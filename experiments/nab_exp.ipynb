{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa0509a",
   "metadata": {},
   "source": [
    "# Experiments on data coming form NAB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b15327ea",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1659123003635,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "b15327ea"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from ad_toolkit.datasets import NabDataset, SupervisedDataset\n",
    "from ad_toolkit.detectors import AutoEncoder, VariationalAutoEncoder, LSTM_AD, LSTM_ED\n",
    "from ad_toolkit.evaluation import Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "l-8OErXJkK0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1659123005405,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "l-8OErXJkK0b",
    "outputId": "5cf72ab9-6028-46a6-8a5a-5439ca584a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "root_dir = pathlib.Path(\"/content/drive/MyDrive/sops_anomaly_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cFya-xg-kFGk",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1659123007205,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "cFya-xg-kFGk"
   },
   "outputs": [],
   "source": [
    "RESULTS_ROOT = root_dir / \"results\" / \"nab_unsupervised\" \n",
    "\n",
    "if not RESULTS_ROOT.exists():\n",
    "    RESULTS_ROOT.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60f61664",
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1659123013999,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "60f61664"
   },
   "outputs": [],
   "source": [
    "def max_metric(scores, targets, threshold_range, metric):\n",
    "    best_metric = -1\n",
    "    best_result = None\n",
    "    best_threshold = 0\n",
    "    for threshold in threshold_range:\n",
    "        labels = (scores > threshold).astype(np.int32)\n",
    "        result = Result(labels, targets)\n",
    "        current_metric = getattr(result, metric, -1)\n",
    "        if current_metric > best_metric:\n",
    "            best_metric = current_metric\n",
    "            best_result = result\n",
    "            best_threshold = threshold\n",
    "    return best_result, best_threshold, best_metric\n",
    "\n",
    "def best_result(predictions, targets, max_error=None, upper_range=None, steps=200):\n",
    "    \"\"\"Try various threshold levels to get best scores.\"\"\"\n",
    "    \n",
    "    if max_error is None:\n",
    "        if upper_range is not None:\n",
    "            upper = upper_range\n",
    "        else:\n",
    "            upper = min(3*np.mean(predictions), np.max(predictions))\n",
    "        threshold_range = np.linspace(\n",
    "            np.min(predictions), \n",
    "            upper,\n",
    "#             np.max(predictions),\n",
    "            steps,\n",
    "        )\n",
    "    else:\n",
    "        threshold_range = np.linspace(0, max_error, 100)\n",
    "        \n",
    "    if np.any(targets):\n",
    "        # If dataset contains anomalies maximize f1 score.\n",
    "        result, threshold, _ = max_metric(predictions, targets, threshold_range, 'f1')\n",
    "    else:\n",
    "        # If dataset contains no anomalies maximize accuracy.\n",
    "        result, threshold, _ = max_metric(predictions, targets, threshold_range, 'accuracy')\n",
    "    return result, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fb63027",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1659123014402,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "0fb63027"
   },
   "outputs": [],
   "source": [
    "def save_results(index, results, anomalies, detector_name):\n",
    "    root_folder = RESULTS_ROOT / detector_name\n",
    "    scores_file = root_folder / \"scores.csv\"\n",
    "    anomalies_file = root_folder / \"anomalies\"\n",
    "    scores = [\n",
    "        [r.accuracy, r.f1, r.precision, r.recall, r.roc_auc]\n",
    "        for r\n",
    "        in results\n",
    "    ]\n",
    "    columns = ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']\n",
    "    \n",
    "    if not root_folder.exists():\n",
    "        root_folder.mkdir(parents=True)\n",
    "    \n",
    "    pd.DataFrame(data=scores, index=index, columns=columns).to_csv(scores_file)\n",
    "    \n",
    "    if not anomalies_file.exists():\n",
    "        anomalies_file.mkdir(parents=True)\n",
    "\n",
    "    for fn, anomaly in zip(index, anomalies):\n",
    "        pd.DataFrame(anomaly).to_csv(anomalies_file / fn.replace('/', '__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8d2d248",
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1659123086051,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "f8d2d248"
   },
   "outputs": [],
   "source": [
    "def test_for_all(model_factory, train_fn, detector_name, datasets, supervised=True):\n",
    "    index = []\n",
    "    all_results = []\n",
    "    all_anomalies = []\n",
    "    for dataset_name, files in datasets.items():\n",
    "        if dataset_name == \"artificialNoAnomaly\":\n",
    "            continue\n",
    "        for filename in files:\n",
    "            print(f\"Testing on {dataset_name}/{filename} ...\")\n",
    "            dataset = NabDataset(dataset=dataset_name, file=filename)\n",
    "            x, y = dataset.get_train_samples(standardize=True)\n",
    "            if supervised:\n",
    "                x_train = x[y != 1]\n",
    "            else:\n",
    "                x_train = x\n",
    "\n",
    "            model = model_factory()\n",
    "            train_fn(model, x_train)\n",
    "            \n",
    "            scores = model.predict(x)\n",
    "            result, threshold = best_result(scores, y)\n",
    "            # result, threshold = best_result(scores, y, upper_range=np.max(scores), steps=300)\n",
    "            print(result)\n",
    "            \n",
    "            anomalies = (scores > threshold).astype(np.float32)\n",
    "        \n",
    "            index.append(f\"{dataset_name}/{filename}\")\n",
    "            all_results.append(result)\n",
    "            all_anomalies.append(anomalies)\n",
    "            \n",
    "    save_results(index, all_results, all_anomalies, detector_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d92b055",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1659123088278,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "4d92b055",
    "outputId": "55296699-ca0e-4bbd-85b9-c77786177d29"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab_simple = NabDataset()\n",
    "nab_simple.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J6JDD_MokFfF",
   "metadata": {
    "id": "J6JDD_MokFfF"
   },
   "source": [
    "## Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f-jeEmXyka9I",
   "metadata": {
    "id": "f-jeEmXyka9I"
   },
   "source": [
    "### Fing best performing on the simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iiWXUjuNklm7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "error",
     "timestamp": 1659123102387,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "iiWXUjuNklm7",
    "outputId": "f56d4c10-8eee-485c-b74b-1564770d63d1"
   },
   "outputs": [],
   "source": [
    "x, y = nab_simple.get_train_samples(standardize=True)\n",
    "x_train = x[y != 1]\n",
    "\n",
    "model = AutoEncoder(window_size=100, layers=(64, 32, 16), latent_size=10, use_gpu=True)\n",
    "model.train(x_train, epochs=40, learning_rate=1e-4)\n",
    "\n",
    "scores = model.predict(x)\n",
    "result, threshold = best_result(scores, y)\n",
    "anomalies = (scores > threshold).astype(np.int32)\n",
    "\n",
    "nab_simple.plot(anomalies={'autoenc': anomalies})\n",
    "print(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EqnK0-xhltYS",
   "metadata": {
    "id": "EqnK0-xhltYS"
   },
   "source": [
    "### Test for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_kZ-6MbulzTh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416771,
     "status": "ok",
     "timestamp": 1659114149166,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "_kZ-6MbulzTh",
    "outputId": "ce2a8cb6-3de9-4577-89ae-0fc8ded773bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on artificialNoAnomaly/art_daily_no_noise.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 0, 4032, 0),\n",
      "\tprecision=1.0,\n",
      "\trecall=1.0,\n",
      "\tf1=1.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.0,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_daily_perfect_square_wave.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 0, 4032, 0),\n",
      "\tprecision=1.0,\n",
      "\trecall=1.0,\n",
      "\tf1=1.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.0,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_daily_small_noise.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 18, 4014, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.004464285714285714,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_flatline.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 0, 4032, 0),\n",
      "\tprecision=1.0,\n",
      "\trecall=1.0,\n",
      "\tf1=1.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.0,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_noisy.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 19, 4013, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.004712301587301587,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_flatmiddle.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsdown.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsup.csv ...\n",
      "Result(accuracy=0.95,\n",
      "\t(tp, fp, tn, fn)=(188, 0, 3629, 215),\n",
      "\tprecision=1.0,\n",
      "\trecall=0.47,\n",
      "\tf1=0.64,\n",
      "\troc_auc=0.73,\n",
      "\ty_pred%=0.04662698412698413,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_nojump.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_increase_spike_density.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(110, 0, 3629, 293),\n",
      "\tprecision=1.0,\n",
      "\trecall=0.27,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.027281746031746032,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_load_balancer_spikes.csv ...\n",
      "Result(accuracy=0.94,\n",
      "\t(tp, fp, tn, fn)=(253, 102, 3527, 150),\n",
      "\tprecision=0.71,\n",
      "\trecall=0.63,\n",
      "\tf1=0.67,\n",
      "\troc_auc=0.8,\n",
      "\ty_pred%=0.08804563492063493,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(100, 4, 3626, 302),\n",
      "\tprecision=0.96,\n",
      "\trecall=0.25,\n",
      "\tf1=0.4,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.025793650793650792,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv ...\n",
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(147, 327, 3303, 255),\n",
      "\tprecision=0.31,\n",
      "\trecall=0.37,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.11755952380952381,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(164, 202, 3428, 238),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.41,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.09077380952380952,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv ...\n",
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(203, 718, 2911, 200),\n",
      "\tprecision=0.22,\n",
      "\trecall=0.5,\n",
      "\tf1=0.31,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.22842261904761904,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(201, 143, 3546, 142),\n",
      "\tprecision=0.58,\n",
      "\trecall=0.59,\n",
      "\tf1=0.59,\n",
      "\troc_auc=0.77,\n",
      "\ty_pred%=0.08531746031746032,\n",
      "\ty_label%=0.08506944444444445,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(202, 521, 3108, 201),\n",
      "\tprecision=0.28,\n",
      "\trecall=0.5,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.1793154761904762,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv ...\n",
      "Result(accuracy=0.96,\n",
      "\t(tp, fp, tn, fn)=(0, 176, 3856, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.04365079365079365,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(125, 119, 3508, 280),\n",
      "\tprecision=0.51,\n",
      "\trecall=0.31,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.060515873015873016,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv ...\n",
      "Result(accuracy=0.76,\n",
      "\t(tp, fp, tn, fn)=(159, 831, 3426, 314),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.34,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.20930232558139536,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv ...\n",
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(126, 244, 3383, 279),\n",
      "\tprecision=0.34,\n",
      "\trecall=0.31,\n",
      "\tf1=0.33,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.09176587301587301,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_257a54.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(106, 1, 3628, 297),\n",
      "\tprecision=0.99,\n",
      "\trecall=0.26,\n",
      "\tf1=0.42,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.026537698412698412,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_5abac7.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(192, 234, 4022, 282),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.41,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.09006342494714588,\n",
      "\ty_label%=0.10021141649048626,\n",
      ")\n",
      "Testing on realAWSCloudwatch/elb_request_count_8c0756.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(192, 132, 3498, 210),\n",
      "\tprecision=0.59,\n",
      "\trecall=0.48,\n",
      "\tf1=0.53,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.08035714285714286,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/grok_asg_anomaly.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(465, 4057, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9785760657866263,\n",
      "\ty_label%=0.10062756979008873,\n",
      ")\n",
      "Testing on realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(81, 83, 1034, 45),\n",
      "\tprecision=0.49,\n",
      "\trecall=0.64,\n",
      "\tf1=0.56,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.13193885760257443,\n",
      "\ty_label%=0.10136765888978279,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv ...\n",
      "Result(accuracy=0.81,\n",
      "\t(tp, fp, tn, fn)=(302, 650, 2980, 100),\n",
      "\tprecision=0.32,\n",
      "\trecall=0.75,\n",
      "\tf1=0.45,\n",
      "\troc_auc=0.79,\n",
      "\ty_pred%=0.2361111111111111,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv ...\n",
      "Result(accuracy=0.62,\n",
      "\t(tp, fp, tn, fn)=(274, 1413, 2217, 128),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.68,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.4184027777777778,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpc_results.csv ...\n",
      "Result(accuracy=0.72,\n",
      "\t(tp, fp, tn, fn)=(122, 418, 1043, 41),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.75,\n",
      "\tf1=0.35,\n",
      "\troc_auc=0.73,\n",
      "\ty_pred%=0.33251231527093594,\n",
      "\ty_label%=0.10036945812807882,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpm_results.csv ...\n",
      "Result(accuracy=0.23,\n",
      "\t(tp, fp, tn, fn)=(162, 1248, 214, 0),\n",
      "\tprecision=0.11,\n",
      "\trecall=1.0,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.8682266009852216,\n",
      "\ty_label%=0.09975369458128079,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpc_results.csv ...\n",
      "Result(accuracy=0.85,\n",
      "\t(tp, fp, tn, fn)=(74, 156, 1229, 79),\n",
      "\tprecision=0.32,\n",
      "\trecall=0.48,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.14954486345903772,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpm_results.csv ...\n",
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(77, 107, 1278, 76),\n",
      "\tprecision=0.42,\n",
      "\trecall=0.5,\n",
      "\tf1=0.46,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.11963589076723016,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpc_results.csv ...\n",
      "Result(accuracy=0.58,\n",
      "\t(tp, fp, tn, fn)=(127, 648, 830, 38),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.77,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.4716981132075472,\n",
      "\ty_label%=0.10042604990870359,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpm_results.csv ...\n",
      "Result(accuracy=0.64,\n",
      "\t(tp, fp, tn, fn)=(83, 510, 969, 81),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.51,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.36092513694461353,\n",
      "\ty_label%=0.09981740718198417,\n",
      ")\n",
      "Testing on realKnownCause/ambient_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(209, 96, 6445, 517),\n",
      "\tprecision=0.69,\n",
      "\trecall=0.29,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.04197055180955002,\n",
      "\ty_label%=0.09990367414338792,\n",
      ")\n",
      "Testing on realKnownCause/cpu_utilization_asg_misconfiguration.csv ...\n",
      "Result(accuracy=0.96,\n",
      "\t(tp, fp, tn, fn)=(888, 132, 16419, 611),\n",
      "\tprecision=0.87,\n",
      "\trecall=0.59,\n",
      "\tf1=0.71,\n",
      "\troc_auc=0.79,\n",
      "\ty_pred%=0.056509695290858725,\n",
      "\ty_label%=0.08304709141274239,\n",
      ")\n",
      "Testing on realKnownCause/ec2_request_latency_system_failure.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(143, 95, 3591, 203),\n",
      "\tprecision=0.6,\n",
      "\trecall=0.41,\n",
      "\tf1=0.49,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.059027777777777776,\n",
      "\ty_label%=0.08581349206349206,\n",
      ")\n",
      "Testing on realKnownCause/machine_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(803, 1441, 18986, 1465),\n",
      "\tprecision=0.36,\n",
      "\trecall=0.35,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.09887640449438202,\n",
      "\ty_label%=0.09993390614672835,\n",
      ")\n",
      "Testing on realKnownCause/nyc_taxi.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(190, 103, 9182, 845),\n",
      "\tprecision=0.65,\n",
      "\trecall=0.18,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.028391472868217055,\n",
      "\ty_label%=0.1002906976744186,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_hold.csv ...\n",
      "Result(accuracy=0.27,\n",
      "\t(tp, fp, tn, fn)=(190, 1381, 311, 0),\n",
      "\tprecision=0.12,\n",
      "\trecall=1.0,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.834750265674814,\n",
      "\ty_label%=0.10095642933049948,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_updown.csv ...\n",
      "Result(accuracy=0.55,\n",
      "\t(tp, fp, tn, fn)=(358, 2206, 2579, 172),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.68,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.48240827845719664,\n",
      "\ty_label%=0.09971777986829727,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_387.csv ...\n",
      "Result(accuracy=0.53,\n",
      "\t(tp, fp, tn, fn)=(248, 1165, 1086, 1),\n",
      "\tprecision=0.18,\n",
      "\trecall=1.0,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.5652,\n",
      "\ty_label%=0.0996,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_451.csv ...\n",
      "Result(accuracy=0.72,\n",
      "\t(tp, fp, tn, fn)=(173, 571, 1374, 44),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.8,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.75,\n",
      "\ty_pred%=0.3441258094357077,\n",
      "\ty_label%=0.10037002775208141,\n",
      ")\n",
      "Testing on realTraffic/occupancy_6005.csv ...\n",
      "Result(accuracy=0.81,\n",
      "\t(tp, fp, tn, fn)=(91, 306, 1835, 148),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.38,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.16680672268907562,\n",
      "\ty_label%=0.1004201680672269,\n",
      ")\n",
      "Testing on realTraffic/occupancy_t4013.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(117, 62, 2188, 133),\n",
      "\tprecision=0.65,\n",
      "\trecall=0.47,\n",
      "\tf1=0.55,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.0716,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realTraffic/speed_6005.csv ...\n",
      "Result(accuracy=0.94,\n",
      "\t(tp, fp, tn, fn)=(100, 2, 2259, 139),\n",
      "\tprecision=0.98,\n",
      "\trecall=0.42,\n",
      "\tf1=0.59,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.0408,\n",
      "\ty_label%=0.0956,\n",
      ")\n",
      "Testing on realTraffic/speed_7578.csv ...\n",
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(60, 97, 914, 56),\n",
      "\tprecision=0.38,\n",
      "\trecall=0.52,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.13930789707187222,\n",
      "\ty_label%=0.10292812777284827,\n",
      ")\n",
      "Testing on realTraffic/speed_t4013.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(129, 77, 2168, 121),\n",
      "\tprecision=0.63,\n",
      "\trecall=0.52,\n",
      "\tf1=0.57,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.08256513026052104,\n",
      "\ty_label%=0.10020040080160321,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AAPL.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(662, 791, 13523, 926),\n",
      "\tprecision=0.46,\n",
      "\trecall=0.42,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.09137215444598164,\n",
      "\ty_label%=0.09986165262231166,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AMZN.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(491, 593, 13658, 1089),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.31,\n",
      "\tf1=0.37,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.06847324868928052,\n",
      "\ty_label%=0.09980418166887751,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CRM.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(474, 270, 14039, 1119),\n",
      "\tprecision=0.64,\n",
      "\trecall=0.3,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.0467865677273299,\n",
      "\ty_label%=0.10017607848069425,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CVS.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(562, 491, 13836, 964),\n",
      "\tprecision=0.53,\n",
      "\trecall=0.37,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.06642275909922413,\n",
      "\ty_label%=0.09625938308206648,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_FB.csv ...\n",
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(599, 2694, 11557, 983),\n",
      "\tprecision=0.18,\n",
      "\trecall=0.38,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.20798332596475716,\n",
      "\ty_label%=0.09991789300827386,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_GOOG.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(448, 443, 13967, 984),\n",
      "\tprecision=0.5,\n",
      "\trecall=0.31,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.056242898623911124,\n",
      "\ty_label%=0.09039262719353618,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_IBM.csv ...\n",
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(374, 654, 13649, 1216),\n",
      "\tprecision=0.36,\n",
      "\trecall=0.24,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.06468256465110427,\n",
      "\ty_label%=0.10004404454791417,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_KO.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(412, 320, 13944, 1175),\n",
      "\tprecision=0.56,\n",
      "\trecall=0.26,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.04618005173175194,\n",
      "\ty_label%=0.10011986625449498,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_PFE.csv ...\n",
      "Result(accuracy=0.84,\n",
      "\t(tp, fp, tn, fn)=(603, 1624, 12646, 985),\n",
      "\tprecision=0.27,\n",
      "\trecall=0.38,\n",
      "\tf1=0.32,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.14043385042249967,\n",
      "\ty_label%=0.10013873123975281,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_UPS.csv ...\n",
      "Result(accuracy=0.68,\n",
      "\t(tp, fp, tn, fn)=(1055, 4608, 9673, 530),\n",
      "\tprecision=0.19,\n",
      "\trecall=0.67,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.35692676162863984,\n",
      "\ty_label%=0.0998991554266986,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_for_all(\n",
    "    lambda: AutoEncoder(window_size=100, layers=(64, 32, 16), latent_size=10, use_gpu=True),\n",
    "    train_fn=lambda model, x: model.train(x, epochs=40, learning_rate=1e-4),\n",
    "    detector_name=\"autoencoder\",\n",
    "    datasets=NabDataset.datasets(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iUqZ9ZnXoWNu",
   "metadata": {
    "id": "iUqZ9ZnXoWNu"
   },
   "source": [
    "## VariationalAutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AKMPgBbooc6J",
   "metadata": {
    "id": "AKMPgBbooc6J"
   },
   "source": [
    "### Find best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WVtzzlkOoeaM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 19093,
     "status": "ok",
     "timestamp": 1659114264069,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "WVtzzlkOoeaM",
    "outputId": "7833401e-272e-4425-d7a4-32f9f5753019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.94,\n",
      "\t(tp, fp, tn, fn)=(195, 15, 3614, 208),\n",
      "\tprecision=0.93,\n",
      "\trecall=0.48,\n",
      "\tf1=0.64,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.052083333333333336,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = nab_simple.get_train_samples(standardize=True)\n",
    "x_train = x[y != 1]\n",
    "\n",
    "model = VariationalAutoEncoder(window_size=100, layers=(64, 32,), latent_size=16, use_gpu=True)\n",
    "model.train(x_train, epochs=40, learning_rate=1e-4)\n",
    "\n",
    "scores = model.predict(x)\n",
    "result, threshold = best_result(scores, y)\n",
    "anomalies = (scores > threshold).astype(np.int32)\n",
    "\n",
    "nab_simple.plot(anomalies={'vae': anomalies})\n",
    "print(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1q-IlKByoiUU",
   "metadata": {
    "id": "1q-IlKByoiUU"
   },
   "source": [
    "### Run for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CQF07DVGonCT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615489,
     "status": "ok",
     "timestamp": 1659115894806,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "CQF07DVGonCT",
    "outputId": "040b43b0-0287-4ea9-ca83-bc47166d2866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on artificialNoAnomaly/art_daily_no_noise.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 20, 4012, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.00496031746031746,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_daily_perfect_square_wave.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 7, 4025, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.001736111111111111,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_daily_small_noise.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 18, 4014, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.004464285714285714,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_flatline.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 0, 4032, 0),\n",
      "\tprecision=1.0,\n",
      "\trecall=1.0,\n",
      "\tf1=1.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.0,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialNoAnomaly/art_noisy.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 20, 4012, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.00496031746031746,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_flatmiddle.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsdown.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsup.csv ...\n",
      "Result(accuracy=0.95,\n",
      "\t(tp, fp, tn, fn)=(193, 9, 3620, 210),\n",
      "\tprecision=0.96,\n",
      "\trecall=0.48,\n",
      "\tf1=0.64,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.05009920634920635,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_nojump.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3530, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_increase_spike_density.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(110, 1, 3628, 293),\n",
      "\tprecision=0.99,\n",
      "\trecall=0.27,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.027529761904761904,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_load_balancer_spikes.csv ...\n",
      "Result(accuracy=0.94,\n",
      "\t(tp, fp, tn, fn)=(253, 103, 3526, 150),\n",
      "\tprecision=0.71,\n",
      "\trecall=0.63,\n",
      "\tf1=0.67,\n",
      "\troc_auc=0.8,\n",
      "\ty_pred%=0.0882936507936508,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(100, 1, 3629, 302),\n",
      "\tprecision=0.99,\n",
      "\trecall=0.25,\n",
      "\tf1=0.4,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.025049603174603176,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv ...\n",
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(135, 248, 3382, 267),\n",
      "\tprecision=0.35,\n",
      "\trecall=0.34,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.09499007936507936,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(78, 106, 3524, 324),\n",
      "\tprecision=0.42,\n",
      "\trecall=0.19,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.04563492063492063,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv ...\n",
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(201, 712, 2917, 202),\n",
      "\tprecision=0.22,\n",
      "\trecall=0.5,\n",
      "\tf1=0.31,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.22643849206349206,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(201, 127, 3562, 142),\n",
      "\tprecision=0.61,\n",
      "\trecall=0.59,\n",
      "\tf1=0.6,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.08134920634920635,\n",
      "\ty_label%=0.08506944444444445,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(202, 521, 3108, 201),\n",
      "\tprecision=0.28,\n",
      "\trecall=0.5,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.1793154761904762,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv ...\n",
      "Result(accuracy=0.96,\n",
      "\t(tp, fp, tn, fn)=(0, 179, 3853, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.04439484126984127,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(125, 102, 3525, 280),\n",
      "\tprecision=0.55,\n",
      "\trecall=0.31,\n",
      "\tf1=0.4,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.056299603174603176,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv ...\n",
      "Result(accuracy=0.76,\n",
      "\t(tp, fp, tn, fn)=(159, 841, 3416, 314),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.34,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.21141649048625794,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv ...\n",
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(126, 263, 3364, 279),\n",
      "\tprecision=0.32,\n",
      "\trecall=0.31,\n",
      "\tf1=0.32,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.09647817460317461,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_257a54.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(106, 0, 3629, 297),\n",
      "\tprecision=1.0,\n",
      "\trecall=0.26,\n",
      "\tf1=0.42,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.02628968253968254,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_5abac7.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(192, 237, 4019, 282),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.41,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.09069767441860466,\n",
      "\ty_label%=0.10021141649048626,\n",
      ")\n",
      "Testing on realAWSCloudwatch/elb_request_count_8c0756.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(187, 121, 3509, 215),\n",
      "\tprecision=0.61,\n",
      "\trecall=0.47,\n",
      "\tf1=0.53,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.0763888888888889,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/grok_asg_anomaly.csv ...\n",
      "Result(accuracy=0.23,\n",
      "\t(tp, fp, tn, fn)=(436, 3533, 623, 29),\n",
      "\tprecision=0.11,\n",
      "\trecall=0.94,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.54,\n",
      "\ty_pred%=0.8589049989179831,\n",
      "\ty_label%=0.10062756979008873,\n",
      ")\n",
      "Testing on realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(80, 82, 1035, 46),\n",
      "\tprecision=0.49,\n",
      "\trecall=0.63,\n",
      "\tf1=0.56,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.13032984714400644,\n",
      "\ty_label%=0.10136765888978279,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv ...\n",
      "Result(accuracy=0.81,\n",
      "\t(tp, fp, tn, fn)=(302, 652, 2978, 100),\n",
      "\tprecision=0.32,\n",
      "\trecall=0.75,\n",
      "\tf1=0.45,\n",
      "\troc_auc=0.79,\n",
      "\ty_pred%=0.23660714285714285,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv ...\n",
      "Result(accuracy=0.62,\n",
      "\t(tp, fp, tn, fn)=(268, 1408, 2222, 134),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.67,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.4156746031746032,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpc_results.csv ...\n",
      "Result(accuracy=0.74,\n",
      "\t(tp, fp, tn, fn)=(112, 374, 1087, 51),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.69,\n",
      "\tf1=0.35,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.29926108374384236,\n",
      "\ty_label%=0.10036945812807882,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpm_results.csv ...\n",
      "Result(accuracy=0.23,\n",
      "\t(tp, fp, tn, fn)=(162, 1246, 216, 0),\n",
      "\tprecision=0.12,\n",
      "\trecall=1.0,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.8669950738916257,\n",
      "\ty_label%=0.09975369458128079,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpc_results.csv ...\n",
      "Result(accuracy=0.83,\n",
      "\t(tp, fp, tn, fn)=(74, 188, 1197, 79),\n",
      "\tprecision=0.28,\n",
      "\trecall=0.48,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.17035110533159947,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpm_results.csv ...\n",
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(77, 107, 1278, 76),\n",
      "\tprecision=0.42,\n",
      "\trecall=0.5,\n",
      "\tf1=0.46,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.11963589076723016,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpc_results.csv ...\n",
      "Result(accuracy=0.55,\n",
      "\t(tp, fp, tn, fn)=(135, 717, 761, 30),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.82,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.5185636031649422,\n",
      "\ty_label%=0.10042604990870359,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpm_results.csv ...\n",
      "Result(accuracy=0.63,\n",
      "\t(tp, fp, tn, fn)=(83, 525, 954, 81),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.51,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.37005477784540475,\n",
      "\ty_label%=0.09981740718198417,\n",
      ")\n",
      "Testing on realKnownCause/ambient_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(223, 145, 6396, 503),\n",
      "\tprecision=0.61,\n",
      "\trecall=0.31,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.0506398789046374,\n",
      "\ty_label%=0.09990367414338792,\n",
      ")\n",
      "Testing on realKnownCause/cpu_utilization_asg_misconfiguration.csv ...\n",
      "Result(accuracy=0.96,\n",
      "\t(tp, fp, tn, fn)=(1022, 222, 16329, 477),\n",
      "\tprecision=0.82,\n",
      "\trecall=0.68,\n",
      "\tf1=0.75,\n",
      "\troc_auc=0.83,\n",
      "\ty_pred%=0.0689196675900277,\n",
      "\ty_label%=0.08304709141274239,\n",
      ")\n",
      "Testing on realKnownCause/ec2_request_latency_system_failure.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(144, 124, 3562, 202),\n",
      "\tprecision=0.54,\n",
      "\trecall=0.42,\n",
      "\tf1=0.47,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.06646825396825397,\n",
      "\ty_label%=0.08581349206349206,\n",
      ")\n",
      "Testing on realKnownCause/machine_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(1333, 597, 19830, 935),\n",
      "\tprecision=0.69,\n",
      "\trecall=0.59,\n",
      "\tf1=0.64,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.08504075787618418,\n",
      "\ty_label%=0.09993390614672835,\n",
      ")\n",
      "Testing on realKnownCause/nyc_taxi.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(184, 70, 9215, 851),\n",
      "\tprecision=0.72,\n",
      "\trecall=0.18,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.024612403100775195,\n",
      "\ty_label%=0.1002906976744186,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_hold.csv ...\n",
      "Result(accuracy=0.28,\n",
      "\t(tp, fp, tn, fn)=(190, 1355, 337, 0),\n",
      "\tprecision=0.12,\n",
      "\trecall=1.0,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.8209351753453773,\n",
      "\ty_label%=0.10095642933049948,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_updown.csv ...\n",
      "Result(accuracy=0.54,\n",
      "\t(tp, fp, tn, fn)=(359, 2279, 2506, 171),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.68,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.4963311382878645,\n",
      "\ty_label%=0.09971777986829727,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_387.csv ...\n",
      "Result(accuracy=0.53,\n",
      "\t(tp, fp, tn, fn)=(249, 1164, 1087, 0),\n",
      "\tprecision=0.18,\n",
      "\trecall=1.0,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.5652,\n",
      "\ty_label%=0.0996,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_451.csv ...\n",
      "Result(accuracy=0.71,\n",
      "\t(tp, fp, tn, fn)=(173, 578, 1367, 44),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.8,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.75,\n",
      "\ty_pred%=0.34736355226641996,\n",
      "\ty_label%=0.10037002775208141,\n",
      ")\n",
      "Testing on realTraffic/occupancy_6005.csv ...\n",
      "Result(accuracy=0.83,\n",
      "\t(tp, fp, tn, fn)=(87, 263, 1878, 152),\n",
      "\tprecision=0.25,\n",
      "\trecall=0.36,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.14705882352941177,\n",
      "\ty_label%=0.1004201680672269,\n",
      ")\n",
      "Testing on realTraffic/occupancy_t4013.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(111, 46, 2204, 139),\n",
      "\tprecision=0.71,\n",
      "\trecall=0.44,\n",
      "\tf1=0.55,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.0628,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realTraffic/speed_6005.csv ...\n",
      "Result(accuracy=0.94,\n",
      "\t(tp, fp, tn, fn)=(100, 2, 2259, 139),\n",
      "\tprecision=0.98,\n",
      "\trecall=0.42,\n",
      "\tf1=0.59,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.0408,\n",
      "\ty_label%=0.0956,\n",
      ")\n",
      "Testing on realTraffic/speed_7578.csv ...\n",
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(60, 98, 913, 56),\n",
      "\tprecision=0.38,\n",
      "\trecall=0.52,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.1401952085181899,\n",
      "\ty_label%=0.10292812777284827,\n",
      ")\n",
      "Testing on realTraffic/speed_t4013.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(129, 77, 2168, 121),\n",
      "\tprecision=0.63,\n",
      "\trecall=0.52,\n",
      "\tf1=0.57,\n",
      "\troc_auc=0.74,\n",
      "\ty_pred%=0.08256513026052104,\n",
      "\ty_label%=0.10020040080160321,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AAPL.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(662, 793, 13521, 926),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.42,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.09149792478933468,\n",
      "\ty_label%=0.09986165262231166,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AMZN.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(494, 622, 13629, 1086),\n",
      "\tprecision=0.44,\n",
      "\trecall=0.31,\n",
      "\tf1=0.37,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.07049459920409323,\n",
      "\ty_label%=0.09980418166887751,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CRM.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(475, 277, 14032, 1118),\n",
      "\tprecision=0.63,\n",
      "\trecall=0.3,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.04728964910074204,\n",
      "\ty_label%=0.10017607848069425,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CVS.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(562, 495, 13832, 964),\n",
      "\tprecision=0.53,\n",
      "\trecall=0.37,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.06667507727244054,\n",
      "\ty_label%=0.09625938308206648,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_FB.csv ...\n",
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(599, 2680, 11571, 983),\n",
      "\tprecision=0.18,\n",
      "\trecall=0.38,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.20709909682309102,\n",
      "\ty_label%=0.09991789300827386,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_GOOG.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(448, 446, 13964, 984),\n",
      "\tprecision=0.5,\n",
      "\trecall=0.31,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.05643226865294786,\n",
      "\ty_label%=0.09039262719353618,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_IBM.csv ...\n",
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(373, 650, 13653, 1217),\n",
      "\tprecision=0.36,\n",
      "\trecall=0.23,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.06436796073743158,\n",
      "\ty_label%=0.10004404454791417,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_KO.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(412, 320, 13944, 1175),\n",
      "\tprecision=0.56,\n",
      "\trecall=0.26,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.04618005173175194,\n",
      "\ty_label%=0.10011986625449498,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_PFE.csv ...\n",
      "Result(accuracy=0.84,\n",
      "\t(tp, fp, tn, fn)=(605, 1629, 12641, 983),\n",
      "\tprecision=0.27,\n",
      "\trecall=0.38,\n",
      "\tf1=0.32,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.14087526800353134,\n",
      "\ty_label%=0.10013873123975281,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_UPS.csv ...\n",
      "Result(accuracy=0.69,\n",
      "\t(tp, fp, tn, fn)=(1029, 4303, 9978, 556),\n",
      "\tprecision=0.19,\n",
      "\trecall=0.65,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.33606454052691287,\n",
      "\ty_label%=0.0998991554266986,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_for_all(\n",
    "    lambda: VariationalAutoEncoder(window_size=100, layers=(64, 32,), latent_size=16, use_gpu=True),\n",
    "    train_fn=lambda model, x: model.train(x, epochs=40, learning_rate=1e-4),\n",
    "    detector_name=\"variationalautoencoder\",\n",
    "    datasets=NabDataset.datasets(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FdnKPTeCoonI",
   "metadata": {
    "id": "FdnKPTeCoonI"
   },
   "source": [
    "## LSTM_AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MXhsQUFMoqjt",
   "metadata": {
    "id": "MXhsQUFMoqjt"
   },
   "source": [
    "### Find best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "av11H90Vosgs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "executionInfo": {
     "elapsed": 44805,
     "status": "ok",
     "timestamp": 1659116015168,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "av11H90Vosgs",
    "outputId": "67b6d3cf-424e-4c2b-eb24-f2d9c298b886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.7937982224624242\n",
      "Epoch 1 loss: 0.7935859989351839\n",
      "Epoch 2 loss: 0.7933739106896154\n",
      "Epoch 3 loss: 0.7931619562800214\n",
      "Epoch 4 loss: 0.7929501311056727\n",
      "Epoch 5 loss: 0.7927384288625029\n",
      "Epoch 6 loss: 0.7925268419829615\n",
      "Epoch 7 loss: 0.7923153608198465\n",
      "Epoch 8 loss: 0.7921039735362392\n",
      "Epoch 9 loss: 0.7918926666003868\n",
      "Epoch 10 loss: 0.791681425171134\n",
      "Epoch 11 loss: 0.7914702331494474\n",
      "Epoch 12 loss: 0.7912590730024728\n",
      "Epoch 13 loss: 0.7910479255669176\n",
      "Epoch 14 loss: 0.790836769971121\n",
      "Epoch 15 loss: 0.7906255836988529\n",
      "Epoch 16 loss: 0.7904143427542389\n",
      "Epoch 17 loss: 0.7902030218785425\n",
      "Epoch 18 loss: 0.789991594779414\n",
      "Epoch 19 loss: 0.789780034342388\n",
      "Epoch 20 loss: 0.7895683128042306\n",
      "Epoch 21 loss: 0.789356401880385\n",
      "Epoch 22 loss: 0.7891442728497489\n",
      "Epoch 23 loss: 0.7889318966049211\n",
      "Epoch 24 loss: 0.7887192436756764\n",
      "Epoch 25 loss: 0.7885062842316051\n",
      "Epoch 26 loss: 0.7882929880692185\n",
      "Epoch 27 loss: 0.78807932458935\n",
      "Epoch 28 loss: 0.7878652627711421\n",
      "Epoch 29 loss: 0.7876507711484743\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(129, 1, 3628, 274),\n",
      "\tprecision=0.99,\n",
      "\trecall=0.32,\n",
      "\tf1=0.48,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.032242063492063495,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = nab_simple.get_train_samples(standardize=True)\n",
    "x_train = x[y != 1]\n",
    "\n",
    "model = LSTM_AD(window_size=20, hidden_size=16, use_gpu=True)\n",
    "model.train(x_train, epochs=30, learning_rate=1e-4, verbose=True)\n",
    "\n",
    "scores = model.predict(x)\n",
    "result, threshold = best_result(scores, y, upper_range=np.max(scores), steps=300)\n",
    "anomalies = (scores > threshold).astype(np.int32)\n",
    "\n",
    "nab_simple.plot(anomalies={'lstm_ad': anomalies})\n",
    "print(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyzzXZpvovy5",
   "metadata": {
    "id": "wyzzXZpvovy5"
   },
   "source": [
    "### Run for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QJNwVDT3ovJQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2241565,
     "status": "ok",
     "timestamp": 1659121211077,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "QJNwVDT3ovJQ",
    "outputId": "cc16d430-3473-4264-9dc4-2d8d0d7d9c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on artificialWithAnomaly/art_daily_flatmiddle.csv ...\n",
      "Result(accuracy=0.64,\n",
      "\t(tp, fp, tn, fn)=(308, 1368, 2261, 95),\n",
      "\tprecision=0.18,\n",
      "\trecall=0.76,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.4156746031746032,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsdown.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 0, 1),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsup.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(128, 1, 3628, 275),\n",
      "\tprecision=0.99,\n",
      "\trecall=0.32,\n",
      "\tf1=0.48,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.031994047619047616,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_nojump.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(403, 3628, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_increase_spike_density.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(403, 3628, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_load_balancer_spikes.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(228, 108, 3521, 175),\n",
      "\tprecision=0.68,\n",
      "\trecall=0.57,\n",
      "\tf1=0.62,\n",
      "\troc_auc=0.77,\n",
      "\ty_pred%=0.08333333333333333,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv ...\n",
      "Result(accuracy=0.78,\n",
      "\t(tp, fp, tn, fn)=(156, 628, 3002, 246),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.39,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.19444444444444445,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv ...\n",
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(92, 255, 3375, 310),\n",
      "\tprecision=0.27,\n",
      "\trecall=0.23,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.08606150793650794,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv ...\n",
      "Result(accuracy=0.57,\n",
      "\t(tp, fp, tn, fn)=(260, 1586, 2044, 142),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.65,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.45783730158730157,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv ...\n",
      "Result(accuracy=0.54,\n",
      "\t(tp, fp, tn, fn)=(273, 1732, 1897, 130),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.68,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.4972718253968254,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(113, 52, 3637, 230),\n",
      "\tprecision=0.68,\n",
      "\trecall=0.33,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.04092261904761905,\n",
      "\ty_label%=0.08506944444444445,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(403, 3628, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 20, 4012, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.00496031746031746,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(73, 116, 3511, 332),\n",
      "\tprecision=0.39,\n",
      "\trecall=0.18,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.046875,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(473, 4256, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997885835095137,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv ...\n",
      "Result(accuracy=0.83,\n",
      "\t(tp, fp, tn, fn)=(116, 379, 3248, 289),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.29,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.12276785714285714,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_257a54.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(403, 3628, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_5abac7.csv ...\n",
      "Result(accuracy=0.74,\n",
      "\t(tp, fp, tn, fn)=(275, 1033, 3223, 199),\n",
      "\tprecision=0.21,\n",
      "\trecall=0.58,\n",
      "\tf1=0.31,\n",
      "\troc_auc=0.67,\n",
      "\ty_pred%=0.27653276955602535,\n",
      "\ty_label%=0.10021141649048626,\n",
      ")\n",
      "Testing on realAWSCloudwatch/elb_request_count_8c0756.csv ...\n",
      "Result(accuracy=0.85,\n",
      "\t(tp, fp, tn, fn)=(106, 321, 3309, 296),\n",
      "\tprecision=0.25,\n",
      "\trecall=0.26,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.10590277777777778,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/grok_asg_anomaly.csv ...\n",
      "Result(accuracy=0.42,\n",
      "\t(tp, fp, tn, fn)=(413, 2614, 1542, 52),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.89,\n",
      "\tf1=0.24,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.6550530188270937,\n",
      "\ty_label%=0.10062756979008873,\n",
      ")\n",
      "Testing on realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(63, 63, 1054, 63),\n",
      "\tprecision=0.5,\n",
      "\trecall=0.5,\n",
      "\tf1=0.5,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.10136765888978279,\n",
      "\ty_label%=0.10136765888978279,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv ...\n",
      "Result(accuracy=0.76,\n",
      "\t(tp, fp, tn, fn)=(298, 845, 2785, 104),\n",
      "\tprecision=0.26,\n",
      "\trecall=0.74,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.75,\n",
      "\ty_pred%=0.28348214285714285,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv ...\n",
      "Result(accuracy=0.72,\n",
      "\t(tp, fp, tn, fn)=(150, 867, 2763, 252),\n",
      "\tprecision=0.15,\n",
      "\trecall=0.37,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.25223214285714285,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpc_results.csv ...\n",
      "Result(accuracy=0.5,\n",
      "\t(tp, fp, tn, fn)=(118, 762, 699, 45),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.72,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.541871921182266,\n",
      "\ty_label%=0.10036945812807882,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpm_results.csv ...\n",
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(70, 287, 1175, 92),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.43,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.21982758620689655,\n",
      "\ty_label%=0.09975369458128079,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpc_results.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(60, 29, 1356, 93),\n",
      "\tprecision=0.67,\n",
      "\trecall=0.39,\n",
      "\tf1=0.5,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.05786736020806242,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpm_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(153, 1384, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9993498049414824,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpc_results.csv ...\n",
      "Result(accuracy=0.78,\n",
      "\t(tp, fp, tn, fn)=(86, 286, 1192, 79),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.52,\n",
      "\tf1=0.32,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.22641509433962265,\n",
      "\ty_label%=0.10042604990870359,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpm_results.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(55, 38, 1441, 109),\n",
      "\tprecision=0.59,\n",
      "\trecall=0.34,\n",
      "\tf1=0.43,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.05660377358490566,\n",
      "\ty_label%=0.09981740718198417,\n",
      ")\n",
      "Testing on realKnownCause/ambient_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(332, 893, 5648, 394),\n",
      "\tprecision=0.27,\n",
      "\trecall=0.46,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.1685702490711435,\n",
      "\ty_label%=0.09990367414338792,\n",
      ")\n",
      "Testing on realKnownCause/cpu_utilization_asg_misconfiguration.csv ...\n",
      "Result(accuracy=0.96,\n",
      "\t(tp, fp, tn, fn)=(1095, 377, 16174, 404),\n",
      "\tprecision=0.74,\n",
      "\trecall=0.73,\n",
      "\tf1=0.74,\n",
      "\troc_auc=0.85,\n",
      "\ty_pred%=0.08155124653739612,\n",
      "\ty_label%=0.08304709141274239,\n",
      ")\n",
      "Testing on realKnownCause/ec2_request_latency_system_failure.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(61, 64, 3622, 285),\n",
      "\tprecision=0.49,\n",
      "\trecall=0.18,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.031001984126984128,\n",
      "\ty_label%=0.08581349206349206,\n",
      ")\n",
      "Testing on realKnownCause/machine_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(1356, 1455, 18972, 912),\n",
      "\tprecision=0.48,\n",
      "\trecall=0.6,\n",
      "\tf1=0.53,\n",
      "\troc_auc=0.76,\n",
      "\ty_pred%=0.12385988103106411,\n",
      "\ty_label%=0.09993390614672835,\n",
      ")\n",
      "Testing on realKnownCause/nyc_taxi.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(153, 215, 9070, 882),\n",
      "\tprecision=0.42,\n",
      "\trecall=0.15,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.56,\n",
      "\ty_pred%=0.03565891472868217,\n",
      "\ty_label%=0.1002906976744186,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_hold.csv ...\n",
      "Result(accuracy=0.57,\n",
      "\t(tp, fp, tn, fn)=(163, 785, 907, 27),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.86,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.7,\n",
      "\ty_pred%=0.5037194473963869,\n",
      "\ty_label%=0.10095642933049948,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_updown.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(530, 4784, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9998118532455316,\n",
      "\ty_label%=0.09971777986829727,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_387.csv ...\n",
      "Result(accuracy=0.75,\n",
      "\t(tp, fp, tn, fn)=(160, 544, 1707, 89),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.64,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.7,\n",
      "\ty_pred%=0.2816,\n",
      "\ty_label%=0.0996,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_451.csv ...\n",
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(57, 148, 1797, 160),\n",
      "\tprecision=0.28,\n",
      "\trecall=0.26,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.09481961147086032,\n",
      "\ty_label%=0.10037002775208141,\n",
      ")\n",
      "Testing on realTraffic/occupancy_6005.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(239, 2140, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9995798319327731,\n",
      "\ty_label%=0.1004201680672269,\n",
      ")\n",
      "Testing on realTraffic/occupancy_t4013.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(46, 3, 2247, 204),\n",
      "\tprecision=0.94,\n",
      "\trecall=0.18,\n",
      "\tf1=0.31,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.0196,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realTraffic/speed_6005.csv ...\n",
      "Result(accuracy=0.63,\n",
      "\t(tp, fp, tn, fn)=(108, 799, 1462, 131),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.45,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.3628,\n",
      "\ty_label%=0.0956,\n",
      ")\n",
      "Testing on realTraffic/speed_7578.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(49, 25, 986, 67),\n",
      "\tprecision=0.66,\n",
      "\trecall=0.42,\n",
      "\tf1=0.52,\n",
      "\troc_auc=0.7,\n",
      "\ty_pred%=0.06566104702750665,\n",
      "\ty_label%=0.10292812777284827,\n",
      ")\n",
      "Testing on realTraffic/speed_t4013.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(105, 118, 2127, 145),\n",
      "\tprecision=0.47,\n",
      "\trecall=0.42,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.68,\n",
      "\ty_pred%=0.08937875751503006,\n",
      "\ty_label%=0.10020040080160321,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AAPL.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(606, 1832, 12482, 982),\n",
      "\tprecision=0.25,\n",
      "\trecall=0.38,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.15331404854735253,\n",
      "\ty_label%=0.09986165262231166,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AMZN.csv ...\n",
      "Result(accuracy=0.8,\n",
      "\t(tp, fp, tn, fn)=(519, 2067, 12184, 1061),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.33,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.16335038847830208,\n",
      "\ty_label%=0.09980418166887751,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CRM.csv ...\n",
      "Result(accuracy=0.84,\n",
      "\t(tp, fp, tn, fn)=(506, 1454, 12855, 1087),\n",
      "\tprecision=0.26,\n",
      "\trecall=0.32,\n",
      "\tf1=0.28,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.1232549364859766,\n",
      "\ty_label%=0.10017607848069425,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CVS.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(206, 452, 13875, 1320),\n",
      "\tprecision=0.31,\n",
      "\trecall=0.13,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.04150633949410206,\n",
      "\ty_label%=0.09625938308206648,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_FB.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1582, 14250, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999368407755953,\n",
      "\ty_label%=0.09991789300827386,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_GOOG.csv ...\n",
      "Result(accuracy=0.78,\n",
      "\t(tp, fp, tn, fn)=(507, 2546, 11864, 925),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.35,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.19271556621638683,\n",
      "\ty_label%=0.09039262719353618,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_IBM.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1590, 14302, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999370792172655,\n",
      "\ty_label%=0.10004404454791417,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_KO.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1586, 14264, 0, 1),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999369124976342,\n",
      "\ty_label%=0.10011986625449498,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_PFE.csv ...\n",
      "Result(accuracy=0.67,\n",
      "\t(tp, fp, tn, fn)=(781, 4430, 9840, 807),\n",
      "\tprecision=0.15,\n",
      "\trecall=0.49,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.3286038592508513,\n",
      "\ty_label%=0.10013873123975281,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_UPS.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(592, 1854, 12427, 993),\n",
      "\tprecision=0.24,\n",
      "\trecall=0.37,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.15416614143451404,\n",
      "\ty_label%=0.0998991554266986,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def train_lstm_ad_model(model, x):\n",
    "    if len(x) > 5000:\n",
    "        model.train_with_slices(x, epochs=10, learning_rate=1e-4)\n",
    "    else:\n",
    "        model.train(x, epochs=30, learning_rate=1e-4)\n",
    "\n",
    "\n",
    "test_for_all(\n",
    "    lambda: LSTM_AD(window_size=20, hidden_size=16, use_gpu=True),\n",
    "    train_fn=train_lstm_ad_model,\n",
    "    detector_name=\"lstm_ad_norm_stand\",\n",
    "    datasets=NabDataset.datasets(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ly-Gplb0o3jk",
   "metadata": {
    "id": "ly-Gplb0o3jk"
   },
   "source": [
    "## LSTM_ED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vCqQpYT9o5z5",
   "metadata": {
    "id": "vCqQpYT9o5z5"
   },
   "source": [
    "### Find best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LF2upbsXo5H7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 17501,
     "status": "ok",
     "timestamp": 1659121301260,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "LF2upbsXo5H7",
    "outputId": "a16a1f54-4386-49cc-cdc0-31c24941bb17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(122, 3, 3626, 281),\n",
      "\tprecision=0.98,\n",
      "\trecall=0.3,\n",
      "\tf1=0.46,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.031001984126984128,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = nab_simple.get_train_samples(standardize=True)\n",
    "x_train = x[y != 1]\n",
    "\n",
    "\n",
    "model = LSTM_ED(sequence_len=30, stride=1, hidden_size=16, use_gpu=True)\n",
    "model.train(x_train, epochs=40, learning_rate=1e-4)\n",
    "\n",
    "scores = model.predict(x)\n",
    "result, threshold = best_result(scores, y, upper_range=np.max(scores), steps=300)\n",
    "anomalies = (scores > threshold).astype(np.int32)\n",
    "\n",
    "nab_simple.plot(anomalies={'lstm_ed': anomalies})\n",
    "print(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wSaJEJjlo9tS",
   "metadata": {
    "id": "wSaJEJjlo9tS"
   },
   "source": [
    "## Run for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v3c1k9duo_sE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1389868,
     "status": "ok",
     "timestamp": 1659122694760,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "v3c1k9duo_sE",
    "outputId": "68b5ea11-b898-4f77-96a6-1829bda068f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on artificialWithAnomaly/art_daily_flatmiddle.csv ...\n",
      "Result(accuracy=0.74,\n",
      "\t(tp, fp, tn, fn)=(269, 931, 2698, 134),\n",
      "\tprecision=0.22,\n",
      "\trecall=0.67,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.2976190476190476,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsdown.csv ...\n",
      "Result(accuracy=0.92,\n",
      "\t(tp, fp, tn, fn)=(104, 6, 3623, 299),\n",
      "\tprecision=0.95,\n",
      "\trecall=0.26,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.027281746031746032,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsup.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(121, 10, 3619, 282),\n",
      "\tprecision=0.92,\n",
      "\trecall=0.3,\n",
      "\tf1=0.45,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.03249007936507937,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_nojump.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(403, 3548, 81, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9799107142857143,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_increase_spike_density.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(403, 3628, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_load_balancer_spikes.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(94, 92, 3537, 309),\n",
      "\tprecision=0.51,\n",
      "\trecall=0.23,\n",
      "\tf1=0.32,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.046130952380952384,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(81, 390, 3239, 322),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.2,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.11681547619047619,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv ...\n",
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(107, 34, 3655, 236),\n",
      "\tprecision=0.76,\n",
      "\trecall=0.31,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.034970238095238096,\n",
      "\ty_label%=0.08506944444444445,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(202, 245, 3384, 201),\n",
      "\tprecision=0.45,\n",
      "\trecall=0.5,\n",
      "\tf1=0.48,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.11086309523809523,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv ...\n",
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 14, 4018, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.003472222222222222,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(405, 3626, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(473, 4256, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997885835095137,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(405, 3626, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_257a54.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 0, 1),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_5abac7.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(474, 4255, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997885835095137,\n",
      "\ty_label%=0.10021141649048626,\n",
      ")\n",
      "Testing on realAWSCloudwatch/elb_request_count_8c0756.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/grok_asg_anomaly.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(465, 4155, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997835966241073,\n",
      "\ty_label%=0.10062756979008873,\n",
      ")\n",
      "Testing on realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv ...\n",
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(21, 47, 1070, 105),\n",
      "\tprecision=0.31,\n",
      "\trecall=0.17,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.56,\n",
      "\ty_pred%=0.054706355591311345,\n",
      "\ty_label%=0.10136765888978279,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv ...\n",
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(302, 640, 2990, 100),\n",
      "\tprecision=0.32,\n",
      "\trecall=0.75,\n",
      "\tf1=0.45,\n",
      "\troc_auc=0.79,\n",
      "\ty_pred%=0.23363095238095238,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(402, 3629, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpc_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(163, 1460, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.999384236453202,\n",
      "\ty_label%=0.10036945812807882,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpm_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(162, 1461, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.999384236453202,\n",
      "\ty_label%=0.09975369458128079,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpc_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(153, 1384, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9993498049414824,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpm_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(153, 1384, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9993498049414824,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpc_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(165, 1477, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9993913572732805,\n",
      "\ty_label%=0.10042604990870359,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpm_results.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(164, 1478, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9993913572732805,\n",
      "\ty_label%=0.09981740718198417,\n",
      ")\n",
      "Testing on realKnownCause/ambient_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(204, 263, 6278, 522),\n",
      "\tprecision=0.44,\n",
      "\trecall=0.28,\n",
      "\tf1=0.34,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.06426310719691758,\n",
      "\ty_label%=0.09990367414338792,\n",
      ")\n",
      "Testing on realKnownCause/cpu_utilization_asg_misconfiguration.csv ...\n",
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(911, 1777, 14774, 588),\n",
      "\tprecision=0.34,\n",
      "\trecall=0.61,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.75,\n",
      "\ty_pred%=0.1489196675900277,\n",
      "\ty_label%=0.08304709141274239,\n",
      ")\n",
      "Testing on realKnownCause/ec2_request_latency_system_failure.csv ...\n",
      "Result(accuracy=0.09,\n",
      "\t(tp, fp, tn, fn)=(346, 3685, 1, 0),\n",
      "\tprecision=0.09,\n",
      "\trecall=1.0,\n",
      "\tf1=0.16,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9997519841269841,\n",
      "\ty_label%=0.08581349206349206,\n",
      ")\n",
      "Testing on realKnownCause/machine_temperature_system_failure.csv ...\n",
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(1360, 1088, 19339, 908),\n",
      "\tprecision=0.56,\n",
      "\trecall=0.6,\n",
      "\tf1=0.58,\n",
      "\troc_auc=0.77,\n",
      "\ty_pred%=0.10786516853932585,\n",
      "\ty_label%=0.09993390614672835,\n",
      ")\n",
      "Testing on realKnownCause/nyc_taxi.csv ...\n",
      "Result(accuracy=0.22,\n",
      "\t(tp, fp, tn, fn)=(918, 7957, 1328, 117),\n",
      "\tprecision=0.1,\n",
      "\trecall=0.89,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.8599806201550387,\n",
      "\ty_label%=0.1002906976744186,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_hold.csv ...\n",
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(190, 1664, 28, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9851222104144527,\n",
      "\ty_label%=0.10095642933049948,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_updown.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(530, 4784, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9998118532455316,\n",
      "\ty_label%=0.09971777986829727,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_387.csv ...\n",
      "Result(accuracy=0.68,\n",
      "\t(tp, fp, tn, fn)=(140, 690, 1561, 109),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.56,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.332,\n",
      "\ty_label%=0.0996,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_451.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(217, 1944, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9995374653098983,\n",
      "\ty_label%=0.10037002775208141,\n",
      ")\n",
      "Testing on realTraffic/occupancy_6005.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(239, 2140, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9995798319327731,\n",
      "\ty_label%=0.1004201680672269,\n",
      ")\n",
      "Testing on realTraffic/occupancy_t4013.csv ...\n",
      "Result(accuracy=0.4,\n",
      "\t(tp, fp, tn, fn)=(172, 1414, 836, 78),\n",
      "\tprecision=0.11,\n",
      "\trecall=0.69,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.53,\n",
      "\ty_pred%=0.6344,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realTraffic/speed_6005.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(239, 2260, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.17,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9996,\n",
      "\ty_label%=0.0956,\n",
      ")\n",
      "Testing on realTraffic/speed_7578.csv ...\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(35, 30, 981, 81),\n",
      "\tprecision=0.54,\n",
      "\trecall=0.3,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.05767524401064774,\n",
      "\ty_label%=0.10292812777284827,\n",
      ")\n",
      "Testing on realTraffic/speed_t4013.csv ...\n",
      "Result(accuracy=0.65,\n",
      "\t(tp, fp, tn, fn)=(100, 724, 1521, 150),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.4,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.54,\n",
      "\ty_pred%=0.33026052104208414,\n",
      "\ty_label%=0.10020040080160321,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AAPL.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1588, 14313, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999371148283235,\n",
      "\ty_label%=0.09986165262231166,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AMZN.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1580, 14250, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999368327964121,\n",
      "\ty_label%=0.09980418166887751,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CRM.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1593, 14308, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999371148283235,\n",
      "\ty_label%=0.10017607848069425,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CVS.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1526, 14326, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999369204566959,\n",
      "\ty_label%=0.09625938308206648,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_FB.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1582, 14250, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999368407755953,\n",
      "\ty_label%=0.09991789300827386,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_GOOG.csv ...\n",
      "Result(accuracy=0.09,\n",
      "\t(tp, fp, tn, fn)=(1432, 14409, 1, 0),\n",
      "\tprecision=0.09,\n",
      "\trecall=1.0,\n",
      "\tf1=0.17,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999368766569877,\n",
      "\ty_label%=0.09039262719353618,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_IBM.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1590, 14302, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999370792172655,\n",
      "\ty_label%=0.10004404454791417,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_KO.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1587, 14263, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999369124976342,\n",
      "\ty_label%=0.10011986625449498,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_PFE.csv ...\n",
      "Result(accuracy=0.57,\n",
      "\t(tp, fp, tn, fn)=(843, 6110, 8160, 745),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.53,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.43845377727330054,\n",
      "\ty_label%=0.10013873123975281,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_UPS.csv ...\n",
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1585, 14280, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999369721416866,\n",
      "\ty_label%=0.0998991554266986,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_for_all(\n",
    "    lambda: LSTM_ED(sequence_len=30, stride=1, hidden_size=16, use_gpu=True),\n",
    "    train_fn=lambda model, x: model.train(x, epochs=40, learning_rate=1e-4),\n",
    "    detector_name=\"lstm_ed\",\n",
    "    datasets=NabDataset.datasets(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeOWH0GepDHY",
   "metadata": {
    "id": "eeOWH0GepDHY"
   },
   "source": [
    "## Donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6hgN7NEpCBQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81838,
     "status": "ok",
     "timestamp": 1659105211443,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "e6hgN7NEpCBQ",
    "outputId": "2ee6d292-43c4-462a-d4f2-64f288f72706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git\n",
      "  Cloning https://github.com/pbudzyns/ad_toolkit.git to /tmp/pip-install-csnruhbl/ad-toolkit_7536b518e061418e853040d42d88a132\n",
      "  Running command git clone -q https://github.com/pbudzyns/ad_toolkit.git /tmp/pip-install-csnruhbl/ad-toolkit_7536b518e061418e853040d42d88a132\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting donut@ git+https://github.com/korepwx/donut.git\n",
      "  Cloning https://github.com/korepwx/donut.git to /tmp/pip-install-csnruhbl/donut_542759d8518947c2924f37fec4492d77\n",
      "  Running command git clone -q https://github.com/korepwx/donut.git /tmp/pip-install-csnruhbl/donut_542759d8518947c2924f37fec4492d77\n",
      "Collecting tfsnippet@ git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2\n",
      "  Cloning https://github.com/haowen-xu/tfsnippet.git (to revision v0.1.2) to /tmp/pip-install-csnruhbl/tfsnippet_7f8a5dabeb7f4984b9ac865c71d403ad\n",
      "  Running command git clone -q https://github.com/haowen-xu/tfsnippet.git /tmp/pip-install-csnruhbl/tfsnippet_7f8a5dabeb7f4984b9ac865c71d403ad\n",
      "  Running command git checkout -q ecc0b4d1e610cf8cfa8c236857a7dabee27d5543\n",
      "Collecting zhusuan@ git+https://github.com/thu-ml/zhusuan.git\n",
      "  Cloning https://github.com/thu-ml/zhusuan.git to /tmp/pip-install-csnruhbl/zhusuan_e8432be8b8b1418b905fecb1b7584b84\n",
      "  Running command git clone -q https://github.com/thu-ml/zhusuan.git /tmp/pip-install-csnruhbl/zhusuan_e8432be8b8b1418b905fecb1b7584b84\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.21.6)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.3.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.7.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.12.0+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (0.13.0+cu113)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2.23.0)\n",
      "Collecting tensorflow<=1.15.1\n",
      "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3 MB 24 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from donut@ git+https://github.com/korepwx/donut.git->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.15.0)\n",
      "Requirement already satisfied: filelock>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from tfsnippet@ git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.7.1)\n",
      "Collecting frozendict>=1.2.0\n",
      "  Downloading frozendict-2.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 11.2 MB/s \n",
      "\u001b[?25hCollecting idx2numpy>=1.2.2\n",
      "  Downloading idx2numpy-1.2.3.tar.gz (6.8 kB)\n",
      "Requirement already satisfied: natsort>=5.3.3 in /usr/local/lib/python3.7/dist-packages (from tfsnippet@ git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (5.5.0)\n",
      "Requirement already satisfied: semver>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from tfsnippet@ git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2.13.0)\n",
      "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.7/dist-packages (from tfsnippet@ git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (4.64.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2022.6.15)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.14.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (0.37.1)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 53.9 MB/s \n",
      "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 47.3 MB/s \n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.47.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.3.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (0.8.1)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 9.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.2.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (4.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.8.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow<=1.15.1->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (1.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->ad_toolkit[donut]@ git+https://github.com/pbudzyns/ad_toolkit.git) (7.1.2)\n",
      "Building wheels for collected packages: ad-toolkit, donut, tfsnippet, zhusuan, idx2numpy, gast\n",
      "  Building wheel for ad-toolkit (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ad-toolkit: filename=ad_toolkit-0.0.1-py3-none-any.whl size=41927 sha256=be38ef2d262953f08497b3e2910df32a8aa13068427de1fcb69910c13ed127ac\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5qjjiagc/wheels/3b/26/41/35e592529c0fe767df03084950d3189ea49d604fec2e0c41ff\n",
      "  Building wheel for donut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for donut: filename=Donut-0.1-py3-none-any.whl size=16172 sha256=fd69d0b42a04ac1c145f954ec4e65b96d743b3dbf4b71984fbe79e914560600c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5qjjiagc/wheels/31/8f/4a/b575aaa55e86d32ba941caf756e44f4ac54488976f82dc8a22\n",
      "  Building wheel for tfsnippet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tfsnippet: filename=TFSnippet-0.1.2-py3-none-any.whl size=166312 sha256=aae4e713a1573b75cb68c767bd62f48bd27a03f2235abfa263c7d25a366ef854\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5qjjiagc/wheels/ca/43/a3/569e5bdd036fff461998cae87868a5de99c46250c68eb1e061\n",
      "  Building wheel for zhusuan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for zhusuan: filename=zhusuan-0.4.0-py2.py3-none-any.whl size=73602 sha256=ae30bd4727ee022f8ac6883d8dc82b31d7bba0133b48383caf019ff421cca80e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5qjjiagc/wheels/b3/25/a5/9fb6a3ac2efd85da9b400198b87d7183287094e23b721ed866\n",
      "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-py3-none-any.whl size=7917 sha256=1a18a59d7c2884affc42da1396de720a6b51a4c6bcf38404b8e3416f2bb8e463\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/ce/ad/d5e95a35cfe34149aade5e500f2edd535c0566d79e9a8e1d8a\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a188a69cfec8d82b522658969f9007e5e71e90fa65212ec4b85fc4a7861e73e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "Successfully built ad-toolkit donut tfsnippet zhusuan idx2numpy gast\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, idx2numpy, gast, frozendict, dataclasses, zhusuan, tfsnippet, tensorflow, donut, ad-toolkit\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.8.0\n",
      "    Uninstalling tensorflow-estimator-2.8.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.3\n",
      "    Uninstalling gast-0.5.3:\n",
      "      Successfully uninstalled gast-0.5.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
      "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
      "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
      "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
      "Successfully installed ad-toolkit-0.0.1 dataclasses-0.6 donut-0.1 frozendict-2.3.4 gast-0.2.2 idx2numpy-1.2.3 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tfsnippet-0.1.2 zhusuan-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install \"ad_toolkit[donut] @ git+https://github.com/pbudzyns/ad_toolkit.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L07eMCJopQuB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4305,
     "status": "ok",
     "timestamp": 1659105215739,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "L07eMCJopQuB",
    "outputId": "b1df94a8-2321-43ed-b6e3-5ffba2ab0527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tfsnippet/utils/scope.py:114: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tfsnippet/mathops/_tfops.py:20: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tfsnippet/mathops/_tfops.py:21: The name tf.log1p is deprecated. Please use tf.math.log1p instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/donut/training.py:78: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/ad_toolkit/detectors/donut_ad.py:87: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ad_toolkit.detectors import Donut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JpW5lUs7pGAQ",
   "metadata": {
    "id": "JpW5lUs7pGAQ"
   },
   "source": [
    "### Find best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lG5WyVrtpiFU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27965,
     "status": "ok",
     "timestamp": 1659105341889,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "lG5WyVrtpiFU",
    "outputId": "c144cd4d-798f-410b-a5d3-8340faea8d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 37.42s] step time: 0.009427s (±0.03043s); valid time: 0.1402s; loss: 133.714 (±15.2337); valid loss: 94.7717 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.02s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.46s] step time: 0.006126s (±0.007024s); valid time: 0.06981s; loss: 64.1744 (±23.1007); valid loss: 57.2592 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.55s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 26.76s] step time: 0.005408s (±0.001151s); valid time: 0.002193s; loss: 14.0795 (±7.09114); valid loss: 109.292\n",
      "[Epoch 30/350, Step 330, ETA 26.06s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.61s] step time: 0.005407s (±0.0009243s); valid time: 0.00202s; loss: 1.93519 (±4.62382); valid loss: 124.568\n",
      "[Epoch 40/350, Step 440, ETA 24.01s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 23.18s] step time: 0.005544s (±0.001647s); valid time: 0.002257s; loss: -6.08965 (±4.93621); valid loss: 120.878\n",
      "[Epoch 50/350, Step 550, ETA 22.49s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.9s] step time: 0.005336s (±0.000716s); valid time: 0.002312s; loss: -13.2569 (±4.65724); valid loss: 92.6986\n",
      "[Epoch 60/350, Step 660, ETA 21.28s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.92s] step time: 0.005549s (±0.001394s); valid time: 0.002223s; loss: -18.2544 (±4.70675); valid loss: 93.3721\n",
      "[Epoch 70/350, Step 770, ETA 20.19s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.91s] step time: 0.005227s (±0.0005747s); valid time: 0.002183s; loss: -21.4336 (±4.7855); valid loss: 78.1269\n",
      "[Epoch 80/350, Step 880, ETA 19.2s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.04s] step time: 0.005376s (±0.0009465s); valid time: 0.002365s; loss: -23.5711 (±4.62384); valid loss: 79.3001\n",
      "[Epoch 90/350, Step 990, ETA 18.28s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.24s] step time: 0.005404s (±0.0009499s); valid time: 0.002139s; loss: -25.5594 (±4.66551); valid loss: 73.8101\n",
      "[Epoch 100/350, Step 1100, ETA 17.45s] step time: 0.005297s (±0.0006748s); valid time: 0.002085s; loss: -26.422 (±4.32046); valid loss: 76.1913\n",
      "[Epoch 100/350, Step 1100, ETA 17.45s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.71s] step time: 0.005354s (±0.0009956s); valid time: 0.00214s; loss: -27.5761 (±4.50791); valid loss: 71.6854\n",
      "[Epoch 110/350, Step 1210, ETA 16.64s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.97s] step time: 0.005224s (±0.000602s); valid time: 0.002224s; loss: -28.5089 (±4.71595); valid loss: 70.5932\n",
      "[Epoch 120/350, Step 1320, ETA 15.82s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.26s] step time: 0.005305s (±0.000898s); valid time: 0.002117s; loss: -28.7815 (±4.35165); valid loss: 72.8267\n",
      "[Epoch 130/350, Step 1430, ETA 15.05s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.59s] step time: 0.005391s (±0.0006646s); valid time: 0.00224s; loss: -29.4959 (±4.03618); valid loss: 68.8281\n",
      "[Epoch 140/350, Step 1540, ETA 14.32s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.91s] step time: 0.005332s (±0.001053s); valid time: 0.002139s; loss: -29.8744 (±5.0732); valid loss: 70.0545\n",
      "[Epoch 150/350, Step 1650, ETA 13.58s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.25s] step time: 0.005297s (±0.0007509s); valid time: 0.002192s; loss: -30.3261 (±4.22545); valid loss: 73.1073\n",
      "[Epoch 160/350, Step 1760, ETA 12.87s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.61s] step time: 0.005459s (±0.00141s); valid time: 0.002165s; loss: -30.3143 (±4.38312); valid loss: 67.0418\n",
      "[Epoch 170/350, Step 1870, ETA 12.18s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.99s] step time: 0.005574s (±0.0009305s); valid time: 0.002435s; loss: -30.2344 (±4.32089); valid loss: 68.7958\n",
      "[Epoch 180/350, Step 1980, ETA 11.49s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.36s] step time: 0.005406s (±0.0005759s); valid time: 0.002196s; loss: -30.8339 (±4.65576); valid loss: 69.2496\n",
      "[Epoch 190/350, Step 2090, ETA 10.79s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.73s] step time: 0.005423s (±0.001039s); valid time: 0.002101s; loss: -30.5153 (±4.54038); valid loss: 69.7913\n",
      "[Epoch 200/350, Step 2200, ETA 10.09s] step time: 0.005353s (±0.0006669s); valid time: 0.002094s; loss: -30.7498 (±4.04063); valid loss: 68.6695\n",
      "[Epoch 200/350, Step 2200, ETA 10.09s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.463s] step time: 0.005269s (±0.0007875s); valid time: 0.002323s; loss: -30.7105 (±4.0155); valid loss: 68.8068\n",
      "[Epoch 210/350, Step 2310, ETA 9.396s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.976s] step time: 0.007596s (±0.004906s); valid time: 0.001974s; loss: -30.6667 (±4.76806); valid loss: 66.6635\n",
      "[Epoch 220/350, Step 2420, ETA 8.844s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.477s] step time: 0.007708s (±0.004828s); valid time: 0.002294s; loss: -31.0693 (±4.36551); valid loss: 66.8779\n",
      "[Epoch 230/350, Step 2530, ETA 8.28s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.902s] step time: 0.006821s (±0.004048s); valid time: 0.002677s; loss: -30.9129 (±4.94639); valid loss: 65.7983\n",
      "[Epoch 240/350, Step 2640, ETA 7.667s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.27s] step time: 0.005756s (±0.001874s); valid time: 0.002196s; loss: -30.8105 (±3.80823); valid loss: 68.2483\n",
      "[Epoch 250/350, Step 2750, ETA 6.94s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.619s] step time: 0.005323s (±0.0008558s); valid time: 0.002255s; loss: -31.0341 (±4.344); valid loss: 66.5551\n",
      "[Epoch 260/350, Step 2860, ETA 6.228s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.973s] step time: 0.005327s (±0.000951s); valid time: 0.002056s; loss: -31.1161 (±4.60319); valid loss: 67.6143\n",
      "[Epoch 270/350, Step 2970, ETA 5.527s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.334s] step time: 0.005433s (±0.0008251s); valid time: 0.002076s; loss: -31.2196 (±3.92266); valid loss: 65.796\n",
      "[Epoch 280/350, Step 3080, ETA 4.824s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.697s] step time: 0.00536s (±0.0007029s); valid time: 0.002127s; loss: -31.029 (±4.0094); valid loss: 67.1633\n",
      "[Epoch 290/350, Step 3190, ETA 4.128s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.065s] step time: 0.005437s (±0.0008753s); valid time: 0.002251s; loss: -30.9373 (±5.27601); valid loss: 64.8063\n",
      "[Epoch 300/350, Step 3300, ETA 3.431s] step time: 0.005244s (±0.0005855s); valid time: 0.002833s; loss: -30.9284 (±4.26186); valid loss: 63.6687\n",
      "[Epoch 300/350, Step 3300, ETA 3.431s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.807s] step time: 0.005665s (±0.00158s); valid time: 0.002047s; loss: -30.9179 (±4.51715); valid loss: 68.3654\n",
      "[Epoch 310/350, Step 3410, ETA 2.743s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.18s] step time: 0.005419s (±0.0009822s); valid time: 0.002291s; loss: -31.051 (±4.16004); valid loss: 69.7365\n",
      "[Epoch 320/350, Step 3520, ETA 2.055s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.556s] step time: 0.005494s (±0.0009094s); valid time: 0.002329s; loss: -31.109 (±4.65673); valid loss: 69.0557\n",
      "[Epoch 330/350, Step 3630, ETA 1.368s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9329s] step time: 0.005555s (±0.001027s); valid time: 0.00209s; loss: -31.107 (±4.76305); valid loss: 65.9583\n",
      "[Epoch 340/350, Step 3740, ETA 0.6837s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3105s] step time: 0.005317s (±0.0006091s); valid time: 0.002083s; loss: -31.0765 (±4.27465); valid loss: 66.2114\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprum0l5aw/variables.dat-200\n",
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(136, 129, 3500, 267),\n",
      "\tprecision=0.51,\n",
      "\trecall=0.34,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.06572420634920635,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = nab_simple.data\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = Donut(window_size=100, latent_size=8, layers=(64,32,16))\n",
    "    model.train(x, labels=y, epochs=350)\n",
    "\n",
    "    scores = model.predict(x)\n",
    "result, threshold = best_result(scores, y, upper_range=np.max(scores)+0.1, steps=400)\n",
    "anomalies = (scores > threshold).astype(np.int32)\n",
    "\n",
    "nab_simple.plot(anomalies={'vae': anomalies})\n",
    "print(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gdxzqBiZqjXh",
   "metadata": {
    "id": "gdxzqBiZqjXh"
   },
   "source": [
    "### Run for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QQbyjb0lN6U",
   "metadata": {
    "id": "7QQbyjb0lN6U"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def fake_index(data):\n",
    "    start = data.index[0]\n",
    "    new_idx = [start]\n",
    "    for i in range(1, len(data)):\n",
    "        new_idx.append(start + datetime.timedelta(minutes=(i*5)))\n",
    "    return new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Am5Ak7FKqm7j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3806443,
     "status": "ok",
     "timestamp": 1659113467191,
     "user": {
      "displayName": "Paweł Budzyński",
      "userId": "10453476179491152064"
     },
     "user_tz": -120
    },
    "id": "Am5Ak7FKqm7j",
    "outputId": "09d4a10a-677e-4b58-af88-7da95e8094db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on artificialWithAnomaly/art_daily_flatmiddle.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 39.67s] step time: 0.01004s (±0.03328s); valid time: 0.1662s; loss: 130.315 (±14.2818); valid loss: 111.81 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.89s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 31.26s] step time: 0.006054s (±0.007324s); valid time: 0.07359s; loss: 67.4134 (±20.1664); valid loss: 53.8715 (*)\n",
      "[Epoch 20/350, Step 220, ETA 30.1s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 28.19s] step time: 0.006192s (±0.007497s); valid time: 0.0745s; loss: 20.5263 (±9.53287); valid loss: 30.0927 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.35s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.43s] step time: 0.006311s (±0.007664s); valid time: 0.0754s; loss: 4.86195 (±7.75844); valid loss: 16.6494 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.47s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.84s] step time: 0.005939s (±0.007396s); valid time: 0.0738s; loss: -6.27559 (±6.1567); valid loss: 6.41049 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24.03s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.7s] step time: 0.006146s (±0.007564s); valid time: 0.07552s; loss: -14.5194 (±6.44609); valid loss: -0.74283 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.83s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.7s] step time: 0.006185s (±0.007923s); valid time: 0.0792s; loss: -19.6095 (±6.47693); valid loss: -4.77892 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.8s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.8s] step time: 0.00623s (±0.008277s); valid time: 0.08306s; loss: -23.0125 (±5.3366); valid loss: -7.49013 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.96s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 21s] step time: 0.006368s (±0.00774s); valid time: 0.07709s; loss: -25.5569 (±5.39754); valid loss: -9.66751 (*)\n",
      "[Epoch 90/350, Step 990, ETA 20s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 20.14s] step time: 0.006076s (±0.007767s); valid time: 0.07673s; loss: -27.8475 (±5.40675); valid loss: -11.1497 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.28s] step time: 0.005977s (±0.00753s); valid time: 0.07538s; loss: -29.0362 (±5.55495); valid loss: -12.4089 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.28s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.5s] step time: 0.006137s (±0.007653s); valid time: 0.07605s; loss: -30.4707 (±5.95573); valid loss: -13.5597 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.4s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.73s] step time: 0.006119s (±0.00848s); valid time: 0.08494s; loss: -31.2266 (±6.62824); valid loss: -14.2638 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.54s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.98s] step time: 0.006135s (±0.007991s); valid time: 0.07963s; loss: -32.0195 (±5.53639); valid loss: -14.8935 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.7s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.24s] step time: 0.006118s (±0.007779s); valid time: 0.0766s; loss: -32.2199 (±6.61273); valid loss: -14.9131 (*)\n",
      "[Epoch 140/350, Step 1540, ETA 15.89s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.5s] step time: 0.00611s (±0.008357s); valid time: 0.08393s; loss: -33.035 (±5.19053); valid loss: -15.8426 (*)\n",
      "[Epoch 150/350, Step 1650, ETA 15.1s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.69s] step time: 0.005381s (±0.001038s); valid time: 0.002089s; loss: -33.2304 (±6.88326); valid loss: -15.581\n",
      "[Epoch 160/350, Step 1760, ETA 14.2s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.89s] step time: 0.005304s (±0.0008232s); valid time: 0.002175s; loss: -33.3046 (±5.98414); valid loss: -15.6747\n",
      "[Epoch 170/350, Step 1870, ETA 13.35s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 13.2s] step time: 0.006149s (±0.00815s); valid time: 0.08145s; loss: -33.6679 (±6.07672); valid loss: -15.9877 (*)\n",
      "[Epoch 180/350, Step 1980, ETA 12.59s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.5s] step time: 0.006031s (±0.00748s); valid time: 0.0746s; loss: -34.0165 (±5.16623); valid loss: -16.1175 (*)\n",
      "[Epoch 190/350, Step 2090, ETA 11.81s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.75s] step time: 0.005323s (±0.00094s); valid time: 0.00253s; loss: -34.1281 (±5.27438); valid loss: -15.9824\n",
      "[Epoch 200/350, Step 2200, ETA 11.07s] step time: 0.006106s (±0.008341s); valid time: 0.08364s; loss: -34.242 (±5.48801); valid loss: -16.2815 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 11.07s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.33s] step time: 0.005207s (±0.0008832s); valid time: 0.002002s; loss: -34.088 (±6.79633); valid loss: -15.7128\n",
      "[Epoch 210/350, Step 2310, ETA 10.26s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.608s] step time: 0.00523s (±0.0005749s); valid time: 0.002177s; loss: -34.4952 (±5.96866); valid loss: -15.9516\n",
      "[Epoch 220/350, Step 2420, ETA 9.463s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.905s] step time: 0.005367s (±0.0008186s); valid time: 0.002293s; loss: -34.1401 (±5.89113); valid loss: -16.1072\n",
      "[Epoch 230/350, Step 2530, ETA 8.693s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.208s] step time: 0.005316s (±0.0006079s); valid time: 0.002121s; loss: -34.3086 (±6.03811); valid loss: -16.1187\n",
      "[Epoch 240/350, Step 2640, ETA 7.937s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.529s] step time: 0.005526s (±0.0009029s); valid time: 0.002311s; loss: -34.3773 (±5.79627); valid loss: -15.9842\n",
      "[Epoch 250/350, Step 2750, ETA 7.186s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.886s] step time: 0.006339s (±0.007961s); valid time: 0.07873s; loss: -34.2375 (±6.21022); valid loss: -16.3665 (*)\n",
      "[Epoch 260/350, Step 2860, ETA 6.478s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.234s] step time: 0.006172s (±0.007927s); valid time: 0.07923s; loss: -34.5466 (±6.45319); valid loss: -16.599 (*)\n",
      "[Epoch 270/350, Step 2970, ETA 5.769s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.569s] step time: 0.005692s (±0.00112s); valid time: 0.002384s; loss: -34.0488 (±6.29899); valid loss: -16.196\n",
      "[Epoch 280/350, Step 3080, ETA 5.029s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.902s] step time: 0.005509s (±0.001212s); valid time: 0.002516s; loss: -34.3921 (±5.97291); valid loss: -15.8842\n",
      "[Epoch 290/350, Step 3190, ETA 4.303s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.236s] step time: 0.005432s (±0.0006943s); valid time: 0.002068s; loss: -34.5407 (±5.56974); valid loss: -16.4243\n",
      "[Epoch 300/350, Step 3300, ETA 3.573s] step time: 0.005348s (±0.0008597s); valid time: 0.002048s; loss: -34.2111 (±5.90471); valid loss: -16.1623\n",
      "[Epoch 300/350, Step 3300, ETA 3.573s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.914s] step time: 0.005217s (±0.0006167s); valid time: 0.00205s; loss: -34.4821 (±6.00224); valid loss: -16.0999\n",
      "[Epoch 310/350, Step 3410, ETA 2.848s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.261s] step time: 0.005377s (±0.0007075s); valid time: 0.002135s; loss: -34.4941 (±5.26125); valid loss: -16.112\n",
      "[Epoch 320/350, Step 3520, ETA 2.13s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.61s] step time: 0.005309s (±0.0005593s); valid time: 0.002268s; loss: -34.2832 (±5.60121); valid loss: -15.8734\n",
      "[Epoch 330/350, Step 3630, ETA 1.416s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9643s] step time: 0.005469s (±0.001101s); valid time: 0.002344s; loss: -34.7007 (±5.71674); valid loss: -16.3795\n",
      "[Epoch 340/350, Step 3740, ETA 0.7063s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3208s] step time: 0.005456s (±0.00112s); valid time: 0.0021s; loss: -34.3513 (±6.48517); valid loss: -16.332\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp7o9luqhg/variables.dat-2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp7o9luqhg/variables.dat-2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.67,\n",
      "\t(tp, fp, tn, fn)=(290, 1206, 2423, 113),\n",
      "\tprecision=0.19,\n",
      "\trecall=0.72,\n",
      "\tf1=0.31,\n",
      "\troc_auc=0.69,\n",
      "\ty_pred%=0.37103174603174605,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsdown.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.1s] step time: 0.009621s (±0.03252s); valid time: 0.153s; loss: 120.624 (±20.38); valid loss: 77.8886 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.58s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.62s] step time: 0.006118s (±0.00715s); valid time: 0.07137s; loss: 45.7306 (±18.5782); valid loss: 53.2161 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.49s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.82s] step time: 0.006221s (±0.007823s); valid time: 0.07739s; loss: 11.7557 (±6.10258); valid loss: 51.8505 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.88s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 25.96s] step time: 0.006074s (±0.007566s); valid time: 0.07526s; loss: 2.63468 (±5.58423); valid loss: 41.5246 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.24s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.68s] step time: 0.006231s (±0.007593s); valid time: 0.07577s; loss: -3.08455 (±4.94304); valid loss: 41.4853 (*)\n",
      "[Epoch 50/350, Step 550, ETA 23.87s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.23s] step time: 0.005518s (±0.001165s); valid time: 0.00226s; loss: -7.45874 (±4.8823); valid loss: 42.5902\n",
      "[Epoch 60/350, Step 660, ETA 22.47s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.02s] step time: 0.005534s (±0.001016s); valid time: 0.002208s; loss: -10.8464 (±4.52954); valid loss: 42.9867\n",
      "[Epoch 70/350, Step 770, ETA 21.27s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 20.97s] step time: 0.00555s (±0.0009887s); valid time: 0.002105s; loss: -13.8033 (±4.93054); valid loss: 44.1699\n",
      "[Epoch 80/350, Step 880, ETA 20.19s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.24s] step time: 0.006186s (±0.007699s); valid time: 0.07731s; loss: -15.9023 (±4.44116); valid loss: 40.7786 (*)\n",
      "[Epoch 90/350, Step 990, ETA 19.43s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.34s] step time: 0.005599s (±0.001132s); valid time: 0.002005s; loss: -17.6113 (±5.07152); valid loss: 42.6736\n",
      "[Epoch 100/350, Step 1100, ETA 18.44s] step time: 0.005424s (±0.0009674s); valid time: 0.002348s; loss: -18.9508 (±4.63381); valid loss: 44.1897\n",
      "[Epoch 100/350, Step 1100, ETA 18.44s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 17.59s] step time: 0.005373s (±0.0006275s); valid time: 0.002219s; loss: -19.9436 (±4.02661); valid loss: 41.0236\n",
      "[Epoch 110/350, Step 1210, ETA 17.5s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.96s] step time: 0.006253s (±0.007925s); valid time: 0.0782s; loss: -20.6703 (±4.04639); valid loss: 39.2261 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 16.79s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.29s] step time: 0.006118s (±0.008224s); valid time: 0.08151s; loss: -21.1452 (±4.49989); valid loss: 38.5021 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.06s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.52s] step time: 0.005472s (±0.0007803s); valid time: 0.002096s; loss: -21.8581 (±4.61401); valid loss: 39.0652\n",
      "[Epoch 140/350, Step 1540, ETA 15.2s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.76s] step time: 0.005406s (±0.001107s); valid time: 0.002217s; loss: -22.1071 (±4.53488); valid loss: 39.8986\n",
      "[Epoch 150/350, Step 1650, ETA 14.39s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.02s] step time: 0.005336s (±0.0008362s); valid time: 0.002075s; loss: -22.3418 (±4.53065); valid loss: 39.809\n",
      "[Epoch 160/350, Step 1760, ETA 13.59s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.31s] step time: 0.005525s (±0.0009491s); valid time: 0.002335s; loss: -22.6519 (±4.15378); valid loss: 40.837\n",
      "[Epoch 170/350, Step 1870, ETA 12.81s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.59s] step time: 0.005345s (±0.0005788s); valid time: 0.002365s; loss: -22.6904 (±4.42426); valid loss: 41.4811\n",
      "[Epoch 180/350, Step 1980, ETA 12.06s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.92s] step time: 0.005582s (±0.00129s); valid time: 0.002243s; loss: -22.9429 (±4.76883); valid loss: 40.6897\n",
      "[Epoch 190/350, Step 2090, ETA 11.29s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.23s] step time: 0.005427s (±0.001341s); valid time: 0.002216s; loss: -22.8014 (±4.44509); valid loss: 39.1565\n",
      "[Epoch 200/350, Step 2200, ETA 10.55s] step time: 0.005405s (±0.0007001s); valid time: 0.002008s; loss: -23.0329 (±4.70996); valid loss: 40.0691\n",
      "[Epoch 200/350, Step 2200, ETA 10.55s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.883s] step time: 0.005485s (±0.001127s); valid time: 0.002002s; loss: -23.0431 (±4.20863); valid loss: 41.4456\n",
      "[Epoch 210/350, Step 2310, ETA 9.814s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.209s] step time: 0.005282s (±0.0005471s); valid time: 0.002217s; loss: -23.0549 (±4.20804); valid loss: 39.2575\n",
      "[Epoch 220/350, Step 2420, ETA 9.075s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.556s] step time: 0.005493s (±0.0009375s); valid time: 0.002235s; loss: -23.312 (±4.83499); valid loss: 39.3701\n",
      "[Epoch 230/350, Step 2530, ETA 8.355s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.897s] step time: 0.005318s (±0.0006302s); valid time: 0.002247s; loss: -22.9334 (±4.28931); valid loss: 39.9637\n",
      "[Epoch 240/350, Step 2640, ETA 7.644s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.252s] step time: 0.005506s (±0.0009944s); valid time: 0.002964s; loss: -23.1886 (±4.64453); valid loss: 39.7109\n",
      "[Epoch 250/350, Step 2750, ETA 6.93s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.611s] step time: 0.005433s (±0.0007553s); valid time: 0.002233s; loss: -23.1854 (±4.49027); valid loss: 40.4122\n",
      "[Epoch 260/350, Step 2860, ETA 6.227s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.973s] step time: 0.00556s (±0.001004s); valid time: 0.002142s; loss: -23.2042 (±4.64661); valid loss: 39.765\n",
      "[Epoch 270/350, Step 2970, ETA 5.527s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.335s] step time: 0.005442s (±0.001049s); valid time: 0.002951s; loss: -23.3975 (±4.38499); valid loss: 40.2373\n",
      "[Epoch 280/350, Step 3080, ETA 4.823s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.695s] step time: 0.005253s (±0.000584s); valid time: 0.002087s; loss: -23.1832 (±4.27977); valid loss: 39.6426\n",
      "[Epoch 290/350, Step 3190, ETA 4.123s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.06s] step time: 0.005324s (±0.001029s); valid time: 0.002128s; loss: -23.3377 (±4.83965); valid loss: 40.2222\n",
      "[Epoch 300/350, Step 3300, ETA 3.43s] step time: 0.005453s (±0.001104s); valid time: 0.002136s; loss: -23.3123 (±4.52107); valid loss: 39.94\n",
      "[Epoch 300/350, Step 3300, ETA 3.43s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.801s] step time: 0.005297s (±0.0005555s); valid time: 0.002053s; loss: -23.2673 (±4.91991); valid loss: 40.5972\n",
      "[Epoch 310/350, Step 3410, ETA 2.738s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.177s] step time: 0.005556s (±0.0009695s); valid time: 0.002172s; loss: -23.3108 (±4.73285); valid loss: 39.1983\n",
      "[Epoch 320/350, Step 3520, ETA 2.052s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.552s] step time: 0.005257s (±0.0005878s); valid time: 0.002226s; loss: -23.2518 (±4.63373); valid loss: 39.4087\n",
      "[Epoch 330/350, Step 3630, ETA 1.365s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9304s] step time: 0.005482s (±0.001243s); valid time: 0.002241s; loss: -23.1358 (±3.8481); valid loss: 40.4186\n",
      "[Epoch 340/350, Step 3740, ETA 0.6818s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3097s] step time: 0.005367s (±0.0008801s); valid time: 0.002182s; loss: -23.4418 (±4.52162); valid loss: 39.2556\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpou8hjbcl/variables.dat-1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpou8hjbcl/variables.dat-1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.45,\n",
      "\t(tp, fp, tn, fn)=(291, 2122, 1507, 112),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.72,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.5984623015873016,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_jumpsup.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.59s] step time: 0.009763s (±0.03218s); valid time: 0.1625s; loss: 126.081 (±19.2377); valid loss: 86.6784 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.01s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 29.53s] step time: 0.005395s (±0.001042s); valid time: 0.002145s; loss: 50.0807 (±21.9133); valid loss: 277.361\n",
      "[Epoch 20/350, Step 220, ETA 28.49s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 25.99s] step time: 0.005235s (±0.0004626s); valid time: 0.002132s; loss: 12.1002 (±6.35186); valid loss: 338.299\n",
      "[Epoch 30/350, Step 330, ETA 25.22s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 23.97s] step time: 0.005325s (±0.001031s); valid time: 0.001971s; loss: 3.24224 (±5.32524); valid loss: 355.939\n",
      "[Epoch 40/350, Step 440, ETA 23.34s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 22.47s] step time: 0.005262s (±0.0009827s); valid time: 0.004798s; loss: -2.67446 (±5.89278); valid loss: 435.767\n",
      "[Epoch 50/350, Step 550, ETA 21.9s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.35s] step time: 0.00536s (±0.0009453s); valid time: 0.002441s; loss: -7.51052 (±4.90434); valid loss: 406.203\n",
      "[Epoch 60/350, Step 660, ETA 20.73s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.38s] step time: 0.005379s (±0.0009889s); valid time: 0.002147s; loss: -11.1931 (±4.28966); valid loss: 419.725\n",
      "[Epoch 70/350, Step 770, ETA 19.74s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.5s] step time: 0.005335s (±0.001133s); valid time: 0.00219s; loss: -14.4709 (±3.98456); valid loss: 441.314\n",
      "[Epoch 80/350, Step 880, ETA 18.86s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 18.7s] step time: 0.005389s (±0.00104s); valid time: 0.002086s; loss: -16.6263 (±5.22694); valid loss: 407.875\n",
      "[Epoch 90/350, Step 990, ETA 17.99s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 17.92s] step time: 0.005335s (±0.0007235s); valid time: 0.002035s; loss: -18.2053 (±4.3853); valid loss: 387.064\n",
      "[Epoch 100/350, Step 1100, ETA 17.17s] step time: 0.005315s (±0.001112s); valid time: 0.002053s; loss: -19.6579 (±4.98472); valid loss: 384.586\n",
      "[Epoch 100/350, Step 1100, ETA 17.17s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.42s] step time: 0.005149s (±0.0006507s); valid time: 0.002154s; loss: -20.5133 (±4.41495); valid loss: 403.537\n",
      "[Epoch 110/350, Step 1210, ETA 16.36s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.73s] step time: 0.005339s (±0.000812s); valid time: 0.002244s; loss: -21.3268 (±4.30409); valid loss: 389.342\n",
      "[Epoch 120/350, Step 1320, ETA 15.58s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.07s] step time: 0.005416s (±0.001016s); valid time: 0.002172s; loss: -21.6925 (±4.38526); valid loss: 391.024\n",
      "[Epoch 130/350, Step 1430, ETA 14.86s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.4s] step time: 0.005287s (±0.0006027s); valid time: 0.002205s; loss: -22.2899 (±4.3851); valid loss: 415.173\n",
      "[Epoch 140/350, Step 1540, ETA 14.13s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.75s] step time: 0.005321s (±0.0008871s); valid time: 0.0023s; loss: -22.5555 (±4.0869); valid loss: 406.034\n",
      "[Epoch 150/350, Step 1650, ETA 13.42s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.1s] step time: 0.005317s (±0.0007048s); valid time: 0.002035s; loss: -22.9447 (±3.96201); valid loss: 402.325\n",
      "[Epoch 160/350, Step 1760, ETA 12.74s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.49s] step time: 0.005594s (±0.001303s); valid time: 0.002143s; loss: -22.7602 (±4.402); valid loss: 390.425\n",
      "[Epoch 170/350, Step 1870, ETA 12.04s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.86s] step time: 0.005398s (±0.001038s); valid time: 0.002162s; loss: -23.0469 (±4.27694); valid loss: 395.677\n",
      "[Epoch 180/350, Step 1980, ETA 11.36s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.24s] step time: 0.005403s (±0.000975s); valid time: 0.002276s; loss: -23.4525 (±4.27505); valid loss: 391.431\n",
      "[Epoch 190/350, Step 2090, ETA 10.67s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.61s] step time: 0.005271s (±0.0007079s); valid time: 0.002079s; loss: -23.2918 (±4.19939); valid loss: 399.465\n",
      "[Epoch 200/350, Step 2200, ETA 9.974s] step time: 0.00521s (±0.0006193s); valid time: 0.002122s; loss: -23.3576 (±3.80528); valid loss: 399.32\n",
      "[Epoch 200/350, Step 2200, ETA 9.974s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.36s] step time: 0.005386s (±0.00105s); valid time: 0.002445s; loss: -23.4948 (±4.5219); valid loss: 388.237\n",
      "[Epoch 210/350, Step 2310, ETA 9.295s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.732s] step time: 0.00516s (±0.0006276s); valid time: 0.00224s; loss: -23.3476 (±3.82484); valid loss: 392.874\n",
      "[Epoch 220/350, Step 2420, ETA 8.608s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.125s] step time: 0.005439s (±0.001142s); valid time: 0.002105s; loss: -23.4818 (±4.13705); valid loss: 388.309\n",
      "[Epoch 230/350, Step 2530, ETA 7.939s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.511s] step time: 0.005261s (±0.0006427s); valid time: 0.002168s; loss: -23.3667 (±3.95603); valid loss: 386.608\n",
      "[Epoch 240/350, Step 2640, ETA 7.275s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.913s] step time: 0.005559s (±0.001334s); valid time: 0.002494s; loss: -23.4368 (±4.48221); valid loss: 394.916\n",
      "[Epoch 250/350, Step 2750, ETA 6.607s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.306s] step time: 0.005356s (±0.0009671s); valid time: 0.002143s; loss: -23.8064 (±4.37182); valid loss: 385.747\n",
      "[Epoch 260/350, Step 2860, ETA 5.945s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.703s] step time: 0.005446s (±0.001142s); valid time: 0.002069s; loss: -23.4256 (±4.44698); valid loss: 390.198\n",
      "[Epoch 270/350, Step 2970, ETA 5.279s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.1s] step time: 0.005376s (±0.0008203s); valid time: 0.002053s; loss: -23.3971 (±4.08671); valid loss: 391.592\n",
      "[Epoch 280/350, Step 3080, ETA 4.615s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.495s] step time: 0.005293s (±0.0005211s); valid time: 0.002289s; loss: -23.5212 (±4.19872); valid loss: 395.294\n",
      "[Epoch 290/350, Step 3190, ETA 3.952s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.893s] step time: 0.005306s (±0.0008964s); valid time: 0.002342s; loss: -23.5935 (±3.94265); valid loss: 393.581\n",
      "[Epoch 300/350, Step 3300, ETA 3.292s] step time: 0.00538s (±0.00089s); valid time: 0.00216s; loss: -23.6541 (±4.34247); valid loss: 394.707\n",
      "[Epoch 300/350, Step 3300, ETA 3.292s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.692s] step time: 0.005369s (±0.0006843s); valid time: 0.002057s; loss: -23.5167 (±4.06538); valid loss: 391.775\n",
      "[Epoch 310/350, Step 3410, ETA 2.631s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.092s] step time: 0.005323s (±0.0008014s); valid time: 0.002016s; loss: -23.6569 (±4.43797); valid loss: 389.942\n",
      "[Epoch 320/350, Step 3520, ETA 1.972s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.493s] step time: 0.00525s (±0.000753s); valid time: 0.00212s; loss: -23.318 (±4.3441); valid loss: 396.099\n",
      "[Epoch 330/350, Step 3630, ETA 1.313s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.8949s] step time: 0.005279s (±0.001205s); valid time: 0.002195s; loss: -23.6055 (±4.15721); valid loss: 389.451\n",
      "[Epoch 340/350, Step 3740, ETA 0.6562s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.2982s] step time: 0.005373s (±0.0008248s); valid time: 0.002035s; loss: -23.5275 (±4.53794); valid loss: 398.643\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp6pdzyh_r/variables.dat-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp6pdzyh_r/variables.dat-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.81,\n",
      "\t(tp, fp, tn, fn)=(139, 507, 3122, 264),\n",
      "\tprecision=0.22,\n",
      "\trecall=0.34,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.16021825396825398,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_daily_nojump.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.68s] step time: 0.009729s (±0.03352s); valid time: 0.1566s; loss: 131.887 (±13.9842); valid loss: 96.8545 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.21s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 31.12s] step time: 0.006194s (±0.007545s); valid time: 0.07451s; loss: 68.7846 (±19.3875); valid loss: 47.4492 (*)\n",
      "[Epoch 20/350, Step 220, ETA 30.03s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.13s] step time: 0.005369s (±0.00104s); valid time: 0.002096s; loss: 16.0721 (±10.368); valid loss: 186.402\n",
      "[Epoch 30/350, Step 330, ETA 26.23s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.75s] step time: 0.005265s (±0.000672s); valid time: 0.002159s; loss: -3.15051 (±5.72915); valid loss: 170.659\n",
      "[Epoch 40/350, Step 440, ETA 24.06s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 23.25s] step time: 0.0055s (±0.001113s); valid time: 0.002304s; loss: -12.0556 (±5.15529); valid loss: 123.352\n",
      "[Epoch 50/350, Step 550, ETA 22.58s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 22.02s] step time: 0.005445s (±0.0009089s); valid time: 0.002271s; loss: -18.3142 (±4.70212); valid loss: 95.8101\n",
      "[Epoch 60/350, Step 660, ETA 21.41s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 21.01s] step time: 0.005492s (±0.0009856s); valid time: 0.002257s; loss: -22.4476 (±4.53073); valid loss: 61.1922\n",
      "[Epoch 70/350, Step 770, ETA 20.26s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 20.02s] step time: 0.005334s (±0.0008247s); valid time: 0.00222s; loss: -25.747 (±4.35176); valid loss: 48.0186\n",
      "[Epoch 80/350, Step 880, ETA 19.25s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.34s] step time: 0.005988s (±0.007716s); valid time: 0.07744s; loss: -28.0286 (±4.90383); valid loss: 34.5366 (*)\n",
      "[Epoch 90/350, Step 990, ETA 18.55s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.68s] step time: 0.006055s (±0.007869s); valid time: 0.07852s; loss: -29.8035 (±3.92363); valid loss: 22.8121 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 17.97s] step time: 0.005802s (±0.007552s); valid time: 0.07529s; loss: -31.172 (±4.29109); valid loss: 22.6743 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 17.97s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 17.33s] step time: 0.006082s (±0.007677s); valid time: 0.07635s; loss: -32.218 (±3.91416); valid loss: 18.0654 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 17.25s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.56s] step time: 0.005456s (±0.001179s); valid time: 0.002187s; loss: -32.8802 (±4.05432); valid loss: 18.4019\n",
      "[Epoch 120/350, Step 1320, ETA 16.4s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.91s] step time: 0.00597s (±0.008207s); valid time: 0.08174s; loss: -33.4184 (±3.67791); valid loss: 11.7347 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 15.67s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.16s] step time: 0.005371s (±0.0008791s); valid time: 0.002188s; loss: -33.6766 (±4.35995); valid loss: 13.2749\n",
      "[Epoch 140/350, Step 1540, ETA 14.86s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.41s] step time: 0.005223s (±0.0006289s); valid time: 0.002095s; loss: -34.3271 (±3.95073); valid loss: 12.6823\n",
      "[Epoch 150/350, Step 1650, ETA 14.06s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.8s] step time: 0.006095s (±0.007651s); valid time: 0.07554s; loss: -34.182 (±4.00597); valid loss: 9.2241 (*)\n",
      "[Epoch 160/350, Step 1760, ETA 13.38s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.12s] step time: 0.005534s (±0.001161s); valid time: 0.00228s; loss: -34.7346 (±4.17404); valid loss: 10.338\n",
      "[Epoch 170/350, Step 1870, ETA 12.64s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.44s] step time: 0.005476s (±0.0006816s); valid time: 0.002054s; loss: -34.7786 (±3.8704); valid loss: 11.159\n",
      "[Epoch 180/350, Step 1980, ETA 11.89s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.75s] step time: 0.005346s (±0.000934s); valid time: 0.002132s; loss: -34.945 (±3.94576); valid loss: 11.5598\n",
      "[Epoch 190/350, Step 2090, ETA 11.14s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.08s] step time: 0.005394s (±0.001123s); valid time: 0.002395s; loss: -34.9157 (±3.84419); valid loss: 9.70469\n",
      "[Epoch 200/350, Step 2200, ETA 10.48s] step time: 0.006268s (±0.007787s); valid time: 0.07746s; loss: -35.1411 (±4.40043); valid loss: 9.20115 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.48s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.813s] step time: 0.005343s (±0.001066s); valid time: 0.002468s; loss: -35.1507 (±4.32525); valid loss: 9.62809\n",
      "[Epoch 210/350, Step 2310, ETA 9.746s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.15s] step time: 0.005329s (±0.0006433s); valid time: 0.002201s; loss: -35.088 (±4.26799); valid loss: 9.33445\n",
      "[Epoch 220/350, Step 2420, ETA 9.016s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.494s] step time: 0.005353s (±0.0009042s); valid time: 0.002242s; loss: -35.0753 (±3.80646); valid loss: 10.1636\n",
      "[Epoch 230/350, Step 2530, ETA 8.295s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.88s] step time: 0.006103s (±0.007968s); valid time: 0.07973s; loss: -35.1048 (±4.65356); valid loss: 9.12704 (*)\n",
      "[Epoch 240/350, Step 2640, ETA 7.619s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.26s] step time: 0.006057s (±0.007665s); valid time: 0.07637s; loss: -35.1882 (±3.81903); valid loss: 8.03356 (*)\n",
      "[Epoch 250/350, Step 2750, ETA 6.932s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.612s] step time: 0.005288s (±0.0008558s); valid time: 0.002082s; loss: -34.813 (±3.97685); valid loss: 9.70057\n",
      "[Epoch 260/350, Step 2860, ETA 6.223s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.968s] step time: 0.005334s (±0.0009791s); valid time: 0.002179s; loss: -35.399 (±3.89664); valid loss: 8.54491\n",
      "[Epoch 270/350, Step 2970, ETA 5.523s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.333s] step time: 0.005544s (±0.001044s); valid time: 0.002135s; loss: -35.2922 (±3.99889); valid loss: 9.46353\n",
      "[Epoch 280/350, Step 3080, ETA 4.824s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.697s] step time: 0.005363s (±0.0005291s); valid time: 0.002398s; loss: -35.35 (±3.94431); valid loss: 10.4246\n",
      "[Epoch 290/350, Step 3190, ETA 4.125s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.063s] step time: 0.005362s (±0.001015s); valid time: 0.002156s; loss: -35.2572 (±4.77769); valid loss: 8.59215\n",
      "[Epoch 300/350, Step 3300, ETA 3.435s] step time: 0.005558s (±0.001571s); valid time: 0.00209s; loss: -35.119 (±3.87708); valid loss: 10.1832\n",
      "[Epoch 300/350, Step 3300, ETA 3.435s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.804s] step time: 0.005242s (±0.0006383s); valid time: 0.002216s; loss: -35.1035 (±4.49348); valid loss: 8.96471\n",
      "[Epoch 310/350, Step 3410, ETA 2.741s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.177s] step time: 0.005356s (±0.001275s); valid time: 0.002112s; loss: -35.2352 (±4.04256); valid loss: 9.85726\n",
      "[Epoch 320/350, Step 3520, ETA 2.052s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.553s] step time: 0.005393s (±0.0006986s); valid time: 0.002331s; loss: -35.0431 (±4.29281); valid loss: 9.21219\n",
      "[Epoch 330/350, Step 3630, ETA 1.366s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9311s] step time: 0.005546s (±0.001311s); valid time: 0.002142s; loss: -35.2584 (±3.9642); valid loss: 9.0158\n",
      "[Epoch 340/350, Step 3740, ETA 0.6824s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.31s] step time: 0.005457s (±0.00119s); valid time: 0.002442s; loss: -35.3733 (±4.30965); valid loss: 9.64127\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmptkrczomo/variables.dat-2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmptkrczomo/variables.dat-2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.43,\n",
      "\t(tp, fp, tn, fn)=(323, 2238, 1391, 80),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.8,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.6351686507936508,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_increase_spike_density.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.14s] step time: 0.009635s (±0.03219s); valid time: 0.1561s; loss: 123.913 (±10.0346); valid loss: 121.127 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.51s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.54s] step time: 0.006043s (±0.007177s); valid time: 0.07164s; loss: 66.0094 (±22.1951); valid loss: 33.9443 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.47s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.91s] step time: 0.006328s (±0.00775s); valid time: 0.07638s; loss: 6.91112 (±10.9285); valid loss: -11.1163 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.98s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 25.97s] step time: 0.006016s (±0.007699s); valid time: 0.07679s; loss: -19.6443 (±6.92778); valid loss: -31.2174 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.39s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.91s] step time: 0.006538s (±0.008358s); valid time: 0.08309s; loss: -34.0866 (±5.803); valid loss: -50.8055 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24.04s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.77s] step time: 0.006187s (±0.007701s); valid time: 0.0766s; loss: -44.9346 (±5.61208); valid loss: -61.0845 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.88s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.73s] step time: 0.00612s (±0.007618s); valid time: 0.07604s; loss: -51.0715 (±5.02485); valid loss: -70.7623 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.89s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.86s] step time: 0.006269s (±0.008467s); valid time: 0.08514s; loss: -59.8795 (±5.57188); valid loss: -77.8293 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.92s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.99s] step time: 0.006184s (±0.007728s); valid time: 0.07666s; loss: -63.1213 (±5.46892); valid loss: -83.1188 (*)\n",
      "[Epoch 90/350, Step 990, ETA 20.01s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 20.14s] step time: 0.006095s (±0.007841s); valid time: 0.07831s; loss: -67.6811 (±5.13719); valid loss: -89.1267 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.33s] step time: 0.006165s (±0.007816s); valid time: 0.07774s; loss: -71.6994 (±6.60676); valid loss: -93.9466 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.33s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.55s] step time: 0.006173s (±0.008873s); valid time: 0.08885s; loss: -74.586 (±6.1305); valid loss: -97.753 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.45s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.78s] step time: 0.006135s (±0.008179s); valid time: 0.08176s; loss: -76.8845 (±7.08797); valid loss: -100.456 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.59s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 17.03s] step time: 0.00616s (±0.008058s); valid time: 0.08072s; loss: -78.2687 (±7.72665); valid loss: -102.548 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.74s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.28s] step time: 0.006066s (±0.008522s); valid time: 0.08449s; loss: -79.8239 (±6.45928); valid loss: -104.53 (*)\n",
      "[Epoch 140/350, Step 1540, ETA 15.93s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.55s] step time: 0.006194s (±0.008565s); valid time: 0.08551s; loss: -81.1046 (±7.60748); valid loss: -105.803 (*)\n",
      "[Epoch 150/350, Step 1650, ETA 15.14s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.84s] step time: 0.006227s (±0.0078s); valid time: 0.07813s; loss: -81.7926 (±6.94978); valid loss: -106.786 (*)\n",
      "[Epoch 160/350, Step 1760, ETA 14.37s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 14.15s] step time: 0.00632s (±0.008552s); valid time: 0.08573s; loss: -83.0282 (±7.30231); valid loss: -107.468 (*)\n",
      "[Epoch 170/350, Step 1870, ETA 13.59s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 13.44s] step time: 0.006205s (±0.008402s); valid time: 0.0842s; loss: -82.6656 (±6.7818); valid loss: -108.137 (*)\n",
      "[Epoch 180/350, Step 1980, ETA 12.82s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.74s] step time: 0.006232s (±0.007623s); valid time: 0.07562s; loss: -83.6334 (±7.97296); valid loss: -108.982 (*)\n",
      "[Epoch 190/350, Step 2090, ETA 12.03s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 12.02s] step time: 0.00606s (±0.00823s); valid time: 0.08253s; loss: -83.9823 (±7.38269); valid loss: -109.625 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 11.32s] step time: 0.006114s (±0.00786s); valid time: 0.07871s; loss: -83.9297 (±7.93869); valid loss: -109.714 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 11.32s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.6s] step time: 0.00589s (±0.007121s); valid time: 0.07106s; loss: -84.2322 (±6.59905); valid loss: -110.426 (*)\n",
      "[Epoch 210/350, Step 2310, ETA 10.53s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.898s] step time: 0.005953s (±0.00743s); valid time: 0.07451s; loss: -84.4663 (±6.63009); valid loss: -110.465 (*)\n",
      "[Epoch 220/350, Step 2420, ETA 9.757s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 9.171s] step time: 0.005464s (±0.001206s); valid time: 0.002259s; loss: -84.5453 (±8.32015); valid loss: -110.189\n",
      "[Epoch 230/350, Step 2530, ETA 8.95s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.486s] step time: 0.006138s (±0.007767s); valid time: 0.07738s; loss: -84.6704 (±8.42014); valid loss: -110.472 (*)\n",
      "[Epoch 240/350, Step 2640, ETA 8.2s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.804s] step time: 0.006218s (±0.007845s); valid time: 0.07812s; loss: -85.4441 (±7.7018); valid loss: -110.865 (*)\n",
      "[Epoch 250/350, Step 2750, ETA 7.446s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 7.09s] step time: 0.005344s (±0.0009648s); valid time: 0.002247s; loss: -84.4047 (±7.77451); valid loss: -110.757\n",
      "[Epoch 260/350, Step 2860, ETA 6.665s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.413s] step time: 0.0062s (±0.008596s); valid time: 0.08585s; loss: -84.7659 (±7.87443); valid loss: -111.279 (*)\n",
      "[Epoch 270/350, Step 2970, ETA 5.918s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.709s] step time: 0.005206s (±0.000543s); valid time: 0.00217s; loss: -85.0674 (±7.89605); valid loss: -110.983\n",
      "[Epoch 280/350, Step 3080, ETA 5.154s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 5.015s] step time: 0.005301s (±0.0009845s); valid time: 0.002167s; loss: -84.8363 (±7.29435); valid loss: -111.228\n",
      "[Epoch 290/350, Step 3190, ETA 4.395s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.328s] step time: 0.005248s (±0.0005454s); valid time: 0.00211s; loss: -85.1046 (±7.46776); valid loss: -110.914\n",
      "[Epoch 300/350, Step 3300, ETA 3.649s] step time: 0.005357s (±0.0009445s); valid time: 0.002235s; loss: -84.9925 (±8.27898); valid loss: -111.076\n",
      "[Epoch 300/350, Step 3300, ETA 3.649s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.974s] step time: 0.005255s (±0.0006736s); valid time: 0.002119s; loss: -85.3235 (±6.78329); valid loss: -111.184\n",
      "[Epoch 310/350, Step 3410, ETA 2.907s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.306s] step time: 0.005353s (±0.0009427s); valid time: 0.00233s; loss: -85.2572 (±8.51822); valid loss: -110.961\n",
      "[Epoch 320/350, Step 3520, ETA 2.173s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.642s] step time: 0.00544s (±0.0009438s); valid time: 0.002144s; loss: -85.1907 (±7.92641); valid loss: -111.127\n",
      "[Epoch 330/350, Step 3630, ETA 1.444s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9863s] step time: 0.006232s (±0.008022s); valid time: 0.08001s; loss: -84.8827 (±8.58853); valid loss: -111.385 (*)\n",
      "[Epoch 340/350, Step 3740, ETA 0.7223s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3279s] step time: 0.005384s (±0.0009881s); valid time: 0.002299s; loss: -85.4567 (±8.02857); valid loss: -110.854\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpigbzfgr0/variables.dat-3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpigbzfgr0/variables.dat-3700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.53,\n",
      "\t(tp, fp, tn, fn)=(248, 1754, 1875, 155),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.62,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.4965277777777778,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on artificialWithAnomaly/art_load_balancer_spikes.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.02s] step time: 0.009604s (±0.03218s); valid time: 0.1622s; loss: 131.926 (±7.87771); valid loss: 139.653 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.38s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.55s] step time: 0.006097s (±0.008083s); valid time: 0.08025s; loss: 112.123 (±4.89865); valid loss: 125.964 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.44s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 26.73s] step time: 0.005341s (±0.0008274s); valid time: 0.00229s; loss: 90.448 (±10.6215); valid loss: 144.977\n",
      "[Epoch 30/350, Step 330, ETA 25.97s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.54s] step time: 0.005349s (±0.0006918s); valid time: 0.002264s; loss: 69.6642 (±6.0558); valid loss: 139.737\n",
      "[Epoch 40/350, Step 440, ETA 23.9s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 22.98s] step time: 0.005336s (±0.000818s); valid time: 0.002195s; loss: 61.3906 (±5.49765); valid loss: 145.232\n",
      "[Epoch 50/350, Step 550, ETA 22.27s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.73s] step time: 0.005326s (±0.0008944s); valid time: 0.002253s; loss: 56.5899 (±5.39052); valid loss: 130.994\n",
      "[Epoch 60/350, Step 660, ETA 21.09s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.72s] step time: 0.005429s (±0.0007445s); valid time: 0.002074s; loss: 53.8245 (±5.4985); valid loss: 200.667\n",
      "[Epoch 70/350, Step 770, ETA 20.04s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.87s] step time: 0.005504s (±0.0008852s); valid time: 0.002245s; loss: 51.4723 (±4.74817); valid loss: 165.71\n",
      "[Epoch 80/350, Step 880, ETA 19.22s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.05s] step time: 0.005504s (±0.001221s); valid time: 0.002074s; loss: 50.3743 (±5.42223); valid loss: 193.176\n",
      "[Epoch 90/350, Step 990, ETA 18.3s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.24s] step time: 0.005364s (±0.001048s); valid time: 0.002182s; loss: 49.1468 (±5.07551); valid loss: 154.332\n",
      "[Epoch 100/350, Step 1100, ETA 17.46s] step time: 0.005337s (±0.000708s); valid time: 0.002224s; loss: 48.2404 (±4.97578); valid loss: 165.711\n",
      "[Epoch 100/350, Step 1100, ETA 17.46s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.7s] step time: 0.005268s (±0.001063s); valid time: 0.002114s; loss: 47.6229 (±4.61079); valid loss: 147.302\n",
      "[Epoch 110/350, Step 1210, ETA 16.62s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.97s] step time: 0.005299s (±0.00106s); valid time: 0.002266s; loss: 47.0559 (±5.42148); valid loss: 167.778\n",
      "[Epoch 120/350, Step 1320, ETA 15.82s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.25s] step time: 0.0052s (±0.0006235s); valid time: 0.002225s; loss: 47.0905 (±5.45495); valid loss: 167.348\n",
      "[Epoch 130/350, Step 1430, ETA 15.04s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.57s] step time: 0.005276s (±0.0007563s); valid time: 0.003273s; loss: 46.6087 (±5.33766); valid loss: 168.524\n",
      "[Epoch 140/350, Step 1540, ETA 14.29s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.88s] step time: 0.005195s (±0.0006625s); valid time: 0.002119s; loss: 46.2568 (±5.25004); valid loss: 186.191\n",
      "[Epoch 150/350, Step 1650, ETA 13.56s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.23s] step time: 0.005395s (±0.00122s); valid time: 0.002304s; loss: 46.5058 (±5.29734); valid loss: 175.558\n",
      "[Epoch 160/350, Step 1760, ETA 12.83s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.57s] step time: 0.005303s (±0.000801s); valid time: 0.00214s; loss: 46.3706 (±5.38254); valid loss: 167.488\n",
      "[Epoch 170/350, Step 1870, ETA 12.13s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.94s] step time: 0.005411s (±0.001051s); valid time: 0.002186s; loss: 46.1952 (±5.57098); valid loss: 166.741\n",
      "[Epoch 180/350, Step 1980, ETA 11.41s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.29s] step time: 0.005224s (±0.0007335s); valid time: 0.00246s; loss: 45.7648 (±5.58416); valid loss: 173.54\n",
      "[Epoch 190/350, Step 2090, ETA 10.73s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.67s] step time: 0.005453s (±0.001093s); valid time: 0.002181s; loss: 46.0993 (±5.69606); valid loss: 192.784\n",
      "[Epoch 200/350, Step 2200, ETA 10.04s] step time: 0.005348s (±0.0007339s); valid time: 0.002248s; loss: 45.9025 (±5.09314); valid loss: 165.921\n",
      "[Epoch 200/350, Step 2200, ETA 10.04s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.414s] step time: 0.005327s (±0.0006622s); valid time: 0.002342s; loss: 45.8369 (±5.39858); valid loss: 143.243\n",
      "[Epoch 210/350, Step 2310, ETA 9.349s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.796s] step time: 0.005388s (±0.001162s); valid time: 0.002135s; loss: 45.931 (±4.76735); valid loss: 193.888\n",
      "[Epoch 220/350, Step 2420, ETA 8.668s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.166s] step time: 0.005157s (±0.0004912s); valid time: 0.002098s; loss: 45.982 (±4.97386); valid loss: 222.635\n",
      "[Epoch 230/350, Step 2530, ETA 7.981s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.564s] step time: 0.005591s (±0.001478s); valid time: 0.002394s; loss: 46.0546 (±5.17994); valid loss: 181.27\n",
      "[Epoch 240/350, Step 2640, ETA 7.319s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.958s] step time: 0.005533s (±0.001052s); valid time: 0.002248s; loss: 45.9651 (±5.05679); valid loss: 191.833\n",
      "[Epoch 250/350, Step 2750, ETA 6.651s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.348s] step time: 0.005409s (±0.0006711s); valid time: 0.002303s; loss: 46.2281 (±5.00114); valid loss: 186.381\n",
      "[Epoch 260/350, Step 2860, ETA 5.987s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.748s] step time: 0.005694s (±0.001415s); valid time: 0.002413s; loss: 45.5454 (±5.85219); valid loss: 190.337\n",
      "[Epoch 270/350, Step 2970, ETA 5.32s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.14s] step time: 0.005421s (±0.0008253s); valid time: 0.0021s; loss: 45.8329 (±4.69431); valid loss: 186.382\n",
      "[Epoch 280/350, Step 3080, ETA 4.653s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.532s] step time: 0.005368s (±0.0007045s); valid time: 0.00207s; loss: 45.917 (±5.86052); valid loss: 204.137\n",
      "[Epoch 290/350, Step 3190, ETA 3.987s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.927s] step time: 0.005515s (±0.001199s); valid time: 0.002046s; loss: 45.6756 (±4.76514); valid loss: 168.938\n",
      "[Epoch 300/350, Step 3300, ETA 3.317s] step time: 0.005223s (±0.0005464s); valid time: 0.002152s; loss: 45.6539 (±4.58162); valid loss: 213.417\n",
      "[Epoch 300/350, Step 3300, ETA 3.317s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.714s] step time: 0.005485s (±0.0009536s); valid time: 0.002179s; loss: 45.8202 (±4.81735); valid loss: 205.327\n",
      "[Epoch 310/350, Step 3410, ETA 2.653s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.11s] step time: 0.005419s (±0.0006717s); valid time: 0.002182s; loss: 45.8615 (±5.73547); valid loss: 173.427\n",
      "[Epoch 320/350, Step 3520, ETA 1.988s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.506s] step time: 0.005434s (±0.001297s); valid time: 0.002252s; loss: 45.8294 (±5.83589); valid loss: 162.183\n",
      "[Epoch 330/350, Step 3630, ETA 1.325s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9025s] step time: 0.005244s (±0.0007481s); valid time: 0.002259s; loss: 45.8646 (±5.76916); valid loss: 211.011\n",
      "[Epoch 340/350, Step 3740, ETA 0.6623s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.301s] step time: 0.005615s (±0.001468s); valid time: 0.002148s; loss: 45.5901 (±5.83603); valid loss: 163.615\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpphipe5rk/variables.dat-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpphipe5rk/variables.dat-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.89,\n",
      "\t(tp, fp, tn, fn)=(193, 231, 3398, 210),\n",
      "\tprecision=0.46,\n",
      "\trecall=0.48,\n",
      "\tf1=0.47,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.10515873015873016,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.56s] step time: 0.009745s (±0.03163s); valid time: 0.1545s; loss: 144.448 (±13.0372); valid loss: 72.1946 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.94s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.76s] step time: 0.00604s (±0.00785s); valid time: 0.0785s; loss: 103.35 (±7.68656); valid loss: 61.0306 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.9s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.84s] step time: 0.00615s (±0.007693s); valid time: 0.07709s; loss: 86.6069 (±3.64317); valid loss: 60.619 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.96s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.09s] step time: 0.006189s (±0.007591s); valid time: 0.07575s; loss: 77.7669 (±3.06502); valid loss: 58.8113 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.21s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.73s] step time: 0.00614s (±0.008646s); valid time: 0.08665s; loss: 71.5169 (±2.40319); valid loss: 56.7011 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.35s] step time: 0.005689s (±0.001173s); valid time: 0.002547s; loss: 69.1992 (±2.16653); valid loss: 58.073\n",
      "[Epoch 60/350, Step 660, ETA 22.53s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.11s] step time: 0.005521s (±0.00103s); valid time: 0.002142s; loss: 67.9646 (±2.35216); valid loss: 58.5398\n",
      "[Epoch 70/350, Step 770, ETA 21.35s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.01s] step time: 0.005463s (±0.0007681s); valid time: 0.002042s; loss: 67.1743 (±2.03659); valid loss: 58.3375\n",
      "[Epoch 80/350, Step 880, ETA 20.19s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.01s] step time: 0.005437s (±0.0009587s); valid time: 0.00241s; loss: 66.804 (±2.26282); valid loss: 61.229\n",
      "[Epoch 90/350, Step 990, ETA 19.14s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.06s] step time: 0.005328s (±0.0007092s); valid time: 0.002153s; loss: 66.4252 (±2.1184); valid loss: 58.9846\n",
      "[Epoch 100/350, Step 1100, ETA 18.19s] step time: 0.005352s (±0.0009044s); valid time: 0.002147s; loss: 66.2174 (±2.09406); valid loss: 60.9072\n",
      "[Epoch 100/350, Step 1100, ETA 18.19s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 17.38s] step time: 0.005391s (±0.000844s); valid time: 0.002127s; loss: 66.1405 (±2.12684); valid loss: 57.7625\n",
      "[Epoch 110/350, Step 1210, ETA 17.29s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.58s] step time: 0.005358s (±0.0006823s); valid time: 0.002225s; loss: 65.939 (±2.23222); valid loss: 59.7712\n",
      "[Epoch 120/350, Step 1320, ETA 16.42s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.84s] step time: 0.005453s (±0.001042s); valid time: 0.002436s; loss: 65.8491 (±2.12567); valid loss: 59.5432\n",
      "[Epoch 130/350, Step 1430, ETA 15.61s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.1s] step time: 0.005325s (±0.000755s); valid time: 0.002007s; loss: 65.7182 (±1.85642); valid loss: 62.7979\n",
      "[Epoch 140/350, Step 1540, ETA 14.8s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.38s] step time: 0.005374s (±0.0009849s); valid time: 0.002116s; loss: 65.7514 (±2.26149); valid loss: 59.6597\n",
      "[Epoch 150/350, Step 1650, ETA 14.03s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.68s] step time: 0.005374s (±0.0007213s); valid time: 0.002069s; loss: 65.7123 (±2.27833); valid loss: 60.3912\n",
      "[Epoch 160/350, Step 1760, ETA 13.28s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.01s] step time: 0.005544s (±0.0009937s); valid time: 0.00211s; loss: 65.715 (±2.2034); valid loss: 61.7453\n",
      "[Epoch 170/350, Step 1870, ETA 12.53s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.34s] step time: 0.005436s (±0.0009334s); valid time: 0.002153s; loss: 65.5598 (±1.97259); valid loss: 62.6003\n",
      "[Epoch 180/350, Step 1980, ETA 11.79s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.65s] step time: 0.005317s (±0.001137s); valid time: 0.00212s; loss: 65.5505 (±2.12631); valid loss: 60.3449\n",
      "[Epoch 190/350, Step 2090, ETA 11.06s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.99s] step time: 0.005435s (±0.0009499s); valid time: 0.002364s; loss: 65.5436 (±2.06068); valid loss: 63.2589\n",
      "[Epoch 200/350, Step 2200, ETA 10.32s] step time: 0.005262s (±0.0005829s); valid time: 0.002113s; loss: 65.5435 (±2.1041); valid loss: 60.9036\n",
      "[Epoch 200/350, Step 2200, ETA 10.33s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.669s] step time: 0.005298s (±0.0007799s); valid time: 0.002008s; loss: 65.5203 (±2.07975); valid loss: 63.0627\n",
      "[Epoch 210/350, Step 2310, ETA 9.603s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.02s] step time: 0.005327s (±0.0007651s); valid time: 0.002301s; loss: 65.5902 (±2.019); valid loss: 60.2517\n",
      "[Epoch 220/350, Step 2420, ETA 8.896s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.383s] step time: 0.00546s (±0.001003s); valid time: 0.002155s; loss: 65.5991 (±2.12678); valid loss: 58.6184\n",
      "[Epoch 230/350, Step 2530, ETA 8.191s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.755s] step time: 0.005511s (±0.001131s); valid time: 0.002185s; loss: 65.5828 (±2.39414); valid loss: 61.1252\n",
      "[Epoch 240/350, Step 2640, ETA 7.499s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.126s] step time: 0.005473s (±0.0007355s); valid time: 0.002321s; loss: 65.4933 (±1.86721); valid loss: 58.1585\n",
      "[Epoch 250/350, Step 2750, ETA 6.814s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.499s] step time: 0.005477s (±0.001046s); valid time: 0.002228s; loss: 65.5135 (±2.11976); valid loss: 57.6802\n",
      "[Epoch 260/350, Step 2860, ETA 6.119s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.869s] step time: 0.005339s (±0.0008292s); valid time: 0.002046s; loss: 65.5622 (±1.94733); valid loss: 61.1294\n",
      "[Epoch 270/350, Step 2970, ETA 5.432s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.246s] step time: 0.005467s (±0.0008326s); valid time: 0.002215s; loss: 65.5961 (±2.01012); valid loss: 61.5785\n",
      "[Epoch 280/350, Step 3080, ETA 4.745s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.625s] step time: 0.00544s (±0.001201s); valid time: 0.002039s; loss: 65.5284 (±2.07612); valid loss: 61.252\n",
      "[Epoch 290/350, Step 3190, ETA 4.065s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.003s] step time: 0.005415s (±0.0008776s); valid time: 0.002023s; loss: 65.5635 (±2.13481); valid loss: 61.055\n",
      "[Epoch 300/350, Step 3300, ETA 3.382s] step time: 0.005319s (±0.00102s); valid time: 0.002078s; loss: 65.4564 (±2.19745); valid loss: 62.6343\n",
      "[Epoch 300/350, Step 3300, ETA 3.382s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.764s] step time: 0.005436s (±0.001067s); valid time: 0.002228s; loss: 65.5167 (±1.91508); valid loss: 58.5881\n",
      "[Epoch 310/350, Step 3410, ETA 2.702s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.15s] step time: 0.005647s (±0.001143s); valid time: 0.002071s; loss: 65.5306 (±1.95733); valid loss: 62.3243\n",
      "[Epoch 320/350, Step 3520, ETA 2.026s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.534s] step time: 0.005365s (±0.0007538s); valid time: 0.002071s; loss: 65.5925 (±2.04049); valid loss: 56.9577\n",
      "[Epoch 330/350, Step 3630, ETA 1.349s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9194s] step time: 0.005367s (±0.000804s); valid time: 0.002124s; loss: 65.4575 (±2.32065); valid loss: 60.8491\n",
      "[Epoch 340/350, Step 3740, ETA 0.6737s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3061s] step time: 0.005366s (±0.0009193s); valid time: 0.002187s; loss: 65.505 (±2.1946); valid loss: 61.3838\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5vyuu1ig/variables.dat-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5vyuu1ig/variables.dat-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.88,\n",
      "\t(tp, fp, tn, fn)=(73, 146, 3484, 329),\n",
      "\tprecision=0.33,\n",
      "\trecall=0.18,\n",
      "\tf1=0.24,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.05431547619047619,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.02s] step time: 0.009589s (±0.03135s); valid time: 0.1592s; loss: 129.147 (±7.32169); valid loss: 141.011 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.57s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.51s] step time: 0.006089s (±0.007493s); valid time: 0.07473s; loss: 115.401 (±3.87942); valid loss: 132.618 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.48s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.61s] step time: 0.006102s (±0.007556s); valid time: 0.07598s; loss: 110.217 (±3.20814); valid loss: 129.798 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.86s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.01s] step time: 0.00629s (±0.008379s); valid time: 0.08375s; loss: 107.682 (±2.8466); valid loss: 127.532 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.14s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.66s] step time: 0.006146s (±0.007608s); valid time: 0.07584s; loss: 105.891 (±3.14187); valid loss: 126.056 (*)\n",
      "[Epoch 50/350, Step 550, ETA 23.89s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.6s] step time: 0.006249s (±0.007924s); valid time: 0.07914s; loss: 105.119 (±3.33593); valid loss: 125.649 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.75s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.69s] step time: 0.006336s (±0.009079s); valid time: 0.09062s; loss: 104.738 (±2.87501); valid loss: 125.352 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.8s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.77s] step time: 0.006155s (±0.008298s); valid time: 0.08317s; loss: 104.473 (±2.71736); valid loss: 125.053 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.85s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.64s] step time: 0.005344s (±0.0008788s); valid time: 0.002124s; loss: 104.284 (±2.94365); valid loss: 125.178\n",
      "[Epoch 90/350, Step 990, ETA 19.75s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.87s] step time: 0.006225s (±0.007735s); valid time: 0.07714s; loss: 103.965 (±2.94497); valid loss: 124.995 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.11s] step time: 0.006218s (±0.007807s); valid time: 0.07771s; loss: 104.101 (±3.13684); valid loss: 124.951 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.11s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.21s] step time: 0.005445s (±0.001103s); valid time: 0.002179s; loss: 103.942 (±3.00086); valid loss: 125.01\n",
      "[Epoch 110/350, Step 1210, ETA 18.14s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.47s] step time: 0.006101s (±0.007153s); valid time: 0.07156s; loss: 103.859 (±2.5725); valid loss: 124.885 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.32s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.78s] step time: 0.006333s (±0.007597s); valid time: 0.07608s; loss: 103.857 (±3.00446); valid loss: 124.734 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.54s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.08s] step time: 0.006235s (±0.007705s); valid time: 0.07682s; loss: 103.786 (±2.68961); valid loss: 124.685 (*)\n",
      "[Epoch 140/350, Step 1540, ETA 15.73s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.24s] step time: 0.005202s (±0.0007579s); valid time: 0.002249s; loss: 103.719 (±3.00263); valid loss: 124.71\n",
      "[Epoch 150/350, Step 1650, ETA 14.85s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.46s] step time: 0.005463s (±0.001083s); valid time: 0.002209s; loss: 103.878 (±2.95244); valid loss: 124.739\n",
      "[Epoch 160/350, Step 1760, ETA 13.98s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.7s] step time: 0.005446s (±0.0007779s); valid time: 0.002275s; loss: 103.841 (±2.9193); valid loss: 124.723\n",
      "[Epoch 170/350, Step 1870, ETA 13.17s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.95s] step time: 0.005427s (±0.001052s); valid time: 0.002084s; loss: 103.735 (±2.70705); valid loss: 124.814\n",
      "[Epoch 180/350, Step 1980, ETA 12.35s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.28s] step time: 0.006111s (±0.007616s); valid time: 0.07641s; loss: 103.658 (±2.72731); valid loss: 124.544 (*)\n",
      "[Epoch 190/350, Step 2090, ETA 11.62s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.61s] step time: 0.00597s (±0.007627s); valid time: 0.0765s; loss: 103.801 (±3.09998); valid loss: 124.507 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.88s] step time: 0.005304s (±0.000912s); valid time: 0.002078s; loss: 103.591 (±2.97568); valid loss: 124.727\n",
      "[Epoch 200/350, Step 2200, ETA 10.88s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.17s] step time: 0.005268s (±0.0006365s); valid time: 0.002215s; loss: 103.825 (±2.95595); valid loss: 124.762\n",
      "[Epoch 210/350, Step 2310, ETA 10.1s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.467s] step time: 0.005302s (±0.0009097s); valid time: 0.00222s; loss: 103.638 (±2.71743); valid loss: 124.825\n",
      "[Epoch 220/350, Step 2420, ETA 9.325s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.784s] step time: 0.005451s (±0.001298s); valid time: 0.002131s; loss: 103.699 (±2.94654); valid loss: 124.756\n",
      "[Epoch 230/350, Step 2530, ETA 8.575s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.095s] step time: 0.005224s (±0.0006497s); valid time: 0.002238s; loss: 103.703 (±2.82093); valid loss: 124.787\n",
      "[Epoch 240/350, Step 2640, ETA 7.823s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.423s] step time: 0.005419s (±0.001254s); valid time: 0.002225s; loss: 103.878 (±3.15976); valid loss: 124.719\n",
      "[Epoch 250/350, Step 2750, ETA 7.09s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.756s] step time: 0.005374s (±0.000775s); valid time: 0.002144s; loss: 103.783 (±3.26534); valid loss: 124.631\n",
      "[Epoch 260/350, Step 2860, ETA 6.363s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.102s] step time: 0.005586s (±0.001362s); valid time: 0.002155s; loss: 103.735 (±2.67979); valid loss: 124.802\n",
      "[Epoch 270/350, Step 2970, ETA 5.638s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.441s] step time: 0.005262s (±0.0005959s); valid time: 0.002235s; loss: 103.727 (±3.3003); valid loss: 124.748\n",
      "[Epoch 280/350, Step 3080, ETA 4.916s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.805s] step time: 0.005989s (±0.007817s); valid time: 0.07781s; loss: 103.861 (±3.29938); valid loss: 124.409 (*)\n",
      "[Epoch 290/350, Step 3190, ETA 4.217s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.153s] step time: 0.005365s (±0.001077s); valid time: 0.002252s; loss: 103.621 (±2.84771); valid loss: 124.555\n",
      "[Epoch 300/350, Step 3300, ETA 3.503s] step time: 0.005252s (±0.0005414s); valid time: 0.002058s; loss: 103.934 (±3.07079); valid loss: 124.62\n",
      "[Epoch 300/350, Step 3300, ETA 3.503s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.859s] step time: 0.005321s (±0.0008367s); valid time: 0.002187s; loss: 103.648 (±2.73766); valid loss: 124.67\n",
      "[Epoch 310/350, Step 3410, ETA 2.795s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.219s] step time: 0.005343s (±0.00084s); valid time: 0.002061s; loss: 103.719 (±2.74614); valid loss: 124.703\n",
      "[Epoch 320/350, Step 3520, ETA 2.091s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.581s] step time: 0.00532s (±0.00092s); valid time: 0.002185s; loss: 103.689 (±2.8989); valid loss: 124.7\n",
      "[Epoch 330/350, Step 3630, ETA 1.39s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9467s] step time: 0.005329s (±0.0009885s); valid time: 0.002169s; loss: 103.529 (±3.37554); valid loss: 124.676\n",
      "[Epoch 340/350, Step 3740, ETA 0.6935s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3151s] step time: 0.005461s (±0.001246s); valid time: 0.00213s; loss: 103.714 (±3.26255); valid loss: 124.597\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpq0sh1r8e/variables.dat-3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpq0sh1r8e/variables.dat-3100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.48,\n",
      "\t(tp, fp, tn, fn)=(254, 1945, 1685, 148),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.63,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.5453869047619048,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.64s] step time: 0.009765s (±0.03205s); valid time: 0.1563s; loss: 113.389 (±9.82576); valid loss: 221.3 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.91s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 29.3s] step time: 0.005232s (±0.0005969s); valid time: 0.002229s; loss: 100.909 (±2.66714); valid loss: 223.804\n",
      "[Epoch 20/350, Step 220, ETA 28.69s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 26.01s] step time: 0.005412s (±0.00114s); valid time: 0.002193s; loss: 99.231 (±2.66194); valid loss: 229.427\n",
      "[Epoch 30/350, Step 330, ETA 25.27s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.12s] step time: 0.005467s (±0.00121s); valid time: 0.002198s; loss: 94.7492 (±2.52258); valid loss: 241.306\n",
      "[Epoch 40/350, Step 440, ETA 23.43s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 22.61s] step time: 0.005285s (±0.0006454s); valid time: 0.002124s; loss: 93.248 (±2.30719); valid loss: 244.838\n",
      "[Epoch 50/350, Step 550, ETA 21.96s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.45s] step time: 0.005353s (±0.0009376s); valid time: 0.002113s; loss: 92.46 (±2.38048); valid loss: 247.034\n",
      "[Epoch 60/350, Step 660, ETA 20.79s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.45s] step time: 0.005332s (±0.000905s); valid time: 0.002279s; loss: 91.572 (±2.41236); valid loss: 249.837\n",
      "[Epoch 70/350, Step 770, ETA 19.86s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.58s] step time: 0.00539s (±0.001113s); valid time: 0.002246s; loss: 91.1827 (±2.28356); valid loss: 253.119\n",
      "[Epoch 80/350, Step 880, ETA 18.93s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 18.78s] step time: 0.005444s (±0.001294s); valid time: 0.006639s; loss: 90.8973 (±2.23602); valid loss: 255.82\n",
      "[Epoch 90/350, Step 990, ETA 18.06s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 17.99s] step time: 0.005289s (±0.0007045s); valid time: 0.002302s; loss: 90.703 (±2.38445); valid loss: 254.702\n",
      "[Epoch 100/350, Step 1100, ETA 17.25s] step time: 0.005376s (±0.00106s); valid time: 0.002018s; loss: 90.4629 (±2.48246); valid loss: 259.261\n",
      "[Epoch 100/350, Step 1100, ETA 17.25s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.54s] step time: 0.005381s (±0.000711s); valid time: 0.002147s; loss: 90.3426 (±2.32732); valid loss: 259.685\n",
      "[Epoch 110/350, Step 1210, ETA 16.46s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.83s] step time: 0.005286s (±0.0009014s); valid time: 0.002142s; loss: 90.3731 (±2.62821); valid loss: 257.623\n",
      "[Epoch 120/350, Step 1320, ETA 15.68s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.16s] step time: 0.005407s (±0.0009579s); valid time: 0.002439s; loss: 90.1472 (±2.27653); valid loss: 260.964\n",
      "[Epoch 130/350, Step 1430, ETA 14.96s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.47s] step time: 0.005243s (±0.0008602s); valid time: 0.002136s; loss: 90.1579 (±2.47262); valid loss: 258.522\n",
      "[Epoch 140/350, Step 1540, ETA 14.2s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.82s] step time: 0.005322s (±0.0008846s); valid time: 0.002079s; loss: 90.0911 (±2.43315); valid loss: 258.595\n",
      "[Epoch 150/350, Step 1650, ETA 13.48s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.16s] step time: 0.005312s (±0.0009163s); valid time: 0.002151s; loss: 90.0116 (±2.40354); valid loss: 262.158\n",
      "[Epoch 160/350, Step 1760, ETA 12.77s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.53s] step time: 0.00547s (±0.001352s); valid time: 0.002456s; loss: 90.1378 (±2.45508); valid loss: 260.472\n",
      "[Epoch 170/350, Step 1870, ETA 12.08s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.88s] step time: 0.005241s (±0.000549s); valid time: 0.002131s; loss: 89.959 (±2.39806); valid loss: 260.198\n",
      "[Epoch 180/350, Step 1980, ETA 11.38s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.26s] step time: 0.005398s (±0.001065s); valid time: 0.002214s; loss: 90.0888 (±2.70713); valid loss: 260.714\n",
      "[Epoch 190/350, Step 2090, ETA 10.68s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.62s] step time: 0.005213s (±0.0006919s); valid time: 0.002223s; loss: 90.0186 (±2.58567); valid loss: 262.021\n",
      "[Epoch 200/350, Step 2200, ETA 9.988s] step time: 0.005295s (±0.0008416s); valid time: 0.002098s; loss: 89.9731 (±2.63631); valid loss: 261.456\n",
      "[Epoch 200/350, Step 2200, ETA 9.988s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.371s] step time: 0.005368s (±0.001363s); valid time: 0.002707s; loss: 90.0054 (±2.21571); valid loss: 262.075\n",
      "[Epoch 210/350, Step 2310, ETA 9.309s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.751s] step time: 0.005278s (±0.0005376s); valid time: 0.002365s; loss: 90.0905 (±2.23227); valid loss: 263.495\n",
      "[Epoch 220/350, Step 2420, ETA 8.624s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.152s] step time: 0.005606s (±0.001103s); valid time: 0.003336s; loss: 90.002 (±2.59236); valid loss: 260.689\n",
      "[Epoch 230/350, Step 2530, ETA 7.962s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.531s] step time: 0.005191s (±0.0005707s); valid time: 0.002262s; loss: 89.9502 (±2.33676); valid loss: 259.196\n",
      "[Epoch 240/350, Step 2640, ETA 7.285s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.921s] step time: 0.00532s (±0.0009162s); valid time: 0.002163s; loss: 89.9521 (±2.3236); valid loss: 260.67\n",
      "[Epoch 250/350, Step 2750, ETA 6.617s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.313s] step time: 0.005346s (±0.0007109s); valid time: 0.00214s; loss: 89.968 (±2.24559); valid loss: 261.376\n",
      "[Epoch 260/350, Step 2860, ETA 5.951s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.709s] step time: 0.005419s (±0.001062s); valid time: 0.002237s; loss: 89.9528 (±2.36358); valid loss: 261.062\n",
      "[Epoch 270/350, Step 2970, ETA 5.284s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.107s] step time: 0.00545s (±0.001014s); valid time: 0.002234s; loss: 90.0304 (±2.24392); valid loss: 262.822\n",
      "[Epoch 280/350, Step 3080, ETA 4.62s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.499s] step time: 0.005212s (±0.0005494s); valid time: 0.002193s; loss: 90.0664 (±2.28054); valid loss: 262.548\n",
      "[Epoch 290/350, Step 3190, ETA 3.96s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.9s] step time: 0.005505s (±0.001513s); valid time: 0.002092s; loss: 90.0207 (±2.66818); valid loss: 262.81\n",
      "[Epoch 300/350, Step 3300, ETA 3.295s] step time: 0.005201s (±0.0006769s); valid time: 0.002048s; loss: 90.0254 (±2.02393); valid loss: 260.339\n",
      "[Epoch 300/350, Step 3300, ETA 3.295s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.695s] step time: 0.005399s (±0.001051s); valid time: 0.002385s; loss: 89.9515 (±2.41864); valid loss: 258.896\n",
      "[Epoch 310/350, Step 3410, ETA 2.634s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.095s] step time: 0.005344s (±0.000952s); valid time: 0.002341s; loss: 89.9271 (±2.36334); valid loss: 261.86\n",
      "[Epoch 320/350, Step 3520, ETA 1.975s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.495s] step time: 0.005315s (±0.0008832s); valid time: 0.002357s; loss: 89.9696 (±2.25542); valid loss: 262.244\n",
      "[Epoch 330/350, Step 3630, ETA 1.315s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.8967s] step time: 0.005379s (±0.001429s); valid time: 0.002318s; loss: 90.3863 (±4.35604); valid loss: 262.237\n",
      "[Epoch 340/350, Step 3740, ETA 0.6571s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.2985s] step time: 0.005212s (±0.0006944s); valid time: 0.002357s; loss: 89.9097 (±2.4118); valid loss: 261.18\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp74yyvkqj/variables.dat-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp74yyvkqj/variables.dat-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.12,\n",
      "\t(tp, fp, tn, fn)=(402, 3531, 99, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9754464285714286,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.74s] step time: 0.009798s (±0.03223s); valid time: 0.1562s; loss: 124.715 (±13.0537); valid loss: 121.194 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.13s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.91s] step time: 0.006107s (±0.008165s); valid time: 0.08182s; loss: 83.1683 (±18.629); valid loss: 84.5921 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.78s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 26.71s] step time: 0.0051s (±0.000545s); valid time: 0.002108s; loss: 44.7128 (±10.2706); valid loss: 88.9044\n",
      "[Epoch 30/350, Step 330, ETA 25.93s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.73s] step time: 0.005603s (±0.00149s); valid time: 0.002277s; loss: 35.3297 (±12.7607); valid loss: 115.293\n",
      "[Epoch 40/350, Step 440, ETA 24s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 23.07s] step time: 0.005272s (±0.0006984s); valid time: 0.002241s; loss: 31.5538 (±12.6429); valid loss: 134.06\n",
      "[Epoch 50/350, Step 550, ETA 22.52s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.93s] step time: 0.005555s (±0.00133s); valid time: 0.002392s; loss: 28.4819 (±12.6771); valid loss: 143.79\n",
      "[Epoch 60/350, Step 660, ETA 21.24s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.83s] step time: 0.005306s (±0.0007486s); valid time: 0.002045s; loss: 26.8806 (±10.5871); valid loss: 161.764\n",
      "[Epoch 70/350, Step 770, ETA 20.17s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.91s] step time: 0.005406s (±0.0008791s); valid time: 0.002112s; loss: 26.231 (±12.5714); valid loss: 213.902\n",
      "[Epoch 80/350, Step 880, ETA 19.2s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.05s] step time: 0.005419s (±0.0009292s); valid time: 0.002209s; loss: 24.7137 (±12.2443); valid loss: 227.858\n",
      "[Epoch 90/350, Step 990, ETA 18.28s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.2s] step time: 0.005223s (±0.0005475s); valid time: 0.002097s; loss: 23.0726 (±11.4167); valid loss: 251.047\n",
      "[Epoch 100/350, Step 1100, ETA 17.44s] step time: 0.005393s (±0.001015s); valid time: 0.002237s; loss: 23.0418 (±13.1879); valid loss: 267.559\n",
      "[Epoch 100/350, Step 1100, ETA 17.44s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.71s] step time: 0.005398s (±0.001008s); valid time: 0.002268s; loss: 22.8011 (±12.574); valid loss: 269.679\n",
      "[Epoch 110/350, Step 1210, ETA 16.62s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16s] step time: 0.005421s (±0.001279s); valid time: 0.00216s; loss: 21.6491 (±12.4038); valid loss: 273.308\n",
      "[Epoch 120/350, Step 1320, ETA 15.87s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.32s] step time: 0.005443s (±0.0006811s); valid time: 0.00222s; loss: 22.0623 (±13.3198); valid loss: 284.061\n",
      "[Epoch 130/350, Step 1430, ETA 15.11s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.62s] step time: 0.005301s (±0.0009983s); valid time: 0.00232s; loss: 21.2635 (±12.617); valid loss: 281.341\n",
      "[Epoch 140/350, Step 1540, ETA 14.35s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.95s] step time: 0.005325s (±0.001028s); valid time: 0.00233s; loss: 22.1544 (±12.8683); valid loss: 299.757\n",
      "[Epoch 150/350, Step 1650, ETA 13.6s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.26s] step time: 0.005215s (±0.0007571s); valid time: 0.002209s; loss: 20.4022 (±14.4596); valid loss: 310.205\n",
      "[Epoch 160/350, Step 1760, ETA 12.88s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.62s] step time: 0.005443s (±0.001155s); valid time: 0.002112s; loss: 21.8217 (±12.4307); valid loss: 317.974\n",
      "[Epoch 170/350, Step 1870, ETA 12.17s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.97s] step time: 0.005343s (±0.0008288s); valid time: 0.002032s; loss: 20.6428 (±12.3016); valid loss: 318.282\n",
      "[Epoch 180/350, Step 1980, ETA 11.46s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.33s] step time: 0.005213s (±0.0007229s); valid time: 0.002145s; loss: 21.3879 (±11.0294); valid loss: 321.814\n",
      "[Epoch 190/350, Step 2090, ETA 10.74s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.68s] step time: 0.005208s (±0.0006763s); valid time: 0.00236s; loss: 20.9093 (±12.8372); valid loss: 323.134\n",
      "[Epoch 200/350, Step 2200, ETA 10.05s] step time: 0.005371s (±0.001332s); valid time: 0.002103s; loss: 20.7445 (±14.0343); valid loss: 321.403\n",
      "[Epoch 200/350, Step 2200, ETA 10.05s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.433s] step time: 0.005389s (±0.0009537s); valid time: 0.002148s; loss: 21.4209 (±13.4369); valid loss: 323.894\n",
      "[Epoch 210/350, Step 2310, ETA 9.368s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.808s] step time: 0.005313s (±0.000804s); valid time: 0.002202s; loss: 21.1673 (±12.2806); valid loss: 324.749\n",
      "[Epoch 220/350, Step 2420, ETA 8.679s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.186s] step time: 0.005295s (±0.001122s); valid time: 0.002203s; loss: 20.7951 (±11.5146); valid loss: 322.537\n",
      "[Epoch 230/350, Step 2530, ETA 7.999s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.566s] step time: 0.005279s (±0.0006643s); valid time: 0.002122s; loss: 21.636 (±12.6851); valid loss: 326.341\n",
      "[Epoch 240/350, Step 2640, ETA 7.33s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.97s] step time: 0.00572s (±0.001197s); valid time: 0.002113s; loss: 20.3497 (±13.253); valid loss: 324.793\n",
      "[Epoch 250/350, Step 2750, ETA 6.659s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.352s] step time: 0.005219s (±0.0007997s); valid time: 0.002234s; loss: 21.197 (±12.9355); valid loss: 329.064\n",
      "[Epoch 260/350, Step 2860, ETA 5.983s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.743s] step time: 0.005419s (±0.001017s); valid time: 0.002266s; loss: 21.6223 (±12.6398); valid loss: 327.667\n",
      "[Epoch 270/350, Step 2970, ETA 5.321s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.138s] step time: 0.005521s (±0.00112s); valid time: 0.002164s; loss: 20.4235 (±11.1634); valid loss: 327.607\n",
      "[Epoch 280/350, Step 3080, ETA 4.651s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.532s] step time: 0.005489s (±0.0007126s); valid time: 0.002268s; loss: 20.5175 (±11.1274); valid loss: 332.151\n",
      "[Epoch 290/350, Step 3190, ETA 3.986s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.926s] step time: 0.005435s (±0.0008987s); valid time: 0.002284s; loss: 21.11 (±12.3433); valid loss: 328.701\n",
      "[Epoch 300/350, Step 3300, ETA 3.316s] step time: 0.005205s (±0.0005164s); valid time: 0.002248s; loss: 21.2601 (±12.2557); valid loss: 328.146\n",
      "[Epoch 300/350, Step 3300, ETA 3.316s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.711s] step time: 0.005299s (±0.0008998s); valid time: 0.002103s; loss: 20.6865 (±12.4732); valid loss: 327.697\n",
      "[Epoch 310/350, Step 3410, ETA 2.65s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.108s] step time: 0.005468s (±0.0009556s); valid time: 0.002265s; loss: 20.8233 (±13.08); valid loss: 326.506\n",
      "[Epoch 320/350, Step 3520, ETA 1.987s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.504s] step time: 0.005308s (±0.0007245s); valid time: 0.00226s; loss: 20.8911 (±13.5206); valid loss: 326.447\n",
      "[Epoch 330/350, Step 3630, ETA 1.324s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9023s] step time: 0.005447s (±0.0008687s); valid time: 0.002131s; loss: 20.6515 (±12.4047); valid loss: 328.039\n",
      "[Epoch 340/350, Step 3740, ETA 0.6613s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3004s] step time: 0.005251s (±0.0006075s); valid time: 0.002087s; loss: 20.812 (±13.2125); valid loss: 328.854\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprd00j597/variables.dat-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmprd00j597/variables.dat-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.76,\n",
      "\t(tp, fp, tn, fn)=(186, 758, 2871, 217),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.46,\n",
      "\tf1=0.28,\n",
      "\troc_auc=0.63,\n",
      "\ty_pred%=0.23412698412698413,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.74s] step time: 0.009786s (±0.03308s); valid time: 0.1528s; loss: 122.902 (±41.7946); valid loss: 65.0138 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.02s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.83s] step time: 0.006055s (±0.007419s); valid time: 0.07445s; loss: 48.2978 (±6.22041); valid loss: 52.5916 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.73s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.94s] step time: 0.006214s (±0.007626s); valid time: 0.07624s; loss: 37.7704 (±3.3839); valid loss: 47.5343 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.11s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.29s] step time: 0.006369s (±0.007601s); valid time: 0.07598s; loss: 33.4329 (±2.3524); valid loss: 46.2369 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.52s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.97s] step time: 0.006282s (±0.008542s); valid time: 0.08449s; loss: 32.1466 (±5.06879); valid loss: 45.0744 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24.09s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.4s] step time: 0.005418s (±0.001182s); valid time: 0.002165s; loss: 30.8839 (±2.77308); valid loss: 45.0893\n",
      "[Epoch 60/350, Step 660, ETA 22.57s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.43s] step time: 0.006145s (±0.008047s); valid time: 0.08019s; loss: 30.5789 (±2.47917); valid loss: 44.551 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.56s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.54s] step time: 0.006143s (±0.008687s); valid time: 0.08654s; loss: 35.4799 (±52.7523); valid loss: 44.155 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.6s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.69s] step time: 0.00614s (±0.008773s); valid time: 0.08623s; loss: 30.6906 (±6.62548); valid loss: 43.7356 (*)\n",
      "[Epoch 90/350, Step 990, ETA 19.73s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.68s] step time: 0.00541s (±0.001464s); valid time: 0.003335s; loss: 30.1082 (±3.63736); valid loss: 43.8493\n",
      "[Epoch 100/350, Step 1100, ETA 18.91s] step time: 0.006097s (±0.007718s); valid time: 0.07641s; loss: 30.2951 (±5.80061); valid loss: 43.6964 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 18.91s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.17s] step time: 0.006081s (±0.007669s); valid time: 0.07694s; loss: 29.4691 (±2.65086); valid loss: 43.5962 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.12s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.47s] step time: 0.006295s (±0.007741s); valid time: 0.07736s; loss: 29.5647 (±2.27387); valid loss: 43.4546 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.29s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.77s] step time: 0.006242s (±0.007664s); valid time: 0.07673s; loss: 44.3876 (±142.714); valid loss: 43.3801 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.51s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.03s] step time: 0.006031s (±0.007617s); valid time: 0.07614s; loss: 31.3961 (±20.4781); valid loss: 43.2841 (*)\n",
      "[Epoch 140/350, Step 1540, ETA 15.71s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.22s] step time: 0.005372s (±0.0009738s); valid time: 0.002243s; loss: 30.0356 (±7.81133); valid loss: 43.4177\n",
      "[Epoch 150/350, Step 1650, ETA 14.81s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.53s] step time: 0.006186s (±0.008356s); valid time: 0.083s; loss: 30.564 (±9.9808); valid loss: 43.1115 (*)\n",
      "[Epoch 160/350, Step 1760, ETA 14.06s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.76s] step time: 0.005367s (±0.001209s); valid time: 0.002172s; loss: 30.1825 (±10.0382); valid loss: 43.157\n",
      "[Epoch 170/350, Step 1870, ETA 13.24s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 13.1s] step time: 0.006304s (±0.008134s); valid time: 0.08133s; loss: 29.3206 (±2.58423); valid loss: 42.9795 (*)\n",
      "[Epoch 180/350, Step 1980, ETA 12.49s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.34s] step time: 0.0053s (±0.0006585s); valid time: 0.002109s; loss: 30.6177 (±11.5903); valid loss: 43.2073\n",
      "[Epoch 190/350, Step 2090, ETA 11.69s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.61s] step time: 0.005454s (±0.000969s); valid time: 0.002382s; loss: 30.8663 (±14.2948); valid loss: 43.1829\n",
      "[Epoch 200/350, Step 2200, ETA 10.9s] step time: 0.005439s (±0.0009132s); valid time: 0.001989s; loss: 48.9774 (±196.432); valid loss: 43.1982\n",
      "[Epoch 200/350, Step 2200, ETA 10.9s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.19s] step time: 0.005306s (±0.0005491s); valid time: 0.002472s; loss: 31.1642 (±17.5828); valid loss: 43.156\n",
      "[Epoch 210/350, Step 2310, ETA 10.11s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.49s] step time: 0.005442s (±0.001092s); valid time: 0.00224s; loss: 29.3748 (±2.38239); valid loss: 43.2376\n",
      "[Epoch 220/350, Step 2420, ETA 9.348s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.795s] step time: 0.005265s (±0.0005101s); valid time: 0.002646s; loss: 29.1279 (±2.6564); valid loss: 43.4289\n",
      "[Epoch 230/350, Step 2530, ETA 8.591s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.12s] step time: 0.005526s (±0.0009815s); valid time: 0.002116s; loss: 29.1713 (±2.60048); valid loss: 43.3063\n",
      "[Epoch 240/350, Step 2640, ETA 7.846s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.45s] step time: 0.005495s (±0.001155s); valid time: 0.002007s; loss: 30.6332 (±10.0602); valid loss: 43.3514\n",
      "[Epoch 250/350, Step 2750, ETA 7.112s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.779s] step time: 0.005374s (±0.0007029s); valid time: 0.002077s; loss: 29.8489 (±4.91679); valid loss: 43.3223\n",
      "[Epoch 260/350, Step 2860, ETA 6.378s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.117s] step time: 0.005411s (±0.0009475s); valid time: 0.002039s; loss: 29.3627 (±2.81581); valid loss: 43.13\n",
      "[Epoch 270/350, Step 2970, ETA 5.654s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.455s] step time: 0.005303s (±0.0008135s); valid time: 0.002038s; loss: 36.9959 (±74.9931); valid loss: 43.1367\n",
      "[Epoch 280/350, Step 3080, ETA 4.931s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.8s] step time: 0.005395s (±0.0009176s); valid time: 0.001993s; loss: 29.1925 (±2.24539); valid loss: 43.04\n",
      "[Epoch 290/350, Step 3190, ETA 4.211s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.147s] step time: 0.005243s (±0.0006815s); valid time: 0.002143s; loss: 29.1874 (±2.30152); valid loss: 43.4459\n",
      "[Epoch 300/350, Step 3300, ETA 3.501s] step time: 0.005369s (±0.001077s); valid time: 0.002167s; loss: 29.5971 (±3.59122); valid loss: 43.4204\n",
      "[Epoch 300/350, Step 3300, ETA 3.501s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.855s] step time: 0.005149s (±0.0005588s); valid time: 0.0021s; loss: 29.2686 (±2.31882); valid loss: 43.406\n",
      "[Epoch 310/350, Step 3410, ETA 2.79s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.216s] step time: 0.005431s (±0.0007879s); valid time: 0.002182s; loss: 526.485 (±4663.07); valid loss: 43.0929\n",
      "[Epoch 320/350, Step 3520, ETA 2.09s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.582s] step time: 0.005588s (±0.0009364s); valid time: 0.00202s; loss: 30.7191 (±12.803); valid loss: 43.262\n",
      "[Epoch 330/350, Step 3630, ETA 1.391s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9498s] step time: 0.005989s (±0.007644s); valid time: 0.07669s; loss: 29.0803 (±2.33372); valid loss: 42.9591 (*)\n",
      "[Epoch 340/350, Step 3740, ETA 0.6962s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.316s] step time: 0.005405s (±0.001024s); valid time: 0.00198s; loss: 31.867 (±26.451); valid loss: 43.302\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp28fqyjqi/variables.dat-3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp28fqyjqi/variables.dat-3700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.93,\n",
      "\t(tp, fp, tn, fn)=(113, 61, 3628, 230),\n",
      "\tprecision=0.65,\n",
      "\trecall=0.33,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.043154761904761904,\n",
      "\ty_label%=0.08506944444444445,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.2s] step time: 0.009644s (±0.03187s); valid time: 0.1565s; loss: 50.1874 (±30.9051); valid loss: 2252.41 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.63s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 29.2s] step time: 0.005318s (±0.0008921s); valid time: 0.002231s; loss: -35.8002 (±8.69858); valid loss: 21530.2\n",
      "[Epoch 20/350, Step 220, ETA 28.22s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 25.91s] step time: 0.0054s (±0.0009627s); valid time: 0.002323s; loss: -46.7073 (±3.71788); valid loss: 3.59181e+07\n",
      "[Epoch 30/350, Step 330, ETA 25.13s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 24.04s] step time: 0.005468s (±0.001213s); valid time: 0.002018s; loss: -51.8136 (±3.4953); valid loss: 1.8354e+09\n",
      "[Epoch 40/350, Step 440, ETA 23.46s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 22.63s] step time: 0.005411s (±0.0008591s); valid time: 0.002142s; loss: -55.6372 (±3.20639); valid loss: 2.80067e+09\n",
      "[Epoch 50/350, Step 550, ETA 21.97s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.49s] step time: 0.005346s (±0.0007695s); valid time: 0.002125s; loss: -57.0132 (±2.85905); valid loss: 3.37777e+09\n",
      "[Epoch 60/350, Step 660, ETA 20.85s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.47s] step time: 0.005305s (±0.0007249s); valid time: 0.002107s; loss: -58.0327 (±3.09933); valid loss: 3.0685e+09\n",
      "[Epoch 70/350, Step 770, ETA 19.83s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.64s] step time: 0.005514s (±0.001078s); valid time: 0.002282s; loss: -58.8921 (±2.9251); valid loss: 2.83455e+09\n",
      "[Epoch 80/350, Step 880, ETA 18.92s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 18.76s] step time: 0.005211s (±0.0006014s); valid time: 0.002084s; loss: -59.2114 (±3.12532); valid loss: 2.87633e+09\n",
      "[Epoch 90/350, Step 990, ETA 18.05s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 17.98s] step time: 0.005353s (±0.0008309s); valid time: 0.002263s; loss: -59.4783 (±3.21215); valid loss: 2.75411e+09\n",
      "[Epoch 100/350, Step 1100, ETA 17.22s] step time: 0.005338s (±0.0009812s); valid time: 0.002141s; loss: -59.6082 (±2.53566); valid loss: 2.17167e+09\n",
      "[Epoch 100/350, Step 1100, ETA 17.22s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.49s] step time: 0.005203s (±0.000695s); valid time: 0.002195s; loss: -59.7851 (±2.70024); valid loss: 1.92331e+09\n",
      "[Epoch 110/350, Step 1210, ETA 16.41s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.8s] step time: 0.005418s (±0.001399s); valid time: 0.002156s; loss: -59.9257 (±2.64611); valid loss: 2.04624e+09\n",
      "[Epoch 120/350, Step 1320, ETA 15.66s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.1s] step time: 0.005236s (±0.000732s); valid time: 0.002164s; loss: -59.9208 (±2.88307); valid loss: 1.6707e+09\n",
      "[Epoch 130/350, Step 1430, ETA 14.89s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.44s] step time: 0.005364s (±0.0008015s); valid time: 0.002293s; loss: -59.9 (±3.00738); valid loss: 1.9057e+09\n",
      "[Epoch 140/350, Step 1540, ETA 14.17s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.79s] step time: 0.00536s (±0.0009525s); valid time: 0.002094s; loss: -60.1823 (±2.99203); valid loss: 1.72183e+09\n",
      "[Epoch 150/350, Step 1650, ETA 13.48s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.17s] step time: 0.005561s (±0.001302s); valid time: 0.002178s; loss: -60.4096 (±2.69468); valid loss: 1.82324e+09\n",
      "[Epoch 160/350, Step 1760, ETA 12.78s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.54s] step time: 0.005362s (±0.0009707s); valid time: 0.002158s; loss: -60.1775 (±2.86065); valid loss: 1.63807e+09\n",
      "[Epoch 170/350, Step 1870, ETA 12.09s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.9s] step time: 0.005355s (±0.0007021s); valid time: 0.002135s; loss: -60.3912 (±2.78882); valid loss: 1.57561e+09\n",
      "[Epoch 180/350, Step 1980, ETA 11.4s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.27s] step time: 0.005404s (±0.0008513s); valid time: 0.002182s; loss: -60.3633 (±2.89967); valid loss: 1.59479e+09\n",
      "[Epoch 190/350, Step 2090, ETA 10.69s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.63s] step time: 0.005266s (±0.0005878s); valid time: 0.002124s; loss: -60.3137 (±2.92042); valid loss: 1.61351e+09\n",
      "[Epoch 200/350, Step 2200, ETA 10s] step time: 0.00529s (±0.0009134s); valid time: 0.002037s; loss: -60.3332 (±2.79988); valid loss: 1.60079e+09\n",
      "[Epoch 200/350, Step 2200, ETA 10s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.388s] step time: 0.005367s (±0.0005955s); valid time: 0.002248s; loss: -60.3014 (±2.56672); valid loss: 1.58818e+09\n",
      "[Epoch 210/350, Step 2310, ETA 9.329s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.779s] step time: 0.00545s (±0.0008908s); valid time: 0.001963s; loss: -60.2838 (±2.64319); valid loss: 1.5836e+09\n",
      "[Epoch 220/350, Step 2420, ETA 8.652s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.164s] step time: 0.005365s (±0.001121s); valid time: 0.002069s; loss: -60.4308 (±2.81305); valid loss: 1.59634e+09\n",
      "[Epoch 230/350, Step 2530, ETA 7.976s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.546s] step time: 0.005287s (±0.0008542s); valid time: 0.002181s; loss: -60.3012 (±2.91372); valid loss: 1.60124e+09\n",
      "[Epoch 240/350, Step 2640, ETA 7.298s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.933s] step time: 0.005304s (±0.0008547s); valid time: 0.002251s; loss: -60.5267 (±2.84081); valid loss: 1.58279e+09\n",
      "[Epoch 250/350, Step 2750, ETA 6.631s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.328s] step time: 0.005479s (±0.001172s); valid time: 0.002048s; loss: -60.4399 (±2.94687); valid loss: 1.57116e+09\n",
      "[Epoch 260/350, Step 2860, ETA 5.965s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.722s] step time: 0.005383s (±0.001037s); valid time: 0.002014s; loss: -60.2423 (±3.20148); valid loss: 1.56195e+09\n",
      "[Epoch 270/350, Step 2970, ETA 5.293s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.113s] step time: 0.00529s (±0.001154s); valid time: 0.002995s; loss: -60.4609 (±2.61143); valid loss: 1.56501e+09\n",
      "[Epoch 280/350, Step 3080, ETA 4.625s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.506s] step time: 0.005253s (±0.0006742s); valid time: 0.002198s; loss: -60.4259 (±2.84047); valid loss: 1.57988e+09\n",
      "[Epoch 290/350, Step 3190, ETA 3.961s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.901s] step time: 0.0053s (±0.000938s); valid time: 0.00208s; loss: -60.5094 (±2.81259); valid loss: 1.57808e+09\n",
      "[Epoch 300/350, Step 3300, ETA 3.294s] step time: 0.005139s (±0.0006144s); valid time: 0.002245s; loss: -60.4866 (±2.90073); valid loss: 1.57009e+09\n",
      "[Epoch 300/350, Step 3300, ETA 3.294s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.695s] step time: 0.005353s (±0.001195s); valid time: 0.002109s; loss: -60.5042 (±2.77851); valid loss: 1.58207e+09\n",
      "[Epoch 310/350, Step 3410, ETA 2.634s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.093s] step time: 0.005165s (±0.0004742s); valid time: 0.002244s; loss: -60.3595 (±2.84069); valid loss: 1.56501e+09\n",
      "[Epoch 320/350, Step 3520, ETA 1.972s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.493s] step time: 0.005291s (±0.0006292s); valid time: 0.002172s; loss: -60.338 (±3.03714); valid loss: 1.57834e+09\n",
      "[Epoch 330/350, Step 3630, ETA 1.313s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.8954s] step time: 0.005315s (±0.001236s); valid time: 0.002171s; loss: -60.3444 (±2.75288); valid loss: 1.56747e+09\n",
      "[Epoch 340/350, Step 3740, ETA 0.6568s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.2985s] step time: 0.005479s (±0.001122s); valid time: 0.003346s; loss: -60.414 (±3.07123); valid loss: 1.57373e+09\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpfkjr58aw/variables.dat-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpfkjr58aw/variables.dat-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.87,\n",
      "\t(tp, fp, tn, fn)=(204, 308, 3321, 199),\n",
      "\tprecision=0.4,\n",
      "\trecall=0.51,\n",
      "\tf1=0.45,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.12698412698412698,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 37.92s] step time: 0.009581s (±0.03139s); valid time: 0.1566s; loss: 142.638 (±15.557); valid loss: 117.139 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.26s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.23s] step time: 0.005959s (±0.007282s); valid time: 0.07282s; loss: 99.7652 (±8.64601); valid loss: 86.4327 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.28s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.36s] step time: 0.006061s (±0.007949s); valid time: 0.0792s; loss: 78.7536 (±4.18212); valid loss: 74.1568 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.41s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 25.5s] step time: 0.005948s (±0.007568s); valid time: 0.07361s; loss: 72.5148 (±2.63125); valid loss: 71.0587 (*)\n",
      "[Epoch 40/350, Step 440, ETA 24.62s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.16s] step time: 0.005987s (±0.007454s); valid time: 0.07445s; loss: 70.1467 (±2.36408); valid loss: 69.3567 (*)\n",
      "[Epoch 50/350, Step 550, ETA 23.53s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.29s] step time: 0.006425s (±0.008127s); valid time: 0.08107s; loss: 68.8751 (±2.44748); valid loss: 68.0737 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.48s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.36s] step time: 0.006201s (±0.007737s); valid time: 0.07557s; loss: 67.6953 (±2.10447); valid loss: 66.9825 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.47s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.45s] step time: 0.006069s (±0.00827s); valid time: 0.08266s; loss: 66.7511 (±2.32708); valid loss: 66.2335 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.57s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.64s] step time: 0.0062s (±0.008165s); valid time: 0.08196s; loss: 65.9861 (±2.00948); valid loss: 65.4373 (*)\n",
      "[Epoch 90/350, Step 990, ETA 19.72s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.85s] step time: 0.006171s (±0.007723s); valid time: 0.07685s; loss: 65.5509 (±2.50222); valid loss: 64.9765 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.03s] step time: 0.005998s (±0.008205s); valid time: 0.08207s; loss: 65.0503 (±2.20179); valid loss: 64.5884 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.04s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.27s] step time: 0.006085s (±0.007638s); valid time: 0.07598s; loss: 64.7238 (±2.07291); valid loss: 64.339 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.19s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.57s] step time: 0.006282s (±0.007678s); valid time: 0.07505s; loss: 64.5796 (±2.11444); valid loss: 64.3013 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.38s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.79s] step time: 0.00587s (±0.007772s); valid time: 0.07818s; loss: 64.4517 (±1.92144); valid loss: 64.1265 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.52s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.04s] step time: 0.005979s (±0.007831s); valid time: 0.07783s; loss: 64.2844 (±1.81631); valid loss: 63.7588 (*)\n",
      "[Epoch 140/350, Step 1540, ETA 15.7s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.21s] step time: 0.00528s (±0.0006515s); valid time: 0.002196s; loss: 64.3076 (±2.1498); valid loss: 63.805\n",
      "[Epoch 150/350, Step 1650, ETA 14.82s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.52s] step time: 0.006137s (±0.007338s); valid time: 0.07295s; loss: 64.1092 (±1.88532); valid loss: 63.5265 (*)\n",
      "[Epoch 160/350, Step 1760, ETA 14.05s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.76s] step time: 0.005489s (±0.001525s); valid time: 0.002322s; loss: 63.9924 (±2.25903); valid loss: 63.6846\n",
      "[Epoch 170/350, Step 1870, ETA 13.22s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.99s] step time: 0.005273s (±0.0006513s); valid time: 0.002255s; loss: 64.1016 (±2.18559); valid loss: 63.7534\n",
      "[Epoch 180/350, Step 1980, ETA 12.4s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.25s] step time: 0.005343s (±0.0009326s); valid time: 0.002129s; loss: 64.0649 (±1.99503); valid loss: 63.6972\n",
      "[Epoch 190/350, Step 2090, ETA 11.57s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.56s] step time: 0.005869s (±0.007499s); valid time: 0.07517s; loss: 63.9119 (±2.05313); valid loss: 63.5057 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.9s] step time: 0.006148s (±0.007274s); valid time: 0.07188s; loss: 63.9497 (±2.09419); valid loss: 63.4383 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.9s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.21s] step time: 0.005507s (±0.001116s); valid time: 0.002467s; loss: 63.9136 (±2.11284); valid loss: 63.5445\n",
      "[Epoch 210/350, Step 2310, ETA 10.15s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.525s] step time: 0.00557s (±0.0007004s); valid time: 0.002157s; loss: 63.8169 (±2.3207); valid loss: 63.486\n",
      "[Epoch 220/350, Step 2420, ETA 9.381s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.836s] step time: 0.005441s (±0.001011s); valid time: 0.002495s; loss: 63.8764 (±2.4597); valid loss: 63.4854\n",
      "[Epoch 230/350, Step 2530, ETA 8.628s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.149s] step time: 0.005386s (±0.000924s); valid time: 0.002292s; loss: 63.7749 (±2.0926); valid loss: 63.611\n",
      "[Epoch 240/350, Step 2640, ETA 7.879s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.47s] step time: 0.005375s (±0.001053s); valid time: 0.002162s; loss: 63.9352 (±2.32277); valid loss: 63.6236\n",
      "[Epoch 250/350, Step 2750, ETA 7.13s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.8s] step time: 0.005458s (±0.001249s); valid time: 0.002225s; loss: 63.8944 (±2.16078); valid loss: 63.6937\n",
      "[Epoch 260/350, Step 2860, ETA 6.399s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.131s] step time: 0.005324s (±0.000768s); valid time: 0.002146s; loss: 63.8899 (±2.04681); valid loss: 63.6247\n",
      "[Epoch 270/350, Step 2970, ETA 5.667s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.469s] step time: 0.005352s (±0.0009659s); valid time: 0.002165s; loss: 63.9517 (±2.15869); valid loss: 63.7133\n",
      "[Epoch 280/350, Step 3080, ETA 4.938s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.807s] step time: 0.005173s (±0.0007531s); valid time: 0.00207s; loss: 63.8389 (±2.09506); valid loss: 63.4589\n",
      "[Epoch 290/350, Step 3190, ETA 4.218s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.169s] step time: 0.006053s (±0.008124s); valid time: 0.08123s; loss: 63.8321 (±2.28205); valid loss: 63.3484 (*)\n",
      "[Epoch 300/350, Step 3300, ETA 3.522s] step time: 0.005566s (±0.001273s); valid time: 0.002158s; loss: 63.846 (±2.12029); valid loss: 63.5168\n",
      "[Epoch 300/350, Step 3300, ETA 3.522s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.874s] step time: 0.00528s (±0.0007204s); valid time: 0.002103s; loss: 63.8092 (±2.10971); valid loss: 63.5575\n",
      "[Epoch 310/350, Step 3410, ETA 2.809s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.229s] step time: 0.005282s (±0.0009163s); valid time: 0.002103s; loss: 63.8357 (±2.00009); valid loss: 63.3904\n",
      "[Epoch 320/350, Step 3520, ETA 2.1s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.588s] step time: 0.005231s (±0.0005255s); valid time: 0.002214s; loss: 63.7144 (±2.04473); valid loss: 63.5801\n",
      "[Epoch 330/350, Step 3630, ETA 1.396s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9508s] step time: 0.0054s (±0.001071s); valid time: 0.002151s; loss: 63.8223 (±1.96121); valid loss: 63.5403\n",
      "[Epoch 340/350, Step 3740, ETA 0.6967s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3164s] step time: 0.005391s (±0.0009131s); valid time: 0.002416s; loss: 63.987 (±2.05232); valid loss: 63.5287\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5uw8nl8x/variables.dat-3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5uw8nl8x/variables.dat-3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=1.0,\n",
      "\t(tp, fp, tn, fn)=(0, 18, 4014, 0),\n",
      "\tprecision=0.0,\n",
      "\trecall=1.0,\n",
      "\tf1=0.0,\n",
      "\troc_auc=nan,\n",
      "\ty_pred%=0.004464285714285714,\n",
      "\ty_label%=0.0,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 39.56s] step time: 0.01001s (±0.03327s); valid time: 0.1689s; loss: 117.328 (±9.29428); valid loss: 141.214 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.99s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 31.64s] step time: 0.006268s (±0.007867s); valid time: 0.07521s; loss: 98.0277 (±5.4813); valid loss: 130.985 (*)\n",
      "[Epoch 20/350, Step 220, ETA 30.48s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 28.36s] step time: 0.006126s (±0.008113s); valid time: 0.08082s; loss: 67.9136 (±14.6819); valid loss: 104.802 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.32s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.4s] step time: 0.006138s (±0.008061s); valid time: 0.0802s; loss: 35.7534 (±9.6); valid loss: 97.3697 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.5s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.99s] step time: 0.006143s (±0.008499s); valid time: 0.08479s; loss: 22.9585 (±8.7994); valid loss: 94.8999 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24.15s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.85s] step time: 0.006212s (±0.007843s); valid time: 0.07862s; loss: 18.1305 (±8.31202); valid loss: 94.4069 (*)\n",
      "[Epoch 60/350, Step 660, ETA 23.03s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.91s] step time: 0.006355s (±0.00774s); valid time: 0.07684s; loss: 17.0174 (±7.34479); valid loss: 93.111 (*)\n",
      "[Epoch 70/350, Step 770, ETA 22s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.99s] step time: 0.006265s (±0.009345s); valid time: 0.09383s; loss: 15.7394 (±8.35657); valid loss: 92.7071 (*)\n",
      "[Epoch 80/350, Step 880, ETA 21.09s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 21.12s] step time: 0.006249s (±0.007719s); valid time: 0.07746s; loss: 14.8783 (±7.68215); valid loss: 92.4525 (*)\n",
      "[Epoch 90/350, Step 990, ETA 20.17s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 20.29s] step time: 0.006231s (±0.007988s); valid time: 0.08031s; loss: 13.9176 (±8.66254); valid loss: 92.0628 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.31s] step time: 0.005531s (±0.0012s); valid time: 0.002301s; loss: 13.477 (±8.52064); valid loss: 92.2825\n",
      "[Epoch 100/350, Step 1100, ETA 19.31s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.58s] step time: 0.006353s (±0.008432s); valid time: 0.08442s; loss: 13.2844 (±7.68901); valid loss: 92.0255 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.48s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.84s] step time: 0.00628s (±0.007636s); valid time: 0.07592s; loss: 12.8736 (±9.15947); valid loss: 91.191 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.65s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 17.07s] step time: 0.006083s (±0.007603s); valid time: 0.07571s; loss: 12.2146 (±7.35684); valid loss: 91.0802 (*)\n",
      "[Epoch 130/350, Step 1430, ETA 16.81s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 16.19s] step time: 0.005342s (±0.0008519s); valid time: 0.002077s; loss: 11.8539 (±8.43407); valid loss: 91.3047\n",
      "[Epoch 140/350, Step 1540, ETA 15.85s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 15.37s] step time: 0.005413s (±0.001238s); valid time: 0.002066s; loss: 11.8524 (±8.75882); valid loss: 91.6305\n",
      "[Epoch 150/350, Step 1650, ETA 14.97s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.67s] step time: 0.00618s (±0.007806s); valid time: 0.07772s; loss: 11.9021 (±8.51098); valid loss: 91.0496 (*)\n",
      "[Epoch 160/350, Step 1760, ETA 14.2s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.89s] step time: 0.00541s (±0.0009157s); valid time: 0.002214s; loss: 11.9789 (±8.49155); valid loss: 91.6531\n",
      "[Epoch 170/350, Step 1870, ETA 13.33s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 13.19s] step time: 0.006077s (±0.009241s); valid time: 0.09131s; loss: 11.321 (±8.67163); valid loss: 90.5729 (*)\n",
      "[Epoch 180/350, Step 1980, ETA 12.58s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12.44s] step time: 0.005368s (±0.0007631s); valid time: 0.002196s; loss: 11.7721 (±8.58644); valid loss: 91.588\n",
      "[Epoch 190/350, Step 2090, ETA 11.77s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.69s] step time: 0.005386s (±0.0008888s); valid time: 0.002204s; loss: 11.6038 (±7.7514); valid loss: 91.2649\n",
      "[Epoch 200/350, Step 2200, ETA 10.96s] step time: 0.005312s (±0.0007478s); valid time: 0.002092s; loss: 11.917 (±7.71958); valid loss: 90.8529\n",
      "[Epoch 200/350, Step 2200, ETA 10.96s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.25s] step time: 0.005315s (±0.0008726s); valid time: 0.00228s; loss: 11.3808 (±9.31149); valid loss: 91.4471\n",
      "[Epoch 210/350, Step 2310, ETA 10.18s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.535s] step time: 0.005296s (±0.0006284s); valid time: 0.002165s; loss: 11.3165 (±8.39777); valid loss: 91.5064\n",
      "[Epoch 220/350, Step 2420, ETA 9.392s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.839s] step time: 0.00535s (±0.0008388s); valid time: 0.002079s; loss: 11.2448 (±8.57062); valid loss: 91.191\n",
      "[Epoch 230/350, Step 2530, ETA 8.627s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.152s] step time: 0.005393s (±0.0008897s); valid time: 0.00213s; loss: 11.3005 (±8.25294); valid loss: 92.6632\n",
      "[Epoch 240/350, Step 2640, ETA 7.88s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.483s] step time: 0.00556s (±0.001228s); valid time: 0.002013s; loss: 11.3129 (±8.54519); valid loss: 91.0661\n",
      "[Epoch 250/350, Step 2750, ETA 7.146s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.814s] step time: 0.005451s (±0.0008874s); valid time: 0.00207s; loss: 11.3687 (±7.54102); valid loss: 91.3506\n",
      "[Epoch 260/350, Step 2860, ETA 6.411s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.147s] step time: 0.005438s (±0.0009716s); valid time: 0.00213s; loss: 11.3273 (±7.28893); valid loss: 91.7099\n",
      "[Epoch 270/350, Step 2970, ETA 5.685s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.487s] step time: 0.005521s (±0.0009014s); valid time: 0.002126s; loss: 11.2727 (±8.67395); valid loss: 91.1654\n",
      "[Epoch 280/350, Step 3080, ETA 4.958s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.826s] step time: 0.005314s (±0.0006676s); valid time: 0.002214s; loss: 10.6572 (±8.52218); valid loss: 90.8292\n",
      "[Epoch 290/350, Step 3190, ETA 4.235s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.171s] step time: 0.005299s (±0.0009351s); valid time: 0.002378s; loss: 11.5603 (±9.82176); valid loss: 91.012\n",
      "[Epoch 300/350, Step 3300, ETA 3.52s] step time: 0.005387s (±0.000877s); valid time: 0.002149s; loss: 11.6192 (±9.51008); valid loss: 92.0083\n",
      "[Epoch 300/350, Step 3300, ETA 3.521s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.873s] step time: 0.00534s (±0.0006513s); valid time: 0.002093s; loss: 11.3967 (±8.4574); valid loss: 90.6213\n",
      "[Epoch 310/350, Step 3410, ETA 2.808s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.229s] step time: 0.005341s (±0.00113s); valid time: 0.002097s; loss: 11.1474 (±7.34253); valid loss: 91.5661\n",
      "[Epoch 320/350, Step 3520, ETA 2.101s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.588s] step time: 0.005275s (±0.001028s); valid time: 0.002261s; loss: 11.4156 (±8.73739); valid loss: 91.5768\n",
      "[Epoch 330/350, Step 3630, ETA 1.397s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9512s] step time: 0.005437s (±0.001056s); valid time: 0.002094s; loss: 11.2229 (±8.85141); valid loss: 90.6119\n",
      "[Epoch 340/350, Step 3740, ETA 0.697s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3166s] step time: 0.005492s (±0.001353s); valid time: 0.002248s; loss: 11.3672 (±7.70043); valid loss: 91.1849\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmppy4en6sn/variables.dat-1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmppy4en6sn/variables.dat-1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(117, 286, 3341, 288),\n",
      "\tprecision=0.29,\n",
      "\trecall=0.29,\n",
      "\tf1=0.29,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.09995039682539683,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 8/350, Step 100, ETA 44.9s] step time: 0.009581s (±0.03229s); valid time: 0.1626s; loss: 103.388 (±6.53587); valid loss: 216.227 (*)\n",
      "[Epoch 10/350, Step 130, ETA 40.17s] Learning rate decreased to 0.00075\n",
      "[Epoch 16/350, Step 200, ETA 36.43s] step time: 0.006153s (±0.00789s); valid time: 0.07804s; loss: 90.364 (±9.2161); valid loss: 135.888 (*)\n",
      "[Epoch 20/350, Step 260, ETA 33.46s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 24/350, Step 300, ETA 33.34s] step time: 0.006213s (±0.007772s); valid time: 0.07713s; loss: 3.14371 (±33.365); valid loss: 3.82162 (*)\n",
      "[Epoch 30/350, Step 390, ETA 30.55s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 400, ETA 31.13s] step time: 0.005985s (±0.007844s); valid time: 0.07834s; loss: -46.7799 (±12.8562); valid loss: -21.3609 (*)\n",
      "[Epoch 39/350, Step 500, ETA 29.83s] step time: 0.006325s (±0.008121s); valid time: 0.08038s; loss: -61.0747 (±13.6804); valid loss: -29.6882 (*)\n",
      "[Epoch 40/350, Step 520, ETA 29.47s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 47/350, Step 600, ETA 28.63s] step time: 0.006146s (±0.00817s); valid time: 0.08218s; loss: -68.5661 (±11.8907); valid loss: -32.9626 (*)\n",
      "[Epoch 50/350, Step 650, ETA 27.74s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 54/350, Step 700, ETA 27.47s] step time: 0.005972s (±0.007814s); valid time: 0.07786s; loss: -72.9138 (±15.0708); valid loss: -35.6161 (*)\n",
      "[Epoch 60/350, Step 780, ETA 26.43s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 62/350, Step 800, ETA 26.53s] step time: 0.006142s (±0.007957s); valid time: 0.07875s; loss: -75.2925 (±15.3473); valid loss: -36.1766 (*)\n",
      "[Epoch 70/350, Step 900, ETA 25.58s] step time: 0.005984s (±0.00821s); valid time: 0.08196s; loss: -77.2811 (±15.1767); valid loss: -37.5711 (*)\n",
      "[Epoch 70/350, Step 910, ETA 25.49s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 77/350, Step 1000, ETA 24.41s] step time: 0.005216s (±0.0009888s); valid time: 0.003557s; loss: -78.3446 (±15.2662); valid loss: -35.1477\n",
      "[Epoch 80/350, Step 1040, ETA 23.99s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 85/350, Step 1100, ETA 23.41s] step time: 0.005389s (±0.001151s); valid time: 0.003488s; loss: -79.996 (±14.6982); valid loss: -36.156\n",
      "[Epoch 90/350, Step 1170, ETA 22.73s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 93/350, Step 1200, ETA 22.71s] step time: 0.006175s (±0.008276s); valid time: 0.08265s; loss: -81.3491 (±15.7946); valid loss: -38.4314 (*)\n",
      "[Epoch 100/350, Step 1300, ETA 21.85s] step time: 0.005468s (±0.0009847s); valid time: 0.003556s; loss: -80.5667 (±16.0635); valid loss: -37.2956\n",
      "[Epoch 100/350, Step 1300, ETA 21.85s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 108/350, Step 1400, ETA 20.95s] step time: 0.005223s (±0.0007151s); valid time: 0.003391s; loss: -81.732 (±14.2435); valid loss: -37.2893\n",
      "[Epoch 110/350, Step 1430, ETA 20.7s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 116/350, Step 1500, ETA 20.13s] step time: 0.00539s (±0.0009309s); valid time: 0.003819s; loss: -82.4972 (±15.5696); valid loss: -36.7561\n",
      "[Epoch 120/350, Step 1560, ETA 19.65s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 124/350, Step 1600, ETA 19.37s] step time: 0.005532s (±0.001421s); valid time: 0.00383s; loss: -83.3152 (±15.5604); valid loss: -37.9518\n",
      "[Epoch 130/350, Step 1690, ETA 18.65s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 1700, ETA 18.58s] step time: 0.005311s (±0.0009134s); valid time: 0.003976s; loss: -82.9032 (±17.2565); valid loss: -38.359\n",
      "[Epoch 139/350, Step 1800, ETA 17.84s] step time: 0.005395s (±0.0008322s); valid time: 0.003649s; loss: -83.4714 (±17.6968); valid loss: -37.1566\n",
      "[Epoch 140/350, Step 1820, ETA 17.68s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 147/350, Step 1900, ETA 17.1s] step time: 0.005367s (±0.0008275s); valid time: 0.003582s; loss: -83.8889 (±15.6298); valid loss: -37.7022\n",
      "[Epoch 150/350, Step 1950, ETA 16.74s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 154/350, Step 2000, ETA 16.39s] step time: 0.005421s (±0.000936s); valid time: 0.00391s; loss: -84.4167 (±15.3189); valid loss: -37.6108\n",
      "[Epoch 160/350, Step 2080, ETA 15.81s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 162/350, Step 2100, ETA 15.79s] step time: 0.006328s (±0.01021s); valid time: 0.1026s; loss: -83.3765 (±15.3961); valid loss: -38.8827 (*)\n",
      "[Epoch 170/350, Step 2200, ETA 15.16s] step time: 0.006042s (±0.007593s); valid time: 0.07579s; loss: -84.7138 (±14.7938); valid loss: -38.8879 (*)\n",
      "[Epoch 170/350, Step 2210, ETA 15.09s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 177/350, Step 2300, ETA 14.46s] step time: 0.005386s (±0.001244s); valid time: 0.003552s; loss: -84.8695 (±17.6474); valid loss: -36.6431\n",
      "[Epoch 180/350, Step 2340, ETA 14.19s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 185/350, Step 2400, ETA 13.77s] step time: 0.005376s (±0.0007741s); valid time: 0.003688s; loss: -84.1111 (±15.4665); valid loss: -38.6261\n",
      "[Epoch 190/350, Step 2470, ETA 13.31s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 193/350, Step 2500, ETA 13.1s] step time: 0.005541s (±0.00116s); valid time: 0.003622s; loss: -85.3054 (±17.675); valid loss: -37.11\n",
      "[Epoch 200/350, Step 2600, ETA 12.43s] step time: 0.005422s (±0.0009995s); valid time: 0.003324s; loss: -84.5096 (±14.9748); valid loss: -37.978\n",
      "[Epoch 200/350, Step 2600, ETA 12.43s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 208/350, Step 2700, ETA 11.75s] step time: 0.005275s (±0.0006664s); valid time: 0.003808s; loss: -84.6957 (±15.6458); valid loss: -37.8278\n",
      "[Epoch 210/350, Step 2730, ETA 11.55s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 216/350, Step 2800, ETA 11.09s] step time: 0.00547s (±0.001158s); valid time: 0.003688s; loss: -84.4323 (±16.5154); valid loss: -38.2371\n",
      "[Epoch 220/350, Step 2860, ETA 10.69s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 224/350, Step 2900, ETA 10.44s] step time: 0.00546s (±0.001811s); valid time: 0.003588s; loss: -85.2098 (±15.8689); valid loss: -38.4862\n",
      "[Epoch 230/350, Step 2990, ETA 9.845s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 3000, ETA 9.781s] step time: 0.005401s (±0.0009596s); valid time: 0.003413s; loss: -84.9634 (±15.7904); valid loss: -37.5581\n",
      "[Epoch 239/350, Step 3100, ETA 9.129s] step time: 0.00537s (±0.000833s); valid time: 0.003532s; loss: -85.1744 (±17.6736); valid loss: -37.7223\n",
      "[Epoch 240/350, Step 3120, ETA 9.003s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 247/350, Step 3200, ETA 8.484s] step time: 0.005384s (±0.0009398s); valid time: 0.00369s; loss: -84.6283 (±15.4326); valid loss: -37.6721\n",
      "[Epoch 250/350, Step 3250, ETA 8.161s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 254/350, Step 3300, ETA 7.849s] step time: 0.005609s (±0.001515s); valid time: 0.003554s; loss: -85.1393 (±16.6344); valid loss: -37.9917\n",
      "[Epoch 260/350, Step 3380, ETA 7.336s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 262/350, Step 3400, ETA 7.211s] step time: 0.005428s (±0.0007093s); valid time: 0.003473s; loss: -85.244 (±15.1666); valid loss: -37.1526\n",
      "[Epoch 270/350, Step 3500, ETA 6.577s] step time: 0.005528s (±0.001046s); valid time: 0.003713s; loss: -84.7105 (±15.9213); valid loss: -37.7491\n",
      "[Epoch 270/350, Step 3510, ETA 6.512s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 277/350, Step 3600, ETA 5.939s] step time: 0.005318s (±0.0006702s); valid time: 0.003631s; loss: -85.2668 (±17.2458); valid loss: -37.5165\n",
      "[Epoch 280/350, Step 3640, ETA 5.689s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 285/350, Step 3700, ETA 5.31s] step time: 0.005556s (±0.001085s); valid time: 0.003567s; loss: -84.7481 (±14.9946); valid loss: -38.2322\n",
      "[Epoch 290/350, Step 3770, ETA 4.865s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 293/350, Step 3800, ETA 4.678s] step time: 0.005349s (±0.0009056s); valid time: 0.003759s; loss: -85.9886 (±15.9148); valid loss: -37.5176\n",
      "[Epoch 300/350, Step 3900, ETA 4.047s] step time: 0.005286s (±0.0006662s); valid time: 0.003432s; loss: -84.3649 (±16.2372); valid loss: -35.6549\n",
      "[Epoch 300/350, Step 3900, ETA 4.047s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 308/350, Step 4000, ETA 3.419s] step time: 0.005332s (±0.001019s); valid time: 0.003628s; loss: -84.9569 (±16.4596); valid loss: -38.3464\n",
      "[Epoch 310/350, Step 4030, ETA 3.23s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 316/350, Step 4100, ETA 2.793s] step time: 0.005309s (±0.0007279s); valid time: 0.00356s; loss: -85.1894 (±14.7099); valid loss: -37.9419\n",
      "[Epoch 320/350, Step 4160, ETA 2.419s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 324/350, Step 4200, ETA 2.17s] step time: 0.005432s (±0.001265s); valid time: 0.00387s; loss: -85.6168 (±14.8332); valid loss: -37.5865\n",
      "[Epoch 330/350, Step 4290, ETA 1.611s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 4300, ETA 1.549s] step time: 0.005522s (±0.001173s); valid time: 0.003831s; loss: -85.5347 (±14.7293); valid loss: -37.2801\n",
      "[Epoch 339/350, Step 4400, ETA 0.9286s] step time: 0.005465s (±0.0009522s); valid time: 0.003572s; loss: -84.3546 (±16.5933); valid loss: -38.0683\n",
      "[Epoch 340/350, Step 4420, ETA 0.8044s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 347/350, Step 4500, ETA 0.3091s] step time: 0.005324s (±0.0008528s); valid time: 0.003774s; loss: -84.7172 (±16.4202); valid loss: -37.7033\n",
      "[Epoch 350/350, Step 4550, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpgamhcl6s/variables.dat-2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpgamhcl6s/variables.dat-2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.37,\n",
      "\t(tp, fp, tn, fn)=(385, 2906, 1351, 88),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.81,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.6957716701902749,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 37.85s] step time: 0.009542s (±0.03093s); valid time: 0.1558s; loss: 134.509 (±16.4633); valid loss: 117.836 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.31s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.45s] step time: 0.006086s (±0.007549s); valid time: 0.07549s; loss: 109.325 (±4.82293); valid loss: 102.314 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.3s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.68s] step time: 0.006195s (±0.007635s); valid time: 0.07668s; loss: 68.5207 (±20.3559); valid loss: 34.792 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.82s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 25.87s] step time: 0.006094s (±0.007724s); valid time: 0.07668s; loss: 32.4635 (±12.5746); valid loss: 27.0733 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.03s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 23.98s] step time: 0.00529s (±0.0009373s); valid time: 0.00208s; loss: 22.2536 (±11.1411); valid loss: 34.0813\n",
      "[Epoch 50/350, Step 550, ETA 23.19s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 22.6s] step time: 0.005405s (±0.0009027s); valid time: 0.002221s; loss: 17.7744 (±11.256); valid loss: 34.9295\n",
      "[Epoch 60/350, Step 660, ETA 21.84s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 21.41s] step time: 0.005359s (±0.0008744s); valid time: 0.002023s; loss: 15.5237 (±11.9041); valid loss: 42.9663\n",
      "[Epoch 70/350, Step 770, ETA 20.69s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 20.4s] step time: 0.005405s (±0.001033s); valid time: 0.002377s; loss: 13.6842 (±12.8996); valid loss: 40.2923\n",
      "[Epoch 80/350, Step 880, ETA 19.63s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.48s] step time: 0.005429s (±0.0009256s); valid time: 0.002202s; loss: 12.2407 (±13.4324); valid loss: 34.099\n",
      "[Epoch 90/350, Step 990, ETA 18.7s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.61s] step time: 0.005331s (±0.0008998s); valid time: 0.0022s; loss: 11.2639 (±14.8352); valid loss: 39.4997\n",
      "[Epoch 100/350, Step 1100, ETA 17.8s] step time: 0.005406s (±0.001046s); valid time: 0.001952s; loss: 10.7885 (±13.0981); valid loss: 41.8716\n",
      "[Epoch 100/350, Step 1100, ETA 17.8s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.99s] step time: 0.005206s (±0.0007478s); valid time: 0.002223s; loss: 9.93931 (±12.4923); valid loss: 39.0012\n",
      "[Epoch 110/350, Step 1210, ETA 16.9s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.24s] step time: 0.005363s (±0.0008726s); valid time: 0.002198s; loss: 9.89234 (±14.0371); valid loss: 42.4188\n",
      "[Epoch 120/350, Step 1320, ETA 16.08s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.49s] step time: 0.005254s (±0.0006518s); valid time: 0.002085s; loss: 9.84755 (±13.2499); valid loss: 43.181\n",
      "[Epoch 130/350, Step 1430, ETA 15.32s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.81s] step time: 0.005465s (±0.001277s); valid time: 0.002044s; loss: 9.00969 (±13.0652); valid loss: 40.7436\n",
      "[Epoch 140/350, Step 1540, ETA 14.53s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.13s] step time: 0.005442s (±0.001331s); valid time: 0.001962s; loss: 8.82639 (±13.7059); valid loss: 39.7204\n",
      "[Epoch 150/350, Step 1650, ETA 13.8s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.47s] step time: 0.005525s (±0.0008562s); valid time: 0.002455s; loss: 9.66321 (±12.9012); valid loss: 41.2723\n",
      "[Epoch 160/350, Step 1760, ETA 13.06s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.81s] step time: 0.005388s (±0.001046s); valid time: 0.002254s; loss: 8.88315 (±14.6918); valid loss: 42.0111\n",
      "[Epoch 170/350, Step 1870, ETA 12.34s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.14s] step time: 0.00537s (±0.0007526s); valid time: 0.002193s; loss: 8.32603 (±14.1448); valid loss: 41.1529\n",
      "[Epoch 180/350, Step 1980, ETA 11.62s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.5s] step time: 0.005522s (±0.001351s); valid time: 0.002074s; loss: 9.36436 (±14.9547); valid loss: 41.6418\n",
      "[Epoch 190/350, Step 2090, ETA 10.91s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.85s] step time: 0.005261s (±0.0005622s); valid time: 0.002153s; loss: 7.81797 (±11.7576); valid loss: 41.6884\n",
      "[Epoch 200/350, Step 2200, ETA 10.2s] step time: 0.005423s (±0.001023s); valid time: 0.002022s; loss: 9.28028 (±13.1371); valid loss: 41.1012\n",
      "[Epoch 200/350, Step 2200, ETA 10.2s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.564s] step time: 0.005344s (±0.0009744s); valid time: 0.003561s; loss: 8.14381 (±13.7166); valid loss: 41.9994\n",
      "[Epoch 210/350, Step 2310, ETA 9.497s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.923s] step time: 0.005287s (±0.0007135s); valid time: 0.00239s; loss: 9.35473 (±13.3873); valid loss: 40.5975\n",
      "[Epoch 220/350, Step 2420, ETA 8.794s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.291s] step time: 0.005327s (±0.0009111s); valid time: 0.002194s; loss: 7.7425 (±13.3731); valid loss: 42.4719\n",
      "[Epoch 230/350, Step 2530, ETA 8.097s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.657s] step time: 0.005252s (±0.0006868s); valid time: 0.002222s; loss: 8.84507 (±13.0416); valid loss: 42.3573\n",
      "[Epoch 240/350, Step 2640, ETA 7.409s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.036s] step time: 0.005391s (±0.001015s); valid time: 0.002113s; loss: 8.1385 (±13.6327); valid loss: 41.353\n",
      "[Epoch 250/350, Step 2750, ETA 6.721s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.41s] step time: 0.005243s (±0.0007085s); valid time: 0.00206s; loss: 8.89492 (±10.9109); valid loss: 43.1977\n",
      "[Epoch 260/350, Step 2860, ETA 6.044s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.798s] step time: 0.005532s (±0.001063s); valid time: 0.002191s; loss: 7.50673 (±15.0698); valid loss: 41.6281\n",
      "[Epoch 270/350, Step 2970, ETA 5.367s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.184s] step time: 0.005474s (±0.0009449s); valid time: 0.002385s; loss: 8.53102 (±15.2615); valid loss: 40.6785\n",
      "[Epoch 280/350, Step 3080, ETA 4.69s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.566s] step time: 0.005211s (±0.0007265s); valid time: 0.002153s; loss: 8.43773 (±13.2176); valid loss: 42.4104\n",
      "[Epoch 290/350, Step 3190, ETA 4.019s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.959s] step time: 0.005652s (±0.001366s); valid time: 0.002114s; loss: 8.71248 (±13.9453); valid loss: 44.346\n",
      "[Epoch 300/350, Step 3300, ETA 3.346s] step time: 0.00535s (±0.0005845s); valid time: 0.002184s; loss: 8.20908 (±14.3847); valid loss: 39.9409\n",
      "[Epoch 300/350, Step 3300, ETA 3.346s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.735s] step time: 0.005355s (±0.0008915s); valid time: 0.002301s; loss: 8.55312 (±12.8334); valid loss: 42.3898\n",
      "[Epoch 310/350, Step 3410, ETA 2.674s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.125s] step time: 0.005374s (±0.001075s); valid time: 0.002372s; loss: 8.30413 (±13.1232); valid loss: 43.8833\n",
      "[Epoch 320/350, Step 3520, ETA 2.003s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.516s] step time: 0.005232s (±0.0005612s); valid time: 0.002142s; loss: 7.64278 (±14.2083); valid loss: 40.406\n",
      "[Epoch 330/350, Step 3630, ETA 1.333s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9094s] step time: 0.005564s (±0.001451s); valid time: 0.002298s; loss: 8.72725 (±13.7289); valid loss: 42.1795\n",
      "[Epoch 340/350, Step 3740, ETA 0.6666s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3029s] step time: 0.005353s (±0.0007806s); valid time: 0.002132s; loss: 8.00412 (±13.419); valid loss: 42.8027\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpayep1us5/variables.dat-400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpayep1us5/variables.dat-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(155, 675, 2952, 250),\n",
      "\tprecision=0.19,\n",
      "\trecall=0.38,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.2058531746031746,\n",
      "\ty_label%=0.10044642857142858,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_257a54.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 39.19s] step time: 0.009899s (±0.03274s); valid time: 0.1627s; loss: 136.132 (±22.1135); valid loss: 63.9337 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.49s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 31.33s] step time: 0.006213s (±0.007778s); valid time: 0.07733s; loss: 78.4693 (±16.3311); valid loss: -57.4953 (*)\n",
      "[Epoch 20/350, Step 220, ETA 30.13s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.96s] step time: 0.00595s (±0.007083s); valid time: 0.07073s; loss: 27.8481 (±11.1419); valid loss: -157.33 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.22s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.32s] step time: 0.006339s (±0.008138s); valid time: 0.07922s; loss: 9.64095 (±8.79905); valid loss: -185.174 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.49s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.94s] step time: 0.006217s (±0.007817s); valid time: 0.07767s; loss: 3.00338 (±8.6055); valid loss: -193.477 (*)\n",
      "[Epoch 50/350, Step 550, ETA 24.07s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.74s] step time: 0.006091s (±0.00777s); valid time: 0.07767s; loss: 0.185903 (±7.96646); valid loss: -195.675 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.92s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.76s] step time: 0.006229s (±0.007956s); valid time: 0.07893s; loss: -2.0119 (±7.85175); valid loss: -199.536 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.83s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.9s] step time: 0.006324s (±0.009146s); valid time: 0.09119s; loss: -4.51595 (±8.32766); valid loss: -199.803 (*)\n",
      "[Epoch 80/350, Step 880, ETA 20.93s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 21s] step time: 0.006106s (±0.007693s); valid time: 0.07675s; loss: -5.14276 (±9.41008); valid loss: -202.747 (*)\n",
      "[Epoch 90/350, Step 990, ETA 20.03s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 20.15s] step time: 0.006095s (±0.007729s); valid time: 0.07698s; loss: -6.02004 (±8.16695); valid loss: -206.037 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 19.16s] step time: 0.005473s (±0.001198s); valid time: 0.002166s; loss: -6.57315 (±8.48293); valid loss: -201.901\n",
      "[Epoch 100/350, Step 1100, ETA 19.16s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.24s] step time: 0.005406s (±0.0008374s); valid time: 0.003053s; loss: -7.3595 (±8.93307); valid loss: -201.181\n",
      "[Epoch 110/350, Step 1210, ETA 18.15s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.36s] step time: 0.00541s (±0.001066s); valid time: 0.002122s; loss: -8.06706 (±9.09942); valid loss: -203.491\n",
      "[Epoch 120/350, Step 1320, ETA 17.17s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.51s] step time: 0.005347s (±0.001094s); valid time: 0.002165s; loss: -8.09958 (±8.14125); valid loss: -203.514\n",
      "[Epoch 130/350, Step 1430, ETA 16.24s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.69s] step time: 0.005308s (±0.0008822s); valid time: 0.002033s; loss: -8.22647 (±9.48774); valid loss: -204.943\n",
      "[Epoch 140/350, Step 1540, ETA 15.37s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.89s] step time: 0.005257s (±0.0008043s); valid time: 0.002195s; loss: -8.67531 (±9.01595); valid loss: -204.06\n",
      "[Epoch 150/350, Step 1650, ETA 14.52s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.15s] step time: 0.005472s (±0.001121s); valid time: 0.002185s; loss: -8.97085 (±9.09398); valid loss: -203.563\n",
      "[Epoch 160/350, Step 1760, ETA 13.7s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.42s] step time: 0.005407s (±0.001022s); valid time: 0.002064s; loss: -8.23417 (±8.8788); valid loss: -203.915\n",
      "[Epoch 170/350, Step 1870, ETA 12.92s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.71s] step time: 0.005447s (±0.000908s); valid time: 0.002689s; loss: -8.93513 (±8.76363); valid loss: -203.513\n",
      "[Epoch 180/350, Step 1980, ETA 12.14s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12s] step time: 0.00539s (±0.0009306s); valid time: 0.002118s; loss: -8.50181 (±8.49559); valid loss: -203.414\n",
      "[Epoch 190/350, Step 2090, ETA 11.38s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.31s] step time: 0.005509s (±0.001438s); valid time: 0.002331s; loss: -8.65545 (±8.73324); valid loss: -204.14\n",
      "[Epoch 200/350, Step 2200, ETA 10.63s] step time: 0.005439s (±0.001033s); valid time: 0.002081s; loss: -9.0116 (±8.51917); valid loss: -203.342\n",
      "[Epoch 200/350, Step 2200, ETA 10.63s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.951s] step time: 0.005465s (±0.001332s); valid time: 0.002066s; loss: -9.03833 (±8.08411); valid loss: -203.782\n",
      "[Epoch 210/350, Step 2310, ETA 9.882s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.28s] step time: 0.005445s (±0.001345s); valid time: 0.00222s; loss: -9.40609 (±9.32193); valid loss: -204.457\n",
      "[Epoch 220/350, Step 2420, ETA 9.144s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.612s] step time: 0.005384s (±0.001009s); valid time: 0.00215s; loss: -8.43832 (±8.2708); valid loss: -203.889\n",
      "[Epoch 230/350, Step 2530, ETA 8.409s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.944s] step time: 0.005266s (±0.0008225s); valid time: 0.002135s; loss: -8.91292 (±9.58848); valid loss: -204.45\n",
      "[Epoch 240/350, Step 2640, ETA 7.681s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.285s] step time: 0.005306s (±0.001098s); valid time: 0.002155s; loss: -9.15948 (±8.28408); valid loss: -203.643\n",
      "[Epoch 250/350, Step 2750, ETA 6.955s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.63s] step time: 0.005271s (±0.0006989s); valid time: 0.002133s; loss: -8.94538 (±7.22687); valid loss: -204.312\n",
      "[Epoch 260/350, Step 2860, ETA 6.245s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.988s] step time: 0.005489s (±0.001254s); valid time: 0.002106s; loss: -9.0808 (±9.21383); valid loss: -203.308\n",
      "[Epoch 270/350, Step 2970, ETA 5.538s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.349s] step time: 0.005478s (±0.001071s); valid time: 0.002449s; loss: -9.44423 (±7.27773); valid loss: -204.297\n",
      "[Epoch 280/350, Step 3080, ETA 4.834s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.706s] step time: 0.005225s (±0.0006237s); valid time: 0.002058s; loss: -8.80825 (±8.31202); valid loss: -204.544\n",
      "[Epoch 290/350, Step 3190, ETA 4.134s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.071s] step time: 0.005411s (±0.0009998s); valid time: 0.002111s; loss: -9.3163 (±8.63891); valid loss: -204.609\n",
      "[Epoch 300/350, Step 3300, ETA 3.436s] step time: 0.005239s (±0.0005445s); valid time: 0.002078s; loss: -9.12776 (±9.53604); valid loss: -204.466\n",
      "[Epoch 300/350, Step 3300, ETA 3.436s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.806s] step time: 0.005371s (±0.001085s); valid time: 0.002102s; loss: -9.02502 (±8.08396); valid loss: -204.894\n",
      "[Epoch 310/350, Step 3410, ETA 2.743s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.179s] step time: 0.00533s (±0.0006941s); valid time: 0.002104s; loss: -9.30478 (±7.56399); valid loss: -204.281\n",
      "[Epoch 320/350, Step 3520, ETA 2.054s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.554s] step time: 0.00542s (±0.0007703s); valid time: 0.00206s; loss: -9.41005 (±9.78893); valid loss: -205.208\n",
      "[Epoch 330/350, Step 3630, ETA 1.367s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9325s] step time: 0.005683s (±0.0011s); valid time: 0.002177s; loss: -9.19925 (±9.7579); valid loss: -204.15\n",
      "[Epoch 340/350, Step 3740, ETA 0.6832s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3106s] step time: 0.005518s (±0.00136s); valid time: 0.00228s; loss: -8.79703 (±8.93047); valid loss: -203.751\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkdsalh_g/variables.dat-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkdsalh_g/variables.dat-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.91,\n",
      "\t(tp, fp, tn, fn)=(98, 49, 3580, 305),\n",
      "\tprecision=0.67,\n",
      "\trecall=0.24,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.036458333333333336,\n",
      "\ty_label%=0.09995039682539683,\n",
      ")\n",
      "Testing on realAWSCloudwatch/ec2_network_in_5abac7.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 8/350, Step 100, ETA 45.35s] step time: 0.009652s (±0.03233s); valid time: 0.1548s; loss: 119.4 (±8.39543); valid loss: 152.083 (*)\n",
      "[Epoch 10/350, Step 130, ETA 40.63s] Learning rate decreased to 0.00075\n",
      "[Epoch 16/350, Step 200, ETA 36.7s] step time: 0.006181s (±0.007955s); valid time: 0.07891s; loss: 93.7057 (±7.1843); valid loss: 111.153 (*)\n",
      "[Epoch 20/350, Step 260, ETA 33.72s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 24/350, Step 300, ETA 33.55s] step time: 0.006224s (±0.008455s); valid time: 0.08481s; loss: 17.0375 (±31.6423); valid loss: 8.82367 (*)\n",
      "[Epoch 30/350, Step 390, ETA 30.85s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 400, ETA 31.43s] step time: 0.006084s (±0.007929s); valid time: 0.07926s; loss: -35.3661 (±12.8639); valid loss: -16.9912 (*)\n",
      "[Epoch 39/350, Step 500, ETA 29.99s] step time: 0.006227s (±0.008055s); valid time: 0.08029s; loss: -53.1488 (±13.4251); valid loss: -29.7143 (*)\n",
      "[Epoch 40/350, Step 520, ETA 29.6s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 47/350, Step 600, ETA 28.81s] step time: 0.006234s (±0.008925s); valid time: 0.08926s; loss: -65.628 (±15.5476); valid loss: -39.0078 (*)\n",
      "[Epoch 50/350, Step 650, ETA 28.06s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 54/350, Step 700, ETA 27.88s] step time: 0.00641s (±0.007998s); valid time: 0.08018s; loss: -72.0047 (±14.3459); valid loss: -44.6081 (*)\n",
      "[Epoch 60/350, Step 780, ETA 26.78s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 62/350, Step 800, ETA 26.89s] step time: 0.006173s (±0.007873s); valid time: 0.07871s; loss: -78.7631 (±15.8042); valid loss: -49.6881 (*)\n",
      "[Epoch 70/350, Step 900, ETA 26.06s] step time: 0.006393s (±0.008659s); valid time: 0.0866s; loss: -83.706 (±14.8957); valid loss: -51.9788 (*)\n",
      "[Epoch 70/350, Step 910, ETA 25.97s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 77/350, Step 1000, ETA 25.17s] step time: 0.006138s (±0.007853s); valid time: 0.07854s; loss: -87.6508 (±14.3359); valid loss: -58.0361 (*)\n",
      "[Epoch 80/350, Step 1040, ETA 24.7s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 85/350, Step 1100, ETA 24.1s] step time: 0.005412s (±0.001011s); valid time: 0.003536s; loss: -91.7556 (±16.5437); valid loss: -56.3448\n",
      "[Epoch 90/350, Step 1170, ETA 23.36s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 93/350, Step 1200, ETA 23.3s] step time: 0.006122s (±0.008466s); valid time: 0.0845s; loss: -94.7363 (±17.562); valid loss: -62.0758 (*)\n",
      "[Epoch 100/350, Step 1300, ETA 22.37s] step time: 0.00553s (±0.001033s); valid time: 0.003897s; loss: -97.2111 (±17.9131); valid loss: -60.257\n",
      "[Epoch 100/350, Step 1300, ETA 22.37s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 108/350, Step 1400, ETA 21.63s] step time: 0.006152s (±0.008032s); valid time: 0.08059s; loss: -100.494 (±17.1145); valid loss: -66.171 (*)\n",
      "[Epoch 110/350, Step 1430, ETA 21.36s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 116/350, Step 1500, ETA 20.76s] step time: 0.005415s (±0.0008224s); valid time: 0.003617s; loss: -100.717 (±18.2259); valid loss: -64.8391\n",
      "[Epoch 120/350, Step 1560, ETA 20.24s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 124/350, Step 1600, ETA 20.06s] step time: 0.006199s (±0.007814s); valid time: 0.07794s; loss: -101.964 (±19.2463); valid loss: -66.4094 (*)\n",
      "[Epoch 130/350, Step 1690, ETA 19.29s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 1700, ETA 19.36s] step time: 0.006184s (±0.008884s); valid time: 0.0888s; loss: -103.788 (±17.3214); valid loss: -69.3003 (*)\n",
      "[Epoch 139/350, Step 1800, ETA 18.54s] step time: 0.005412s (±0.001211s); valid time: 0.00356s; loss: -104.447 (±16.8819); valid loss: -68.6662\n",
      "[Epoch 140/350, Step 1820, ETA 18.37s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 147/350, Step 1900, ETA 17.85s] step time: 0.006119s (±0.007693s); valid time: 0.07672s; loss: -105.906 (±19.6034); valid loss: -70.3871 (*)\n",
      "[Epoch 150/350, Step 1950, ETA 17.44s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 154/350, Step 2000, ETA 17.15s] step time: 0.006054s (±0.007999s); valid time: 0.07966s; loss: -106.83 (±17.7171); valid loss: -70.9976 (*)\n",
      "[Epoch 160/350, Step 2080, ETA 16.54s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 162/350, Step 2100, ETA 16.38s] step time: 0.005403s (±0.001102s); valid time: 0.003576s; loss: -107.261 (±18.2864); valid loss: -70.9713\n",
      "[Epoch 170/350, Step 2200, ETA 15.73s] step time: 0.006342s (±0.008365s); valid time: 0.08362s; loss: -107.622 (±19.4372); valid loss: -71.9967 (*)\n",
      "[Epoch 170/350, Step 2210, ETA 15.65s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 177/350, Step 2300, ETA 15.04s] step time: 0.005949s (±0.007774s); valid time: 0.07805s; loss: -107.775 (±16.5999); valid loss: -72.2408 (*)\n",
      "[Epoch 180/350, Step 2340, ETA 14.75s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 185/350, Step 2400, ETA 14.39s] step time: 0.006357s (±0.008987s); valid time: 0.08919s; loss: -107.774 (±15.5764); valid loss: -72.3357 (*)\n",
      "[Epoch 190/350, Step 2470, ETA 13.88s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 193/350, Step 2500, ETA 13.67s] step time: 0.005555s (±0.001277s); valid time: 0.00379s; loss: -107.992 (±16.4292); valid loss: -72.0015\n",
      "[Epoch 200/350, Step 2600, ETA 12.95s] step time: 0.005486s (±0.0008666s); valid time: 0.003754s; loss: -108.496 (±19.2862); valid loss: -70.462\n",
      "[Epoch 200/350, Step 2600, ETA 12.95s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 208/350, Step 2700, ETA 12.29s] step time: 0.006115s (±0.008215s); valid time: 0.08215s; loss: -108.825 (±17.884); valid loss: -72.9944 (*)\n",
      "[Epoch 210/350, Step 2730, ETA 12.07s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 216/350, Step 2800, ETA 11.62s] step time: 0.006102s (±0.008674s); valid time: 0.08691s; loss: -109.018 (±18.111); valid loss: -73.7554 (*)\n",
      "[Epoch 220/350, Step 2860, ETA 11.2s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 224/350, Step 2900, ETA 10.92s] step time: 0.005387s (±0.0009409s); valid time: 0.003624s; loss: -107.673 (±15.5452); valid loss: -73.3107\n",
      "[Epoch 230/350, Step 2990, ETA 10.28s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 3000, ETA 10.22s] step time: 0.005396s (±0.001252s); valid time: 0.004768s; loss: -109.996 (±17.4517); valid loss: -72.5949\n",
      "[Epoch 239/350, Step 3100, ETA 9.524s] step time: 0.005357s (±0.0008029s); valid time: 0.003725s; loss: -109.083 (±19.9514); valid loss: -72.0194\n",
      "[Epoch 240/350, Step 3120, ETA 9.383s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 247/350, Step 3200, ETA 8.841s] step time: 0.005451s (±0.001008s); valid time: 0.003383s; loss: -108.442 (±17.3318); valid loss: -72.649\n",
      "[Epoch 250/350, Step 3250, ETA 8.497s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 254/350, Step 3300, ETA 8.184s] step time: 0.00603s (±0.007252s); valid time: 0.07262s; loss: -109.458 (±18.138); valid loss: -74.6383 (*)\n",
      "[Epoch 260/350, Step 3380, ETA 7.645s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 262/350, Step 3400, ETA 7.511s] step time: 0.005456s (±0.001125s); valid time: 0.003615s; loss: -109.827 (±17.0751); valid loss: -71.2976\n",
      "[Epoch 270/350, Step 3500, ETA 6.839s] step time: 0.005423s (±0.001117s); valid time: 0.0037s; loss: -109.782 (±17.86); valid loss: -73.5565\n",
      "[Epoch 270/350, Step 3510, ETA 6.773s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 277/350, Step 3600, ETA 6.19s] step time: 0.006104s (±0.008143s); valid time: 0.08101s; loss: -108.647 (±16.3105); valid loss: -74.9944 (*)\n",
      "[Epoch 280/350, Step 3640, ETA 5.923s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 285/350, Step 3700, ETA 5.526s] step time: 0.00545s (±0.001295s); valid time: 0.003716s; loss: -108.463 (±16.0705); valid loss: -74.3617\n",
      "[Epoch 290/350, Step 3770, ETA 5.059s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 293/350, Step 3800, ETA 4.862s] step time: 0.005322s (±0.0009474s); valid time: 0.005821s; loss: -110.157 (±17.1187); valid loss: -74.5088\n",
      "[Epoch 300/350, Step 3900, ETA 4.217s] step time: 0.006218s (±0.007937s); valid time: 0.07975s; loss: -109.181 (±18.1268); valid loss: -75.4693 (*)\n",
      "[Epoch 300/350, Step 3900, ETA 4.218s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 308/350, Step 4000, ETA 3.561s] step time: 0.005382s (±0.001206s); valid time: 0.003581s; loss: -109.184 (±18.7955); valid loss: -74.3847\n",
      "[Epoch 310/350, Step 4030, ETA 3.364s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 316/350, Step 4100, ETA 2.906s] step time: 0.005319s (±0.0007567s); valid time: 0.004001s; loss: -109.781 (±19.1562); valid loss: -72.8037\n",
      "[Epoch 320/350, Step 4160, ETA 2.517s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 324/350, Step 4200, ETA 2.258s] step time: 0.005669s (±0.001466s); valid time: 0.003487s; loss: -109.758 (±18.3899); valid loss: -74.9226\n",
      "[Epoch 330/350, Step 4290, ETA 1.674s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 4300, ETA 1.615s] step time: 0.006251s (±0.008566s); valid time: 0.08553s; loss: -109.119 (±16.372); valid loss: -75.633 (*)\n",
      "[Epoch 339/350, Step 4400, ETA 0.9664s] step time: 0.005269s (±0.0009254s); valid time: 0.003565s; loss: -109.542 (±17.5156); valid loss: -75.5133\n",
      "[Epoch 340/350, Step 4420, ETA 0.8371s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 347/350, Step 4500, ETA 0.3216s] step time: 0.005409s (±0.0009513s); valid time: 0.003593s; loss: -110.265 (±17.0078); valid loss: -74.8613\n",
      "[Epoch 350/350, Step 4550, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpb12h996u/variables.dat-4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpb12h996u/variables.dat-4300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.64,\n",
      "\t(tp, fp, tn, fn)=(311, 1546, 2710, 163),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.66,\n",
      "\tf1=0.27,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.39260042283298097,\n",
      "\ty_label%=0.10021141649048626,\n",
      ")\n",
      "Testing on realAWSCloudwatch/elb_request_count_8c0756.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.36s] step time: 0.009693s (±0.03181s); valid time: 0.1628s; loss: 139.652 (±7.86515); valid loss: 104.561 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.74s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.96s] step time: 0.006189s (±0.007964s); valid time: 0.07871s; loss: 128.919 (±2.69021); valid loss: 102.195 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.92s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 28.09s] step time: 0.006262s (±0.00789s); valid time: 0.07914s; loss: 127.181 (±2.22872); valid loss: 101.713 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.05s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.08s] step time: 0.006006s (±0.007512s); valid time: 0.07508s; loss: 126.889 (±2.39103); valid loss: 101.547 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.16s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.66s] step time: 0.006062s (±0.007834s); valid time: 0.07718s; loss: 126.742 (±2.40145); valid loss: 101.512 (*)\n",
      "[Epoch 50/350, Step 550, ETA 23.97s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.69s] step time: 0.006376s (±0.007755s); valid time: 0.07666s; loss: 126.477 (±2.36253); valid loss: 101.415 (*)\n",
      "[Epoch 60/350, Step 660, ETA 22.83s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.72s] step time: 0.006248s (±0.008418s); valid time: 0.0842s; loss: 126.616 (±2.68905); valid loss: 101.339 (*)\n",
      "[Epoch 70/350, Step 770, ETA 21.8s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 21.43s] step time: 0.005214s (±0.0005834s); valid time: 0.002187s; loss: 126.389 (±2.44586); valid loss: 101.363\n",
      "[Epoch 80/350, Step 880, ETA 20.64s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 20.67s] step time: 0.00635s (±0.0076s); valid time: 0.07536s; loss: 126.315 (±2.33217); valid loss: 101.258 (*)\n",
      "[Epoch 90/350, Step 990, ETA 19.68s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.82s] step time: 0.005897s (±0.008198s); valid time: 0.08212s; loss: 126.238 (±2.32685); valid loss: 101.256 (*)\n",
      "[Epoch 100/350, Step 1100, ETA 18.83s] step time: 0.005281s (±0.0005753s); valid time: 0.002133s; loss: 126.337 (±2.46944); valid loss: 101.322\n",
      "[Epoch 100/350, Step 1100, ETA 18.83s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 18.12s] step time: 0.006213s (±0.007793s); valid time: 0.07638s; loss: 126.303 (±2.20795); valid loss: 101.253 (*)\n",
      "[Epoch 110/350, Step 1210, ETA 18.02s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 17.38s] step time: 0.006099s (±0.008418s); valid time: 0.08416s; loss: 126.237 (±2.2401); valid loss: 101.151 (*)\n",
      "[Epoch 120/350, Step 1320, ETA 17.23s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 16.54s] step time: 0.00531s (±0.000757s); valid time: 0.002371s; loss: 126.477 (±2.6044); valid loss: 101.195\n",
      "[Epoch 130/350, Step 1430, ETA 16.28s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.73s] step time: 0.00541s (±0.001106s); valid time: 0.002165s; loss: 126.233 (±2.63787); valid loss: 101.209\n",
      "[Epoch 140/350, Step 1540, ETA 15.41s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.94s] step time: 0.0053s (±0.0008675s); valid time: 0.00214s; loss: 126.35 (±2.37771); valid loss: 101.181\n",
      "[Epoch 150/350, Step 1650, ETA 14.54s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 14.17s] step time: 0.00533s (±0.001203s); valid time: 0.002083s; loss: 126.283 (±2.67333); valid loss: 101.21\n",
      "[Epoch 160/350, Step 1760, ETA 13.72s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.44s] step time: 0.005438s (±0.001158s); valid time: 0.002031s; loss: 126.337 (±2.56782); valid loss: 101.158\n",
      "[Epoch 170/350, Step 1870, ETA 12.94s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.72s] step time: 0.005415s (±0.001052s); valid time: 0.002073s; loss: 126.302 (±2.21088); valid loss: 101.186\n",
      "[Epoch 180/350, Step 1980, ETA 12.14s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 12s] step time: 0.0053s (±0.0009313s); valid time: 0.002203s; loss: 126.289 (±2.4596); valid loss: 101.183\n",
      "[Epoch 190/350, Step 2090, ETA 11.37s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 11.36s] step time: 0.006099s (±0.007716s); valid time: 0.07721s; loss: 126.298 (±2.95053); valid loss: 101.127 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.74s] step time: 0.00636s (±0.007739s); valid time: 0.07651s; loss: 126.368 (±2.25263); valid loss: 101.09 (*)\n",
      "[Epoch 200/350, Step 2200, ETA 10.74s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 10.05s] step time: 0.005311s (±0.001158s); valid time: 0.002064s; loss: 126.241 (±2.49432); valid loss: 101.097\n",
      "[Epoch 210/350, Step 2310, ETA 9.974s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.359s] step time: 0.005331s (±0.001023s); valid time: 0.002198s; loss: 126.322 (±2.46979); valid loss: 101.198\n",
      "[Epoch 220/350, Step 2420, ETA 9.22s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.674s] step time: 0.005224s (±0.0008205s); valid time: 0.002128s; loss: 126.431 (±2.58033); valid loss: 101.103\n",
      "[Epoch 230/350, Step 2530, ETA 8.474s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 8.003s] step time: 0.005288s (±0.0008044s); valid time: 0.002172s; loss: 126.215 (±2.29467); valid loss: 101.197\n",
      "[Epoch 240/350, Step 2640, ETA 7.738s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.347s] step time: 0.005523s (±0.001186s); valid time: 0.002158s; loss: 126.382 (±2.40319); valid loss: 101.239\n",
      "[Epoch 250/350, Step 2750, ETA 7.016s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.69s] step time: 0.005372s (±0.0006849s); valid time: 0.002162s; loss: 126.453 (±2.29523); valid loss: 101.159\n",
      "[Epoch 260/350, Step 2860, ETA 6.301s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 6.042s] step time: 0.005529s (±0.001196s); valid time: 0.00224s; loss: 126.203 (±2.35899); valid loss: 101.162\n",
      "[Epoch 270/350, Step 2970, ETA 5.584s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.392s] step time: 0.005384s (±0.001232s); valid time: 0.002302s; loss: 126.333 (±2.4635); valid loss: 101.22\n",
      "[Epoch 280/350, Step 3080, ETA 4.876s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.747s] step time: 0.005381s (±0.0008895s); valid time: 0.002037s; loss: 126.184 (±2.44255); valid loss: 101.18\n",
      "[Epoch 290/350, Step 3190, ETA 4.167s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 4.105s] step time: 0.00539s (±0.0009495s); valid time: 0.002301s; loss: 126.413 (±2.44292); valid loss: 101.128\n",
      "[Epoch 300/350, Step 3300, ETA 3.466s] step time: 0.00537s (±0.0005993s); valid time: 0.002205s; loss: 126.311 (±2.42093); valid loss: 101.134\n",
      "[Epoch 300/350, Step 3300, ETA 3.466s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.832s] step time: 0.005456s (±0.000966s); valid time: 0.002206s; loss: 126.272 (±2.73928); valid loss: 101.179\n",
      "[Epoch 310/350, Step 3410, ETA 2.768s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.198s] step time: 0.005281s (±0.00092s); valid time: 0.002162s; loss: 126.254 (±2.46217); valid loss: 101.208\n",
      "[Epoch 320/350, Step 3520, ETA 2.072s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.568s] step time: 0.005471s (±0.001006s); valid time: 0.002203s; loss: 126.337 (±2.15475); valid loss: 101.194\n",
      "[Epoch 330/350, Step 3630, ETA 1.379s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.939s] step time: 0.005361s (±0.0009035s); valid time: 0.004544s; loss: 126.372 (±2.19866); valid loss: 101.158\n",
      "[Epoch 340/350, Step 3740, ETA 0.6887s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3127s] step time: 0.005543s (±0.0014s); valid time: 0.002186s; loss: 126.219 (±2.48615); valid loss: 101.18\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmphjr225bk/variables.dat-2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmphjr225bk/variables.dat-2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.51,\n",
      "\t(tp, fp, tn, fn)=(255, 1828, 1802, 147),\n",
      "\tprecision=0.12,\n",
      "\trecall=0.63,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.5166170634920635,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/grok_asg_anomaly.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 8/350, Step 100, ETA 45.82s] step time: 0.009795s (±0.03239s); valid time: 0.158s; loss: 27.992 (±34.3771); valid loss: 37305.2 (*)\n",
      "[Epoch 10/350, Step 130, ETA 40.82s] Learning rate decreased to 0.00075\n",
      "[Epoch 16/350, Step 200, ETA 35.06s] step time: 0.005329s (±0.0008368s); valid time: 0.003675s; loss: -55.2628 (±5.85997); valid loss: 72589.2\n",
      "[Epoch 20/350, Step 260, ETA 32.48s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 24/350, Step 300, ETA 31.36s] step time: 0.005501s (±0.001041s); valid time: 0.003616s; loss: -60.869 (±2.08507); valid loss: 81850.2\n",
      "[Epoch 30/350, Step 390, ETA 29.02s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 400, ETA 29.04s] step time: 0.005386s (±0.001288s); valid time: 0.00368s; loss: -62.2521 (±1.79696); valid loss: 63702.2\n",
      "[Epoch 39/350, Step 500, ETA 27.39s] step time: 0.005354s (±0.00073s); valid time: 0.003489s; loss: -62.8691 (±1.57572); valid loss: 90565.5\n",
      "[Epoch 40/350, Step 520, ETA 27.09s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 47/350, Step 600, ETA 26.14s] step time: 0.005397s (±0.0011s); valid time: 0.003274s; loss: -63.4622 (±1.52539); valid loss: 88158.9\n",
      "[Epoch 50/350, Step 650, ETA 25.55s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 54/350, Step 700, ETA 25.02s] step time: 0.005322s (±0.0007028s); valid time: 0.003509s; loss: -63.6561 (±1.72432); valid loss: 82408.1\n",
      "[Epoch 60/350, Step 780, ETA 24.32s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 62/350, Step 800, ETA 24.16s] step time: 0.005538s (±0.001096s); valid time: 0.003711s; loss: -63.8588 (±1.4732); valid loss: 90830.6\n",
      "[Epoch 70/350, Step 900, ETA 23.25s] step time: 0.005314s (±0.0007468s); valid time: 0.003645s; loss: -63.9646 (±1.80449); valid loss: 71237.9\n",
      "[Epoch 70/350, Step 910, ETA 23.16s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 77/350, Step 1000, ETA 22.44s] step time: 0.005367s (±0.000873s); valid time: 0.003391s; loss: -63.9867 (±1.72325); valid loss: 103355\n",
      "[Epoch 80/350, Step 1040, ETA 22.11s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 85/350, Step 1100, ETA 21.68s] step time: 0.005405s (±0.001007s); valid time: 0.003893s; loss: -64.1463 (±1.85428); valid loss: 127137\n",
      "[Epoch 90/350, Step 1170, ETA 21.17s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 93/350, Step 1200, ETA 20.95s] step time: 0.005407s (±0.001197s); valid time: 0.003374s; loss: -64.1518 (±1.8664); valid loss: 89022.3\n",
      "[Epoch 100/350, Step 1300, ETA 20.23s] step time: 0.00542s (±0.001075s); valid time: 0.003861s; loss: -64.367 (±1.88417); valid loss: 84556.2\n",
      "[Epoch 100/350, Step 1300, ETA 20.23s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 108/350, Step 1400, ETA 19.52s] step time: 0.005346s (±0.0007714s); valid time: 0.004061s; loss: -64.373 (±2.02148); valid loss: 90169.5\n",
      "[Epoch 110/350, Step 1430, ETA 19.34s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 116/350, Step 1500, ETA 18.88s] step time: 0.005584s (±0.001098s); valid time: 0.003662s; loss: -64.345 (±1.58329); valid loss: 97088.7\n",
      "[Epoch 120/350, Step 1560, ETA 18.45s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 124/350, Step 1600, ETA 18.22s] step time: 0.005457s (±0.001125s); valid time: 0.003609s; loss: -64.3814 (±1.84413); valid loss: 76761.3\n",
      "[Epoch 130/350, Step 1690, ETA 17.61s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 1700, ETA 17.55s] step time: 0.005394s (±0.0007903s); valid time: 0.004312s; loss: -64.4282 (±1.75916); valid loss: 115823\n",
      "[Epoch 139/350, Step 1800, ETA 16.89s] step time: 0.005399s (±0.00103s); valid time: 0.003319s; loss: -64.4438 (±1.61124); valid loss: 103833\n",
      "[Epoch 140/350, Step 1820, ETA 16.76s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 147/350, Step 1900, ETA 16.25s] step time: 0.005399s (±0.001091s); valid time: 0.003541s; loss: -64.4408 (±1.61252); valid loss: 108841\n",
      "[Epoch 150/350, Step 1950, ETA 15.94s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 154/350, Step 2000, ETA 15.61s] step time: 0.005494s (±0.001094s); valid time: 0.003386s; loss: -64.4238 (±1.77651); valid loss: 140462\n",
      "[Epoch 160/350, Step 2080, ETA 15.09s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 162/350, Step 2100, ETA 14.97s] step time: 0.005322s (±0.0007557s); valid time: 0.00354s; loss: -64.5152 (±1.85498); valid loss: 155385\n",
      "[Epoch 170/350, Step 2200, ETA 14.34s] step time: 0.005436s (±0.001129s); valid time: 0.003779s; loss: -64.094 (±4.44089); valid loss: 242617\n",
      "[Epoch 170/350, Step 2210, ETA 14.27s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 177/350, Step 2300, ETA 13.73s] step time: 0.005616s (±0.001167s); valid time: 0.003481s; loss: -64.4636 (±1.81648); valid loss: 100218\n",
      "[Epoch 180/350, Step 2340, ETA 13.48s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 185/350, Step 2400, ETA 13.1s] step time: 0.005391s (±0.0009847s); valid time: 0.003531s; loss: -64.4571 (±1.75203); valid loss: 130505\n",
      "[Epoch 190/350, Step 2470, ETA 12.66s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 193/350, Step 2500, ETA 12.48s] step time: 0.005424s (±0.001065s); valid time: 0.003869s; loss: -64.4457 (±1.85424); valid loss: 112235\n",
      "[Epoch 200/350, Step 2600, ETA 11.84s] step time: 0.005233s (±0.0006264s); valid time: 0.00343s; loss: -64.4821 (±1.51651); valid loss: 99904.1\n",
      "[Epoch 200/350, Step 2600, ETA 11.84s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 208/350, Step 2700, ETA 11.22s] step time: 0.005442s (±0.001123s); valid time: 0.003654s; loss: -64.4259 (±1.75173); valid loss: 90063.8\n",
      "[Epoch 210/350, Step 2730, ETA 11.04s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 216/350, Step 2800, ETA 10.61s] step time: 0.005536s (±0.001071s); valid time: 0.004503s; loss: -64.5681 (±1.71642); valid loss: 135453\n",
      "[Epoch 220/350, Step 2860, ETA 10.24s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 224/350, Step 2900, ETA 9.998s] step time: 0.005373s (±0.0009912s); valid time: 0.003695s; loss: -64.4615 (±1.62794); valid loss: 115685\n",
      "[Epoch 230/350, Step 2990, ETA 9.453s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 3000, ETA 9.397s] step time: 0.005702s (±0.001688s); valid time: 0.003411s; loss: -64.4373 (±1.71441); valid loss: 96726.9\n",
      "[Epoch 239/350, Step 3100, ETA 8.775s] step time: 0.005251s (±0.0007461s); valid time: 0.003675s; loss: -64.4958 (±1.78962); valid loss: 118751\n",
      "[Epoch 240/350, Step 3120, ETA 8.649s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 247/350, Step 3200, ETA 8.16s] step time: 0.005325s (±0.0009955s); valid time: 0.003502s; loss: -64.5451 (±1.70411); valid loss: 114376\n",
      "[Epoch 250/350, Step 3250, ETA 7.849s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 254/350, Step 3300, ETA 7.542s] step time: 0.005202s (±0.0007989s); valid time: 0.003425s; loss: -64.4904 (±1.62671); valid loss: 94833.7\n",
      "[Epoch 260/350, Step 3380, ETA 7.056s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 262/350, Step 3400, ETA 6.935s] step time: 0.005423s (±0.001001s); valid time: 0.003419s; loss: -64.4987 (±1.57992); valid loss: 134114\n",
      "[Epoch 270/350, Step 3500, ETA 6.328s] step time: 0.005393s (±0.00104s); valid time: 0.003777s; loss: -64.4494 (±1.65927); valid loss: 110105\n",
      "[Epoch 270/350, Step 3510, ETA 6.266s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 277/350, Step 3600, ETA 5.721s] step time: 0.00536s (±0.0006837s); valid time: 0.003381s; loss: -64.5459 (±1.93688); valid loss: 161447\n",
      "[Epoch 280/350, Step 3640, ETA 5.477s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 285/350, Step 3700, ETA 5.117s] step time: 0.005452s (±0.001098s); valid time: 0.003521s; loss: -64.498 (±1.80418); valid loss: 116104\n",
      "[Epoch 290/350, Step 3770, ETA 4.698s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 293/350, Step 3800, ETA 4.516s] step time: 0.005602s (±0.001103s); valid time: 0.003568s; loss: -64.4516 (±1.72427); valid loss: 102981\n",
      "[Epoch 300/350, Step 3900, ETA 3.912s] step time: 0.005408s (±0.001067s); valid time: 0.003496s; loss: -64.488 (±1.65577); valid loss: 84815.4\n",
      "[Epoch 300/350, Step 3900, ETA 3.912s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 308/350, Step 4000, ETA 3.307s] step time: 0.005294s (±0.0007698s); valid time: 0.003477s; loss: -64.4951 (±1.93723); valid loss: 112698\n",
      "[Epoch 310/350, Step 4030, ETA 3.127s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 316/350, Step 4100, ETA 2.707s] step time: 0.005602s (±0.001046s); valid time: 0.003674s; loss: -64.4804 (±1.93239); valid loss: 116710\n",
      "[Epoch 320/350, Step 4160, ETA 2.344s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 324/350, Step 4200, ETA 2.105s] step time: 0.005443s (±0.001319s); valid time: 0.003845s; loss: -64.4752 (±2.02686); valid loss: 121590\n",
      "[Epoch 330/350, Step 4290, ETA 1.562s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 4300, ETA 1.502s] step time: 0.005354s (±0.0009491s); valid time: 0.003654s; loss: -64.4342 (±1.84377); valid loss: 98214.7\n",
      "[Epoch 339/350, Step 4400, ETA 0.9009s] step time: 0.005399s (±0.001159s); valid time: 0.003525s; loss: -64.4695 (±1.60485); valid loss: 206855\n",
      "[Epoch 340/350, Step 4420, ETA 0.7805s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 347/350, Step 4500, ETA 0.3001s] step time: 0.005287s (±0.0008147s); valid time: 0.00375s; loss: -64.4823 (±1.71884); valid loss: 125412\n",
      "[Epoch 350/350, Step 4550, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkzfa9ofg/variables.dat-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkzfa9ofg/variables.dat-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.14,\n",
      "\t(tp, fp, tn, fn)=(449, 3935, 221, 16),\n",
      "\tprecision=0.1,\n",
      "\trecall=0.97,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9487123999134387,\n",
      "\ty_label%=0.10062756979008873,\n",
      ")\n",
      "Testing on realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 30, ETA 15.05s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 60, ETA 10.14s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 30/350, Step 90, ETA 8.397s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 34/350, Step 100, ETA 9.626s] step time: 0.009539s (±0.03171s); valid time: 0.1678s; loss: 94.2092 (±13.8423); valid loss: 68.8871 (*)\n",
      "[Epoch 40/350, Step 120, ETA 8.759s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 150, ETA 7.855s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 180, ETA 7.15s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 67/350, Step 200, ETA 7.14s] step time: 0.006108s (±0.007448s); valid time: 0.07371s; loss: 63.4593 (±5.4267); valid loss: 63.9973 (*)\n",
      "[Epoch 70/350, Step 210, ETA 6.942s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 80/350, Step 240, ETA 6.453s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 270, ETA 6.041s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 300, ETA 5.869s] step time: 0.006098s (±0.007529s); valid time: 0.07499s; loss: 51.5902 (±3.16311); valid loss: 59.0155 (*)\n",
      "[Epoch 100/350, Step 300, ETA 5.869s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 330, ETA 5.499s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 360, ETA 5.172s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 130/350, Step 390, ETA 4.864s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 134/350, Step 400, ETA 4.906s] step time: 0.00613s (±0.008297s); valid time: 0.08315s; loss: 46.7494 (±2.29748); valid loss: 57.8871 (*)\n",
      "[Epoch 140/350, Step 420, ETA 4.708s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 450, ETA 4.427s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 480, ETA 4.149s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 167/350, Step 500, ETA 4.061s] step time: 0.006155s (±0.007518s); valid time: 0.07529s; loss: 44.8722 (±2.55274); valid loss: 56.0374 (*)\n",
      "[Epoch 170/350, Step 510, ETA 3.972s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 180/350, Step 540, ETA 3.713s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 570, ETA 3.463s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 600, ETA 3.215s] step time: 0.005352s (±0.0009201s); valid time: 0.001499s; loss: 44.1785 (±2.54886); valid loss: 57.0238\n",
      "[Epoch 200/350, Step 600, ETA 3.216s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 630, ETA 2.974s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 660, ETA 2.739s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 230/350, Step 690, ETA 2.512s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 234/350, Step 700, ETA 2.478s] step time: 0.006102s (±0.007608s); valid time: 0.07517s; loss: 43.8934 (±2.56166); valid loss: 55.2576 (*)\n",
      "[Epoch 240/350, Step 720, ETA 2.328s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 750, ETA 2.109s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 780, ETA 1.884s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 267/350, Step 800, ETA 1.739s] step time: 0.005483s (±0.001299s); valid time: 0.001475s; loss: 43.8838 (±2.27322); valid loss: 55.5849\n",
      "[Epoch 270/350, Step 810, ETA 1.667s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 280/350, Step 840, ETA 1.451s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 870, ETA 1.238s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 900, ETA 1.029s] step time: 0.005483s (±0.001044s); valid time: 0.001568s; loss: 43.5421 (±2.46297); valid loss: 55.8847\n",
      "[Epoch 300/350, Step 900, ETA 1.029s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 930, ETA 0.8194s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 960, ETA 0.6119s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 330/350, Step 990, ETA 0.4059s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 334/350, Step 1000, ETA 0.3381s] step time: 0.005319s (±0.001028s); valid time: 0.001503s; loss: 43.6934 (±2.34555); valid loss: 56.8478\n",
      "[Epoch 340/350, Step 1020, ETA 0.2024s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1050, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmptc63a0n3/variables.dat-700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmptc63a0n3/variables.dat-700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.81,\n",
      "\t(tp, fp, tn, fn)=(78, 189, 928, 48),\n",
      "\tprecision=0.29,\n",
      "\trecall=0.62,\n",
      "\tf1=0.4,\n",
      "\troc_auc=0.72,\n",
      "\ty_pred%=0.21480289621882542,\n",
      "\ty_label%=0.10136765888978279,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 38.79s] step time: 0.009803s (±0.03171s); valid time: 0.1521s; loss: 29.6288 (±39.0206); valid loss: 63726.7 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.08s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 29.27s] step time: 0.005174s (±0.0003992s); valid time: 0.002127s; loss: -70.2889 (±8.00572); valid loss: 431690\n",
      "[Epoch 20/350, Step 220, ETA 28.46s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 25.97s] step time: 0.005411s (±0.001402s); valid time: 0.002116s; loss: -78.8291 (±1.0302); valid loss: 502086\n",
      "[Epoch 30/350, Step 330, ETA 25.17s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 23.97s] step time: 0.005319s (±0.0009951s); valid time: 0.00218s; loss: -80.0986 (±0.971168); valid loss: 581063\n",
      "[Epoch 40/350, Step 440, ETA 23.37s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 22.57s] step time: 0.005369s (±0.0008484s); valid time: 0.002154s; loss: -80.7575 (±0.827499); valid loss: 551576\n",
      "[Epoch 50/350, Step 550, ETA 21.96s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 21.44s] step time: 0.005394s (±0.001187s); valid time: 0.00207s; loss: -81.0735 (±0.842028); valid loss: 540446\n",
      "[Epoch 60/350, Step 660, ETA 20.79s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 20.44s] step time: 0.005353s (±0.001142s); valid time: 0.002218s; loss: -81.281 (±0.842388); valid loss: 455498\n",
      "[Epoch 70/350, Step 770, ETA 19.8s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 19.55s] step time: 0.005364s (±0.00116s); valid time: 0.002105s; loss: -81.4836 (±0.874144); valid loss: 504922\n",
      "[Epoch 80/350, Step 880, ETA 18.89s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 18.74s] step time: 0.005386s (±0.0006438s); valid time: 0.00231s; loss: -81.6596 (±0.918084); valid loss: 483679\n",
      "[Epoch 90/350, Step 990, ETA 18.02s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 17.95s] step time: 0.005259s (±0.0007675s); valid time: 0.002274s; loss: -81.7493 (±0.949857); valid loss: 498889\n",
      "[Epoch 100/350, Step 1100, ETA 17.25s] step time: 0.005508s (±0.001099s); valid time: 0.002145s; loss: -81.8252 (±0.936774); valid loss: 471562\n",
      "[Epoch 100/350, Step 1100, ETA 17.25s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.53s] step time: 0.005308s (±0.000772s); valid time: 0.002406s; loss: -81.9051 (±0.869911); valid loss: 464995\n",
      "[Epoch 110/350, Step 1210, ETA 16.45s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 15.86s] step time: 0.005479s (±0.001281s); valid time: 0.002161s; loss: -81.9208 (±0.835545); valid loss: 477555\n",
      "[Epoch 120/350, Step 1320, ETA 15.7s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.15s] step time: 0.005215s (±0.0006848s); valid time: 0.002115s; loss: -81.9687 (±0.855478); valid loss: 496147\n",
      "[Epoch 130/350, Step 1430, ETA 14.94s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.48s] step time: 0.00534s (±0.0008279s); valid time: 0.002102s; loss: -81.9509 (±0.921253); valid loss: 474894\n",
      "[Epoch 140/350, Step 1540, ETA 14.21s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 13.84s] step time: 0.0055s (±0.001392s); valid time: 0.003057s; loss: -81.9682 (±0.832408); valid loss: 473444\n",
      "[Epoch 150/350, Step 1650, ETA 13.5s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.18s] step time: 0.005325s (±0.0007109s); valid time: 0.00214s; loss: -82.0262 (±0.922247); valid loss: 477418\n",
      "[Epoch 160/350, Step 1760, ETA 12.8s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.55s] step time: 0.005463s (±0.001071s); valid time: 0.002102s; loss: -81.9566 (±0.92742); valid loss: 469524\n",
      "[Epoch 170/350, Step 1870, ETA 12.1s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 11.91s] step time: 0.005276s (±0.0006844s); valid time: 0.002094s; loss: -82.0665 (±0.880396); valid loss: 481103\n",
      "[Epoch 180/350, Step 1980, ETA 11.4s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.28s] step time: 0.005467s (±0.001139s); valid time: 0.002035s; loss: -82.0173 (±1.14114); valid loss: 475869\n",
      "[Epoch 190/350, Step 2090, ETA 10.72s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.66s] step time: 0.005397s (±0.001017s); valid time: 0.002106s; loss: -82.0533 (±0.867217); valid loss: 473238\n",
      "[Epoch 200/350, Step 2200, ETA 10.03s] step time: 0.005386s (±0.00101s); valid time: 0.002187s; loss: -82.0256 (±0.911573); valid loss: 498091\n",
      "[Epoch 200/350, Step 2200, ETA 10.03s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.424s] step time: 0.005523s (±0.001197s); valid time: 0.002365s; loss: -82.0585 (±0.891033); valid loss: 491474\n",
      "[Epoch 210/350, Step 2310, ETA 9.361s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.799s] step time: 0.005281s (±0.0007296s); valid time: 0.002143s; loss: -82.0755 (±0.915843); valid loss: 494645\n",
      "[Epoch 220/350, Step 2420, ETA 8.674s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.184s] step time: 0.005408s (±0.001026s); valid time: 0.002236s; loss: -82.0834 (±0.750044); valid loss: 481378\n",
      "[Epoch 230/350, Step 2530, ETA 7.996s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.571s] step time: 0.00541s (±0.001187s); valid time: 0.002076s; loss: -82.0366 (±0.828611); valid loss: 489100\n",
      "[Epoch 240/350, Step 2640, ETA 7.327s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 6.955s] step time: 0.005322s (±0.001025s); valid time: 0.002779s; loss: -81.9947 (±0.945471); valid loss: 481960\n",
      "[Epoch 250/350, Step 2750, ETA 6.648s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.343s] step time: 0.005368s (±0.0009396s); valid time: 0.002203s; loss: -82.0919 (±0.857402); valid loss: 466893\n",
      "[Epoch 260/350, Step 2860, ETA 5.98s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.742s] step time: 0.005608s (±0.001292s); valid time: 0.00218s; loss: -82.0776 (±0.852262); valid loss: 483927\n",
      "[Epoch 270/350, Step 2970, ETA 5.317s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.143s] step time: 0.005747s (±0.001984s); valid time: 0.003121s; loss: -82.1015 (±1.02087); valid loss: 495413\n",
      "[Epoch 280/350, Step 3080, ETA 4.662s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.539s] step time: 0.005597s (±0.001442s); valid time: 0.002077s; loss: -82.0565 (±0.914921); valid loss: 475655\n",
      "[Epoch 290/350, Step 3190, ETA 3.989s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.929s] step time: 0.005321s (±0.0008755s); valid time: 0.002142s; loss: -82.0994 (±1.06423); valid loss: 486505\n",
      "[Epoch 300/350, Step 3300, ETA 3.318s] step time: 0.005191s (±0.000785s); valid time: 0.002287s; loss: -82.038 (±0.797556); valid loss: 498019\n",
      "[Epoch 300/350, Step 3300, ETA 3.318s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.714s] step time: 0.00539s (±0.001069s); valid time: 0.002075s; loss: -82.0401 (±0.904541); valid loss: 482165\n",
      "[Epoch 310/350, Step 3410, ETA 2.653s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.109s] step time: 0.005397s (±0.001023s); valid time: 0.002276s; loss: -82.0367 (±0.853515); valid loss: 489679\n",
      "[Epoch 320/350, Step 3520, ETA 1.988s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.505s] step time: 0.005258s (±0.0009611s); valid time: 0.002234s; loss: -82.1049 (±1.06281); valid loss: 508852\n",
      "[Epoch 330/350, Step 3630, ETA 1.323s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9018s] step time: 0.005289s (±0.0009197s); valid time: 0.002126s; loss: -82.0364 (±1.01768); valid loss: 469192\n",
      "[Epoch 340/350, Step 3740, ETA 0.6612s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3005s] step time: 0.005422s (±0.001197s); valid time: 0.002073s; loss: -82.0629 (±0.876665); valid loss: 465972\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpq60hpxcs/variables.dat-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpq60hpxcs/variables.dat-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.8,\n",
      "\t(tp, fp, tn, fn)=(304, 690, 2940, 98),\n",
      "\tprecision=0.31,\n",
      "\trecall=0.76,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.2465277777777778,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 39.4s] step time: 0.009965s (±0.03249s); valid time: 0.1573s; loss: 76.7471 (±27.6485); valid loss: 113.744 (*)\n",
      "[Epoch 10/350, Step 110, ETA 37.65s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 31.69s] step time: 0.006347s (±0.009667s); valid time: 0.09678s; loss: -7.70291 (±13.6745); valid loss: 65.2281 (*)\n",
      "[Epoch 20/350, Step 220, ETA 30.6s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 28.65s] step time: 0.006329s (±0.007821s); valid time: 0.07861s; loss: -29.5459 (±4.26023); valid loss: 37.9838 (*)\n",
      "[Epoch 30/350, Step 330, ETA 27.81s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 26.76s] step time: 0.006297s (±0.007667s); valid time: 0.07605s; loss: -40.5495 (±3.66737); valid loss: 34.0702 (*)\n",
      "[Epoch 40/350, Step 440, ETA 25.83s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 24.78s] step time: 0.005438s (±0.0009881s); valid time: 0.002246s; loss: -49.3636 (±3.0038); valid loss: 44.2661\n",
      "[Epoch 50/350, Step 550, ETA 24.04s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 23.32s] step time: 0.005546s (±0.001415s); valid time: 0.002141s; loss: -55.9225 (±2.83717); valid loss: 56.7403\n",
      "[Epoch 60/350, Step 660, ETA 22.55s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 22.09s] step time: 0.005487s (±0.001128s); valid time: 0.002329s; loss: -60.7146 (±2.43303); valid loss: 82.4536\n",
      "[Epoch 70/350, Step 770, ETA 21.24s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 20.91s] step time: 0.005248s (±0.0005303s); valid time: 0.002087s; loss: -63.3774 (±2.21376); valid loss: 101.547\n",
      "[Epoch 80/350, Step 880, ETA 20.13s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.93s] step time: 0.005466s (±0.0009887s); valid time: 0.002241s; loss: -64.927 (±2.21636); valid loss: 99.2557\n",
      "[Epoch 90/350, Step 990, ETA 19.12s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 19.04s] step time: 0.005489s (±0.001074s); valid time: 0.002316s; loss: -65.794 (±2.04273); valid loss: 86.1664\n",
      "[Epoch 100/350, Step 1100, ETA 18.18s] step time: 0.00541s (±0.0007681s); valid time: 0.002215s; loss: -66.6793 (±2.17769); valid loss: 108.929\n",
      "[Epoch 100/350, Step 1100, ETA 18.18s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 17.43s] step time: 0.005669s (±0.00134s); valid time: 0.006025s; loss: -66.9758 (±1.90446); valid loss: 90.5711\n",
      "[Epoch 110/350, Step 1210, ETA 17.35s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.61s] step time: 0.00522s (±0.0006081s); valid time: 0.002115s; loss: -67.3201 (±2.08588); valid loss: 91.7903\n",
      "[Epoch 120/350, Step 1320, ETA 16.44s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.85s] step time: 0.005394s (±0.001037s); valid time: 0.002079s; loss: -67.5525 (±2.23023); valid loss: 102.386\n",
      "[Epoch 130/350, Step 1430, ETA 15.62s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 15.13s] step time: 0.005496s (±0.000901s); valid time: 0.002136s; loss: -67.5569 (±2.05439); valid loss: 95.6456\n",
      "[Epoch 140/350, Step 1540, ETA 14.84s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.4s] step time: 0.005351s (±0.0008277s); valid time: 0.002241s; loss: -67.8004 (±2.06356); valid loss: 106.741\n",
      "[Epoch 150/350, Step 1650, ETA 14.05s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.71s] step time: 0.005456s (±0.001161s); valid time: 0.002189s; loss: -67.8635 (±2.20479); valid loss: 107.573\n",
      "[Epoch 160/350, Step 1760, ETA 13.28s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 13.01s] step time: 0.005384s (±0.0008835s); valid time: 0.002224s; loss: -67.9255 (±2.21511); valid loss: 93.8994\n",
      "[Epoch 170/350, Step 1870, ETA 12.53s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.33s] step time: 0.005362s (±0.001002s); valid time: 0.002147s; loss: -68.0061 (±2.1085); valid loss: 95.0921\n",
      "[Epoch 180/350, Step 1980, ETA 11.79s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.66s] step time: 0.005428s (±0.001501s); valid time: 0.002161s; loss: -67.9689 (±1.88727); valid loss: 96.541\n",
      "[Epoch 190/350, Step 2090, ETA 11.06s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.99s] step time: 0.005315s (±0.000755s); valid time: 0.002193s; loss: -68.1271 (±1.95046); valid loss: 106.33\n",
      "[Epoch 200/350, Step 2200, ETA 10.33s] step time: 0.005369s (±0.001064s); valid time: 0.002472s; loss: -68.0729 (±2.05796); valid loss: 95.2799\n",
      "[Epoch 200/350, Step 2200, ETA 10.33s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.673s] step time: 0.005239s (±0.0006611s); valid time: 0.002076s; loss: -68.1095 (±2.04634); valid loss: 87.4368\n",
      "[Epoch 210/350, Step 2310, ETA 9.605s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 9.022s] step time: 0.005259s (±0.0009577s); valid time: 0.002192s; loss: -68.1595 (±2.18984); valid loss: 99.0011\n",
      "[Epoch 220/350, Step 2420, ETA 8.893s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.374s] step time: 0.005259s (±0.0007375s); valid time: 0.002118s; loss: -68.1158 (±2.25654); valid loss: 98.698\n",
      "[Epoch 230/350, Step 2530, ETA 8.18s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.743s] step time: 0.00546s (±0.001227s); valid time: 0.002203s; loss: -68.2166 (±1.89927); valid loss: 93.6155\n",
      "[Epoch 240/350, Step 2640, ETA 7.487s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.108s] step time: 0.005323s (±0.00078s); valid time: 0.002215s; loss: -68.1214 (±2.08007); valid loss: 100.112\n",
      "[Epoch 250/350, Step 2750, ETA 6.799s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.487s] step time: 0.00559s (±0.001069s); valid time: 0.002339s; loss: -68.2746 (±2.49805); valid loss: 98.8516\n",
      "[Epoch 260/350, Step 2860, ETA 6.11s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.864s] step time: 0.005473s (±0.001078s); valid time: 0.002354s; loss: -68.2486 (±2.10997); valid loss: 98.1323\n",
      "[Epoch 270/350, Step 2970, ETA 5.427s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.238s] step time: 0.005342s (±0.0007409s); valid time: 0.002046s; loss: -68.1117 (±2.14556); valid loss: 98.3195\n",
      "[Epoch 280/350, Step 3080, ETA 4.738s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.613s] step time: 0.005319s (±0.0008734s); valid time: 0.002065s; loss: -68.2563 (±2.12795); valid loss: 101.005\n",
      "[Epoch 290/350, Step 3190, ETA 4.052s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.99s] step time: 0.005255s (±0.0006517s); valid time: 0.002186s; loss: -68.1524 (±1.91091); valid loss: 83.7815\n",
      "[Epoch 300/350, Step 3300, ETA 3.372s] step time: 0.005397s (±0.001311s); valid time: 0.002444s; loss: -68.1457 (±1.85249); valid loss: 101.933\n",
      "[Epoch 300/350, Step 3300, ETA 3.372s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.754s] step time: 0.005259s (±0.0008294s); valid time: 0.00242s; loss: -68.3139 (±2.19494); valid loss: 90.3227\n",
      "[Epoch 310/350, Step 3410, ETA 2.694s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.141s] step time: 0.00547s (±0.001486s); valid time: 0.002202s; loss: -68.0683 (±2.09338); valid loss: 96.2245\n",
      "[Epoch 320/350, Step 3520, ETA 2.017s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.528s] step time: 0.005416s (±0.001014s); valid time: 0.002128s; loss: -68.3028 (±2.45373); valid loss: 93.7517\n",
      "[Epoch 330/350, Step 3630, ETA 1.343s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9154s] step time: 0.005316s (±0.00102s); valid time: 0.00218s; loss: -68.1937 (±2.03494); valid loss: 88.8858\n",
      "[Epoch 340/350, Step 3740, ETA 0.6709s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3049s] step time: 0.005412s (±0.001051s); valid time: 0.002094s; loss: -68.2417 (±2.12874); valid loss: 103.347\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpro4bac02/variables.dat-400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpro4bac02/variables.dat-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.58,\n",
      "\t(tp, fp, tn, fn)=(236, 1547, 2083, 166),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.59,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.44221230158730157,\n",
      "\ty_label%=0.09970238095238096,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpc_results.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 17.45s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 12.6s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.6s] step time: 0.009893s (±0.03236s); valid time: 0.1649s; loss: 123.453 (±6.5837); valid loss: 141.208 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.39s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 10.84s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.27s] step time: 0.006107s (±0.007638s); valid time: 0.07463s; loss: 109.946 (±4.82023); valid loss: 129.246 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.27s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.528s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.855s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.822s] step time: 0.006378s (±0.007442s); valid time: 0.07392s; loss: 98.9102 (±3.48601); valid loss: 124.57 (*)\n",
      "[Epoch 80/350, Step 320, ETA 8.579s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 8.074s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.755s] step time: 0.006352s (±0.007507s); valid time: 0.07433s; loss: 94.3726 (±2.76767); valid loss: 122.66 (*)\n",
      "[Epoch 100/350, Step 400, ETA 7.755s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 7.279s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.848s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.786s] step time: 0.006123s (±0.008045s); valid time: 0.0799s; loss: 92.5534 (±2.51668); valid loss: 122.093 (*)\n",
      "[Epoch 130/350, Step 520, ETA 6.597s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 6.208s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.83s] step time: 0.005485s (±0.001332s); valid time: 0.001569s; loss: 91.6539 (±2.59498); valid loss: 122.234\n",
      "[Epoch 150/350, Step 600, ETA 5.83s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.474s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 5.146s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 4.976s] step time: 0.005486s (±0.001217s); valid time: 0.001641s; loss: 91.3036 (±2.6659); valid loss: 122.489\n",
      "[Epoch 180/350, Step 720, ETA 4.821s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.509s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 4.268s] step time: 0.006569s (±0.007789s); valid time: 0.07679s; loss: 91.2465 (±2.65929); valid loss: 121.975 (*)\n",
      "[Epoch 200/350, Step 800, ETA 4.268s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.954s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.645s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.491s] step time: 0.005375s (±0.001028s); valid time: 0.001603s; loss: 91.088 (±2.663); valid loss: 122.01\n",
      "[Epoch 230/350, Step 920, ETA 3.346s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 3.052s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.766s] step time: 0.005757s (±0.001438s); valid time: 0.001521s; loss: 91.0701 (±2.24128); valid loss: 122.4\n",
      "[Epoch 250/350, Step 1000, ETA 2.766s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.476s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.188s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 2.045s] step time: 0.005279s (±0.0007159s); valid time: 0.001745s; loss: 91.0339 (±2.46762); valid loss: 122.083\n",
      "[Epoch 280/350, Step 1120, ETA 1.904s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.626s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.35s] step time: 0.005426s (±0.001137s); valid time: 0.001646s; loss: 90.854 (±2.48091); valid loss: 122.336\n",
      "[Epoch 300/350, Step 1200, ETA 1.35s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.075s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.8037s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6744s] step time: 0.006087s (±0.007779s); valid time: 0.07765s; loss: 91.0845 (±2.63922); valid loss: 121.831 (*)\n",
      "[Epoch 330/350, Step 1320, ETA 0.539s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.269s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.005631s (±0.001401s); valid time: 0.001629s; loss: 90.9316 (±2.36882); valid loss: 121.883\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpvjmwt0x1/variables.dat-1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpvjmwt0x1/variables.dat-1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.16,\n",
      "\t(tp, fp, tn, fn)=(163, 1362, 99, 0),\n",
      "\tprecision=0.11,\n",
      "\trecall=1.0,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.53,\n",
      "\ty_pred%=0.9390394088669951,\n",
      "\ty_label%=0.10036945812807882,\n",
      ")\n",
      "Testing on realAdExchange/exchange-2_cpm_results.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 17.18s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 12.12s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.28s] step time: 0.00962s (±0.03205s); valid time: 0.1684s; loss: 122.786 (±4.72934); valid loss: 151.155 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.21s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 10.68s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.16s] step time: 0.006159s (±0.007766s); valid time: 0.07579s; loss: 110.343 (±4.15966); valid loss: 136.118 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.16s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.33s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.717s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.713s] step time: 0.006273s (±0.008173s); valid time: 0.08132s; loss: 101.015 (±2.68251); valid loss: 128.911 (*)\n",
      "[Epoch 80/350, Step 320, ETA 8.414s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 7.915s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.618s] step time: 0.006134s (±0.007339s); valid time: 0.07326s; loss: 94.911 (±2.25483); valid loss: 123.182 (*)\n",
      "[Epoch 100/350, Step 400, ETA 7.618s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 7.174s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.755s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.699s] step time: 0.006176s (±0.008351s); valid time: 0.08326s; loss: 90.7829 (±1.69176); valid loss: 120.027 (*)\n",
      "[Epoch 130/350, Step 520, ETA 6.506s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 6.123s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.872s] step time: 0.006246s (±0.0075s); valid time: 0.0746s; loss: 88.9808 (±1.84289); valid loss: 119.281 (*)\n",
      "[Epoch 150/350, Step 600, ETA 5.872s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.515s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 5.179s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 5.079s] step time: 0.006206s (±0.007479s); valid time: 0.074s; loss: 88.0494 (±1.95366); valid loss: 118.416 (*)\n",
      "[Epoch 180/350, Step 720, ETA 4.926s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.593s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 4.272s] step time: 0.005616s (±0.001357s); valid time: 0.00153s; loss: 87.7366 (±1.66468); valid loss: 118.575\n",
      "[Epoch 200/350, Step 800, ETA 4.272s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.954s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.648s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.495s] step time: 0.005407s (±0.001093s); valid time: 0.001549s; loss: 87.3909 (±1.55084); valid loss: 118.485\n",
      "[Epoch 230/350, Step 920, ETA 3.344s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 3.048s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.791s] step time: 0.006277s (±0.008912s); valid time: 0.0888s; loss: 87.467 (±1.77087); valid loss: 118.347 (*)\n",
      "[Epoch 250/350, Step 1000, ETA 2.791s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.5s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.21s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 2.068s] step time: 0.005481s (±0.001175s); valid time: 0.001634s; loss: 87.3041 (±1.72557); valid loss: 118.468\n",
      "[Epoch 280/350, Step 1120, ETA 1.926s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.646s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.379s] step time: 0.006343s (±0.00782s); valid time: 0.07749s; loss: 87.3453 (±1.62969); valid loss: 118.188 (*)\n",
      "[Epoch 300/350, Step 1200, ETA 1.379s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.098s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.8209s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6827s] step time: 0.005453s (±0.0008756s); valid time: 0.001918s; loss: 87.15 (±1.91013); valid loss: 118.359\n",
      "[Epoch 330/350, Step 1320, ETA 0.5455s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.2722s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.00572s (±0.001236s); valid time: 0.001697s; loss: 87.2694 (±1.58204); valid loss: 118.309\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpd8zaurp4/variables.dat-1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpd8zaurp4/variables.dat-1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.25,\n",
      "\t(tp, fp, tn, fn)=(149, 1210, 252, 13),\n",
      "\tprecision=0.11,\n",
      "\trecall=0.92,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.8368226600985221,\n",
      "\ty_label%=0.09975369458128079,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpc_results.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 17.65s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 12.3s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.29s] step time: 0.009644s (±0.03208s); valid time: 0.1579s; loss: 111.229 (±5.08615); valid loss: 125.05 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.13s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 10.78s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.2s] step time: 0.006204s (±0.007321s); valid time: 0.07177s; loss: 99.234 (±3.3054); valid loss: 123.174 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.2s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.351s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.757s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.472s] step time: 0.005471s (±0.001258s); valid time: 0.001625s; loss: 95.3179 (±2.26234); valid loss: 123.782\n",
      "[Epoch 80/350, Step 320, ETA 8.2s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 7.704s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.263s] step time: 0.005389s (±0.00111s); valid time: 0.002789s; loss: 93.7235 (±2.2859); valid loss: 123.505\n",
      "[Epoch 100/350, Step 400, ETA 7.263s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 6.855s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.49s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.309s] step time: 0.005425s (±0.001018s); valid time: 0.002178s; loss: 93.2376 (±2.45275); valid loss: 124.558\n",
      "[Epoch 130/350, Step 520, ETA 6.138s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 5.806s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.476s] step time: 0.005421s (±0.0008729s); valid time: 0.001566s; loss: 93.312 (±2.34931); valid loss: 123.63\n",
      "[Epoch 150/350, Step 600, ETA 5.477s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.162s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 4.865s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 4.71s] step time: 0.005473s (±0.001727s); valid time: 0.001726s; loss: 93.0568 (±2.19309); valid loss: 123.267\n",
      "[Epoch 180/350, Step 720, ETA 4.562s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.264s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 3.972s] step time: 0.005313s (±0.0008786s); valid time: 0.001659s; loss: 92.9757 (±2.41478); valid loss: 123.509\n",
      "[Epoch 200/350, Step 800, ETA 3.972s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.696s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.422s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.325s] step time: 0.00633s (±0.007724s); valid time: 0.07731s; loss: 93.0487 (±2.30456); valid loss: 123.002 (*)\n",
      "[Epoch 230/350, Step 920, ETA 3.188s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 2.913s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.639s] step time: 0.005539s (±0.001501s); valid time: 0.001764s; loss: 92.9284 (±2.3099); valid loss: 123.522\n",
      "[Epoch 250/350, Step 1000, ETA 2.639s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.368s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.095s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 1.96s] step time: 0.00535s (±0.0009017s); valid time: 0.001504s; loss: 92.8095 (±2.362); valid loss: 123.313\n",
      "[Epoch 280/350, Step 1120, ETA 1.828s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.567s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.302s] step time: 0.005684s (±0.001534s); valid time: 0.002885s; loss: 93.0609 (±2.35169); valid loss: 123.442\n",
      "[Epoch 300/350, Step 1200, ETA 1.302s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.039s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.7766s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6473s] step time: 0.005447s (±0.00114s); valid time: 0.001594s; loss: 92.8596 (±2.27177); valid loss: 123.608\n",
      "[Epoch 330/350, Step 1320, ETA 0.5181s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.2586s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.005579s (±0.001541s); valid time: 0.001681s; loss: 92.9046 (±2.30136); valid loss: 123.866\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeo_bbbi3/variables.dat-900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeo_bbbi3/variables.dat-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.73,\n",
      "\t(tp, fp, tn, fn)=(88, 355, 1030, 65),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.58,\n",
      "\tf1=0.3,\n",
      "\troc_auc=0.66,\n",
      "\ty_pred%=0.288036410923277,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-3_cpm_results.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 18.02s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 13.03s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.88s] step time: 0.01009s (±0.03346s); valid time: 0.1655s; loss: 107.94 (±4.03684); valid loss: 188.941 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.58s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 10.95s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.34s] step time: 0.005992s (±0.007383s); valid time: 0.07331s; loss: 101.225 (±3.15421); valid loss: 161.802 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.35s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.514s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.866s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.853s] step time: 0.006326s (±0.008046s); valid time: 0.08039s; loss: 96.5422 (±2.09693); valid loss: 152.22 (*)\n",
      "[Epoch 80/350, Step 320, ETA 8.548s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 7.976s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.717s] step time: 0.006126s (±0.007661s); valid time: 0.07659s; loss: 95.1347 (±1.87381); valid loss: 149.372 (*)\n",
      "[Epoch 100/350, Step 400, ETA 7.718s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 7.264s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.841s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.791s] step time: 0.006292s (±0.008281s); valid time: 0.08281s; loss: 94.5068 (±1.97429); valid loss: 148.381 (*)\n",
      "[Epoch 130/350, Step 520, ETA 6.599s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 6.216s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.841s] step time: 0.005497s (±0.001104s); valid time: 0.001631s; loss: 94.4501 (±1.76305); valid loss: 148.728\n",
      "[Epoch 150/350, Step 600, ETA 5.841s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.485s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 5.138s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 5.071s] step time: 0.006344s (±0.0102s); valid time: 0.1014s; loss: 94.2426 (±2.17108); valid loss: 148.108 (*)\n",
      "[Epoch 180/350, Step 720, ETA 4.911s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.583s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 4.315s] step time: 0.006235s (±0.007863s); valid time: 0.07807s; loss: 94.1906 (±2.18718); valid loss: 147.617 (*)\n",
      "[Epoch 200/350, Step 800, ETA 4.315s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.985s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.687s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.533s] step time: 0.005499s (±0.001323s); valid time: 0.001458s; loss: 94.1726 (±1.77188); valid loss: 148.249\n",
      "[Epoch 230/350, Step 920, ETA 3.379s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 3.082s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.789s] step time: 0.005563s (±0.001158s); valid time: 0.001604s; loss: 94.0866 (±2.11766); valid loss: 148.328\n",
      "[Epoch 250/350, Step 1000, ETA 2.789s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.5s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.21s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 2.066s] step time: 0.005453s (±0.001005s); valid time: 0.00202s; loss: 94.1411 (±2.05093); valid loss: 148.063\n",
      "[Epoch 280/350, Step 1120, ETA 1.925s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.642s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.363s] step time: 0.005477s (±0.00119s); valid time: 0.001579s; loss: 94.1757 (±2.07744); valid loss: 147.78\n",
      "[Epoch 300/350, Step 1200, ETA 1.363s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.087s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.8121s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6752s] step time: 0.005418s (±0.000849s); valid time: 0.001539s; loss: 94.2693 (±1.95416); valid loss: 148.068\n",
      "[Epoch 330/350, Step 1320, ETA 0.5396s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.2693s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.005634s (±0.0012s); valid time: 0.001603s; loss: 93.9541 (±1.83809); valid loss: 148.151\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpry15d6vd/variables.dat-800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpry15d6vd/variables.dat-800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.34,\n",
      "\t(tp, fp, tn, fn)=(148, 1009, 376, 5),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.97,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.7522756827048115,\n",
      "\ty_label%=0.09947984395318596,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpc_results.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 17.31s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 12.31s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.53s] step time: 0.009834s (±0.0331s); valid time: 0.1748s; loss: 95.8452 (±8.16258); valid loss: 335.95 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.39s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 10.84s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.27s] step time: 0.00616s (±0.007343s); valid time: 0.07252s; loss: 69.977 (±11.8022); valid loss: 330.048 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.27s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.415s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.814s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.812s] step time: 0.006363s (±0.008165s); valid time: 0.08052s; loss: 49.6621 (±5.40706); valid loss: 302.661 (*)\n",
      "[Epoch 80/350, Step 320, ETA 8.525s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 7.974s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.696s] step time: 0.006175s (±0.0082s); valid time: 0.08228s; loss: 43.3956 (±4.30304); valid loss: 299.44 (*)\n",
      "[Epoch 100/350, Step 400, ETA 7.697s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 7.245s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.841s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.775s] step time: 0.006262s (±0.007925s); valid time: 0.0797s; loss: 41.4883 (±4.86173); valid loss: 286.278 (*)\n",
      "[Epoch 130/350, Step 520, ETA 6.576s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 6.199s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.827s] step time: 0.005509s (±0.001328s); valid time: 0.001812s; loss: 40.7037 (±4.0858); valid loss: 287.999\n",
      "[Epoch 150/350, Step 600, ETA 5.828s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.478s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 5.139s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 5.051s] step time: 0.006231s (±0.007604s); valid time: 0.07599s; loss: 40.5052 (±4.51161); valid loss: 283.229 (*)\n",
      "[Epoch 180/350, Step 720, ETA 4.903s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.571s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 4.251s] step time: 0.005618s (±0.001378s); valid time: 0.001599s; loss: 40.0172 (±4.00521); valid loss: 285.322\n",
      "[Epoch 200/350, Step 800, ETA 4.252s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.937s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.635s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.523s] step time: 0.006153s (±0.0076s); valid time: 0.07618s; loss: 39.9416 (±4.62457); valid loss: 281.508 (*)\n",
      "[Epoch 230/350, Step 920, ETA 3.372s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 3.068s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.772s] step time: 0.005339s (±0.001312s); valid time: 0.001494s; loss: 39.8309 (±4.33409); valid loss: 286.367\n",
      "[Epoch 250/350, Step 1000, ETA 2.772s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.487s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.199s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 2.056s] step time: 0.005451s (±0.0008901s); valid time: 0.00165s; loss: 39.609 (±3.88553); valid loss: 282.84\n",
      "[Epoch 280/350, Step 1120, ETA 1.916s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.635s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.358s] step time: 0.005552s (±0.001269s); valid time: 0.001488s; loss: 39.6865 (±4.08125); valid loss: 288.032\n",
      "[Epoch 300/350, Step 1200, ETA 1.358s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.083s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.8087s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6729s] step time: 0.005431s (±0.0009434s); valid time: 0.001577s; loss: 39.6123 (±3.61933); valid loss: 286.361\n",
      "[Epoch 330/350, Step 1320, ETA 0.5379s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.2683s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.005804s (±0.001629s); valid time: 0.001668s; loss: 39.3325 (±4.28471); valid loss: 284.717\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpz6ggocu8/variables.dat-900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpz6ggocu8/variables.dat-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.51,\n",
      "\t(tp, fp, tn, fn)=(111, 751, 727, 54),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.67,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.5246500304321363,\n",
      "\ty_label%=0.10042604990870359,\n",
      ")\n",
      "Testing on realAdExchange/exchange-4_cpm_results.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 40, ETA 17.65s] Learning rate decreased to 0.00075\n",
      "[Epoch 20/350, Step 80, ETA 12.55s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 25/350, Step 100, ETA 13.7s] step time: 0.009966s (±0.03401s); valid time: 0.1799s; loss: 134.31 (±17.7283); valid loss: 130.967 (*)\n",
      "[Epoch 30/350, Step 120, ETA 12.54s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 40/350, Step 160, ETA 11.01s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 200, ETA 10.45s] step time: 0.0063s (±0.007287s); valid time: 0.07216s; loss: 98.4505 (±8.75212); valid loss: 124.797 (*)\n",
      "[Epoch 50/350, Step 200, ETA 10.45s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 240, ETA 9.548s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 70/350, Step 280, ETA 8.85s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 75/350, Step 300, ETA 8.611s] step time: 0.005479s (±0.001362s); valid time: 0.001705s; loss: 81.3924 (±5.76739); valid loss: 131.951\n",
      "[Epoch 80/350, Step 320, ETA 8.343s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 90/350, Step 360, ETA 7.822s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 400, ETA 7.365s] step time: 0.005432s (±0.001152s); valid time: 0.001734s; loss: 74.8119 (±3.97071); valid loss: 132.427\n",
      "[Epoch 100/350, Step 400, ETA 7.367s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 440, ETA 6.946s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 120/350, Step 480, ETA 6.572s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 125/350, Step 500, ETA 6.389s] step time: 0.005481s (±0.001356s); valid time: 0.001761s; loss: 71.9879 (±3.83895); valid loss: 134.713\n",
      "[Epoch 130/350, Step 520, ETA 6.217s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 140/350, Step 560, ETA 5.857s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 600, ETA 5.526s] step time: 0.005377s (±0.0009575s); valid time: 0.001503s; loss: 70.8578 (±3.48948); valid loss: 134.77\n",
      "[Epoch 150/350, Step 600, ETA 5.526s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 640, ETA 5.211s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 170/350, Step 680, ETA 4.893s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 175/350, Step 700, ETA 4.737s] step time: 0.005351s (±0.001148s); valid time: 0.001443s; loss: 70.6569 (±3.78966); valid loss: 136.478\n",
      "[Epoch 180/350, Step 720, ETA 4.588s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 190/350, Step 760, ETA 4.291s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 800, ETA 4.005s] step time: 0.005493s (±0.0009392s); valid time: 0.00169s; loss: 70.3632 (±3.77528); valid loss: 139.583\n",
      "[Epoch 200/350, Step 800, ETA 4.005s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 840, ETA 3.719s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 220/350, Step 880, ETA 3.44s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 225/350, Step 900, ETA 3.301s] step time: 0.005446s (±0.001068s); valid time: 0.001662s; loss: 70.2391 (±3.36937); valid loss: 137.278\n",
      "[Epoch 230/350, Step 920, ETA 3.164s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 240/350, Step 960, ETA 2.887s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1000, ETA 2.624s] step time: 0.005602s (±0.001381s); valid time: 0.001602s; loss: 70.0077 (±4.33755); valid loss: 138.487\n",
      "[Epoch 250/350, Step 1000, ETA 2.624s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1040, ETA 2.352s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 270/350, Step 1080, ETA 2.083s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 275/350, Step 1100, ETA 1.949s] step time: 0.005317s (±0.0008033s); valid time: 0.001647s; loss: 70.2844 (±3.89944); valid loss: 135.477\n",
      "[Epoch 280/350, Step 1120, ETA 1.818s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 290/350, Step 1160, ETA 1.557s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1200, ETA 1.296s] step time: 0.005729s (±0.001501s); valid time: 0.001621s; loss: 70.1327 (±4.08089); valid loss: 139.532\n",
      "[Epoch 300/350, Step 1200, ETA 1.296s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1240, ETA 1.035s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 320/350, Step 1280, ETA 0.7739s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 325/350, Step 1300, ETA 0.6443s] step time: 0.005438s (±0.001042s); valid time: 0.001638s; loss: 70.0705 (±3.74851); valid loss: 140.405\n",
      "[Epoch 330/350, Step 1320, ETA 0.5161s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 340/350, Step 1360, ETA 0.2573s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 1400, ETA 0s] step time: 0.005554s (±0.00133s); valid time: 0.00158s; loss: 70.0411 (±4.04855); valid loss: 138.412\n",
      "[Epoch 350/350, Step 1400, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbpeyqoxg/variables.dat-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbpeyqoxg/variables.dat-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.65,\n",
      "\t(tp, fp, tn, fn)=(78, 483, 996, 86),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.48,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.57,\n",
      "\ty_pred%=0.3414485696895922,\n",
      "\ty_label%=0.09981740718198417,\n",
      ")\n",
      "Testing on realKnownCause/ambient_temperature_system_failure.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 5/350, Step 100, ETA 1m 16.71s] step time: 0.009573s (±0.0324s); valid time: 0.1591s; loss: 111.907 (±35.7145); valid loss: 180.887 (*)\n",
      "[Epoch 10/350, Step 200, ETA 1m 2.094s] step time: 0.005935s (±0.007371s); valid time: 0.0742s; loss: 75.0855 (±5.90886); valid loss: 160.846 (*)\n",
      "[Epoch 10/350, Step 220, ETA 1m 0.002862s] Learning rate decreased to 0.00075\n",
      "[Epoch 14/350, Step 300, ETA 57.34s] step time: 0.006216s (±0.007695s); valid time: 0.0763s; loss: 63.629 (±3.53623); valid loss: 114.439 (*)\n",
      "[Epoch 19/350, Step 400, ETA 54.29s] step time: 0.00599s (±0.007675s); valid time: 0.0772s; loss: 58.3838 (±2.50928); valid loss: 111.322 (*)\n",
      "[Epoch 20/350, Step 440, ETA 53.08s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 23/350, Step 500, ETA 51.48s] step time: 0.005531s (±0.001538s); valid time: 0.004283s; loss: 56.341 (±6.27815); valid loss: 112.653\n",
      "[Epoch 28/350, Step 600, ETA 49.44s] step time: 0.005505s (±0.001371s); valid time: 0.004375s; loss: 54.2991 (±2.33556); valid loss: 111.712\n",
      "[Epoch 30/350, Step 660, ETA 48.29s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 700, ETA 48.41s] step time: 0.006171s (±0.007827s); valid time: 0.07792s; loss: 52.507 (±2.19518); valid loss: 107.849 (*)\n",
      "[Epoch 37/350, Step 800, ETA 47.44s] step time: 0.00609s (±0.007931s); valid time: 0.07831s; loss: 51.5566 (±2.38136); valid loss: 104.413 (*)\n",
      "[Epoch 40/350, Step 880, ETA 46.14s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 900, ETA 45.95s] step time: 0.005329s (±0.001108s); valid time: 0.008421s; loss: 50.4054 (±2.09972); valid loss: 105.914\n",
      "[Epoch 46/350, Step 1000, ETA 44.81s] step time: 0.005545s (±0.001429s); valid time: 0.004497s; loss: 49.1794 (±2.41822); valid loss: 105.422\n",
      "[Epoch 50/350, Step 1100, ETA 43.67s] step time: 0.005421s (±0.001312s); valid time: 0.004291s; loss: 49.8081 (±24.2074); valid loss: 105.612\n",
      "[Epoch 50/350, Step 1100, ETA 43.67s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 1200, ETA 42.67s] step time: 0.005476s (±0.001006s); valid time: 0.004401s; loss: 46 (±2.79646); valid loss: 106.596\n",
      "[Epoch 60/350, Step 1300, ETA 42.05s] step time: 0.006122s (±0.007648s); valid time: 0.07482s; loss: 45.4446 (±9.722); valid loss: 103.873 (*)\n",
      "[Epoch 60/350, Step 1320, ETA 41.84s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 1400, ETA 41.09s] step time: 0.005425s (±0.001417s); valid time: 0.007302s; loss: 43.5524 (±2.19896); valid loss: 104.831\n",
      "[Epoch 69/350, Step 1500, ETA 40.21s] step time: 0.005444s (±0.0008902s); valid time: 0.004584s; loss: 42.8391 (±2.41858); valid loss: 107.127\n",
      "[Epoch 70/350, Step 1540, ETA 39.82s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 1600, ETA 39.34s] step time: 0.005462s (±0.001301s); valid time: 0.004456s; loss: 42.0159 (±2.57933); valid loss: 105.467\n",
      "[Epoch 78/350, Step 1700, ETA 38.5s] step time: 0.005369s (±0.001131s); valid time: 0.004278s; loss: 41.3488 (±2.46807); valid loss: 105.808\n",
      "[Epoch 80/350, Step 1760, ETA 38.05s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 1800, ETA 37.73s] step time: 0.005543s (±0.001533s); valid time: 0.004504s; loss: 40.6909 (±2.4085); valid loss: 105.992\n",
      "[Epoch 87/350, Step 1900, ETA 36.93s] step time: 0.005376s (±0.001034s); valid time: 0.004396s; loss: 40.2554 (±2.38966); valid loss: 105.318\n",
      "[Epoch 90/350, Step 1980, ETA 36.32s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 2000, ETA 36.17s] step time: 0.005447s (±0.0009782s); valid time: 0.004419s; loss: 39.7339 (±2.03479); valid loss: 105.055\n",
      "[Epoch 96/350, Step 2100, ETA 35.4s] step time: 0.005306s (±0.001005s); valid time: 0.00446s; loss: 39.4673 (±2.29819); valid loss: 109.688\n",
      "[Epoch 100/350, Step 2200, ETA 34.82s] step time: 0.006068s (±0.007945s); valid time: 0.07888s; loss: 39.0279 (±2.535); valid loss: 103.815 (*)\n",
      "[Epoch 100/350, Step 2200, ETA 34.82s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 105/350, Step 2300, ETA 34.3s] step time: 0.006282s (±0.007929s); valid time: 0.07886s; loss: 38.7203 (±2.10929); valid loss: 103.721 (*)\n",
      "[Epoch 110/350, Step 2400, ETA 33.53s] step time: 0.005259s (±0.0006644s); valid time: 0.004323s; loss: 38.67 (±2.62495); valid loss: 104.344\n",
      "[Epoch 110/350, Step 2420, ETA 33.39s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 114/350, Step 2500, ETA 32.95s] step time: 0.006134s (±0.008373s); valid time: 0.08342s; loss: 38.586 (±2.13631); valid loss: 101.036 (*)\n",
      "[Epoch 119/350, Step 2600, ETA 32.21s] step time: 0.005206s (±0.0009004s); valid time: 0.004714s; loss: 38.2612 (±2.39802); valid loss: 101.994\n",
      "[Epoch 120/350, Step 2640, ETA 31.95s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 123/350, Step 2700, ETA 31.52s] step time: 0.005522s (±0.001563s); valid time: 0.004087s; loss: 38.702 (±5.61987); valid loss: 101.458\n",
      "[Epoch 128/350, Step 2800, ETA 30.8s] step time: 0.005279s (±0.001045s); valid time: 0.004522s; loss: 38.215 (±2.00033); valid loss: 101.991\n",
      "[Epoch 130/350, Step 2860, ETA 30.37s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 2900, ETA 30.08s] step time: 0.005291s (±0.001101s); valid time: 0.004256s; loss: 37.9788 (±1.99455); valid loss: 101.384\n",
      "[Epoch 137/350, Step 3000, ETA 29.5s] step time: 0.006064s (±0.007655s); valid time: 0.07633s; loss: 37.8571 (±2.37573); valid loss: 100.988 (*)\n",
      "[Epoch 140/350, Step 3080, ETA 28.92s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 3100, ETA 28.91s] step time: 0.005995s (±0.008254s); valid time: 0.08235s; loss: 37.7654 (±2.05681); valid loss: 100.852 (*)\n",
      "[Epoch 146/350, Step 3200, ETA 28.31s] step time: 0.006006s (±0.008149s); valid time: 0.08141s; loss: 37.6891 (±2.39666); valid loss: 100.824 (*)\n",
      "[Epoch 150/350, Step 3300, ETA 27.63s] step time: 0.00539s (±0.001653s); valid time: 0.00545s; loss: 37.7191 (±2.05678); valid loss: 100.983\n",
      "[Epoch 150/350, Step 3300, ETA 27.63s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 3400, ETA 27.05s] step time: 0.006181s (±0.008652s); valid time: 0.0866s; loss: 37.8219 (±3.04949); valid loss: 100.795 (*)\n",
      "[Epoch 160/350, Step 3500, ETA 26.45s] step time: 0.005984s (±0.008088s); valid time: 0.08065s; loss: 38.8953 (±13.0455); valid loss: 100.123 (*)\n",
      "[Epoch 160/350, Step 3520, ETA 26.3s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 3600, ETA 25.85s] step time: 0.006057s (±0.007964s); valid time: 0.07976s; loss: 37.4416 (±2.3086); valid loss: 99.947 (*)\n",
      "[Epoch 169/350, Step 3700, ETA 25.16s] step time: 0.005285s (±0.000866s); valid time: 0.004236s; loss: 37.5134 (±1.92788); valid loss: 100.939\n",
      "[Epoch 170/350, Step 3740, ETA 24.9s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 3800, ETA 24.49s] step time: 0.005444s (±0.001064s); valid time: 0.00432s; loss: 39.3131 (±17.6379); valid loss: 100.916\n",
      "[Epoch 178/350, Step 3900, ETA 23.83s] step time: 0.005398s (±0.001138s); valid time: 0.007214s; loss: 37.5489 (±2.26503); valid loss: 100.506\n",
      "[Epoch 180/350, Step 3960, ETA 23.42s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 4000, ETA 23.23s] step time: 0.006108s (±0.008029s); valid time: 0.08005s; loss: 37.2888 (±2.20298); valid loss: 99.2911 (*)\n",
      "[Epoch 187/350, Step 4100, ETA 22.57s] step time: 0.005384s (±0.0009744s); valid time: 0.004196s; loss: 37.4165 (±2.06466); valid loss: 101.382\n",
      "[Epoch 190/350, Step 4180, ETA 22.04s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 4200, ETA 21.91s] step time: 0.005347s (±0.0009258s); valid time: 0.004542s; loss: 37.4637 (±2.54208); valid loss: 100.09\n",
      "[Epoch 196/350, Step 4300, ETA 21.28s] step time: 0.005648s (±0.001308s); valid time: 0.004479s; loss: 37.2288 (±2.26643); valid loss: 101.827\n",
      "[Epoch 200/350, Step 4400, ETA 20.63s] step time: 0.005436s (±0.001333s); valid time: 0.0048s; loss: 37.4728 (±2.36336); valid loss: 100.165\n",
      "[Epoch 200/350, Step 4400, ETA 20.63s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 205/350, Step 4500, ETA 19.99s] step time: 0.005578s (±0.0009698s); valid time: 0.004179s; loss: 37.396 (±2.2307); valid loss: 99.9833\n",
      "[Epoch 210/350, Step 4600, ETA 19.35s] step time: 0.005515s (±0.001147s); valid time: 0.004463s; loss: 37.2791 (±1.94226); valid loss: 100.169\n",
      "[Epoch 210/350, Step 4620, ETA 19.22s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 214/350, Step 4700, ETA 18.7s] step time: 0.005387s (±0.001102s); valid time: 0.004392s; loss: 37.3725 (±2.57013); valid loss: 100.516\n",
      "[Epoch 219/350, Step 4800, ETA 18.07s] step time: 0.005532s (±0.001078s); valid time: 0.004319s; loss: 37.2767 (±2.10329); valid loss: 104.415\n",
      "[Epoch 220/350, Step 4840, ETA 17.81s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 223/350, Step 4900, ETA 17.42s] step time: 0.005311s (±0.001015s); valid time: 0.004377s; loss: 37.2184 (±2.09598); valid loss: 100.033\n",
      "[Epoch 228/350, Step 5000, ETA 16.79s] step time: 0.005539s (±0.001248s); valid time: 0.004658s; loss: 37.219 (±2.14611); valid loss: 99.3446\n",
      "[Epoch 230/350, Step 5060, ETA 16.41s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 5100, ETA 16.16s] step time: 0.0056s (±0.001173s); valid time: 0.005299s; loss: 37.3191 (±2.07549); valid loss: 101.725\n",
      "[Epoch 237/350, Step 5200, ETA 15.53s] step time: 0.005402s (±0.000906s); valid time: 0.004107s; loss: 37.2036 (±2.15131); valid loss: 100.582\n",
      "[Epoch 240/350, Step 5280, ETA 15.02s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 5300, ETA 14.89s] step time: 0.005373s (±0.001001s); valid time: 0.00429s; loss: 37.5012 (±2.23613); valid loss: 100.573\n",
      "[Epoch 246/350, Step 5400, ETA 14.25s] step time: 0.005288s (±0.0007733s); valid time: 0.004211s; loss: 37.2099 (±2.18461); valid loss: 100.564\n",
      "[Epoch 250/350, Step 5500, ETA 13.62s] step time: 0.005401s (±0.0008417s); valid time: 0.004247s; loss: 37.2468 (±2.28505); valid loss: 100.002\n",
      "[Epoch 250/350, Step 5500, ETA 13.62s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 5600, ETA 12.99s] step time: 0.005378s (±0.001047s); valid time: 0.004507s; loss: 37.213 (±2.28783); valid loss: 100.98\n",
      "[Epoch 260/350, Step 5700, ETA 12.36s] step time: 0.005338s (±0.001269s); valid time: 0.004146s; loss: 37.249 (±1.93918); valid loss: 99.8305\n",
      "[Epoch 260/350, Step 5720, ETA 12.23s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 5800, ETA 11.73s] step time: 0.005296s (±0.0009207s); valid time: 0.004396s; loss: 37.4057 (±2.31529); valid loss: 101.367\n",
      "[Epoch 269/350, Step 5900, ETA 11.1s] step time: 0.005458s (±0.001189s); valid time: 0.004363s; loss: 37.1126 (±2.19029); valid loss: 102.143\n",
      "[Epoch 270/350, Step 5940, ETA 10.86s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 6000, ETA 10.48s] step time: 0.005534s (±0.001399s); valid time: 0.004312s; loss: 37.4743 (±2.90582); valid loss: 101.795\n",
      "[Epoch 278/350, Step 6100, ETA 9.861s] step time: 0.005478s (±0.001235s); valid time: 0.004371s; loss: 37.2247 (±2.31607); valid loss: 100.445\n",
      "[Epoch 280/350, Step 6160, ETA 9.489s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 6200, ETA 9.241s] step time: 0.005524s (±0.001242s); valid time: 0.004628s; loss: 37.333 (±1.997); valid loss: 100.531\n",
      "[Epoch 287/350, Step 6300, ETA 8.622s] step time: 0.005448s (±0.00113s); valid time: 0.00472s; loss: 37.2653 (±2.29346); valid loss: 100.122\n",
      "[Epoch 290/350, Step 6380, ETA 8.122s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 6400, ETA 7.998s] step time: 0.005323s (±0.0008314s); valid time: 0.004284s; loss: 37.2689 (±2.1881); valid loss: 100.228\n",
      "[Epoch 296/350, Step 6500, ETA 7.378s] step time: 0.005386s (±0.0009795s); valid time: 0.004316s; loss: 37.2568 (±2.04074); valid loss: 99.9538\n",
      "[Epoch 300/350, Step 6600, ETA 6.76s] step time: 0.005466s (±0.001015s); valid time: 0.004373s; loss: 37.2252 (±2.29878); valid loss: 99.4917\n",
      "[Epoch 300/350, Step 6600, ETA 6.76s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 305/350, Step 6700, ETA 6.142s] step time: 0.005439s (±0.001204s); valid time: 0.004385s; loss: 37.2379 (±2.34156); valid loss: 99.7192\n",
      "[Epoch 310/350, Step 6800, ETA 5.526s] step time: 0.005442s (±0.001045s); valid time: 0.004997s; loss: 37.2168 (±2.01134); valid loss: 100.083\n",
      "[Epoch 310/350, Step 6820, ETA 5.401s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 314/350, Step 6900, ETA 4.918s] step time: 0.006181s (±0.008115s); valid time: 0.08131s; loss: 37.2779 (±1.965); valid loss: 99.2115 (*)\n",
      "[Epoch 319/350, Step 7000, ETA 4.301s] step time: 0.005404s (±0.001321s); valid time: 0.004351s; loss: 37.1398 (±2.45221); valid loss: 99.7257\n",
      "[Epoch 320/350, Step 7040, ETA 4.053s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 323/350, Step 7100, ETA 3.684s] step time: 0.005375s (±0.00101s); valid time: 0.004333s; loss: 37.8431 (±6.08429); valid loss: 99.897\n",
      "[Epoch 328/350, Step 7200, ETA 3.069s] step time: 0.005458s (±0.001318s); valid time: 0.004321s; loss: 38.2288 (±10.9966); valid loss: 100.628\n",
      "[Epoch 330/350, Step 7260, ETA 2.699s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 7300, ETA 2.454s] step time: 0.005478s (±0.001593s); valid time: 0.004691s; loss: 37.2354 (±2.31436); valid loss: 99.6096\n",
      "[Epoch 337/350, Step 7400, ETA 1.84s] step time: 0.005414s (±0.001236s); valid time: 0.004275s; loss: 37.2924 (±2.38077); valid loss: 100.18\n",
      "[Epoch 340/350, Step 7480, ETA 1.349s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 7500, ETA 1.226s] step time: 0.005506s (±0.00102s); valid time: 0.004307s; loss: 37.2985 (±2.15526); valid loss: 100.175\n",
      "[Epoch 346/350, Step 7600, ETA 0.6126s] step time: 0.005307s (±0.0009503s); valid time: 0.004156s; loss: 37.2159 (±2.15677); valid loss: 100.227\n",
      "[Epoch 350/350, Step 7700, ETA 0s] step time: 0.005425s (±0.001323s); valid time: 0.006005s; loss: 37.2148 (±2.31421); valid loss: 99.5851\n",
      "[Epoch 350/350, Step 7700, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpu11wd5pe/variables.dat-6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpu11wd5pe/variables.dat-6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.84,\n",
      "\t(tp, fp, tn, fn)=(193, 654, 5887, 533),\n",
      "\tprecision=0.23,\n",
      "\trecall=0.27,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.11655428650061923,\n",
      "\ty_label%=0.09990367414338792,\n",
      ")\n",
      "Testing on realKnownCause/cpu_utilization_asg_misconfiguration.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 51.42s] step time: 0.009669s (±0.03232s); valid time: 0.1614s; loss: 133.945 (±10.511); valid loss: 140.58 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 20.29s] step time: 0.006151s (±0.00743s); valid time: 0.07379s; loss: 83.553 (±16.5436); valid loss: 80.655 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 9.485s] step time: 0.006175s (±0.009023s); valid time: 0.08974s; loss: 48.7686 (±7.95324); valid loss: 60.3173 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 3.117s] step time: 0.005957s (±0.008077s); valid time: 0.08066s; loss: 32.9677 (±5.90829); valid loss: 48.9515 (*)\n",
      "[Epoch 10/350, Step 480, ETA 1m 58.12s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 2m 0.009577s] step time: 0.006314s (±0.008555s); valid time: 0.0846s; loss: 22.273 (±5.12359); valid loss: 41.5442 (*)\n",
      "[Epoch 13/350, Step 600, ETA 1m 57.48s] step time: 0.006231s (±0.008756s); valid time: 0.08687s; loss: 14.9934 (±4.65795); valid loss: 36.4157 (*)\n",
      "[Epoch 15/350, Step 700, ETA 1m 55.47s] step time: 0.006228s (±0.008155s); valid time: 0.08175s; loss: 9.93396 (±4.75485); valid loss: 31.7189 (*)\n",
      "[Epoch 17/350, Step 800, ETA 1m 54.3s] step time: 0.00646s (±0.008507s); valid time: 0.08456s; loss: 5.45247 (±4.79017); valid loss: 28.5948 (*)\n",
      "[Epoch 19/350, Step 900, ETA 1m 52.74s] step time: 0.006197s (±0.008436s); valid time: 0.08439s; loss: 2.25432 (±4.02311); valid loss: 23.4226 (*)\n",
      "[Epoch 20/350, Step 960, ETA 1m 51.29s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 1000, ETA 1m 51.72s] step time: 0.006379s (±0.008286s); valid time: 0.08232s; loss: -0.802912 (±4.34193); valid loss: 21.358 (*)\n",
      "[Epoch 23/350, Step 1100, ETA 1m 50.36s] step time: 0.006064s (±0.007552s); valid time: 0.07577s; loss: -3.44325 (±4.26186); valid loss: 18.0435 (*)\n",
      "[Epoch 25/350, Step 1200, ETA 1m 49.36s] step time: 0.006313s (±0.00852s); valid time: 0.08443s; loss: -5.08511 (±5.0542); valid loss: 16.1953 (*)\n",
      "[Epoch 28/350, Step 1300, ETA 1m 48.44s] step time: 0.00617s (±0.008455s); valid time: 0.08268s; loss: -7.04771 (±4.81135); valid loss: 14.3374 (*)\n",
      "[Epoch 30/350, Step 1400, ETA 1m 46.7s] step time: 0.005582s (±0.001384s); valid time: 0.009148s; loss: -9.08314 (±4.21487); valid loss: 14.4264\n",
      "[Epoch 30/350, Step 1440, ETA 1m 46.05s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 1500, ETA 1m 46.05s] step time: 0.006475s (±0.00842s); valid time: 0.08379s; loss: -10.6164 (±4.41109); valid loss: 12.955 (*)\n",
      "[Epoch 34/350, Step 1600, ETA 1m 45.21s] step time: 0.006307s (±0.00905s); valid time: 0.09059s; loss: -12.2387 (±4.0465); valid loss: 7.90963 (*)\n",
      "[Epoch 36/350, Step 1700, ETA 1m 44.36s] step time: 0.006293s (±0.008489s); valid time: 0.08443s; loss: -13.1233 (±4.64385); valid loss: 5.9635 (*)\n",
      "[Epoch 38/350, Step 1800, ETA 1m 42.81s] step time: 0.00541s (±0.001768s); valid time: 0.008752s; loss: -14.7038 (±4.41801); valid loss: 7.50806\n",
      "[Epoch 40/350, Step 1900, ETA 1m 42.01s] step time: 0.006209s (±0.008744s); valid time: 0.08685s; loss: -15.7739 (±4.61293); valid loss: 5.33116 (*)\n",
      "[Epoch 40/350, Step 1920, ETA 1m 41.67s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 2000, ETA 1m 41.32s] step time: 0.006362s (±0.008468s); valid time: 0.08357s; loss: -16.6408 (±4.45181); valid loss: 4.47714 (*)\n",
      "[Epoch 44/350, Step 2100, ETA 1m 40.51s] step time: 0.006199s (±0.008402s); valid time: 0.08413s; loss: -17.7663 (±4.51879); valid loss: 3.85271 (*)\n",
      "[Epoch 46/350, Step 2200, ETA 1m 39.72s] step time: 0.006222s (±0.008561s); valid time: 0.08518s; loss: -18.7932 (±4.01151); valid loss: 3.73672 (*)\n",
      "[Epoch 48/350, Step 2300, ETA 1m 39.03s] step time: 0.006352s (±0.01009s); valid time: 0.08692s; loss: -20.1281 (±3.84222); valid loss: 1.91295 (*)\n",
      "[Epoch 50/350, Step 2400, ETA 1m 38.25s] step time: 0.006212s (±0.008489s); valid time: 0.08542s; loss: -20.6853 (±4.05629); valid loss: 1.85063 (*)\n",
      "[Epoch 50/350, Step 2400, ETA 1m 38.25s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2500, ETA 1m 37.63s] step time: 0.006352s (±0.008796s); valid time: 0.08686s; loss: -21.8831 (±4.17347); valid loss: 0.681474 (*)\n",
      "[Epoch 55/350, Step 2600, ETA 1m 36.49s] step time: 0.00552s (±0.001481s); valid time: 0.009025s; loss: -22.4586 (±3.8172); valid loss: 1.28483\n",
      "[Epoch 57/350, Step 2700, ETA 1m 35.77s] step time: 0.006249s (±0.008463s); valid time: 0.08513s; loss: -23.1493 (±4.07533); valid loss: 0.521037 (*)\n",
      "[Epoch 59/350, Step 2800, ETA 1m 35s] step time: 0.006072s (±0.008083s); valid time: 0.08086s; loss: -23.4675 (±4.10272); valid loss: -0.012402 (*)\n",
      "[Epoch 60/350, Step 2880, ETA 1m 34.06s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2900, ETA 1m 34.3s] step time: 0.006285s (±0.009603s); valid time: 0.09633s; loss: -24.0692 (±4.36201); valid loss: -0.428292 (*)\n",
      "[Epoch 63/350, Step 3000, ETA 1m 33.17s] step time: 0.005325s (±0.001168s); valid time: 0.008752s; loss: -24.6526 (±3.76432); valid loss: -0.376167\n",
      "[Epoch 65/350, Step 3100, ETA 1m 32.48s] step time: 0.00626s (±0.008333s); valid time: 0.08312s; loss: -25.3677 (±4.0962); valid loss: -2.089 (*)\n",
      "[Epoch 67/350, Step 3200, ETA 1m 31.43s] step time: 0.005399s (±0.001021s); valid time: 0.008743s; loss: -25.5094 (±3.95873); valid loss: -1.34165\n",
      "[Epoch 69/350, Step 3300, ETA 1m 30.79s] step time: 0.006295s (±0.00882s); valid time: 0.08815s; loss: -26.376 (±4.36976); valid loss: -2.79666 (*)\n",
      "[Epoch 70/350, Step 3360, ETA 1m 30.18s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 71/350, Step 3400, ETA 1m 29.89s] step time: 0.005677s (±0.001669s); valid time: 0.01137s; loss: -26.7236 (±3.91849); valid loss: -2.36503\n",
      "[Epoch 73/350, Step 3500, ETA 1m 28.91s] step time: 0.005389s (±0.001085s); valid time: 0.008581s; loss: -27.187 (±4.13623); valid loss: -0.976059\n",
      "[Epoch 75/350, Step 3600, ETA 1m 28.26s] step time: 0.006274s (±0.00827s); valid time: 0.08246s; loss: -27.6125 (±3.68304); valid loss: -2.89628 (*)\n",
      "[Epoch 78/350, Step 3700, ETA 1m 27.58s] step time: 0.006074s (±0.009005s); valid time: 0.09034s; loss: -27.9 (±4.07389); valid loss: -3.55466 (*)\n",
      "[Epoch 80/350, Step 3800, ETA 1m 26.92s] step time: 0.006238s (±0.008315s); valid time: 0.08317s; loss: -28.5657 (±4.06327); valid loss: -4.641 (*)\n",
      "[Epoch 80/350, Step 3840, ETA 1m 26.5s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 3900, ETA 1m 26.11s] step time: 0.005784s (±0.001722s); valid time: 0.009263s; loss: -28.5346 (±3.87885); valid loss: -2.73121\n",
      "[Epoch 84/350, Step 4000, ETA 1m 25.48s] step time: 0.00632s (±0.00758s); valid time: 0.07593s; loss: -29.036 (±4.07012); valid loss: -5.04781 (*)\n",
      "[Epoch 86/350, Step 4100, ETA 1m 24.63s] step time: 0.005536s (±0.001609s); valid time: 0.008839s; loss: -29.1753 (±4.0172); valid loss: -4.86787\n",
      "[Epoch 88/350, Step 4200, ETA 1m 23.75s] step time: 0.005496s (±0.00132s); valid time: 0.009045s; loss: -29.6866 (±3.70947); valid loss: -4.22392\n",
      "[Epoch 90/350, Step 4300, ETA 1m 22.89s] step time: 0.005479s (±0.00118s); valid time: 0.008846s; loss: -29.6371 (±4.38715); valid loss: -4.73901\n",
      "[Epoch 90/350, Step 4320, ETA 1m 22.68s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4400, ETA 1m 22.26s] step time: 0.006289s (±0.008529s); valid time: 0.08558s; loss: -30.2526 (±4.04315); valid loss: -5.15392 (*)\n",
      "[Epoch 94/350, Step 4500, ETA 1m 21.42s] step time: 0.00552s (±0.001845s); valid time: 0.01625s; loss: -30.2076 (±3.98919); valid loss: -3.82503\n",
      "[Epoch 96/350, Step 4600, ETA 1m 20.79s] step time: 0.006252s (±0.008288s); valid time: 0.08256s; loss: -30.6347 (±4.08576); valid loss: -5.85135 (*)\n",
      "[Epoch 98/350, Step 4700, ETA 1m 19.98s] step time: 0.005514s (±0.001367s); valid time: 0.008876s; loss: -30.826 (±4.04915); valid loss: -5.07535\n",
      "[Epoch 100/350, Step 4800, ETA 1m 19.29s] step time: 0.006055s (±0.008329s); valid time: 0.08324s; loss: -30.9358 (±4.17526); valid loss: -5.87247 (*)\n",
      "[Epoch 100/350, Step 4800, ETA 1m 19.29s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4900, ETA 1m 18.55s] step time: 0.00567s (±0.00144s); valid time: 0.009099s; loss: -31.1036 (±4.12065); valid loss: -4.32001\n",
      "[Epoch 105/350, Step 5000, ETA 1m 17.78s] step time: 0.005658s (±0.001346s); valid time: 0.008968s; loss: -31.235 (±4.02075); valid loss: -4.43314\n",
      "[Epoch 107/350, Step 5100, ETA 1m 16.98s] step time: 0.005525s (±0.001236s); valid time: 0.00916s; loss: -31.4437 (±4.33395); valid loss: -5.73978\n",
      "[Epoch 109/350, Step 5200, ETA 1m 16.4s] step time: 0.006429s (±0.008461s); valid time: 0.08432s; loss: -31.5673 (±4.32522); valid loss: -5.99904 (*)\n",
      "[Epoch 110/350, Step 5280, ETA 1m 15.75s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5300, ETA 1m 15.82s] step time: 0.006486s (±0.009769s); valid time: 0.09774s; loss: -31.3714 (±3.9749); valid loss: -6.139 (*)\n",
      "[Epoch 113/350, Step 5400, ETA 1m 15.21s] step time: 0.006365s (±0.008508s); valid time: 0.0829s; loss: -32.0721 (±3.77503); valid loss: -6.21315 (*)\n",
      "[Epoch 115/350, Step 5500, ETA 1m 14.52s] step time: 0.005931s (±0.001848s); valid time: 0.01045s; loss: -31.8127 (±4.15341); valid loss: -5.12261\n",
      "[Epoch 117/350, Step 5600, ETA 1m 13.75s] step time: 0.005594s (±0.001318s); valid time: 0.008714s; loss: -32.0578 (±3.88709); valid loss: -5.54827\n",
      "[Epoch 119/350, Step 5700, ETA 1m 12.98s] step time: 0.005537s (±0.00155s); valid time: 0.008312s; loss: -31.9445 (±3.94434); valid loss: -5.6326\n",
      "[Epoch 120/350, Step 5760, ETA 1m 12.49s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 5800, ETA 1m 12.37s] step time: 0.006305s (±0.00854s); valid time: 0.08542s; loss: -32.19 (±3.9592); valid loss: -7.0833 (*)\n",
      "[Epoch 123/350, Step 5900, ETA 1m 11.63s] step time: 0.005679s (±0.001528s); valid time: 0.01153s; loss: -32.5251 (±3.88662); valid loss: -5.99445\n",
      "[Epoch 125/350, Step 6000, ETA 1m 10.88s] step time: 0.005584s (±0.001613s); valid time: 0.008713s; loss: -32.4562 (±4.05821); valid loss: -6.30471\n",
      "[Epoch 128/350, Step 6100, ETA 1m 10.15s] step time: 0.005546s (±0.001406s); valid time: 0.009004s; loss: -32.4998 (±4.03428); valid loss: -6.14758\n",
      "[Epoch 130/350, Step 6200, ETA 1m 9.377s] step time: 0.005433s (±0.001068s); valid time: 0.008916s; loss: -32.6075 (±3.79609); valid loss: -6.04721\n",
      "[Epoch 130/350, Step 6240, ETA 1m 9.039s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 6300, ETA 1m 8.629s] step time: 0.005513s (±0.001581s); valid time: 0.008811s; loss: -32.7137 (±3.7936); valid loss: -6.94011\n",
      "[Epoch 134/350, Step 6400, ETA 1m 7.892s] step time: 0.005532s (±0.001364s); valid time: 0.01238s; loss: -32.8813 (±4.25416); valid loss: -6.27427\n",
      "[Epoch 136/350, Step 6500, ETA 1m 7.17s] step time: 0.005608s (±0.001631s); valid time: 0.008774s; loss: -32.7428 (±4.15798); valid loss: -6.01044\n",
      "[Epoch 138/350, Step 6600, ETA 1m 6.415s] step time: 0.005387s (±0.001298s); valid time: 0.008683s; loss: -32.837 (±3.87117); valid loss: -6.7678\n",
      "[Epoch 140/350, Step 6700, ETA 1m 5.669s] step time: 0.00541s (±0.001319s); valid time: 0.009018s; loss: -32.9097 (±3.83206); valid loss: -6.31203\n",
      "[Epoch 140/350, Step 6720, ETA 1m 5.503s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6800, ETA 1m 5.03s] step time: 0.006109s (±0.008446s); valid time: 0.08438s; loss: -32.9963 (±4.25634); valid loss: -7.55701 (*)\n",
      "[Epoch 144/350, Step 6900, ETA 1m 4.282s] step time: 0.00537s (±0.001199s); valid time: 0.009114s; loss: -32.995 (±3.87024); valid loss: -6.76799\n",
      "[Epoch 146/350, Step 7000, ETA 1m 3.672s] step time: 0.006298s (±0.008332s); valid time: 0.0831s; loss: -33.0789 (±4.10174); valid loss: -7.79638 (*)\n",
      "[Epoch 148/350, Step 7100, ETA 1m 2.937s] step time: 0.005411s (±0.001534s); valid time: 0.0133s; loss: -33.1464 (±4.1933); valid loss: -7.49458\n",
      "[Epoch 150/350, Step 7200, ETA 1m 2.254s] step time: 0.005737s (±0.001519s); valid time: 0.009004s; loss: -33.1582 (±3.61679); valid loss: -7.48814\n",
      "[Epoch 150/350, Step 7200, ETA 1m 2.254s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7300, ETA 1m 1.549s] step time: 0.005447s (±0.001556s); valid time: 0.009186s; loss: -33.2371 (±3.81417); valid loss: -7.16203\n",
      "[Epoch 155/350, Step 7400, ETA 1m 0.8324s] step time: 0.005473s (±0.001585s); valid time: 0.009211s; loss: -33.4148 (±4.60367); valid loss: -7.38876\n",
      "[Epoch 157/350, Step 7500, ETA 1m 0.1284s] step time: 0.005549s (±0.001333s); valid time: 0.009073s; loss: -32.9171 (±4.34761); valid loss: -6.57535\n",
      "[Epoch 159/350, Step 7600, ETA 59.39s] step time: 0.005161s (±0.001345s); valid time: 0.008657s; loss: -33.4885 (±4.11904); valid loss: -5.98203\n",
      "[Epoch 160/350, Step 7680, ETA 58.8s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7700, ETA 58.68s] step time: 0.005433s (±0.001362s); valid time: 0.009497s; loss: -33.3004 (±3.6645); valid loss: -5.77411\n",
      "[Epoch 163/350, Step 7800, ETA 58.06s] step time: 0.006203s (±0.008451s); valid time: 0.0816s; loss: -33.3537 (±4.00374); valid loss: -7.80366 (*)\n",
      "[Epoch 165/350, Step 7900, ETA 57.34s] step time: 0.005294s (±0.001123s); valid time: 0.009178s; loss: -33.4454 (±4.14631); valid loss: -6.50531\n",
      "[Epoch 167/350, Step 8000, ETA 56.64s] step time: 0.005464s (±0.001456s); valid time: 0.008837s; loss: -33.5142 (±4.24334); valid loss: -6.98654\n",
      "[Epoch 169/350, Step 8100, ETA 55.93s] step time: 0.005388s (±0.001619s); valid time: 0.009398s; loss: -33.5824 (±3.90243); valid loss: -6.77129\n",
      "[Epoch 170/350, Step 8160, ETA 55.51s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 171/350, Step 8200, ETA 55.24s] step time: 0.005529s (±0.001355s); valid time: 0.008805s; loss: -33.3834 (±4.1192); valid loss: -7.73436\n",
      "[Epoch 173/350, Step 8300, ETA 54.56s] step time: 0.005596s (±0.001583s); valid time: 0.009874s; loss: -33.3332 (±4.24469); valid loss: -6.61852\n",
      "[Epoch 175/350, Step 8400, ETA 53.85s] step time: 0.00526s (±0.001176s); valid time: 0.008475s; loss: -33.5876 (±4.90248); valid loss: -6.56436\n",
      "[Epoch 178/350, Step 8500, ETA 53.17s] step time: 0.005382s (±0.001285s); valid time: 0.009034s; loss: -33.5192 (±4.28229); valid loss: -5.73646\n",
      "[Epoch 180/350, Step 8600, ETA 52.47s] step time: 0.005392s (±0.00128s); valid time: 0.009348s; loss: -33.6278 (±4.30361); valid loss: -7.49285\n",
      "[Epoch 180/350, Step 8640, ETA 52.2s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 8700, ETA 51.81s] step time: 0.005661s (±0.001403s); valid time: 0.00875s; loss: -33.5296 (±4.17958); valid loss: -6.95846\n",
      "[Epoch 184/350, Step 8800, ETA 51.13s] step time: 0.0054s (±0.001187s); valid time: 0.008895s; loss: -33.4991 (±3.78621); valid loss: -7.76464\n",
      "[Epoch 186/350, Step 8900, ETA 50.46s] step time: 0.005555s (±0.001908s); valid time: 0.009888s; loss: -33.6425 (±3.6756); valid loss: -6.904\n",
      "[Epoch 188/350, Step 9000, ETA 49.79s] step time: 0.005585s (±0.001533s); valid time: 0.009378s; loss: -33.8376 (±3.81831); valid loss: -7.23324\n",
      "[Epoch 190/350, Step 9100, ETA 49.11s] step time: 0.005462s (±0.001248s); valid time: 0.008722s; loss: -33.4492 (±4.5114); valid loss: -7.33746\n",
      "[Epoch 190/350, Step 9120, ETA 48.97s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 9200, ETA 48.51s] step time: 0.006364s (±0.008141s); valid time: 0.08123s; loss: -33.7278 (±4.39123); valid loss: -7.96165 (*)\n",
      "[Epoch 194/350, Step 9300, ETA 47.83s] step time: 0.005447s (±0.001853s); valid time: 0.01252s; loss: -33.704 (±4.25983); valid loss: -7.45103\n",
      "[Epoch 196/350, Step 9400, ETA 47.16s] step time: 0.005498s (±0.001169s); valid time: 0.00889s; loss: -33.8631 (±3.82149); valid loss: -7.18158\n",
      "[Epoch 198/350, Step 9500, ETA 46.49s] step time: 0.005482s (±0.001189s); valid time: 0.009304s; loss: -33.6073 (±4.05554); valid loss: -7.65574\n",
      "[Epoch 200/350, Step 9600, ETA 45.82s] step time: 0.005391s (±0.001172s); valid time: 0.008624s; loss: -33.7305 (±3.94514); valid loss: -6.97475\n",
      "[Epoch 200/350, Step 9600, ETA 45.82s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9700, ETA 45.16s] step time: 0.005482s (±0.001302s); valid time: 0.00906s; loss: -33.8173 (±3.87133); valid loss: -7.20425\n",
      "[Epoch 205/350, Step 9800, ETA 44.49s] step time: 0.005412s (±0.001154s); valid time: 0.009411s; loss: -33.6375 (±4.48032); valid loss: -7.18618\n",
      "[Epoch 207/350, Step 9900, ETA 43.83s] step time: 0.005556s (±0.001413s); valid time: 0.008271s; loss: -34.0244 (±4.12609); valid loss: -7.36363\n",
      "[Epoch 209/350, Step 10000, ETA 43.17s] step time: 0.005446s (±0.001222s); valid time: 0.008878s; loss: -33.3532 (±4.66983); valid loss: -6.97884\n",
      "[Epoch 210/350, Step 10080, ETA 42.63s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 10100, ETA 42.5s] step time: 0.005331s (±0.001132s); valid time: 0.008267s; loss: -33.5065 (±3.95075); valid loss: -7.8724\n",
      "[Epoch 213/350, Step 10200, ETA 41.83s] step time: 0.005364s (±0.001245s); valid time: 0.008903s; loss: -34.0128 (±3.64131); valid loss: -7.63552\n",
      "[Epoch 215/350, Step 10300, ETA 41.16s] step time: 0.005269s (±0.001112s); valid time: 0.008917s; loss: -33.587 (±3.53857); valid loss: -7.28238\n",
      "[Epoch 217/350, Step 10400, ETA 40.52s] step time: 0.005795s (±0.001796s); valid time: 0.008622s; loss: -33.8989 (±3.90534); valid loss: -7.26681\n",
      "[Epoch 219/350, Step 10500, ETA 39.87s] step time: 0.005538s (±0.002024s); valid time: 0.009267s; loss: -34.0628 (±3.96205); valid loss: -7.50718\n",
      "[Epoch 220/350, Step 10560, ETA 39.47s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 10600, ETA 39.22s] step time: 0.005484s (±0.001283s); valid time: 0.009084s; loss: -33.4261 (±4.16159); valid loss: -7.25762\n",
      "[Epoch 223/350, Step 10700, ETA 38.57s] step time: 0.005585s (±0.001633s); valid time: 0.009773s; loss: -33.9267 (±4.64118); valid loss: -7.54958\n",
      "[Epoch 225/350, Step 10800, ETA 37.91s] step time: 0.005308s (±0.001135s); valid time: 0.00878s; loss: -33.7896 (±4.60405); valid loss: -6.59158\n",
      "[Epoch 228/350, Step 10900, ETA 37.26s] step time: 0.005465s (±0.00131s); valid time: 0.008808s; loss: -33.7834 (±3.61195); valid loss: -7.71254\n",
      "[Epoch 230/350, Step 11000, ETA 36.61s] step time: 0.005413s (±0.001148s); valid time: 0.008849s; loss: -33.778 (±3.7824); valid loss: -7.60371\n",
      "[Epoch 230/350, Step 11040, ETA 36.34s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 11100, ETA 35.95s] step time: 0.005426s (±0.001467s); valid time: 0.008787s; loss: -33.7482 (±3.80597); valid loss: -7.75784\n",
      "[Epoch 234/350, Step 11200, ETA 35.31s] step time: 0.005651s (±0.00172s); valid time: 0.009129s; loss: -33.7378 (±3.96441); valid loss: -7.38007\n",
      "[Epoch 236/350, Step 11300, ETA 34.66s] step time: 0.005286s (±0.001266s); valid time: 0.009884s; loss: -33.7962 (±4.229); valid loss: -6.53487\n",
      "[Epoch 238/350, Step 11400, ETA 34.01s] step time: 0.005397s (±0.001227s); valid time: 0.008972s; loss: -33.9256 (±4.29191); valid loss: -6.93762\n",
      "[Epoch 240/350, Step 11500, ETA 33.35s] step time: 0.005346s (±0.001084s); valid time: 0.009331s; loss: -33.6127 (±4.33624); valid loss: -7.05239\n",
      "[Epoch 240/350, Step 11520, ETA 33.22s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11600, ETA 32.71s] step time: 0.005457s (±0.001282s); valid time: 0.008837s; loss: -33.8448 (±4.1728); valid loss: -5.96766\n",
      "[Epoch 244/350, Step 11700, ETA 32.06s] step time: 0.005469s (±0.001743s); valid time: 0.01193s; loss: -33.9407 (±4.0461); valid loss: -7.55136\n",
      "[Epoch 246/350, Step 11800, ETA 31.42s] step time: 0.005475s (±0.001561s); valid time: 0.009021s; loss: -33.9678 (±4.3589); valid loss: -7.38762\n",
      "[Epoch 248/350, Step 11900, ETA 30.77s] step time: 0.005392s (±0.001339s); valid time: 0.008892s; loss: -33.7668 (±4.30025); valid loss: -6.89193\n",
      "[Epoch 250/350, Step 12000, ETA 30.16s] step time: 0.006149s (±0.008391s); valid time: 0.08412s; loss: -33.9161 (±4.08467); valid loss: -8.08277 (*)\n",
      "[Epoch 250/350, Step 12000, ETA 30.16s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 12100, ETA 29.52s] step time: 0.005489s (±0.001378s); valid time: 0.008922s; loss: -33.7719 (±4.07158); valid loss: -6.28543\n",
      "[Epoch 255/350, Step 12200, ETA 28.88s] step time: 0.005347s (±0.00118s); valid time: 0.008874s; loss: -34.056 (±4.12115); valid loss: -6.96071\n",
      "[Epoch 257/350, Step 12300, ETA 28.24s] step time: 0.005526s (±0.001304s); valid time: 0.008564s; loss: -33.5369 (±3.68608); valid loss: -7.14538\n",
      "[Epoch 259/350, Step 12400, ETA 27.61s] step time: 0.005776s (±0.001711s); valid time: 0.009239s; loss: -33.9583 (±4.36438); valid loss: -7.62715\n",
      "[Epoch 260/350, Step 12480, ETA 27.1s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12500, ETA 26.98s] step time: 0.005584s (±0.001301s); valid time: 0.01022s; loss: -34.0823 (±4.85194); valid loss: -6.70591\n",
      "[Epoch 263/350, Step 12600, ETA 26.34s] step time: 0.005516s (±0.001458s); valid time: 0.009028s; loss: -33.6003 (±4.23211); valid loss: -7.55675\n",
      "[Epoch 265/350, Step 12700, ETA 25.7s] step time: 0.005334s (±0.001129s); valid time: 0.009047s; loss: -33.7465 (±4.21056); valid loss: -7.81955\n",
      "[Epoch 267/350, Step 12800, ETA 25.06s] step time: 0.005434s (±0.001528s); valid time: 0.008676s; loss: -33.9855 (±4.28341); valid loss: -7.39902\n",
      "[Epoch 269/350, Step 12900, ETA 24.42s] step time: 0.00539s (±0.001355s); valid time: 0.008561s; loss: -33.7392 (±4.41438); valid loss: -6.92489\n",
      "[Epoch 270/350, Step 12960, ETA 24.04s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 271/350, Step 13000, ETA 23.78s] step time: 0.005448s (±0.001325s); valid time: 0.009176s; loss: -34.03 (±3.7863); valid loss: -6.76371\n",
      "[Epoch 273/350, Step 13100, ETA 23.15s] step time: 0.005618s (±0.00141s); valid time: 0.008613s; loss: -33.4345 (±4.14119); valid loss: -6.77179\n",
      "[Epoch 275/350, Step 13200, ETA 22.52s] step time: 0.00536s (±0.001333s); valid time: 0.008793s; loss: -34.158 (±4.48121); valid loss: -7.16613\n",
      "[Epoch 278/350, Step 13300, ETA 21.89s] step time: 0.005459s (±0.001287s); valid time: 0.008922s; loss: -33.9393 (±3.87613); valid loss: -7.53302\n",
      "[Epoch 280/350, Step 13400, ETA 21.25s] step time: 0.005437s (±0.001343s); valid time: 0.008803s; loss: -33.6817 (±4.03565); valid loss: -7.81785\n",
      "[Epoch 280/350, Step 13440, ETA 20.99s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 13500, ETA 20.62s] step time: 0.005483s (±0.001261s); valid time: 0.009083s; loss: -33.7657 (±4.334); valid loss: -7.99957\n",
      "[Epoch 284/350, Step 13600, ETA 19.99s] step time: 0.005457s (±0.001309s); valid time: 0.008974s; loss: -33.9351 (±3.79814); valid loss: -7.35308\n",
      "[Epoch 286/350, Step 13700, ETA 19.36s] step time: 0.005481s (±0.001258s); valid time: 0.009342s; loss: -33.9287 (±4.28332); valid loss: -7.60409\n",
      "[Epoch 288/350, Step 13800, ETA 18.75s] step time: 0.006361s (±0.008888s); valid time: 0.0876s; loss: -33.7596 (±4.38986); valid loss: -8.23523 (*)\n",
      "[Epoch 290/350, Step 13900, ETA 18.12s] step time: 0.005758s (±0.001667s); valid time: 0.009062s; loss: -33.9579 (±4.20134); valid loss: -6.73396\n",
      "[Epoch 290/350, Step 13920, ETA 17.99s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 14000, ETA 17.49s] step time: 0.005429s (±0.001242s); valid time: 0.009029s; loss: -33.8133 (±4); valid loss: -7.67383\n",
      "[Epoch 294/350, Step 14100, ETA 16.86s] step time: 0.005685s (±0.002042s); valid time: 0.01245s; loss: -33.7901 (±4.55506); valid loss: -6.9129\n",
      "[Epoch 296/350, Step 14200, ETA 16.23s] step time: 0.005326s (±0.001266s); valid time: 0.00896s; loss: -33.9504 (±3.93701); valid loss: -7.21113\n",
      "[Epoch 298/350, Step 14300, ETA 15.6s] step time: 0.005729s (±0.001486s); valid time: 0.009017s; loss: -33.6473 (±4.43279); valid loss: -6.52253\n",
      "[Epoch 300/350, Step 14400, ETA 14.98s] step time: 0.005483s (±0.001427s); valid time: 0.008628s; loss: -33.9976 (±4.1251); valid loss: -7.27791\n",
      "[Epoch 300/350, Step 14400, ETA 14.98s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 14500, ETA 14.35s] step time: 0.005506s (±0.001201s); valid time: 0.009039s; loss: -33.9251 (±3.92814); valid loss: -6.86432\n",
      "[Epoch 305/350, Step 14600, ETA 13.73s] step time: 0.006289s (±0.00867s); valid time: 0.08698s; loss: -33.946 (±4.20872); valid loss: -8.27766 (*)\n",
      "[Epoch 307/350, Step 14700, ETA 13.11s] step time: 0.005492s (±0.001252s); valid time: 0.009296s; loss: -33.7673 (±4.45576); valid loss: -6.35456\n",
      "[Epoch 309/350, Step 14800, ETA 12.48s] step time: 0.005586s (±0.001488s); valid time: 0.009218s; loss: -33.9266 (±4.72462); valid loss: -6.92567\n",
      "[Epoch 310/350, Step 14880, ETA 11.97s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14900, ETA 11.85s] step time: 0.005538s (±0.001754s); valid time: 0.0101s; loss: -33.5529 (±4.01705); valid loss: -7.12035\n",
      "[Epoch 313/350, Step 15000, ETA 11.22s] step time: 0.005437s (±0.001214s); valid time: 0.009129s; loss: -34.2013 (±4.39425); valid loss: -6.84546\n",
      "[Epoch 315/350, Step 15100, ETA 10.6s] step time: 0.005733s (±0.001526s); valid time: 0.008546s; loss: -33.4198 (±4.47777); valid loss: -7.69026\n",
      "[Epoch 317/350, Step 15200, ETA 9.973s] step time: 0.005404s (±0.001135s); valid time: 0.008805s; loss: -34.4575 (±4.17636); valid loss: -7.59232\n",
      "[Epoch 319/350, Step 15300, ETA 9.347s] step time: 0.005496s (±0.001779s); valid time: 0.009553s; loss: -33.6471 (±3.76744); valid loss: -7.35205\n",
      "[Epoch 320/350, Step 15360, ETA 8.973s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 15400, ETA 8.727s] step time: 0.005985s (±0.001711s); valid time: 0.008781s; loss: -33.7735 (±4.12745); valid loss: -7.22753\n",
      "[Epoch 323/350, Step 15500, ETA 8.101s] step time: 0.005498s (±0.001639s); valid time: 0.009111s; loss: -33.7444 (±4.0683); valid loss: -6.83391\n",
      "[Epoch 325/350, Step 15600, ETA 7.477s] step time: 0.005573s (±0.001385s); valid time: 0.008556s; loss: -34.0482 (±4.20434); valid loss: -6.9931\n",
      "[Epoch 328/350, Step 15700, ETA 6.852s] step time: 0.005476s (±0.001315s); valid time: 0.00914s; loss: -33.995 (±4.29436); valid loss: -6.98792\n",
      "[Epoch 330/350, Step 15800, ETA 6.229s] step time: 0.005663s (±0.001791s); valid time: 0.009297s; loss: -33.5902 (±3.92685); valid loss: -7.07405\n",
      "[Epoch 330/350, Step 15840, ETA 5.979s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 15900, ETA 5.607s] step time: 0.005874s (±0.001488s); valid time: 0.009392s; loss: -34.1954 (±4.59269); valid loss: -7.29713\n",
      "[Epoch 334/350, Step 16000, ETA 4.982s] step time: 0.005369s (±0.001166s); valid time: 0.008692s; loss: -33.6114 (±4.47275); valid loss: -7.61824\n",
      "[Epoch 336/350, Step 16100, ETA 4.358s] step time: 0.005384s (±0.00139s); valid time: 0.009246s; loss: -33.8132 (±3.98669); valid loss: -7.26203\n",
      "[Epoch 338/350, Step 16200, ETA 3.734s] step time: 0.005467s (±0.001169s); valid time: 0.009566s; loss: -33.7806 (±3.63363); valid loss: -8.09098\n",
      "[Epoch 340/350, Step 16300, ETA 3.111s] step time: 0.005481s (±0.001489s); valid time: 0.008784s; loss: -34.0939 (±4.0774); valid loss: -7.82279\n",
      "[Epoch 340/350, Step 16320, ETA 2.986s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 16400, ETA 2.488s] step time: 0.005327s (±0.001134s); valid time: 0.009122s; loss: -33.9337 (±4.27594); valid loss: -6.84588\n",
      "[Epoch 344/350, Step 16500, ETA 1.865s] step time: 0.005504s (±0.001284s); valid time: 0.0086s; loss: -33.7002 (±4.04198); valid loss: -8.19322\n",
      "[Epoch 346/350, Step 16600, ETA 1.243s] step time: 0.005501s (±0.001304s); valid time: 0.00812s; loss: -33.9213 (±3.63928); valid loss: -7.37672\n",
      "[Epoch 348/350, Step 16700, ETA 0.6214s] step time: 0.005311s (±0.001219s); valid time: 0.009541s; loss: -33.7837 (±3.92155); valid loss: -7.70388\n",
      "[Epoch 350/350, Step 16800, ETA 0s] step time: 0.00546s (±0.001167s); valid time: 0.009122s; loss: -33.9041 (±4.0618); valid loss: -7.13971\n",
      "[Epoch 350/350, Step 16800, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj1vnclpx/variables.dat-14600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj1vnclpx/variables.dat-14600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.86,\n",
      "\t(tp, fp, tn, fn)=(1055, 2031, 14520, 444),\n",
      "\tprecision=0.34,\n",
      "\trecall=0.7,\n",
      "\tf1=0.46,\n",
      "\troc_auc=0.79,\n",
      "\ty_pred%=0.17096952908587257,\n",
      "\ty_label%=0.08304709141274239,\n",
      ")\n",
      "Testing on realKnownCause/ec2_request_latency_system_failure.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 100, ETA 37.9s] step time: 0.009579s (±0.03149s); valid time: 0.1577s; loss: 147.433 (±12.7756); valid loss: 117.775 (*)\n",
      "[Epoch 10/350, Step 110, ETA 36.28s] Learning rate decreased to 0.00075\n",
      "[Epoch 19/350, Step 200, ETA 30.48s] step time: 0.006101s (±0.007462s); valid time: 0.07396s; loss: 135.886 (±1.92456); valid loss: 116.786 (*)\n",
      "[Epoch 20/350, Step 220, ETA 29.39s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 28/350, Step 300, ETA 27.42s] step time: 0.005983s (±0.007913s); valid time: 0.07898s; loss: 135.25 (±1.8825); valid loss: 116.699 (*)\n",
      "[Epoch 30/350, Step 330, ETA 26.65s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 37/350, Step 400, ETA 25.06s] step time: 0.005388s (±0.0009704s); valid time: 0.002255s; loss: 133.88 (±1.87836); valid loss: 117.196\n",
      "[Epoch 40/350, Step 440, ETA 24.34s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 46/350, Step 500, ETA 23.47s] step time: 0.005473s (±0.001379s); valid time: 0.002108s; loss: 132.568 (±1.80146); valid loss: 117.185\n",
      "[Epoch 50/350, Step 550, ETA 22.76s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 55/350, Step 600, ETA 22.16s] step time: 0.005385s (±0.001001s); valid time: 0.002197s; loss: 131.931 (±1.72185); valid loss: 117.208\n",
      "[Epoch 60/350, Step 660, ETA 21.46s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 64/350, Step 700, ETA 21.12s] step time: 0.005476s (±0.0009094s); valid time: 0.00197s; loss: 131.806 (±1.77926); valid loss: 117.5\n",
      "[Epoch 70/350, Step 770, ETA 20.49s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 800, ETA 20.21s] step time: 0.005542s (±0.001431s); valid time: 0.002165s; loss: 131.585 (±1.89987); valid loss: 117.089\n",
      "[Epoch 80/350, Step 880, ETA 19.51s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 900, ETA 19.34s] step time: 0.005452s (±0.001193s); valid time: 0.002073s; loss: 131.417 (±1.91297); valid loss: 117.016\n",
      "[Epoch 90/350, Step 990, ETA 18.55s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 1000, ETA 18.48s] step time: 0.005347s (±0.0009823s); valid time: 0.00374s; loss: 131.437 (±1.70684); valid loss: 116.997\n",
      "[Epoch 100/350, Step 1100, ETA 17.67s] step time: 0.005354s (±0.001093s); valid time: 0.002071s; loss: 131.281 (±1.59878); valid loss: 117.058\n",
      "[Epoch 100/350, Step 1100, ETA 17.67s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 1200, ETA 16.88s] step time: 0.005249s (±0.0009592s); valid time: 0.002197s; loss: 131.279 (±1.54088); valid loss: 117.14\n",
      "[Epoch 110/350, Step 1210, ETA 16.81s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 119/350, Step 1300, ETA 16.15s] step time: 0.005361s (±0.0008403s); valid time: 0.002151s; loss: 131.345 (±1.75391); valid loss: 116.887\n",
      "[Epoch 120/350, Step 1320, ETA 15.99s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 128/350, Step 1400, ETA 15.44s] step time: 0.005431s (±0.001075s); valid time: 0.00215s; loss: 131.192 (±1.94653); valid loss: 116.97\n",
      "[Epoch 130/350, Step 1430, ETA 15.22s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 137/350, Step 1500, ETA 14.74s] step time: 0.005347s (±0.000798s); valid time: 0.002209s; loss: 131.293 (±1.63837); valid loss: 116.951\n",
      "[Epoch 140/350, Step 1540, ETA 14.5s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 146/350, Step 1600, ETA 14.11s] step time: 0.005709s (±0.001352s); valid time: 0.002115s; loss: 131.169 (±1.76146); valid loss: 117.032\n",
      "[Epoch 150/350, Step 1650, ETA 13.78s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 155/350, Step 1700, ETA 13.46s] step time: 0.005614s (±0.001208s); valid time: 0.002355s; loss: 131.272 (±1.79364); valid loss: 116.86\n",
      "[Epoch 160/350, Step 1760, ETA 13.06s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 164/350, Step 1800, ETA 12.79s] step time: 0.005382s (±0.0009546s); valid time: 0.002135s; loss: 131.105 (±2.02612); valid loss: 116.896\n",
      "[Epoch 170/350, Step 1870, ETA 12.34s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 1900, ETA 12.14s] step time: 0.005491s (±0.001349s); valid time: 0.002203s; loss: 131.217 (±1.99469); valid loss: 116.813\n",
      "[Epoch 180/350, Step 1980, ETA 11.61s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 2000, ETA 11.48s] step time: 0.005321s (±0.0007968s); valid time: 0.002109s; loss: 131.185 (±2.01151); valid loss: 116.918\n",
      "[Epoch 190/350, Step 2090, ETA 10.9s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 2100, ETA 10.83s] step time: 0.005355s (±0.0009536s); valid time: 0.002099s; loss: 131.166 (±1.9819); valid loss: 116.947\n",
      "[Epoch 200/350, Step 2200, ETA 10.21s] step time: 0.005632s (±0.001527s); valid time: 0.005775s; loss: 131.159 (±1.93239); valid loss: 116.75\n",
      "[Epoch 200/350, Step 2200, ETA 10.21s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 2300, ETA 9.579s] step time: 0.005427s (±0.0008732s); valid time: 0.002257s; loss: 131.146 (±1.6624); valid loss: 116.788\n",
      "[Epoch 210/350, Step 2310, ETA 9.514s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 219/350, Step 2400, ETA 8.947s] step time: 0.005461s (±0.001257s); valid time: 0.002166s; loss: 131.171 (±1.91242); valid loss: 116.866\n",
      "[Epoch 220/350, Step 2420, ETA 8.818s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 228/350, Step 2500, ETA 8.318s] step time: 0.005461s (±0.00141s); valid time: 0.002399s; loss: 131.151 (±1.90837); valid loss: 116.904\n",
      "[Epoch 230/350, Step 2530, ETA 8.125s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 237/350, Step 2600, ETA 7.686s] step time: 0.005329s (±0.0009599s); valid time: 0.002219s; loss: 131.117 (±1.79155); valid loss: 116.87\n",
      "[Epoch 240/350, Step 2640, ETA 7.436s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 246/350, Step 2700, ETA 7.072s] step time: 0.005668s (±0.001568s); valid time: 0.002339s; loss: 131.189 (±1.83487); valid loss: 116.831\n",
      "[Epoch 250/350, Step 2750, ETA 6.76s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 255/350, Step 2800, ETA 6.448s] step time: 0.005427s (±0.001589s); valid time: 0.003309s; loss: 131.097 (±1.6595); valid loss: 116.871\n",
      "[Epoch 260/350, Step 2860, ETA 6.075s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 264/350, Step 2900, ETA 5.831s] step time: 0.005527s (±0.001454s); valid time: 0.002091s; loss: 131.138 (±2.06649); valid loss: 116.912\n",
      "[Epoch 270/350, Step 2970, ETA 5.392s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 3000, ETA 5.205s] step time: 0.005183s (±0.000694s); valid time: 0.002055s; loss: 131.167 (±1.9044); valid loss: 116.786\n",
      "[Epoch 280/350, Step 3080, ETA 4.711s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 3100, ETA 4.588s] step time: 0.005428s (±0.001469s); valid time: 0.002122s; loss: 131.141 (±1.9635); valid loss: 116.878\n",
      "[Epoch 290/350, Step 3190, ETA 4.03s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 3200, ETA 3.968s] step time: 0.00525s (±0.000645s); valid time: 0.002042s; loss: 131.177 (±1.69494); valid loss: 116.906\n",
      "[Epoch 300/350, Step 3300, ETA 3.355s] step time: 0.005433s (±0.001376s); valid time: 0.002235s; loss: 131.142 (±1.74727); valid loss: 116.957\n",
      "[Epoch 300/350, Step 3300, ETA 3.355s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 3400, ETA 2.74s] step time: 0.005221s (±0.0007225s); valid time: 0.002188s; loss: 131.168 (±1.91756); valid loss: 116.886\n",
      "[Epoch 310/350, Step 3410, ETA 2.679s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 319/350, Step 3500, ETA 2.128s] step time: 0.005313s (±0.001096s); valid time: 0.002044s; loss: 131.193 (±1.57932); valid loss: 116.864\n",
      "[Epoch 320/350, Step 3520, ETA 2.006s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 328/350, Step 3600, ETA 1.518s] step time: 0.005337s (±0.001141s); valid time: 0.002272s; loss: 131.138 (±1.93499); valid loss: 116.907\n",
      "[Epoch 330/350, Step 3630, ETA 1.335s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 337/350, Step 3700, ETA 0.9101s] step time: 0.005357s (±0.0009394s); valid time: 0.002244s; loss: 131.156 (±1.90126); valid loss: 116.864\n",
      "[Epoch 340/350, Step 3740, ETA 0.667s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 346/350, Step 3800, ETA 0.3032s] step time: 0.00545s (±0.001219s); valid time: 0.002263s; loss: 131.255 (±1.79175); valid loss: 116.792\n",
      "[Epoch 350/350, Step 3850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk4mst_ui/variables.dat-300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk4mst_ui/variables.dat-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.18,\n",
      "\t(tp, fp, tn, fn)=(329, 3305, 381, 17),\n",
      "\tprecision=0.09,\n",
      "\trecall=0.95,\n",
      "\tf1=0.17,\n",
      "\troc_auc=0.53,\n",
      "\ty_pred%=0.9012896825396826,\n",
      "\ty_label%=0.08581349206349206,\n",
      ")\n",
      "Testing on realKnownCause/machine_temperature_system_failure.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 2/350, Step 100, ETA 4m 4.425s] step time: 0.01006s (±0.03301s); valid time: 0.1744s; loss: 116.744 (±16.3357); valid loss: 99.9339 (*)\n",
      "[Epoch 4/350, Step 200, ETA 3m 19.66s] step time: 0.006226s (±0.008375s); valid time: 0.08398s; loss: 74.3997 (±11.9613); valid loss: 48.6641 (*)\n",
      "[Epoch 5/350, Step 300, ETA 3m 2.773s] step time: 0.00623s (±0.008783s); valid time: 0.08772s; loss: 31.2231 (±11.038); valid loss: 19.2506 (*)\n",
      "[Epoch 7/350, Step 400, ETA 2m 55.02s] step time: 0.006178s (±0.008879s); valid time: 0.08871s; loss: 16.1041 (±4.24923); valid loss: 13.1262 (*)\n",
      "[Epoch 8/350, Step 500, ETA 2m 49.47s] step time: 0.006258s (±0.008152s); valid time: 0.08078s; loss: 12.4088 (±4.24786); valid loss: 9.49016 (*)\n",
      "[Epoch 10/350, Step 600, ETA 2m 42.92s] step time: 0.005401s (±0.001624s); valid time: 0.01259s; loss: 10.7943 (±4.26974); valid loss: 11.2173\n",
      "[Epoch 10/350, Step 660, ETA 2m 39.27s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 700, ETA 2m 40.51s] step time: 0.006314s (±0.009018s); valid time: 0.08843s; loss: 9.28747 (±4.0644); valid loss: 8.35033 (*)\n",
      "[Epoch 13/350, Step 800, ETA 2m 38.97s] step time: 0.006325s (±0.009671s); valid time: 0.09657s; loss: 9.44736 (±9.4624); valid loss: 7.51729 (*)\n",
      "[Epoch 14/350, Step 900, ETA 2m 36.64s] step time: 0.006079s (±0.009111s); valid time: 0.08926s; loss: 8.21449 (±13.0392); valid loss: 5.8588 (*)\n",
      "[Epoch 16/350, Step 1000, ETA 2m 35.45s] step time: 0.006291s (±0.008764s); valid time: 0.08605s; loss: 8.89119 (±33.0417); valid loss: 0.164749 (*)\n",
      "[Epoch 17/350, Step 1100, ETA 2m 33.86s] step time: 0.006194s (±0.008859s); valid time: 0.08881s; loss: -5.26534 (±5.50177); valid loss: -10.2493 (*)\n",
      "[Epoch 19/350, Step 1200, ETA 2m 33.11s] step time: 0.006391s (±0.009021s); valid time: 0.09022s; loss: -7.41841 (±82.5768); valid loss: -14.1845 (*)\n",
      "[Epoch 20/350, Step 1300, ETA 2m 32.14s] step time: 0.00643s (±0.009412s); valid time: 0.09358s; loss: -18.7664 (±3.72373); valid loss: -16.3182 (*)\n",
      "[Epoch 20/350, Step 1320, ETA 2m 31.51s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1400, ETA 2m 29.88s] step time: 0.005404s (±0.001448s); valid time: 0.0127s; loss: -20.4289 (±4.05606); valid loss: -15.8681\n",
      "[Epoch 23/350, Step 1500, ETA 2m 27.67s] step time: 0.005464s (±0.001621s); valid time: 0.01268s; loss: -21.6396 (±3.5036); valid loss: -16.2513\n",
      "[Epoch 25/350, Step 1600, ETA 2m 26.95s] step time: 0.006266s (±0.009145s); valid time: 0.09115s; loss: -23.3154 (±3.52062); valid loss: -17.2099 (*)\n",
      "[Epoch 26/350, Step 1700, ETA 2m 25.88s] step time: 0.006136s (±0.00928s); valid time: 0.09284s; loss: -13.4782 (±117.354); valid loss: -20.4524 (*)\n",
      "[Epoch 28/350, Step 1800, ETA 2m 25.25s] step time: 0.006298s (±0.008775s); valid time: 0.08762s; loss: 2469.29 (±24862.6); valid loss: -24.6974 (*)\n",
      "[Epoch 29/350, Step 1900, ETA 2m 24.49s] step time: 0.006353s (±0.008727s); valid time: 0.08734s; loss: 3602.24 (±36164.4); valid loss: -26.3452 (*)\n",
      "[Epoch 30/350, Step 1980, ETA 2m 22.97s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 2000, ETA 2m 22.89s] step time: 0.005358s (±0.001397s); valid time: 0.0125s; loss: -32.0192 (±22.1439); valid loss: -26.0599\n",
      "[Epoch 32/350, Step 2100, ETA 2m 22.14s] step time: 0.006313s (±0.00915s); valid time: 0.09139s; loss: -36.1474 (±4.89853); valid loss: -28.0783 (*)\n",
      "[Epoch 34/350, Step 2200, ETA 2m 20.82s] step time: 0.005542s (±0.001652s); valid time: 0.01189s; loss: -35.8184 (±12.9651); valid loss: -26.9103\n",
      "[Epoch 35/350, Step 2300, ETA 2m 19.32s] step time: 0.005437s (±0.001433s); valid time: 0.01168s; loss: 343175 (±3.41491e+06); valid loss: -27.3611\n",
      "[Epoch 37/350, Step 2400, ETA 2m 18.65s] step time: 0.006129s (±0.008409s); valid time: 0.08436s; loss: -34.5951 (±44.3255); valid loss: -28.7885 (*)\n",
      "[Epoch 38/350, Step 2500, ETA 2m 17.33s] step time: 0.005508s (±0.00155s); valid time: 0.01381s; loss: -30.7792 (±82.2116); valid loss: -28.2103\n",
      "[Epoch 40/350, Step 2600, ETA 2m 16.22s] step time: 0.00553s (±0.00166s); valid time: 0.01231s; loss: -25.3913 (±109.651); valid loss: 1.32051e+06\n",
      "[Epoch 40/350, Step 2640, ETA 2m 15.71s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 2700, ETA 2m 16.03s] step time: 0.006842s (±0.009149s); valid time: 0.09101s; loss: 4027.63 (±40470.3); valid loss: -29.0841 (*)\n",
      "[Epoch 43/350, Step 2800, ETA 2m 15.07s] step time: 0.005688s (±0.001767s); valid time: 0.01292s; loss: -40.6301 (±3.10152); valid loss: -23.3217\n",
      "[Epoch 44/350, Step 2900, ETA 2m 14.09s] step time: 0.005726s (±0.001811s); valid time: 0.01291s; loss: 205437 (±2.04448e+06); valid loss: -28.5907\n",
      "[Epoch 46/350, Step 3000, ETA 2m 13.02s] step time: 0.005476s (±0.001651s); valid time: 0.013s; loss: -41.4563 (±3.5596); valid loss: -28.5379\n",
      "[Epoch 47/350, Step 3100, ETA 2m 12.05s] step time: 0.005736s (±0.00182s); valid time: 0.01251s; loss: 80680.8 (±803137); valid loss: 331803\n",
      "[Epoch 49/350, Step 3200, ETA 2m 11.1s] step time: 0.005592s (±0.001714s); valid time: 0.01243s; loss: -41.5799 (±3.08268); valid loss: -28.7268\n",
      "[Epoch 50/350, Step 3300, ETA 2m 9.936s] step time: 0.00535s (±0.001341s); valid time: 0.01261s; loss: 186.189 (±2264.82); valid loss: 657.285\n",
      "[Epoch 50/350, Step 3300, ETA 2m 9.937s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 52/350, Step 3400, ETA 2m 8.962s] step time: 0.005462s (±0.001754s); valid time: 0.01357s; loss: 24.6888 (±606.063); valid loss: -28.0903\n",
      "[Epoch 54/350, Step 3500, ETA 2m 8.425s] step time: 0.006201s (±0.009176s); valid time: 0.09131s; loss: 196182 (±1.95176e+06); valid loss: -29.4773 (*)\n",
      "[Epoch 55/350, Step 3600, ETA 2m 7.336s] step time: 0.005361s (±0.001471s); valid time: 0.01249s; loss: -42.671 (±3.55044); valid loss: -25.5876\n",
      "[Epoch 57/350, Step 3700, ETA 2m 6.463s] step time: 0.005524s (±0.002098s); valid time: 0.01226s; loss: -34.7758 (±81.5572); valid loss: 1527.96\n",
      "[Epoch 58/350, Step 3800, ETA 2m 5.454s] step time: 0.005425s (±0.001524s); valid time: 0.01312s; loss: -43.1273 (±4.90342); valid loss: 1.08435e+06\n",
      "[Epoch 60/350, Step 3900, ETA 2m 4.591s] step time: 0.005516s (±0.001633s); valid time: 0.01252s; loss: -43.7327 (±2.74623); valid loss: -28.9473\n",
      "[Epoch 60/350, Step 3960, ETA 2m 3.877s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 4000, ETA 2m 3.547s] step time: 0.005266s (±0.001588s); valid time: 0.01272s; loss: -42.7344 (±9.71864); valid loss: -29.454\n",
      "[Epoch 63/350, Step 4100, ETA 2m 2.806s] step time: 0.005714s (±0.001827s); valid time: 0.01341s; loss: -44.1555 (±3.60508); valid loss: 16372.2\n",
      "[Epoch 64/350, Step 4200, ETA 2m 1.907s] step time: 0.00551s (±0.001591s); valid time: 0.01305s; loss: -44.5493 (±4.53364); valid loss: 281.407\n",
      "[Epoch 66/350, Step 4300, ETA 2m 1.104s] step time: 0.00554s (±0.001835s); valid time: 0.01265s; loss: 484.91 (±5272.12); valid loss: -28.8485\n",
      "[Epoch 67/350, Step 4400, ETA 2m 0.2784s] step time: 0.005563s (±0.001629s); valid time: 0.01278s; loss: 50247.7 (±500411); valid loss: -29.315\n",
      "[Epoch 69/350, Step 4500, ETA 1m 59.78s] step time: 0.006227s (±0.00901s); valid time: 0.09054s; loss: -42.5967 (±35.9995); valid loss: -30.7368 (*)\n",
      "[Epoch 70/350, Step 4600, ETA 1m 59s] step time: 0.005718s (±0.001829s); valid time: 0.01274s; loss: -46.1919 (±3.28914); valid loss: -24.7139\n",
      "[Epoch 70/350, Step 4620, ETA 1m 58.81s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 4700, ETA 1m 58.51s] step time: 0.006255s (±0.008918s); valid time: 0.08921s; loss: -22.4878 (±190.008); valid loss: -30.9241 (*)\n",
      "[Epoch 73/350, Step 4800, ETA 1m 57.62s] step time: 0.005373s (±0.001503s); valid time: 0.01303s; loss: 16191.6 (±161508); valid loss: -30.505\n",
      "[Epoch 75/350, Step 4900, ETA 1m 56.9s] step time: 0.005674s (±0.001895s); valid time: 0.01553s; loss: -33.515 (±105.515); valid loss: -29.6544\n",
      "[Epoch 76/350, Step 5000, ETA 1m 56.06s] step time: 0.005478s (±0.001365s); valid time: 0.01228s; loss: 3198.87 (±26447.1); valid loss: 19927.2\n",
      "[Epoch 78/350, Step 5100, ETA 1m 55.3s] step time: 0.005505s (±0.001676s); valid time: 0.01291s; loss: -48.0101 (±3.43919); valid loss: -30.1592\n",
      "[Epoch 79/350, Step 5200, ETA 1m 54.78s] step time: 0.006344s (±0.00885s); valid time: 0.08817s; loss: 1.36914e+06 (±1.36233e+07); valid loss: -31.5452 (*)\n",
      "[Epoch 80/350, Step 5280, ETA 1m 54.15s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 5300, ETA 1m 54.05s] step time: 0.00558s (±0.001775s); valid time: 0.01295s; loss: -4.16379 (±439.51); valid loss: -30.0274\n",
      "[Epoch 82/350, Step 5400, ETA 1m 53.27s] step time: 0.005586s (±0.001668s); valid time: 0.01282s; loss: 4587.46 (±45902.9); valid loss: -30.1348\n",
      "[Epoch 84/350, Step 5500, ETA 1m 52.53s] step time: 0.005534s (±0.001805s); valid time: 0.01291s; loss: 3542.4 (±34201.8); valid loss: 133.197\n",
      "[Epoch 85/350, Step 5600, ETA 1m 51.98s] step time: 0.006275s (±0.008999s); valid time: 0.08959s; loss: -47.265 (±16.9826); valid loss: -32.2471 (*)\n",
      "[Epoch 87/350, Step 5700, ETA 1m 51.25s] step time: 0.005578s (±0.001661s); valid time: 0.01265s; loss: 1690.27 (±17311.8); valid loss: -30.3869\n",
      "[Epoch 88/350, Step 5800, ETA 1m 50.45s] step time: 0.005457s (±0.001385s); valid time: 0.01288s; loss: 196.202 (±2447.55); valid loss: -32.2418\n",
      "[Epoch 90/350, Step 5900, ETA 1m 49.98s] step time: 0.006385s (±0.008821s); valid time: 0.08771s; loss: 160.328 (±2089.56); valid loss: -32.9982 (*)\n",
      "[Epoch 90/350, Step 5940, ETA 1m 49.66s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 6000, ETA 1m 49.26s] step time: 0.005672s (±0.001933s); valid time: 0.01215s; loss: -49.8373 (±3.95304); valid loss: 142835\n",
      "[Epoch 93/350, Step 6100, ETA 1m 48.5s] step time: 0.005424s (±0.001481s); valid time: 0.01306s; loss: 252.738 (±3015.37); valid loss: -32.8059\n",
      "[Epoch 94/350, Step 6200, ETA 1m 47.72s] step time: 0.005456s (±0.001733s); valid time: 0.01285s; loss: 5.1097 (±553.092); valid loss: 2307.09\n",
      "[Epoch 96/350, Step 6300, ETA 1m 46.99s] step time: 0.005401s (±0.001617s); valid time: 0.01227s; loss: 43421.3 (±432542); valid loss: 106.844\n",
      "[Epoch 97/350, Step 6400, ETA 1m 46.23s] step time: 0.005504s (±0.001544s); valid time: 0.01198s; loss: 1509.67 (±15522.1); valid loss: 20.9612\n",
      "[Epoch 99/350, Step 6500, ETA 1m 45.53s] step time: 0.005581s (±0.00188s); valid time: 0.01247s; loss: 51.1515 (±985.817); valid loss: -32.5309\n",
      "[Epoch 100/350, Step 6600, ETA 1m 44.77s] step time: 0.005434s (±0.00149s); valid time: 0.01251s; loss: -50.9857 (±3.23094); valid loss: -32.5776\n",
      "[Epoch 100/350, Step 6600, ETA 1m 44.77s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 102/350, Step 6700, ETA 1m 44.25s] step time: 0.006278s (±0.00812s); valid time: 0.08011s; loss: 439908 (±4.21666e+06); valid loss: -34.0941 (*)\n",
      "[Epoch 104/350, Step 6800, ETA 1m 43.56s] step time: 0.005519s (±0.001721s); valid time: 0.01232s; loss: -50.073 (±13.9041); valid loss: -32.2905\n",
      "[Epoch 105/350, Step 6900, ETA 1m 42.8s] step time: 0.005428s (±0.001646s); valid time: 0.01239s; loss: -30.0808 (±163.626); valid loss: -31.7931\n",
      "[Epoch 107/350, Step 7000, ETA 1m 42.3s] step time: 0.006351s (±0.009343s); valid time: 0.0933s; loss: 3035.91 (±29832); valid loss: -34.137 (*)\n",
      "[Epoch 108/350, Step 7100, ETA 1m 41.58s] step time: 0.005552s (±0.001518s); valid time: 0.01251s; loss: -51.4197 (±3.14716); valid loss: -33.8205\n",
      "[Epoch 110/350, Step 7200, ETA 1m 40.89s] step time: 0.005554s (±0.001572s); valid time: 0.01312s; loss: -50.6801 (±9.31003); valid loss: 106.105\n",
      "[Epoch 110/350, Step 7260, ETA 1m 40.43s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 7300, ETA 1m 40.18s] step time: 0.005601s (±0.001703s); valid time: 0.01236s; loss: 19.4579 (±657.941); valid loss: -32.9396\n",
      "[Epoch 113/350, Step 7400, ETA 1m 39.51s] step time: 0.00562s (±0.001599s); valid time: 0.0127s; loss: 52405.8 (±521794); valid loss: 5771.26\n",
      "[Epoch 114/350, Step 7500, ETA 1m 38.8s] step time: 0.005568s (±0.001638s); valid time: 0.0138s; loss: 280690 (±2.78797e+06); valid loss: -33.6411\n",
      "[Epoch 116/350, Step 7600, ETA 1m 38.09s] step time: 0.005433s (±0.00148s); valid time: 0.01271s; loss: 763.859 (±8115.24); valid loss: -33.5687\n",
      "[Epoch 117/350, Step 7700, ETA 1m 37.38s] step time: 0.005524s (±0.001632s); valid time: 0.01225s; loss: 2355.4 (±23873.7); valid loss: -33.2518\n",
      "[Epoch 119/350, Step 7800, ETA 1m 36.68s] step time: 0.005433s (±0.001638s); valid time: 0.01279s; loss: 27.7663 (±756.992); valid loss: 9450.95\n",
      "[Epoch 120/350, Step 7900, ETA 1m 36.17s] step time: 0.006543s (±0.009602s); valid time: 0.09587s; loss: -50.3267 (±16.9649); valid loss: -34.4697 (*)\n",
      "[Epoch 120/350, Step 7920, ETA 1m 36s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 8000, ETA 1m 35.59s] step time: 0.006065s (±0.008696s); valid time: 0.08719s; loss: 17772.7 (±177352); valid loss: -34.68 (*)\n",
      "[Epoch 123/350, Step 8100, ETA 1m 34.87s] step time: 0.005449s (±0.001387s); valid time: 0.01309s; loss: 1.87429e+06 (±1.85293e+07); valid loss: -33.4209\n",
      "[Epoch 125/350, Step 8200, ETA 1m 34.21s] step time: 0.005592s (±0.001972s); valid time: 0.01256s; loss: 1.43755e+06 (±1.39238e+07); valid loss: -34.5542\n",
      "[Epoch 126/350, Step 8300, ETA 1m 33.47s] step time: 0.005369s (±0.001486s); valid time: 0.01239s; loss: 33049.6 (±283744); valid loss: -32.5379\n",
      "[Epoch 128/350, Step 8400, ETA 1m 32.8s] step time: 0.005519s (±0.0018s); valid time: 0.01252s; loss: 739574 (±7.17996e+06); valid loss: -33.9587\n",
      "[Epoch 129/350, Step 8500, ETA 1m 32.09s] step time: 0.005434s (±0.001578s); valid time: 0.01263s; loss: -50.5188 (±17.5465); valid loss: -34.4913\n",
      "[Epoch 130/350, Step 8580, ETA 1m 31.54s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 8600, ETA 1m 31.42s] step time: 0.005523s (±0.001472s); valid time: 0.01255s; loss: 65089.7 (±450734); valid loss: -34.1817\n",
      "[Epoch 132/350, Step 8700, ETA 1m 30.71s] step time: 0.005423s (±0.001628s); valid time: 0.01276s; loss: 275.17 (±3251.03); valid loss: -33.7046\n",
      "[Epoch 134/350, Step 8800, ETA 1m 30.02s] step time: 0.005349s (±0.001467s); valid time: 0.01237s; loss: 18.9362 (±505.264); valid loss: -33.4566\n",
      "[Epoch 135/350, Step 8900, ETA 1m 29.33s] step time: 0.005532s (±0.001646s); valid time: 0.01261s; loss: -52.2833 (±2.90675); valid loss: 434.686\n",
      "[Epoch 137/350, Step 9000, ETA 1m 28.66s] step time: 0.005478s (±0.001754s); valid time: 0.01278s; loss: 1064.9 (±11118.3); valid loss: -33.1168\n",
      "[Epoch 138/350, Step 9100, ETA 1m 27.94s] step time: 0.005307s (±0.001518s); valid time: 0.01288s; loss: 503.572 (±5532.54); valid loss: -34.2441\n",
      "[Epoch 140/350, Step 9200, ETA 1m 27.28s] step time: 0.005536s (±0.001506s); valid time: 0.01237s; loss: 11432.8 (±108864); valid loss: 23.3006\n",
      "[Epoch 140/350, Step 9240, ETA 1m 27s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 9300, ETA 1m 26.6s] step time: 0.005487s (±0.001702s); valid time: 0.01236s; loss: 3243.03 (±32767.5); valid loss: 168430\n",
      "[Epoch 143/350, Step 9400, ETA 1m 25.94s] step time: 0.005539s (±0.001684s); valid time: 0.01238s; loss: 109925 (±1.09364e+06); valid loss: -23.8758\n",
      "[Epoch 144/350, Step 9500, ETA 1m 25.24s] step time: 0.005285s (±0.001334s); valid time: 0.01283s; loss: -47.5865 (±34.8007); valid loss: 454.251\n",
      "[Epoch 146/350, Step 9600, ETA 1m 24.61s] step time: 0.005732s (±0.002061s); valid time: 0.01278s; loss: 446125 (±4.24618e+06); valid loss: -33.2582\n",
      "[Epoch 147/350, Step 9700, ETA 1m 23.93s] step time: 0.005414s (±0.001453s); valid time: 0.01191s; loss: 1.05563e+06 (±1.04999e+07); valid loss: -33.5449\n",
      "[Epoch 149/350, Step 9800, ETA 1m 23.27s] step time: 0.005472s (±0.001472s); valid time: 0.01213s; loss: -46.3777 (±60.5105); valid loss: -33.9503\n",
      "[Epoch 150/350, Step 9900, ETA 1m 22.6s] step time: 0.005475s (±0.001666s); valid time: 0.01369s; loss: 27079.9 (±209286); valid loss: 11549.1\n",
      "[Epoch 150/350, Step 9900, ETA 1m 22.6s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 152/350, Step 10000, ETA 1m 21.92s] step time: 0.005272s (±0.001336s); valid time: 0.01238s; loss: -52.9079 (±2.8923); valid loss: -33.6268\n",
      "[Epoch 154/350, Step 10100, ETA 1m 21.28s] step time: 0.005553s (±0.001897s); valid time: 0.01281s; loss: 1.78134e+06 (±1.28996e+07); valid loss: -28.9697\n",
      "[Epoch 155/350, Step 10200, ETA 1m 20.62s] step time: 0.005598s (±0.001712s); valid time: 0.01247s; loss: 524.799 (±4104.86); valid loss: -34.1684\n",
      "[Epoch 157/350, Step 10300, ETA 1m 19.96s] step time: 0.005427s (±0.001576s); valid time: 0.01293s; loss: 1.27712e+06 (±9.31314e+06); valid loss: -34.0621\n",
      "[Epoch 158/350, Step 10400, ETA 1m 19.29s] step time: 0.005482s (±0.001585s); valid time: 0.01242s; loss: -52.5269 (±3.3735); valid loss: -34.4704\n",
      "[Epoch 160/350, Step 10500, ETA 1m 18.63s] step time: 0.005371s (±0.001533s); valid time: 0.01266s; loss: 151.283 (±2028.54); valid loss: -33.8236\n",
      "[Epoch 160/350, Step 10560, ETA 1m 18.21s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 10600, ETA 1m 17.97s] step time: 0.005561s (±0.00194s); valid time: 0.01255s; loss: 665.772 (±7133.44); valid loss: 105452\n",
      "[Epoch 163/350, Step 10700, ETA 1m 17.32s] step time: 0.005478s (±0.001685s); valid time: 0.01332s; loss: 522626 (±4.42732e+06); valid loss: 79090.9\n",
      "[Epoch 164/350, Step 10800, ETA 1m 16.64s] step time: 0.005353s (±0.001466s); valid time: 0.01308s; loss: -49.7646 (±29.2133); valid loss: -33.9449\n",
      "[Epoch 166/350, Step 10900, ETA 1m 15.99s] step time: 0.005418s (±0.001816s); valid time: 0.01363s; loss: 978817 (±8.8554e+06); valid loss: -34.5969\n",
      "[Epoch 167/350, Step 11000, ETA 1m 15.31s] step time: 0.005315s (±0.001477s); valid time: 0.01349s; loss: 3.03901e+06 (±2.99282e+07); valid loss: -33.631\n",
      "[Epoch 169/350, Step 11100, ETA 1m 14.76s] step time: 0.006294s (±0.008824s); valid time: 0.08835s; loss: -51.5137 (±13.9626); valid loss: -34.873 (*)\n",
      "[Epoch 170/350, Step 11200, ETA 1m 14.11s] step time: 0.005561s (±0.002135s); valid time: 0.01589s; loss: 5923.36 (±58518.8); valid loss: -34.3234\n",
      "[Epoch 170/350, Step 11220, ETA 1m 13.97s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 11300, ETA 1m 13.47s] step time: 0.005508s (±0.001536s); valid time: 0.01234s; loss: 168.426 (±2202.04); valid loss: 175.879\n",
      "[Epoch 173/350, Step 11400, ETA 1m 12.83s] step time: 0.005723s (±0.001933s); valid time: 0.01296s; loss: 844.86 (±8931.25); valid loss: 9136.63\n",
      "[Epoch 175/350, Step 11500, ETA 1m 12.18s] step time: 0.005411s (±0.001452s); valid time: 0.0122s; loss: -52.974 (±3.0372); valid loss: -32.43\n",
      "[Epoch 176/350, Step 11600, ETA 1m 11.52s] step time: 0.005446s (±0.001658s); valid time: 0.01232s; loss: -52.5897 (±4.46219); valid loss: -33.9238\n",
      "[Epoch 178/350, Step 11700, ETA 1m 10.89s] step time: 0.005559s (±0.001864s); valid time: 0.01271s; loss: 1.21123e+06 (±1.20521e+07); valid loss: 5.00377e+06\n",
      "[Epoch 179/350, Step 11800, ETA 1m 10.25s] step time: 0.005617s (±0.001674s); valid time: 0.01228s; loss: 8683.44 (±86916.5); valid loss: -34.22\n",
      "[Epoch 180/350, Step 11880, ETA 1m 9.72s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 11900, ETA 1m 9.624s] step time: 0.005576s (±0.002001s); valid time: 0.01759s; loss: 227539 (±2.26115e+06); valid loss: -33.4172\n",
      "[Epoch 182/350, Step 12000, ETA 1m 8.974s] step time: 0.005503s (±0.001487s); valid time: 0.01231s; loss: -47.8696 (±29.4983); valid loss: 4.4174\n",
      "[Epoch 184/350, Step 12100, ETA 1m 8.333s] step time: 0.005453s (±0.001675s); valid time: 0.01319s; loss: 58.7789 (±1031.47); valid loss: 1.12608e+06\n",
      "[Epoch 185/350, Step 12200, ETA 1m 7.678s] step time: 0.005439s (±0.001482s); valid time: 0.01256s; loss: 118764 (±1.10132e+06); valid loss: -33.3699\n",
      "[Epoch 187/350, Step 12300, ETA 1m 7.047s] step time: 0.005523s (±0.001692s); valid time: 0.01275s; loss: 2536.28 (±25755.8); valid loss: -33.1705\n",
      "[Epoch 188/350, Step 12400, ETA 1m 6.401s] step time: 0.00552s (±0.001795s); valid time: 0.01508s; loss: -47.1487 (±57.1158); valid loss: -34.3073\n",
      "[Epoch 190/350, Step 12500, ETA 1m 5.771s] step time: 0.005532s (±0.001438s); valid time: 0.01244s; loss: -52.248 (±6.34696); valid loss: 192477\n",
      "[Epoch 190/350, Step 12540, ETA 1m 5.512s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 12600, ETA 1m 5.142s] step time: 0.00566s (±0.001769s); valid time: 0.01225s; loss: 997.245 (±9854.32); valid loss: 600.552\n",
      "[Epoch 193/350, Step 12700, ETA 1m 4.51s] step time: 0.005525s (±0.001551s); valid time: 0.01268s; loss: 699296 (±6.18591e+06); valid loss: -34.1245\n",
      "[Epoch 194/350, Step 12800, ETA 1m 3.86s] step time: 0.005445s (±0.001626s); valid time: 0.01275s; loss: -52.6765 (±4.29079); valid loss: 6752.41\n",
      "[Epoch 196/350, Step 12900, ETA 1m 3.222s] step time: 0.005425s (±0.001719s); valid time: 0.01207s; loss: 3999.51 (±32367.5); valid loss: -34.7582\n",
      "[Epoch 197/350, Step 13000, ETA 1m 2.57s] step time: 0.005332s (±0.001376s); valid time: 0.01314s; loss: 1.11826e+06 (±1.11267e+07); valid loss: 713.766\n",
      "[Epoch 199/350, Step 13100, ETA 1m 1.952s] step time: 0.005661s (±0.002011s); valid time: 0.01248s; loss: -43.7586 (±78.2269); valid loss: 77.6272\n",
      "[Epoch 200/350, Step 13200, ETA 1m 1.3s] step time: 0.005373s (±0.001485s); valid time: 0.01264s; loss: 181657 (±1.80798e+06); valid loss: -34.5189\n",
      "[Epoch 200/350, Step 13200, ETA 1m 1.3s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 202/350, Step 13300, ETA 1m 0.6631s] step time: 0.005403s (±0.001786s); valid time: 0.01228s; loss: 306216 (±3.04733e+06); valid loss: -34.0815\n",
      "[Epoch 204/350, Step 13400, ETA 1m 0.02815s] step time: 0.005416s (±0.00145s); valid time: 0.01378s; loss: 1.11194e+06 (±1.1064e+07); valid loss: -33.954\n",
      "[Epoch 205/350, Step 13500, ETA 59.4s] step time: 0.005618s (±0.001756s); valid time: 0.01266s; loss: -17.8764 (±328.107); valid loss: -34.2585\n",
      "[Epoch 207/350, Step 13600, ETA 58.77s] step time: 0.00552s (±0.001595s); valid time: 0.01271s; loss: 40.9149 (±723.069); valid loss: -33.5555\n",
      "[Epoch 208/350, Step 13700, ETA 58.13s] step time: 0.005413s (±0.001752s); valid time: 0.01626s; loss: 3107.51 (±22931.7); valid loss: -34.0853\n",
      "[Epoch 210/350, Step 13800, ETA 57.5s] step time: 0.005509s (±0.001691s); valid time: 0.01313s; loss: 18331.6 (±181750); valid loss: 138509\n",
      "[Epoch 210/350, Step 13860, ETA 57.1s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 13900, ETA 56.85s] step time: 0.005292s (±0.001512s); valid time: 0.01245s; loss: 269780 (±1.73506e+06); valid loss: -34.4828\n",
      "[Epoch 213/350, Step 14000, ETA 56.23s] step time: 0.005651s (±0.001993s); valid time: 0.01584s; loss: 1.70158e+06 (±1.19235e+07); valid loss: -34.1749\n",
      "[Epoch 214/350, Step 14100, ETA 55.6s] step time: 0.005571s (±0.001845s); valid time: 0.01252s; loss: 2785.41 (±28223); valid loss: -13.4345\n",
      "[Epoch 216/350, Step 14200, ETA 54.97s] step time: 0.005377s (±0.001452s); valid time: 0.01206s; loss: 131725 (±1.31115e+06); valid loss: 219742\n",
      "[Epoch 217/350, Step 14300, ETA 54.33s] step time: 0.005464s (±0.001751s); valid time: 0.01283s; loss: -2.17266 (±479.9); valid loss: -33.8884\n",
      "[Epoch 219/350, Step 14400, ETA 53.7s] step time: 0.005434s (±0.001528s); valid time: 0.01258s; loss: -52.3239 (±5.31344); valid loss: -34.2342\n",
      "[Epoch 220/350, Step 14500, ETA 53.06s] step time: 0.005443s (±0.001817s); valid time: 0.01252s; loss: 46182.6 (±450119); valid loss: -33.896\n",
      "[Epoch 220/350, Step 14520, ETA 52.93s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 14600, ETA 52.43s] step time: 0.005423s (±0.001358s); valid time: 0.01173s; loss: 287056 (±2.85669e+06); valid loss: -34.8056\n",
      "[Epoch 223/350, Step 14700, ETA 51.8s] step time: 0.005519s (±0.001532s); valid time: 0.01226s; loss: 552077 (±4.25965e+06); valid loss: 371.221\n",
      "[Epoch 225/350, Step 14800, ETA 51.2s] step time: 0.005873s (±0.002451s); valid time: 0.01317s; loss: -51.4484 (±12.564); valid loss: -31.1343\n",
      "[Epoch 226/350, Step 14900, ETA 50.56s] step time: 0.00536s (±0.001464s); valid time: 0.01253s; loss: 33.6346 (±862.246); valid loss: -34.4853\n",
      "[Epoch 228/350, Step 15000, ETA 49.93s] step time: 0.005371s (±0.001689s); valid time: 0.01182s; loss: -50.8977 (±21.8818); valid loss: 586.015\n",
      "[Epoch 229/350, Step 15100, ETA 49.29s] step time: 0.005436s (±0.001534s); valid time: 0.01243s; loss: -52.4799 (±4.1728); valid loss: 703142\n",
      "[Epoch 230/350, Step 15180, ETA 48.79s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 15200, ETA 48.67s] step time: 0.005439s (±0.001584s); valid time: 0.01237s; loss: -33.6912 (±190.745); valid loss: -34.124\n",
      "[Epoch 232/350, Step 15300, ETA 48.08s] step time: 0.006216s (±0.008673s); valid time: 0.08672s; loss: -53.1652 (±3.13126); valid loss: -34.9017 (*)\n",
      "[Epoch 234/350, Step 15400, ETA 47.45s] step time: 0.005437s (±0.001488s); valid time: 0.01245s; loss: 8988.15 (±67649); valid loss: 32.4836\n",
      "[Epoch 235/350, Step 15500, ETA 46.82s] step time: 0.00551s (±0.001593s); valid time: 0.01256s; loss: 120.317 (±1701.59); valid loss: -34.1067\n",
      "[Epoch 237/350, Step 15600, ETA 46.2s] step time: 0.00542s (±0.001509s); valid time: 0.01277s; loss: 16236.6 (±162080); valid loss: 27247\n",
      "[Epoch 238/350, Step 15700, ETA 45.57s] step time: 0.005536s (±0.001641s); valid time: 0.01269s; loss: 68907.3 (±686146); valid loss: -33.6506\n",
      "[Epoch 240/350, Step 15800, ETA 44.95s] step time: 0.005493s (±0.001485s); valid time: 0.01276s; loss: -50.3777 (±27.4695); valid loss: 4.64908e+06\n",
      "[Epoch 240/350, Step 15840, ETA 44.69s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 15900, ETA 44.32s] step time: 0.005432s (±0.001616s); valid time: 0.01237s; loss: 75925.8 (±715773); valid loss: -34.0523\n",
      "[Epoch 243/350, Step 16000, ETA 43.72s] step time: 0.00589s (±0.001958s); valid time: 0.01442s; loss: -46.6704 (±60.2127); valid loss: -34.4813\n",
      "[Epoch 244/350, Step 16100, ETA 43.09s] step time: 0.0054s (±0.001458s); valid time: 0.01223s; loss: 5856.27 (±58796.7); valid loss: -33.8286\n",
      "[Epoch 246/350, Step 16200, ETA 42.47s] step time: 0.005474s (±0.001706s); valid time: 0.0124s; loss: -19.0233 (±335.277); valid loss: -34.5104\n",
      "[Epoch 247/350, Step 16300, ETA 41.84s] step time: 0.005507s (±0.001668s); valid time: 0.01236s; loss: -44.7458 (±56.91); valid loss: 1.40965e+06\n",
      "[Epoch 249/350, Step 16400, ETA 41.22s] step time: 0.005388s (±0.001468s); valid time: 0.01251s; loss: 1950.29 (±19666.4); valid loss: -34.5345\n",
      "[Epoch 250/350, Step 16500, ETA 40.6s] step time: 0.005556s (±0.001653s); valid time: 0.01273s; loss: -42.5349 (±100.64); valid loss: -32.9317\n",
      "[Epoch 250/350, Step 16500, ETA 40.6s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 252/350, Step 16600, ETA 39.97s] step time: 0.005392s (±0.001486s); valid time: 0.01214s; loss: 486754 (±4.84367e+06); valid loss: -34.3286\n",
      "[Epoch 254/350, Step 16700, ETA 39.35s] step time: 0.005462s (±0.00184s); valid time: 0.01375s; loss: 78068 (±777295); valid loss: 2301.92\n",
      "[Epoch 255/350, Step 16800, ETA 38.73s] step time: 0.005528s (±0.001532s); valid time: 0.01165s; loss: -52.0594 (±7.83027); valid loss: -34.1408\n",
      "[Epoch 257/350, Step 16900, ETA 38.12s] step time: 0.005559s (±0.001734s); valid time: 0.01241s; loss: -53.1432 (±3.02756); valid loss: -33.6398\n",
      "[Epoch 258/350, Step 17000, ETA 37.5s] step time: 0.005593s (±0.001746s); valid time: 0.01226s; loss: 42908.7 (±426784); valid loss: -34.1424\n",
      "[Epoch 260/350, Step 17100, ETA 36.87s] step time: 0.005362s (±0.001443s); valid time: 0.01264s; loss: -51.8306 (±10.0017); valid loss: -33.9948\n",
      "[Epoch 260/350, Step 17160, ETA 36.49s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 17200, ETA 36.25s] step time: 0.005453s (±0.001683s); valid time: 0.01307s; loss: 146287 (±1.45025e+06); valid loss: -33.8819\n",
      "[Epoch 263/350, Step 17300, ETA 35.63s] step time: 0.005525s (±0.00151s); valid time: 0.01275s; loss: -52.9825 (±3.02017); valid loss: 67348.6\n",
      "[Epoch 264/350, Step 17400, ETA 35.01s] step time: 0.005482s (±0.001509s); valid time: 0.0123s; loss: 1893.66 (±19362.6); valid loss: -34.112\n",
      "[Epoch 266/350, Step 17500, ETA 34.39s] step time: 0.005452s (±0.001616s); valid time: 0.01373s; loss: 1.0943e+06 (±1.08885e+07); valid loss: -33.9456\n",
      "[Epoch 267/350, Step 17600, ETA 33.78s] step time: 0.005646s (±0.001491s); valid time: 0.01353s; loss: 8939.75 (±85419); valid loss: -34.3888\n",
      "[Epoch 269/350, Step 17700, ETA 33.16s] step time: 0.00558s (±0.001785s); valid time: 0.01319s; loss: -31.2919 (±211.951); valid loss: 142.611\n",
      "[Epoch 270/350, Step 17800, ETA 32.54s] step time: 0.005534s (±0.001695s); valid time: 0.01286s; loss: 150152 (±1.04793e+06); valid loss: 1.39111e+06\n",
      "[Epoch 270/350, Step 17820, ETA 32.41s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 17900, ETA 31.93s] step time: 0.005638s (±0.002754s); valid time: 0.02494s; loss: -52.9607 (±3.17501); valid loss: -34.1661\n",
      "[Epoch 273/350, Step 18000, ETA 31.31s] step time: 0.005478s (±0.001459s); valid time: 0.01276s; loss: 15338.1 (±121480); valid loss: -33.0139\n",
      "[Epoch 275/350, Step 18100, ETA 30.69s] step time: 0.005458s (±0.001491s); valid time: 0.01297s; loss: 611617 (±5.09123e+06); valid loss: -34.4365\n",
      "[Epoch 276/350, Step 18200, ETA 30.07s] step time: 0.005551s (±0.001589s); valid time: 0.01288s; loss: -52.9742 (±2.98917); valid loss: -33.4356\n",
      "[Epoch 278/350, Step 18300, ETA 29.46s] step time: 0.005634s (±0.001792s); valid time: 0.01287s; loss: 61569.4 (±613103); valid loss: 18340.7\n",
      "[Epoch 279/350, Step 18400, ETA 28.84s] step time: 0.005558s (±0.001862s); valid time: 0.01212s; loss: 1921.55 (±19341.2); valid loss: -32.7597\n",
      "[Epoch 280/350, Step 18480, ETA 28.34s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 18500, ETA 28.23s] step time: 0.005591s (±0.002106s); valid time: 0.0123s; loss: 14708.9 (±144578); valid loss: 534.335\n",
      "[Epoch 282/350, Step 18600, ETA 27.61s] step time: 0.005472s (±0.001488s); valid time: 0.01261s; loss: 12563.3 (±112011); valid loss: 2063.58\n",
      "[Epoch 284/350, Step 18700, ETA 26.99s] step time: 0.00552s (±0.001785s); valid time: 0.01286s; loss: -52.235 (±10.0722); valid loss: 2.71822e+06\n",
      "[Epoch 285/350, Step 18800, ETA 26.37s] step time: 0.005454s (±0.001554s); valid time: 0.01297s; loss: -34.3593 (±157.715); valid loss: -33.4989\n",
      "[Epoch 287/350, Step 18900, ETA 25.76s] step time: 0.005585s (±0.001617s); valid time: 0.01286s; loss: 3515.53 (±35504.8); valid loss: 1.11494e+06\n",
      "[Epoch 288/350, Step 19000, ETA 25.14s] step time: 0.005458s (±0.001666s); valid time: 0.0127s; loss: 2110.24 (±21515.7); valid loss: -34.5353\n",
      "[Epoch 290/350, Step 19100, ETA 24.53s] step time: 0.005615s (±0.001679s); valid time: 0.01226s; loss: 20189.6 (±201317); valid loss: -34.6832\n",
      "[Epoch 290/350, Step 19140, ETA 24.28s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 19200, ETA 23.91s] step time: 0.005678s (±0.001723s); valid time: 0.01267s; loss: 1841.96 (±18854); valid loss: -33.7167\n",
      "[Epoch 293/350, Step 19300, ETA 23.3s] step time: 0.005487s (±0.001668s); valid time: 0.01335s; loss: -50.5389 (±20.9361); valid loss: -34.1648\n",
      "[Epoch 294/350, Step 19400, ETA 22.69s] step time: 0.005679s (±0.001883s); valid time: 0.01398s; loss: -44.3775 (±61.2462); valid loss: 23.0964\n",
      "[Epoch 296/350, Step 19500, ETA 22.07s] step time: 0.005323s (±0.001488s); valid time: 0.01198s; loss: -53.0146 (±3.23375); valid loss: -34.7642\n",
      "[Epoch 297/350, Step 19600, ETA 21.45s] step time: 0.005551s (±0.001572s); valid time: 0.01295s; loss: -4.52755 (±480.919); valid loss: -32.6035\n",
      "[Epoch 299/350, Step 19700, ETA 20.84s] step time: 0.005749s (±0.001593s); valid time: 0.01241s; loss: -52.7685 (±2.9535); valid loss: -6.41247\n",
      "[Epoch 300/350, Step 19800, ETA 20.23s] step time: 0.005487s (±0.001495s); valid time: 0.01302s; loss: -53.0822 (±2.55339); valid loss: -34.1488\n",
      "[Epoch 300/350, Step 19800, ETA 20.23s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 302/350, Step 19900, ETA 19.61s] step time: 0.005519s (±0.001638s); valid time: 0.0133s; loss: -23.6842 (±290.18); valid loss: -34.2116\n",
      "[Epoch 304/350, Step 20000, ETA 19s] step time: 0.005473s (±0.001695s); valid time: 0.01333s; loss: 227960 (±2.26864e+06); valid loss: -34.0238\n",
      "[Epoch 305/350, Step 20100, ETA 18.38s] step time: 0.005507s (±0.001556s); valid time: 0.01233s; loss: -53.1064 (±3.21044); valid loss: 108602\n",
      "[Epoch 307/350, Step 20200, ETA 17.77s] step time: 0.00554s (±0.001904s); valid time: 0.0137s; loss: -49.5531 (±31.7437); valid loss: -33.3118\n",
      "[Epoch 308/350, Step 20300, ETA 17.15s] step time: 0.00539s (±0.001536s); valid time: 0.01249s; loss: 31675.6 (±315320); valid loss: -33.5144\n",
      "[Epoch 310/350, Step 20400, ETA 16.54s] step time: 0.005412s (±0.001719s); valid time: 0.01478s; loss: -51.1674 (±15.678); valid loss: -34.1468\n",
      "[Epoch 310/350, Step 20460, ETA 16.16s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 20500, ETA 15.92s] step time: 0.005425s (±0.001653s); valid time: 0.01306s; loss: 8287.81 (±82992.4); valid loss: -33.6501\n",
      "[Epoch 313/350, Step 20600, ETA 15.31s] step time: 0.005688s (±0.001655s); valid time: 0.0127s; loss: 750064 (±7.09372e+06); valid loss: -30.5158\n",
      "[Epoch 314/350, Step 20700, ETA 14.69s] step time: 0.005464s (±0.002087s); valid time: 0.01982s; loss: 205083 (±2.04107e+06); valid loss: -34.5172\n",
      "[Epoch 316/350, Step 20800, ETA 14.08s] step time: 0.005533s (±0.001528s); valid time: 0.01302s; loss: -52.9243 (±3.1287); valid loss: -33.6618\n",
      "[Epoch 317/350, Step 20900, ETA 13.47s] step time: 0.00557s (±0.001723s); valid time: 0.01405s; loss: 2218.45 (±16830.9); valid loss: -27.8423\n",
      "[Epoch 319/350, Step 21000, ETA 12.86s] step time: 0.005474s (±0.001361s); valid time: 0.01241s; loss: 492.907 (±5392.05); valid loss: -34.6035\n",
      "[Epoch 320/350, Step 21100, ETA 12.24s] step time: 0.005616s (±0.001737s); valid time: 0.01207s; loss: 406165 (±4.04108e+06); valid loss: -34.2958\n",
      "[Epoch 320/350, Step 21120, ETA 12.12s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 21200, ETA 11.63s] step time: 0.00545s (±0.001489s); valid time: 0.01251s; loss: 3283.43 (±33099.2); valid loss: -33.5563\n",
      "[Epoch 323/350, Step 21300, ETA 11.02s] step time: 0.005548s (±0.001804s); valid time: 0.01279s; loss: -47.6321 (±53.3016); valid loss: -33.5867\n",
      "[Epoch 325/350, Step 21400, ETA 10.4s] step time: 0.005614s (±0.001641s); valid time: 0.01314s; loss: 65.8618 (±1160.92); valid loss: -8.87308\n",
      "[Epoch 326/350, Step 21500, ETA 9.79s] step time: 0.005464s (±0.001438s); valid time: 0.01323s; loss: 107.114 (±1591.28); valid loss: 12106\n",
      "[Epoch 328/350, Step 21600, ETA 9.178s] step time: 0.005513s (±0.001633s); valid time: 0.01223s; loss: -49.6586 (±32.0246); valid loss: -33.5367\n",
      "[Epoch 329/350, Step 21700, ETA 8.563s] step time: 0.005356s (±0.001436s); valid time: 0.01221s; loss: -43.2048 (±85.2166); valid loss: -33.4453\n",
      "[Epoch 330/350, Step 21780, ETA 8.074s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 21800, ETA 7.952s] step time: 0.00556s (±0.00173s); valid time: 0.013s; loss: -51.9945 (±12.1139); valid loss: -34.3125\n",
      "[Epoch 332/350, Step 21900, ETA 7.34s] step time: 0.005661s (±0.001808s); valid time: 0.01294s; loss: 1.10363e+06 (±1.09735e+07); valid loss: -34.0229\n",
      "[Epoch 334/350, Step 22000, ETA 6.728s] step time: 0.005469s (±0.001533s); valid time: 0.01289s; loss: -49.8624 (±30.4607); valid loss: -33.1104\n",
      "[Epoch 335/350, Step 22100, ETA 6.116s] step time: 0.005613s (±0.001567s); valid time: 0.01278s; loss: 2.85791e+06 (±1.81794e+07); valid loss: -33.0561\n",
      "[Epoch 337/350, Step 22200, ETA 5.504s] step time: 0.005393s (±0.001501s); valid time: 0.01251s; loss: 2252.72 (±18641.1); valid loss: -33.7979\n",
      "[Epoch 338/350, Step 22300, ETA 4.891s] step time: 0.005473s (±0.001615s); valid time: 0.01273s; loss: 579187 (±5.38589e+06); valid loss: 3.86956e+06\n",
      "[Epoch 340/350, Step 22400, ETA 4.28s] step time: 0.005568s (±0.001777s); valid time: 0.014s; loss: -53.0524 (±2.92485); valid loss: 1669.74\n",
      "[Epoch 340/350, Step 22440, ETA 4.034s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 22500, ETA 3.668s] step time: 0.005406s (±0.001635s); valid time: 0.01259s; loss: 73808.1 (±731240); valid loss: -33.8857\n",
      "[Epoch 343/350, Step 22600, ETA 3.057s] step time: 0.005882s (±0.001865s); valid time: 0.01434s; loss: 652.51 (±7014.68); valid loss: 169181\n",
      "[Epoch 344/350, Step 22700, ETA 2.446s] step time: 0.00562s (±0.001414s); valid time: 0.01248s; loss: 4117.19 (±41490.9); valid loss: -30.7685\n",
      "[Epoch 346/350, Step 22800, ETA 1.834s] step time: 0.005333s (±0.001632s); valid time: 0.01301s; loss: -26.0256 (±265.066); valid loss: -30.6494\n",
      "[Epoch 347/350, Step 22900, ETA 1.222s] step time: 0.005344s (±0.001683s); valid time: 0.01624s; loss: 158.109 (±2086.77); valid loss: -33.7871\n",
      "[Epoch 349/350, Step 23000, ETA 0.6111s] step time: 0.005499s (±0.001739s); valid time: 0.01239s; loss: 322089 (±3.20526e+06); valid loss: -34.0403\n",
      "[Epoch 350/350, Step 23100, ETA 0s] step time: 0.005517s (±0.00166s); valid time: 0.01301s; loss: 153226 (±1.37922e+06); valid loss: 3770.95\n",
      "[Epoch 350/350, Step 23100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk8dvb3vt/variables.dat-15300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk8dvb3vt/variables.dat-15300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.8,\n",
      "\t(tp, fp, tn, fn)=(1727, 3945, 16482, 541),\n",
      "\tprecision=0.3,\n",
      "\trecall=0.76,\n",
      "\tf1=0.44,\n",
      "\troc_auc=0.78,\n",
      "\ty_pred%=0.2499228905045164,\n",
      "\ty_label%=0.09993390614672835,\n",
      ")\n",
      "Testing on realKnownCause/nyc_taxi.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 4/350, Step 100, ETA 1m 42.13s] step time: 0.009629s (±0.03148s); valid time: 0.1624s; loss: 138.303 (±8.49849); valid loss: 96.09 (*)\n",
      "[Epoch 7/350, Step 200, ETA 1m 23.75s] step time: 0.006218s (±0.007903s); valid time: 0.07944s; loss: 105.384 (±9.416); valid loss: 75.2998 (*)\n",
      "[Epoch 10/350, Step 290, ETA 1m 14.96s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 300, ETA 1m 17.02s] step time: 0.006097s (±0.00803s); valid time: 0.08s; loss: 74.8604 (±8.30253); valid loss: 59.5652 (*)\n",
      "[Epoch 14/350, Step 400, ETA 1m 13.32s] step time: 0.006178s (±0.007874s); valid time: 0.07866s; loss: 61.2041 (±2.58023); valid loss: 55.3989 (*)\n",
      "[Epoch 18/350, Step 500, ETA 1m 11.09s] step time: 0.006217s (±0.008131s); valid time: 0.08101s; loss: 56.411 (±2.7863); valid loss: 52.9257 (*)\n",
      "[Epoch 20/350, Step 580, ETA 1m 8.233s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 600, ETA 1m 8.941s] step time: 0.006027s (±0.008309s); valid time: 0.08193s; loss: 51.8618 (±2.59854); valid loss: 49.8848 (*)\n",
      "[Epoch 25/350, Step 700, ETA 1m 7.517s] step time: 0.006179s (±0.008091s); valid time: 0.08034s; loss: 46.3865 (±2.38107); valid loss: 47.6857 (*)\n",
      "[Epoch 28/350, Step 800, ETA 1m 6.146s] step time: 0.006133s (±0.008948s); valid time: 0.0898s; loss: 41.3917 (±2.82718); valid loss: 46.2968 (*)\n",
      "[Epoch 30/350, Step 870, ETA 1m 4.646s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 900, ETA 1m 4.196s] step time: 0.005323s (±0.0008383s); valid time: 0.006089s; loss: 37.4994 (±2.21529); valid loss: 46.3373\n",
      "[Epoch 35/350, Step 1000, ETA 1m 2.524s] step time: 0.005423s (±0.001282s); valid time: 0.00633s; loss: 35.377 (±2.34533); valid loss: 46.3442\n",
      "[Epoch 38/350, Step 1100, ETA 1m 1.665s] step time: 0.006165s (±0.008014s); valid time: 0.08013s; loss: 33.6868 (±2.11969); valid loss: 45.7783 (*)\n",
      "[Epoch 40/350, Step 1160, ETA 1m 0.9101s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1200, ETA 1m 1.057s] step time: 0.006384s (±0.008517s); valid time: 0.08422s; loss: 32.4103 (±2.04336); valid loss: 45.7565 (*)\n",
      "[Epoch 45/350, Step 1300, ETA 59.75s] step time: 0.005375s (±0.001075s); valid time: 0.005892s; loss: 31.495 (±1.95101); valid loss: 46.749\n",
      "[Epoch 49/350, Step 1400, ETA 59.03s] step time: 0.006162s (±0.008178s); valid time: 0.08194s; loss: 30.4843 (±2.32668); valid loss: 45.6221 (*)\n",
      "[Epoch 50/350, Step 1450, ETA 58.45s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 52/350, Step 1500, ETA 57.9s] step time: 0.005487s (±0.001406s); valid time: 0.006608s; loss: 29.8189 (±1.91774); valid loss: 46.73\n",
      "[Epoch 56/350, Step 1600, ETA 56.81s] step time: 0.005393s (±0.001175s); valid time: 0.006123s; loss: 29.1017 (±2.24701); valid loss: 46.8778\n",
      "[Epoch 59/350, Step 1700, ETA 55.84s] step time: 0.005575s (±0.001154s); valid time: 0.006256s; loss: 28.3555 (±2.07399); valid loss: 47.5667\n",
      "[Epoch 60/350, Step 1740, ETA 55.42s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 63/350, Step 1800, ETA 55.01s] step time: 0.005694s (±0.001512s); valid time: 0.006435s; loss: 27.9541 (±2.32405); valid loss: 47.6049\n",
      "[Epoch 66/350, Step 1900, ETA 54.03s] step time: 0.005419s (±0.0009623s); valid time: 0.005974s; loss: 27.6077 (±1.97886); valid loss: 47.5266\n",
      "[Epoch 69/350, Step 2000, ETA 53.19s] step time: 0.005571s (±0.001179s); valid time: 0.006269s; loss: 26.9036 (±2.05529); valid loss: 47.6671\n",
      "[Epoch 70/350, Step 2030, ETA 52.89s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 73/350, Step 2100, ETA 52.24s] step time: 0.005233s (±0.0009048s); valid time: 0.006007s; loss: 26.6757 (±2.33436); valid loss: 48.9769\n",
      "[Epoch 76/350, Step 2200, ETA 51.35s] step time: 0.005328s (±0.001027s); valid time: 0.006349s; loss: 26.4746 (±2.46213); valid loss: 48.7111\n",
      "[Epoch 80/350, Step 2300, ETA 50.59s] step time: 0.005529s (±0.001617s); valid time: 0.006551s; loss: 26.0381 (±2.25957); valid loss: 49.5873\n",
      "[Epoch 80/350, Step 2320, ETA 50.38s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 83/350, Step 2400, ETA 49.75s] step time: 0.005382s (±0.001106s); valid time: 0.00597s; loss: 25.5798 (±1.81207); valid loss: 49.2381\n",
      "[Epoch 87/350, Step 2500, ETA 48.97s] step time: 0.005438s (±0.001215s); valid time: 0.006589s; loss: 25.4037 (±2.43189); valid loss: 49.7046\n",
      "[Epoch 90/350, Step 2600, ETA 48.18s] step time: 0.005423s (±0.0009086s); valid time: 0.006001s; loss: 25.3947 (±1.89226); valid loss: 49.3061\n",
      "[Epoch 90/350, Step 2610, ETA 48.1s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 94/350, Step 2700, ETA 47.4s] step time: 0.005359s (±0.001347s); valid time: 0.005992s; loss: 25.1724 (±2.17784); valid loss: 50.0581\n",
      "[Epoch 97/350, Step 2800, ETA 46.63s] step time: 0.005419s (±0.001265s); valid time: 0.009533s; loss: 24.7335 (±2.27043); valid loss: 51.3073\n",
      "[Epoch 100/350, Step 2900, ETA 45.93s] step time: 0.005614s (±0.001356s); valid time: 0.006225s; loss: 24.8069 (±2.04246); valid loss: 51.2629\n",
      "[Epoch 100/350, Step 2900, ETA 45.93s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 104/350, Step 3000, ETA 45.24s] step time: 0.005578s (±0.001398s); valid time: 0.006273s; loss: 24.4359 (±1.89879); valid loss: 51.7502\n",
      "[Epoch 107/350, Step 3100, ETA 44.52s] step time: 0.005482s (±0.001112s); valid time: 0.006136s; loss: 24.4726 (±2.03841); valid loss: 50.4982\n",
      "[Epoch 110/350, Step 3190, ETA 43.88s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 3200, ETA 43.83s] step time: 0.005483s (±0.001336s); valid time: 0.006047s; loss: 24.2492 (±1.87408); valid loss: 50.687\n",
      "[Epoch 114/350, Step 3300, ETA 43.11s] step time: 0.005449s (±0.001463s); valid time: 0.008931s; loss: 24.3374 (±2.20613); valid loss: 51.6825\n",
      "[Epoch 118/350, Step 3400, ETA 42.4s] step time: 0.005348s (±0.001138s); valid time: 0.006167s; loss: 24.1081 (±2.04122); valid loss: 52.1965\n",
      "[Epoch 120/350, Step 3480, ETA 41.82s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 3500, ETA 41.71s] step time: 0.005499s (±0.001199s); valid time: 0.006034s; loss: 24.0047 (±1.95676); valid loss: 51.9272\n",
      "[Epoch 125/350, Step 3600, ETA 41.04s] step time: 0.005465s (±0.0009956s); valid time: 0.006568s; loss: 24.0899 (±1.96295); valid loss: 52.7268\n",
      "[Epoch 128/350, Step 3700, ETA 40.35s] step time: 0.00548s (±0.001194s); valid time: 0.00607s; loss: 23.7594 (±2.23108); valid loss: 51.6696\n",
      "[Epoch 130/350, Step 3770, ETA 39.85s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 3800, ETA 39.66s] step time: 0.005313s (±0.0009831s); valid time: 0.006181s; loss: 23.9084 (±2.0628); valid loss: 52.1684\n",
      "[Epoch 135/350, Step 3900, ETA 38.97s] step time: 0.005396s (±0.001243s); valid time: 0.006015s; loss: 23.8267 (±1.96698); valid loss: 52.0169\n",
      "[Epoch 138/350, Step 4000, ETA 38.27s] step time: 0.005287s (±0.001512s); valid time: 0.009933s; loss: 23.7551 (±2.11536); valid loss: 52.9702\n",
      "[Epoch 140/350, Step 4060, ETA 37.86s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 4100, ETA 37.61s] step time: 0.005427s (±0.001251s); valid time: 0.00591s; loss: 23.7029 (±2.23356); valid loss: 53.1764\n",
      "[Epoch 145/350, Step 4200, ETA 36.97s] step time: 0.00562s (±0.0015s); valid time: 0.007052s; loss: 23.7525 (±2.08984); valid loss: 52.3294\n",
      "[Epoch 149/350, Step 4300, ETA 36.3s] step time: 0.005267s (±0.0008384s); valid time: 0.006036s; loss: 23.5323 (±2.21789); valid loss: 52.3183\n",
      "[Epoch 150/350, Step 4350, ETA 35.97s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 152/350, Step 4400, ETA 35.65s] step time: 0.005546s (±0.001325s); valid time: 0.006346s; loss: 23.6978 (±1.92725); valid loss: 52.9684\n",
      "[Epoch 156/350, Step 4500, ETA 35s] step time: 0.005373s (±0.001138s); valid time: 0.006519s; loss: 23.5153 (±2.1552); valid loss: 52.6555\n",
      "[Epoch 159/350, Step 4600, ETA 34.34s] step time: 0.005442s (±0.001175s); valid time: 0.006207s; loss: 23.4763 (±2.13669); valid loss: 53.5375\n",
      "[Epoch 160/350, Step 4640, ETA 34.07s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 163/350, Step 4700, ETA 33.7s] step time: 0.005474s (±0.001281s); valid time: 0.006285s; loss: 23.4296 (±2.15771); valid loss: 52.3292\n",
      "[Epoch 166/350, Step 4800, ETA 33.05s] step time: 0.005399s (±0.0009747s); valid time: 0.006037s; loss: 23.4911 (±2.31283); valid loss: 52.8831\n",
      "[Epoch 169/350, Step 4900, ETA 32.4s] step time: 0.005453s (±0.001214s); valid time: 0.006244s; loss: 23.4327 (±2.25742); valid loss: 52.6677\n",
      "[Epoch 170/350, Step 4930, ETA 32.21s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 173/350, Step 5000, ETA 31.76s] step time: 0.00542s (±0.001109s); valid time: 0.0063s; loss: 23.3161 (±1.99311); valid loss: 53.523\n",
      "[Epoch 176/350, Step 5100, ETA 31.12s] step time: 0.005401s (±0.001081s); valid time: 0.006043s; loss: 23.4419 (±2.27666); valid loss: 52.6141\n",
      "[Epoch 180/350, Step 5200, ETA 30.47s] step time: 0.005311s (±0.0008603s); valid time: 0.006071s; loss: 23.3426 (±2.0436); valid loss: 52.9059\n",
      "[Epoch 180/350, Step 5220, ETA 30.34s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 183/350, Step 5300, ETA 29.82s] step time: 0.00535s (±0.001011s); valid time: 0.006613s; loss: 23.4687 (±2.01021); valid loss: 52.5741\n",
      "[Epoch 187/350, Step 5400, ETA 29.21s] step time: 0.005568s (±0.001397s); valid time: 0.006004s; loss: 23.2903 (±2.0826); valid loss: 53.0907\n",
      "[Epoch 190/350, Step 5500, ETA 28.56s] step time: 0.005298s (±0.0009614s); valid time: 0.006094s; loss: 23.2463 (±2.15308); valid loss: 53.8539\n",
      "[Epoch 190/350, Step 5510, ETA 28.49s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 194/350, Step 5600, ETA 27.93s] step time: 0.005404s (±0.001213s); valid time: 0.00596s; loss: 23.3237 (±2.15541); valid loss: 53.2184\n",
      "[Epoch 197/350, Step 5700, ETA 27.28s] step time: 0.005268s (±0.0009147s); valid time: 0.005992s; loss: 23.4577 (±2.08386); valid loss: 53.4399\n",
      "[Epoch 200/350, Step 5800, ETA 26.64s] step time: 0.005314s (±0.001162s); valid time: 0.006115s; loss: 23.2437 (±2.16158); valid loss: 53.0308\n",
      "[Epoch 200/350, Step 5800, ETA 26.64s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 204/350, Step 5900, ETA 26s] step time: 0.005244s (±0.0009375s); valid time: 0.006384s; loss: 23.2296 (±2.15236); valid loss: 52.8871\n",
      "[Epoch 207/350, Step 6000, ETA 25.38s] step time: 0.005461s (±0.00119s); valid time: 0.006155s; loss: 23.4257 (±2.05076); valid loss: 53.1143\n",
      "[Epoch 210/350, Step 6090, ETA 24.81s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 6100, ETA 24.76s] step time: 0.005504s (±0.001139s); valid time: 0.006441s; loss: 23.3357 (±2.26774); valid loss: 52.9853\n",
      "[Epoch 214/350, Step 6200, ETA 24.13s] step time: 0.005458s (±0.001193s); valid time: 0.005958s; loss: 23.1805 (±2.31566); valid loss: 53.1086\n",
      "[Epoch 218/350, Step 6300, ETA 23.52s] step time: 0.005415s (±0.000956s); valid time: 0.00632s; loss: 23.2115 (±2.25186); valid loss: 53.317\n",
      "[Epoch 220/350, Step 6380, ETA 23.01s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 6400, ETA 22.89s] step time: 0.005418s (±0.001088s); valid time: 0.00636s; loss: 23.2546 (±1.93406); valid loss: 53.4882\n",
      "[Epoch 225/350, Step 6500, ETA 22.27s] step time: 0.005491s (±0.001305s); valid time: 0.006241s; loss: 23.2475 (±2.07738); valid loss: 53.3074\n",
      "[Epoch 228/350, Step 6600, ETA 21.66s] step time: 0.005575s (±0.001576s); valid time: 0.006361s; loss: 23.2906 (±2.1143); valid loss: 53.5439\n",
      "[Epoch 230/350, Step 6670, ETA 21.22s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 6700, ETA 21.04s] step time: 0.00543s (±0.001198s); valid time: 0.006198s; loss: 23.2642 (±1.82825); valid loss: 52.9307\n",
      "[Epoch 235/350, Step 6800, ETA 20.43s] step time: 0.005589s (±0.001353s); valid time: 0.00624s; loss: 23.4693 (±2.02881); valid loss: 52.9808\n",
      "[Epoch 238/350, Step 6900, ETA 19.81s] step time: 0.005422s (±0.000876s); valid time: 0.005995s; loss: 23.1094 (±2.17339); valid loss: 52.919\n",
      "[Epoch 240/350, Step 6960, ETA 19.44s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 7000, ETA 19.2s] step time: 0.005587s (±0.001305s); valid time: 0.006611s; loss: 23.2545 (±2.16399); valid loss: 52.9147\n",
      "[Epoch 245/350, Step 7100, ETA 18.6s] step time: 0.005742s (±0.001334s); valid time: 0.006671s; loss: 23.3384 (±2.40465); valid loss: 53.382\n",
      "[Epoch 249/350, Step 7200, ETA 17.99s] step time: 0.005475s (±0.0007465s); valid time: 0.006243s; loss: 23.2163 (±2.04634); valid loss: 53.2274\n",
      "[Epoch 250/350, Step 7250, ETA 17.68s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 252/350, Step 7300, ETA 17.38s] step time: 0.005738s (±0.00149s); valid time: 0.006504s; loss: 23.3045 (±2.00489); valid loss: 52.7687\n",
      "[Epoch 256/350, Step 7400, ETA 16.78s] step time: 0.005637s (±0.001162s); valid time: 0.006413s; loss: 23.2933 (±2.34785); valid loss: 53.1426\n",
      "[Epoch 259/350, Step 7500, ETA 16.17s] step time: 0.005625s (±0.001087s); valid time: 0.006407s; loss: 23.339 (±2.2595); valid loss: 53.3042\n",
      "[Epoch 260/350, Step 7540, ETA 15.92s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 263/350, Step 7600, ETA 15.57s] step time: 0.00597s (±0.001891s); valid time: 0.007672s; loss: 23.2355 (±1.98827); valid loss: 53.4771\n",
      "[Epoch 266/350, Step 7700, ETA 14.96s] step time: 0.005421s (±0.0009834s); valid time: 0.006128s; loss: 23.2798 (±1.92527); valid loss: 53.4589\n",
      "[Epoch 269/350, Step 7800, ETA 14.34s] step time: 0.00554s (±0.001308s); valid time: 0.006149s; loss: 23.2789 (±2.10919); valid loss: 53.3872\n",
      "[Epoch 270/350, Step 7830, ETA 14.16s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 273/350, Step 7900, ETA 13.73s] step time: 0.005437s (±0.0009408s); valid time: 0.006428s; loss: 23.1233 (±2.12757); valid loss: 53.2757\n",
      "[Epoch 276/350, Step 8000, ETA 13.12s] step time: 0.00569s (±0.00137s); valid time: 0.006261s; loss: 23.3461 (±2.17873); valid loss: 52.7887\n",
      "[Epoch 280/350, Step 8100, ETA 12.51s] step time: 0.00558s (±0.001501s); valid time: 0.006322s; loss: 23.1425 (±2.16384); valid loss: 53.3922\n",
      "[Epoch 280/350, Step 8120, ETA 12.39s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 283/350, Step 8200, ETA 11.9s] step time: 0.005408s (±0.001181s); valid time: 0.006051s; loss: 23.1633 (±2.21621); valid loss: 53.0072\n",
      "[Epoch 287/350, Step 8300, ETA 11.28s] step time: 0.005463s (±0.00116s); valid time: 0.005906s; loss: 23.2962 (±2.53422); valid loss: 52.8029\n",
      "[Epoch 290/350, Step 8400, ETA 10.67s] step time: 0.005346s (±0.001102s); valid time: 0.006152s; loss: 23.0894 (±1.92763); valid loss: 52.3049\n",
      "[Epoch 290/350, Step 8410, ETA 10.61s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 294/350, Step 8500, ETA 10.06s] step time: 0.005479s (±0.001192s); valid time: 0.00631s; loss: 23.3196 (±2.49211); valid loss: 53.1284\n",
      "[Epoch 297/350, Step 8600, ETA 9.444s] step time: 0.005459s (±0.001004s); valid time: 0.006892s; loss: 23.2419 (±2.1201); valid loss: 53.4756\n",
      "[Epoch 300/350, Step 8700, ETA 8.839s] step time: 0.005856s (±0.001391s); valid time: 0.00632s; loss: 23.2856 (±2.21368); valid loss: 53.4769\n",
      "[Epoch 300/350, Step 8700, ETA 8.839s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 304/350, Step 8800, ETA 8.226s] step time: 0.005383s (±0.001131s); valid time: 0.006209s; loss: 23.24 (±2.04997); valid loss: 53.6876\n",
      "[Epoch 307/350, Step 8900, ETA 7.612s] step time: 0.005263s (±0.000847s); valid time: 0.006078s; loss: 23.2432 (±2.30574); valid loss: 53.2525\n",
      "[Epoch 310/350, Step 8990, ETA 7.061s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 9000, ETA 7.002s] step time: 0.00548s (±0.001273s); valid time: 0.007379s; loss: 23.146 (±2.18589); valid loss: 53.1234\n",
      "[Epoch 314/350, Step 9100, ETA 6.389s] step time: 0.005332s (±0.001112s); valid time: 0.006572s; loss: 23.2269 (±1.90335); valid loss: 53.4355\n",
      "[Epoch 318/350, Step 9200, ETA 5.779s] step time: 0.005397s (±0.001035s); valid time: 0.005982s; loss: 23.2203 (±2.40166); valid loss: 53.8405\n",
      "[Epoch 320/350, Step 9280, ETA 5.291s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 9300, ETA 5.171s] step time: 0.005593s (±0.001384s); valid time: 0.006741s; loss: 23.3172 (±2.11712); valid loss: 53.4263\n",
      "[Epoch 325/350, Step 9400, ETA 4.561s] step time: 0.005389s (±0.0012s); valid time: 0.005922s; loss: 23.2269 (±2.17996); valid loss: 53.4989\n",
      "[Epoch 328/350, Step 9500, ETA 3.952s] step time: 0.005438s (±0.001017s); valid time: 0.006344s; loss: 23.2185 (±2.0717); valid loss: 53.1549\n",
      "[Epoch 330/350, Step 9570, ETA 3.525s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 9600, ETA 3.343s] step time: 0.005433s (±0.0009658s); valid time: 0.006407s; loss: 23.1672 (±1.88602); valid loss: 53.5991\n",
      "[Epoch 335/350, Step 9700, ETA 2.735s] step time: 0.005468s (±0.001424s); valid time: 0.008332s; loss: 23.2431 (±2.09321); valid loss: 53.4626\n",
      "[Epoch 338/350, Step 9800, ETA 2.126s] step time: 0.005266s (±0.0009802s); valid time: 0.005897s; loss: 23.2158 (±2.26445); valid loss: 53.6988\n",
      "[Epoch 340/350, Step 9860, ETA 1.761s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 9900, ETA 1.518s] step time: 0.005536s (±0.001332s); valid time: 0.006068s; loss: 23.2426 (±2.23429); valid loss: 53.2085\n",
      "[Epoch 345/350, Step 10000, ETA 0.9109s] step time: 0.005477s (±0.001193s); valid time: 0.006297s; loss: 23.1602 (±1.87152); valid loss: 53.3045\n",
      "[Epoch 349/350, Step 10100, ETA 0.3035s] step time: 0.00533s (±0.001007s); valid time: 0.005818s; loss: 23.3384 (±2.04719); valid loss: 53.4584\n",
      "[Epoch 350/350, Step 10150, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3j04nqfn/variables.dat-1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3j04nqfn/variables.dat-1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.74,\n",
      "\t(tp, fp, tn, fn)=(390, 2029, 7256, 645),\n",
      "\tprecision=0.16,\n",
      "\trecall=0.38,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.23439922480620154,\n",
      "\ty_label%=0.1002906976744186,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_hold.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 7/350, Step 100, ETA 51.84s] step time: 0.009558s (±0.03167s); valid time: 0.1569s; loss: 33.3494 (±4.63136); valid loss: 64.5607 (*)\n",
      "[Epoch 10/350, Step 150, ETA 44.21s] Learning rate decreased to 0.00075\n",
      "[Epoch 14/350, Step 200, ETA 42.05s] step time: 0.006071s (±0.0073s); valid time: 0.0729s; loss: 29.6556 (±3.70675); valid loss: 62.1741 (*)\n",
      "[Epoch 20/350, Step 300, ETA 38.57s] step time: 0.006258s (±0.007729s); valid time: 0.07673s; loss: 25.4031 (±3.96648); valid loss: 54.7083 (*)\n",
      "[Epoch 20/350, Step 300, ETA 38.57s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 27/350, Step 400, ETA 36.35s] step time: 0.006093s (±0.008569s); valid time: 0.0849s; loss: 21.4363 (±3.60823); valid loss: 49.0814 (*)\n",
      "[Epoch 30/350, Step 450, ETA 35.02s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 34/350, Step 500, ETA 34.7s] step time: 0.006036s (±0.007842s); valid time: 0.07809s; loss: 18.5131 (±3.52759); valid loss: 48.1356 (*)\n",
      "[Epoch 40/350, Step 600, ETA 33.39s] step time: 0.006098s (±0.007558s); valid time: 0.07505s; loss: 16.6929 (±2.97313); valid loss: 47.3703 (*)\n",
      "[Epoch 40/350, Step 600, ETA 33.4s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 47/350, Step 700, ETA 32.15s] step time: 0.005878s (±0.007631s); valid time: 0.07658s; loss: 15.6308 (±2.86328); valid loss: 46.2738 (*)\n",
      "[Epoch 50/350, Step 750, ETA 31.46s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 54/350, Step 800, ETA 31.24s] step time: 0.006171s (±0.007944s); valid time: 0.07904s; loss: 14.8252 (±3.02941); valid loss: 45.7653 (*)\n",
      "[Epoch 60/350, Step 900, ETA 30.5s] step time: 0.006491s (±0.008902s); valid time: 0.08791s; loss: 14.2819 (±3.06018); valid loss: 45.403 (*)\n",
      "[Epoch 60/350, Step 900, ETA 30.51s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 67/350, Step 1000, ETA 29.58s] step time: 0.005993s (±0.008574s); valid time: 0.08489s; loss: 13.8303 (±3.15015); valid loss: 45.2696 (*)\n",
      "[Epoch 70/350, Step 1050, ETA 29.08s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 74/350, Step 1100, ETA 28.85s] step time: 0.006354s (±0.007746s); valid time: 0.07722s; loss: 13.4586 (±2.91891); valid loss: 45.2024 (*)\n",
      "[Epoch 80/350, Step 1200, ETA 27.8s] step time: 0.005439s (±0.001284s); valid time: 0.003593s; loss: 13.1839 (±2.93863); valid loss: 45.3826\n",
      "[Epoch 80/350, Step 1200, ETA 27.8s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 87/350, Step 1300, ETA 27s] step time: 0.006013s (±0.007759s); valid time: 0.07755s; loss: 12.8536 (±2.94063); valid loss: 44.941 (*)\n",
      "[Epoch 90/350, Step 1350, ETA 26.53s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 94/350, Step 1400, ETA 26.15s] step time: 0.005655s (±0.001288s); valid time: 0.003628s; loss: 12.8204 (±3.22682); valid loss: 45.0845\n",
      "[Epoch 100/350, Step 1500, ETA 25.25s] step time: 0.00545s (±0.001066s); valid time: 0.003925s; loss: 12.5331 (±3.13943); valid loss: 45.1534\n",
      "[Epoch 100/350, Step 1500, ETA 25.25s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 107/350, Step 1600, ETA 24.36s] step time: 0.005265s (±0.000765s); valid time: 0.003827s; loss: 12.248 (±2.92405); valid loss: 45.3265\n",
      "[Epoch 110/350, Step 1650, ETA 23.92s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 114/350, Step 1700, ETA 23.69s] step time: 0.006167s (±0.007667s); valid time: 0.07619s; loss: 12.3787 (±3.08964); valid loss: 44.6088 (*)\n",
      "[Epoch 120/350, Step 1800, ETA 22.85s] step time: 0.005281s (±0.0007351s); valid time: 0.003435s; loss: 12.2239 (±3.13994); valid loss: 45.0867\n",
      "[Epoch 120/350, Step 1800, ETA 22.85s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 127/350, Step 1900, ETA 22.06s] step time: 0.005422s (±0.001403s); valid time: 0.003524s; loss: 11.9672 (±3.00405); valid loss: 45.0202\n",
      "[Epoch 130/350, Step 1950, ETA 21.65s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 134/350, Step 2000, ETA 21.27s] step time: 0.005235s (±0.001194s); valid time: 0.003744s; loss: 12.1162 (±3.0223); valid loss: 44.9047\n",
      "[Epoch 140/350, Step 2100, ETA 20.55s] step time: 0.005656s (±0.001628s); valid time: 0.003586s; loss: 12.1165 (±2.49346); valid loss: 44.824\n",
      "[Epoch 140/350, Step 2100, ETA 20.55s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 147/350, Step 2200, ETA 19.82s] step time: 0.005485s (±0.001021s); valid time: 0.003529s; loss: 11.9159 (±2.75805); valid loss: 44.8262\n",
      "[Epoch 150/350, Step 2250, ETA 19.46s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 154/350, Step 2300, ETA 19.11s] step time: 0.005425s (±0.0008308s); valid time: 0.00357s; loss: 11.8047 (±3.14162); valid loss: 44.7967\n",
      "[Epoch 160/350, Step 2400, ETA 18.39s] step time: 0.00548s (±0.001259s); valid time: 0.004658s; loss: 11.9759 (±2.76822); valid loss: 44.7067\n",
      "[Epoch 160/350, Step 2400, ETA 18.4s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 167/350, Step 2500, ETA 17.69s] step time: 0.005392s (±0.0011s); valid time: 0.003826s; loss: 11.8294 (±3.0773); valid loss: 44.9043\n",
      "[Epoch 170/350, Step 2550, ETA 17.35s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 174/350, Step 2600, ETA 17s] step time: 0.005536s (±0.001432s); valid time: 0.003682s; loss: 11.889 (±3.26547); valid loss: 44.6833\n",
      "[Epoch 180/350, Step 2700, ETA 16.33s] step time: 0.005595s (±0.001214s); valid time: 0.004991s; loss: 11.8749 (±2.90901); valid loss: 44.8689\n",
      "[Epoch 180/350, Step 2700, ETA 16.33s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 187/350, Step 2800, ETA 15.64s] step time: 0.005394s (±0.001019s); valid time: 0.003608s; loss: 11.879 (±2.94506); valid loss: 44.9131\n",
      "[Epoch 190/350, Step 2850, ETA 15.3s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 194/350, Step 2900, ETA 14.97s] step time: 0.005428s (±0.001211s); valid time: 0.003442s; loss: 11.6742 (±3.18878); valid loss: 44.9277\n",
      "[Epoch 200/350, Step 3000, ETA 14.29s] step time: 0.005296s (±0.0008117s); valid time: 0.003439s; loss: 11.7645 (±3.4805); valid loss: 44.8888\n",
      "[Epoch 200/350, Step 3000, ETA 14.29s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 207/350, Step 3100, ETA 13.63s] step time: 0.00549s (±0.001502s); valid time: 0.00349s; loss: 11.8327 (±2.72545); valid loss: 44.9634\n",
      "[Epoch 210/350, Step 3150, ETA 13.29s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 214/350, Step 3200, ETA 12.97s] step time: 0.005456s (±0.0008803s); valid time: 0.003632s; loss: 11.6878 (±3.38967); valid loss: 45.0867\n",
      "[Epoch 220/350, Step 3300, ETA 12.32s] step time: 0.005476s (±0.001081s); valid time: 0.003529s; loss: 11.8024 (±3.2124); valid loss: 44.7103\n",
      "[Epoch 220/350, Step 3300, ETA 12.32s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 227/350, Step 3400, ETA 11.71s] step time: 0.006203s (±0.00792s); valid time: 0.0787s; loss: 11.7121 (±3.15356); valid loss: 44.5361 (*)\n",
      "[Epoch 230/350, Step 3450, ETA 11.38s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 234/350, Step 3500, ETA 11.05s] step time: 0.005263s (±0.001104s); valid time: 0.003587s; loss: 11.9124 (±3.04857); valid loss: 44.8364\n",
      "[Epoch 240/350, Step 3600, ETA 10.4s] step time: 0.005446s (±0.0014s); valid time: 0.003592s; loss: 11.761 (±2.67951); valid loss: 44.9081\n",
      "[Epoch 240/350, Step 3600, ETA 10.4s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 247/350, Step 3700, ETA 9.743s] step time: 0.005218s (±0.000862s); valid time: 0.003816s; loss: 11.6573 (±3.10623); valid loss: 44.8347\n",
      "[Epoch 250/350, Step 3750, ETA 9.425s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 254/350, Step 3800, ETA 9.106s] step time: 0.005544s (±0.001404s); valid time: 0.00404s; loss: 11.7109 (±3.17639); valid loss: 44.8716\n",
      "[Epoch 260/350, Step 3900, ETA 8.467s] step time: 0.005501s (±0.001132s); valid time: 0.003755s; loss: 11.6075 (±3.37941); valid loss: 45.1703\n",
      "[Epoch 260/350, Step 3900, ETA 8.467s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 267/350, Step 4000, ETA 7.826s] step time: 0.005338s (±0.0007032s); valid time: 0.00376s; loss: 11.8208 (±3.66806); valid loss: 45.0214\n",
      "[Epoch 270/350, Step 4050, ETA 7.509s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 274/350, Step 4100, ETA 7.194s] step time: 0.005509s (±0.001235s); valid time: 0.003569s; loss: 11.7179 (±3.36597); valid loss: 44.7843\n",
      "[Epoch 280/350, Step 4200, ETA 6.558s] step time: 0.005405s (±0.0008694s); valid time: 0.003469s; loss: 11.745 (±3.07071); valid loss: 44.8491\n",
      "[Epoch 280/350, Step 4200, ETA 6.558s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 287/350, Step 4300, ETA 5.945s] step time: 0.006223s (±0.007937s); valid time: 0.07951s; loss: 11.7478 (±3.10188); valid loss: 44.5319 (*)\n",
      "[Epoch 290/350, Step 4350, ETA 5.627s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 294/350, Step 4400, ETA 5.314s] step time: 0.005442s (±0.001142s); valid time: 0.003381s; loss: 11.7447 (±3.18109); valid loss: 44.6628\n",
      "[Epoch 300/350, Step 4500, ETA 4.681s] step time: 0.005299s (±0.000884s); valid time: 0.003602s; loss: 11.6866 (±3.00517); valid loss: 44.7923\n",
      "[Epoch 300/350, Step 4500, ETA 4.681s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 307/350, Step 4600, ETA 4.051s] step time: 0.005318s (±0.00103s); valid time: 0.004626s; loss: 11.5978 (±3.23504); valid loss: 44.8122\n",
      "[Epoch 310/350, Step 4650, ETA 3.736s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 314/350, Step 4700, ETA 3.423s] step time: 0.005331s (±0.001092s); valid time: 0.003557s; loss: 11.7526 (±3.31435); valid loss: 44.9995\n",
      "[Epoch 320/350, Step 4800, ETA 2.796s] step time: 0.005326s (±0.001134s); valid time: 0.003653s; loss: 11.7306 (±3.03251); valid loss: 45.0148\n",
      "[Epoch 320/350, Step 4800, ETA 2.797s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 327/350, Step 4900, ETA 2.173s] step time: 0.005402s (±0.0009173s); valid time: 0.003526s; loss: 11.7676 (±3.05284); valid loss: 44.6008\n",
      "[Epoch 330/350, Step 4950, ETA 1.862s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 334/350, Step 5000, ETA 1.551s] step time: 0.005497s (±0.001257s); valid time: 0.003712s; loss: 11.7145 (±2.98853); valid loss: 45.0163\n",
      "[Epoch 340/350, Step 5100, ETA 0.9299s] step time: 0.005503s (±0.001243s); valid time: 0.003643s; loss: 11.689 (±2.74176); valid loss: 44.6374\n",
      "[Epoch 340/350, Step 5100, ETA 0.9299s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 347/350, Step 5200, ETA 0.3097s] step time: 0.005428s (±0.001265s); valid time: 0.003686s; loss: 11.8335 (±3.02711); valid loss: 45.1255\n",
      "[Epoch 350/350, Step 5250, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpi174gt3p/variables.dat-4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpi174gt3p/variables.dat-4300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(190, 1691, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9994686503719448,\n",
      "\ty_label%=0.10095642933049948,\n",
      ")\n",
      "Testing on realKnownCause/rogue_agent_key_updown.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 7/350, Step 100, ETA 53.04s] step time: 0.009776s (±0.03198s); valid time: 0.1569s; loss: 140.391 (±39.1421); valid loss: 155.103 (*)\n",
      "[Epoch 10/350, Step 150, ETA 44.92s] Learning rate decreased to 0.00075\n",
      "[Epoch 14/350, Step 200, ETA 43.11s] step time: 0.006184s (±0.007752s); valid time: 0.07504s; loss: 104.767 (±21.0064); valid loss: 130.169 (*)\n",
      "[Epoch 20/350, Step 300, ETA 38.96s] step time: 0.006075s (±0.007943s); valid time: 0.07954s; loss: 69.65 (±15.7152); valid loss: 111.235 (*)\n",
      "[Epoch 20/350, Step 300, ETA 38.96s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 27/350, Step 400, ETA 36.83s] step time: 0.006218s (±0.008025s); valid time: 0.07879s; loss: 80258.8 (±345676); valid loss: 13.9402 (*)\n",
      "[Epoch 30/350, Step 450, ETA 35.51s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 34/350, Step 500, ETA 35.51s] step time: 0.006463s (±0.008751s); valid time: 0.08261s; loss: -110.808 (±42.4776); valid loss: -12.9522 (*)\n",
      "[Epoch 40/350, Step 600, ETA 34.33s] step time: 0.006436s (±0.00789s); valid time: 0.07906s; loss: -140.597 (±14.5835); valid loss: -23.8436 (*)\n",
      "[Epoch 40/350, Step 600, ETA 34.33s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 47/350, Step 700, ETA 33.15s] step time: 0.006211s (±0.007629s); valid time: 0.07569s; loss: -155.925 (±14.3225); valid loss: -31.153 (*)\n",
      "[Epoch 50/350, Step 750, ETA 32.39s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 54/350, Step 800, ETA 32.2s] step time: 0.00632s (±0.008028s); valid time: 0.07891s; loss: -165.418 (±16.3866); valid loss: -38.812 (*)\n",
      "[Epoch 60/350, Step 900, ETA 31.15s] step time: 0.006111s (±0.007694s); valid time: 0.07723s; loss: -173.63 (±15.7165); valid loss: -42.8993 (*)\n",
      "[Epoch 60/350, Step 900, ETA 31.16s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 67/350, Step 1000, ETA 30.25s] step time: 0.006229s (±0.008014s); valid time: 0.07949s; loss: -180.302 (±18.2422); valid loss: -45.8854 (*)\n",
      "[Epoch 70/350, Step 1050, ETA 29.6s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 74/350, Step 1100, ETA 29.36s] step time: 0.006139s (±0.007943s); valid time: 0.07939s; loss: -185.675 (±16.7183); valid loss: -49.0467 (*)\n",
      "[Epoch 80/350, Step 1200, ETA 28.47s] step time: 0.006064s (±0.007966s); valid time: 0.07975s; loss: -190.395 (±17.2674); valid loss: -51.9724 (*)\n",
      "[Epoch 80/350, Step 1200, ETA 28.47s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 87/350, Step 1300, ETA 27.7s] step time: 0.006309s (±0.008308s); valid time: 0.08029s; loss: -195.056 (±15.7091); valid loss: -55.5915 (*)\n",
      "[Epoch 90/350, Step 1350, ETA 27.17s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 94/350, Step 1400, ETA 26.94s] step time: 0.006275s (±0.008129s); valid time: 0.08085s; loss: -197.913 (±16.6024); valid loss: -56.7846 (*)\n",
      "[Epoch 100/350, Step 1500, ETA 26.13s] step time: 0.006078s (±0.007915s); valid time: 0.07925s; loss: -201.325 (±17.3088); valid loss: -57.392 (*)\n",
      "[Epoch 100/350, Step 1500, ETA 26.13s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 107/350, Step 1600, ETA 25.37s] step time: 0.006195s (±0.007742s); valid time: 0.07685s; loss: -204.754 (±17.7298); valid loss: -59.5315 (*)\n",
      "[Epoch 110/350, Step 1650, ETA 24.88s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 114/350, Step 1700, ETA 24.61s] step time: 0.006141s (±0.008075s); valid time: 0.08008s; loss: -205.669 (±17.8935); valid loss: -60.4169 (*)\n",
      "[Epoch 120/350, Step 1800, ETA 23.81s] step time: 0.005909s (±0.007805s); valid time: 0.07659s; loss: -207.503 (±14.4715); valid loss: -61.3257 (*)\n",
      "[Epoch 120/350, Step 1800, ETA 23.81s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 127/350, Step 1900, ETA 22.94s] step time: 0.005301s (±0.001014s); valid time: 0.003594s; loss: -209.552 (±16.9615); valid loss: -60.9025\n",
      "[Epoch 130/350, Step 1950, ETA 22.5s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 134/350, Step 2000, ETA 22.21s] step time: 0.006088s (±0.008574s); valid time: 0.08509s; loss: -210.663 (±18.0582); valid loss: -63.1424 (*)\n",
      "[Epoch 140/350, Step 2100, ETA 21.48s] step time: 0.006066s (±0.007856s); valid time: 0.07863s; loss: -212.406 (±16.4567); valid loss: -63.6732 (*)\n",
      "[Epoch 140/350, Step 2100, ETA 21.48s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 147/350, Step 2200, ETA 20.67s] step time: 0.005388s (±0.001208s); valid time: 0.003906s; loss: -213.578 (±18.2921); valid loss: -63.4488\n",
      "[Epoch 150/350, Step 2250, ETA 20.27s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 154/350, Step 2300, ETA 19.99s] step time: 0.006233s (±0.008117s); valid time: 0.08092s; loss: -214.036 (±20.1754); valid loss: -64.4422 (*)\n",
      "[Epoch 160/350, Step 2400, ETA 19.32s] step time: 0.006407s (±0.008259s); valid time: 0.0813s; loss: -215.057 (±20.2179); valid loss: -65.0421 (*)\n",
      "[Epoch 160/350, Step 2400, ETA 19.32s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 167/350, Step 2500, ETA 18.56s] step time: 0.005504s (±0.001321s); valid time: 0.003617s; loss: -215.461 (±17.0958); valid loss: -64.1313\n",
      "[Epoch 170/350, Step 2550, ETA 18.18s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 174/350, Step 2600, ETA 17.88s] step time: 0.006201s (±0.008023s); valid time: 0.08006s; loss: -215.809 (±16.8917); valid loss: -65.3103 (*)\n",
      "[Epoch 180/350, Step 2700, ETA 17.21s] step time: 0.006232s (±0.008057s); valid time: 0.08028s; loss: -217.525 (±18.8809); valid loss: -65.4481 (*)\n",
      "[Epoch 180/350, Step 2700, ETA 17.21s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 187/350, Step 2800, ETA 16.45s] step time: 0.00534s (±0.001143s); valid time: 0.003746s; loss: -217.167 (±18.5848); valid loss: -64.9381\n",
      "[Epoch 190/350, Step 2850, ETA 16.08s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 194/350, Step 2900, ETA 15.72s] step time: 0.00538s (±0.00081s); valid time: 0.003559s; loss: -217.176 (±18.8678); valid loss: -65.1049\n",
      "[Epoch 200/350, Step 3000, ETA 15.06s] step time: 0.006353s (±0.00811s); valid time: 0.08104s; loss: -217.261 (±17.9494); valid loss: -65.8642 (*)\n",
      "[Epoch 200/350, Step 3000, ETA 15.06s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 207/350, Step 3100, ETA 14.34s] step time: 0.005404s (±0.0009171s); valid time: 0.004041s; loss: -217.507 (±18.1884); valid loss: -65.2431\n",
      "[Epoch 210/350, Step 3150, ETA 13.98s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 214/350, Step 3200, ETA 13.63s] step time: 0.005608s (±0.00149s); valid time: 0.003768s; loss: -218.46 (±19.8194); valid loss: -64.7211\n",
      "[Epoch 220/350, Step 3300, ETA 12.98s] step time: 0.00635s (±0.008161s); valid time: 0.08099s; loss: -218.038 (±19.1656); valid loss: -66.4499 (*)\n",
      "[Epoch 220/350, Step 3300, ETA 12.98s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 227/350, Step 3400, ETA 12.28s] step time: 0.005451s (±0.0009873s); valid time: 0.003416s; loss: -217.751 (±20.7808); valid loss: -65.9801\n",
      "[Epoch 230/350, Step 3450, ETA 11.93s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 234/350, Step 3500, ETA 11.58s] step time: 0.005451s (±0.00115s); valid time: 0.003574s; loss: -219.073 (±19.1924); valid loss: -65.7577\n",
      "[Epoch 240/350, Step 3600, ETA 10.93s] step time: 0.006269s (±0.008998s); valid time: 0.08951s; loss: -218.88 (±19.8623); valid loss: -66.7395 (*)\n",
      "[Epoch 240/350, Step 3600, ETA 10.93s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 247/350, Step 3700, ETA 10.24s] step time: 0.005607s (±0.00109s); valid time: 0.003988s; loss: -219.44 (±19.666); valid loss: -66.6112\n",
      "[Epoch 250/350, Step 3750, ETA 9.897s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 254/350, Step 3800, ETA 9.564s] step time: 0.00559s (±0.001596s); valid time: 0.003667s; loss: -218.179 (±19.1501); valid loss: -66.2608\n",
      "[Epoch 260/350, Step 3900, ETA 8.873s] step time: 0.005246s (±0.0008596s); valid time: 0.004267s; loss: -218.303 (±19.3406); valid loss: -66.01\n",
      "[Epoch 260/350, Step 3900, ETA 8.874s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 267/350, Step 4000, ETA 8.198s] step time: 0.005497s (±0.001456s); valid time: 0.003632s; loss: -218.047 (±19.2609); valid loss: -66.5299\n",
      "[Epoch 270/350, Step 4050, ETA 7.858s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 274/350, Step 4100, ETA 7.523s] step time: 0.005361s (±0.001176s); valid time: 0.003949s; loss: -219.775 (±22.934); valid loss: -66.1505\n",
      "[Epoch 280/350, Step 4200, ETA 6.854s] step time: 0.005476s (±0.001145s); valid time: 0.003887s; loss: -218.622 (±20.5888); valid loss: -65.9069\n",
      "[Epoch 280/350, Step 4200, ETA 6.854s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 287/350, Step 4300, ETA 6.194s] step time: 0.005681s (±0.001347s); valid time: 0.003697s; loss: -219.117 (±18.7216); valid loss: -65.2237\n",
      "[Epoch 290/350, Step 4350, ETA 5.861s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 294/350, Step 4400, ETA 5.546s] step time: 0.006159s (±0.008373s); valid time: 0.08339s; loss: -219.411 (±20.2595); valid loss: -67.06 (*)\n",
      "[Epoch 300/350, Step 4500, ETA 4.883s] step time: 0.005446s (±0.001527s); valid time: 0.003618s; loss: -218.395 (±18.8477); valid loss: -66.8283\n",
      "[Epoch 300/350, Step 4500, ETA 4.883s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 307/350, Step 4600, ETA 4.222s] step time: 0.005323s (±0.000747s); valid time: 0.003556s; loss: -218.696 (±16.3245); valid loss: -66.5145\n",
      "[Epoch 310/350, Step 4650, ETA 3.894s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 314/350, Step 4700, ETA 3.567s] step time: 0.005516s (±0.001378s); valid time: 0.003758s; loss: -218.73 (±18.5064); valid loss: -66.192\n",
      "[Epoch 320/350, Step 4800, ETA 2.912s] step time: 0.00537s (±0.001005s); valid time: 0.003713s; loss: -219.697 (±17.6524); valid loss: -66.7385\n",
      "[Epoch 320/350, Step 4800, ETA 2.912s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 327/350, Step 4900, ETA 2.261s] step time: 0.005379s (±0.0007894s); valid time: 0.003985s; loss: -218.727 (±18.5317); valid loss: -66.8279\n",
      "[Epoch 330/350, Step 4950, ETA 1.936s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 334/350, Step 5000, ETA 1.613s] step time: 0.005467s (±0.001271s); valid time: 0.003515s; loss: -218.893 (±18.2517); valid loss: -64.6546\n",
      "[Epoch 340/350, Step 5100, ETA 0.9657s] step time: 0.005355s (±0.0008775s); valid time: 0.00383s; loss: -218.693 (±19.7682); valid loss: -65.84\n",
      "[Epoch 340/350, Step 5100, ETA 0.9657s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 347/350, Step 5200, ETA 0.3213s] step time: 0.005316s (±0.0008603s); valid time: 0.003591s; loss: -218.308 (±17.9088); valid loss: -66.6841\n",
      "[Epoch 350/350, Step 5250, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp6qtugtca/variables.dat-4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp6qtugtca/variables.dat-4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.51,\n",
      "\t(tp, fp, tn, fn)=(358, 2442, 2343, 172),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.68,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.5268109125117592,\n",
      "\ty_label%=0.09971777986829727,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_387.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 1/350, Step 100] step time: 0.009991s (±0.0346s); valid time: 0.2087s; loss: -18.3128 (±5.19178); valid loss: -10.0965 (*)\n",
      "[Epoch 1/350, Step 200] step time: 0.006858s (±0.01334s); valid time: 0.1338s; loss: -22.0636 (±0.675047); valid loss: -10.1538 (*)\n",
      "[Epoch 2/350, Step 300, ETA 14m 1.789s] step time: 0.005848s (±0.005218s); valid time: 0.0516s; loss: -22.2056 (±0.849974); valid loss: -9.92939\n",
      "[Epoch 2/350, Step 400, ETA 13m 22.16s] step time: 0.006518s (±0.01304s); valid time: 0.1308s; loss: -22.6349 (±0.766822); valid loss: -10.2731 (*)\n",
      "[Epoch 2/350, Step 500, ETA 12m 47.72s] step time: 0.005996s (±0.005177s); valid time: 0.05101s; loss: -23.3456 (±0.754923); valid loss: -10.1301\n",
      "[Epoch 3/350, Step 600, ETA 12m 46.26s] step time: 0.006531s (±0.0127s); valid time: 0.1273s; loss: -23.8319 (±0.819708); valid loss: -10.8064 (*)\n",
      "[Epoch 3/350, Step 700, ETA 12m 36.13s] step time: 0.006674s (±0.01304s); valid time: 0.1304s; loss: -24.5563 (±0.814532); valid loss: -10.992 (*)\n",
      "[Epoch 3/350, Step 800, ETA 12m 18.13s] step time: 0.005852s (±0.004957s); valid time: 0.04978s; loss: -24.9464 (±0.816875); valid loss: -9.76021\n",
      "[Epoch 4/350, Step 900, ETA 12m 12.12s] step time: 0.005823s (±0.005405s); valid time: 0.05339s; loss: -25.4273 (±0.830577); valid loss: -10.8154\n",
      "[Epoch 4/350, Step 1000, ETA 12m 7.012s] step time: 0.006566s (±0.0128s); valid time: 0.128s; loss: -25.7841 (±0.910715); valid loss: -11.9199 (*)\n",
      "[Epoch 4/350, Step 1100, ETA 11m 55.7s] step time: 0.00581s (±0.005107s); valid time: 0.05067s; loss: -25.9926 (±1.12941); valid loss: -11.4721\n",
      "[Epoch 5/350, Step 1200, ETA 11m 58.98s] step time: 0.006637s (±0.01261s); valid time: 0.1258s; loss: -26.0205 (±1.1419); valid loss: -12.8255 (*)\n",
      "[Epoch 5/350, Step 1300, ETA 11m 49.48s] step time: 0.005765s (±0.005318s); valid time: 0.05275s; loss: -26.368 (±0.843831); valid loss: -11.3201\n",
      "[Epoch 5/350, Step 1400, ETA 11m 47.6s] step time: 0.006646s (±0.01271s); valid time: 0.1273s; loss: -26.712 (±0.965162); valid loss: -13.5076 (*)\n",
      "[Epoch 6/350, Step 1500, ETA 11m 45.7s] step time: 0.005923s (±0.00533s); valid time: 0.05271s; loss: -26.6857 (±1.15127); valid loss: -12.3021\n",
      "[Epoch 6/350, Step 1600, ETA 11m 39.31s] step time: 0.005884s (±0.005518s); valid time: 0.05429s; loss: -26.8633 (±0.999986); valid loss: -12.4107\n",
      "[Epoch 6/350, Step 1700, ETA 11m 34.1s] step time: 0.005972s (±0.005324s); valid time: 0.05096s; loss: -26.8989 (±2.10719); valid loss: -12.2827\n",
      "[Epoch 7/350, Step 1800, ETA 11m 36.39s] step time: 0.006504s (±0.01234s); valid time: 0.1235s; loss: -27.0666 (±0.927606); valid loss: -14.1217 (*)\n",
      "[Epoch 7/350, Step 1900, ETA 11m 32.11s] step time: 0.006024s (±0.005271s); valid time: 0.05228s; loss: -26.8065 (±2.89202); valid loss: -12.3326\n",
      "[Epoch 7/350, Step 2000, ETA 11m 27.48s] step time: 0.00589s (±0.005325s); valid time: 0.0515s; loss: -27.1608 (±1.06255); valid loss: -10.471\n",
      "[Epoch 8/350, Step 2100, ETA 11m 25.77s] step time: 0.005685s (±0.005914s); valid time: 0.05915s; loss: -27.0163 (±1.11979); valid loss: -13.331\n",
      "[Epoch 8/350, Step 2200, ETA 11m 21.26s] step time: 0.005794s (±0.005243s); valid time: 0.0523s; loss: -27.3722 (±1.18335); valid loss: -11.7905\n",
      "[Epoch 8/350, Step 2300, ETA 11m 19.15s] step time: 0.006257s (±0.005899s); valid time: 0.05823s; loss: -27.8483 (±0.886569); valid loss: -10.3754\n",
      "[Epoch 9/350, Step 2400, ETA 11m 18.47s] step time: 0.005821s (±0.005063s); valid time: 0.05096s; loss: -27.241 (±1.27526); valid loss: -11.8761\n",
      "[Epoch 9/350, Step 2500, ETA 11m 15.33s] step time: 0.005959s (±0.005433s); valid time: 0.0535s; loss: -27.7284 (±1.06084); valid loss: 38.539\n",
      "[Epoch 9/350, Step 2600, ETA 11m 12.51s] step time: 0.005999s (±0.005321s); valid time: 0.0525s; loss: -28.0998 (±1.24098); valid loss: -11.4787\n",
      "[Epoch 10/350, Step 2700, ETA 11m 11.23s] step time: 0.005682s (±0.005088s); valid time: 0.05015s; loss: -28.032 (±1.2348); valid loss: 34.6916\n",
      "[Epoch 10/350, Step 2800, ETA 11m 7.935s] step time: 0.005786s (±0.005356s); valid time: 0.05323s; loss: -27.9526 (±1.21771); valid loss: 5.95433\n",
      "[Epoch 10/350, Step 2900, ETA 11m 4.983s] step time: 0.005837s (±0.004995s); valid time: 0.04967s; loss: -28.374 (±1.15352); valid loss: 91.7953\n",
      "[Epoch 10/350, Step 2910, ETA 11m 4.821s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 3000, ETA 11m 4.917s] step time: 0.005945s (±0.005244s); valid time: 0.05088s; loss: -28.229 (±1.30184); valid loss: 9.14118\n",
      "[Epoch 11/350, Step 3100, ETA 11m 2.63s] step time: 0.005974s (±0.005509s); valid time: 0.05393s; loss: -29.0548 (±1.12995); valid loss: 40323.2\n",
      "[Epoch 11/350, Step 3200, ETA 11m 0.4087s] step time: 0.005957s (±0.00526s); valid time: 0.05142s; loss: -28.181 (±1.37878); valid loss: 64.2546\n",
      "[Epoch 12/350, Step 3300, ETA 11m 0.1483s] step time: 0.005906s (±0.00537s); valid time: 0.05284s; loss: -29.4469 (±1.14866); valid loss: 549.297\n",
      "[Epoch 12/350, Step 3400, ETA 11m 0.2073s] step time: 0.006699s (±0.01289s); valid time: 0.1288s; loss: -28.392 (±3.34893); valid loss: -14.8104 (*)\n",
      "[Epoch 13/350, Step 3500, ETA 11m 0.2285s] step time: 0.005995s (±0.005261s); valid time: 0.05198s; loss: -29.4587 (±1.2965); valid loss: -11.0305\n",
      "[Epoch 13/350, Step 3600, ETA 10m 57.92s] step time: 0.005867s (±0.005582s); valid time: 0.0556s; loss: -29.6583 (±1.13145); valid loss: 208.325\n",
      "[Epoch 13/350, Step 3700, ETA 10m 55.83s] step time: 0.005907s (±0.00569s); valid time: 0.05635s; loss: -29.8075 (±1.20255); valid loss: 415.824\n",
      "[Epoch 14/350, Step 3800, ETA 10m 55.38s] step time: 0.005839s (±0.005302s); valid time: 0.05285s; loss: -29.5922 (±1.44137); valid loss: -11.8637\n",
      "[Epoch 14/350, Step 3900, ETA 10m 53.53s] step time: 0.005968s (±0.005255s); valid time: 0.05134s; loss: -29.7313 (±1.32026); valid loss: -8.3484\n",
      "[Epoch 14/350, Step 4000, ETA 10m 53.48s] step time: 0.006683s (±0.01328s); valid time: 0.1336s; loss: -29.2404 (±4.20543); valid loss: -14.9272 (*)\n",
      "[Epoch 15/350, Step 4100, ETA 10m 55.05s] step time: 0.00661s (±0.0132s); valid time: 0.1321s; loss: -29.7127 (±1.33433); valid loss: -15.1755 (*)\n",
      "[Epoch 15/350, Step 4200, ETA 10m 54.8s] step time: 0.006638s (±0.01259s); valid time: 0.126s; loss: -29.3406 (±1.7631); valid loss: -15.3394 (*)\n",
      "[Epoch 15/350, Step 4300, ETA 10m 52.86s] step time: 0.005904s (±0.005733s); valid time: 0.05678s; loss: -30.2753 (±1.10993); valid loss: -0.797714\n",
      "[Epoch 16/350, Step 4400, ETA 10m 52.59s] step time: 0.005901s (±0.005329s); valid time: 0.05304s; loss: -29.6804 (±1.75746); valid loss: -10.9365\n",
      "[Epoch 16/350, Step 4500, ETA 10m 52.28s] step time: 0.006618s (±0.01254s); valid time: 0.1255s; loss: -30.6327 (±1.3395); valid loss: -16.4766 (*)\n",
      "[Epoch 16/350, Step 4600, ETA 10m 50.53s] step time: 0.0059s (±0.006074s); valid time: 0.06056s; loss: -30.5283 (±1.18616); valid loss: -15.2828\n",
      "[Epoch 17/350, Step 4700, ETA 10m 51.86s] step time: 0.006706s (±0.01363s); valid time: 0.1365s; loss: -30.2946 (±2.5813); valid loss: -17.2498 (*)\n",
      "[Epoch 17/350, Step 4800, ETA 10m 50.05s] step time: 0.005889s (±0.00521s); valid time: 0.05129s; loss: -29.9182 (±1.83572); valid loss: -16.6681\n",
      "[Epoch 17/350, Step 4900, ETA 10m 48.11s] step time: 0.005818s (±0.005205s); valid time: 0.05166s; loss: -30.503 (±1.26022); valid loss: 0.104849\n",
      "[Epoch 18/350, Step 5000, ETA 10m 48.15s] step time: 0.006118s (±0.00564s); valid time: 0.0558s; loss: -30.2595 (±2.1406); valid loss: -9.17881\n",
      "[Epoch 18/350, Step 5100, ETA 10m 46.31s] step time: 0.005839s (±0.005423s); valid time: 0.05347s; loss: -30.0017 (±1.7043); valid loss: -11.9157\n",
      "[Epoch 18/350, Step 5200, ETA 10m 44.4s] step time: 0.005777s (±0.005056s); valid time: 0.05s; loss: -30.3624 (±1.89407); valid loss: -4.29844\n",
      "[Epoch 19/350, Step 5300, ETA 10m 44.18s] step time: 0.005971s (±0.005165s); valid time: 0.05125s; loss: -30.2916 (±1.6287); valid loss: 24.0568\n",
      "[Epoch 19/350, Step 5400, ETA 10m 42.32s] step time: 0.005764s (±0.004968s); valid time: 0.04924s; loss: -30.7586 (±1.53094); valid loss: 15.1627\n",
      "[Epoch 19/350, Step 5500, ETA 10m 40.58s] step time: 0.005804s (±0.005466s); valid time: 0.05402s; loss: -30.687 (±1.80997); valid loss: -5.53109\n",
      "[Epoch 20/350, Step 5600, ETA 10m 40.16s] step time: 0.00578s (±0.0052s); valid time: 0.05146s; loss: -31.6502 (±1.27975); valid loss: -10.3899\n",
      "[Epoch 20/350, Step 5700, ETA 10m 38.4s] step time: 0.005751s (±0.005124s); valid time: 0.0506s; loss: -31.0365 (±1.39584); valid loss: -17.0424\n",
      "[Epoch 20/350, Step 5800, ETA 10m 37.02s] step time: 0.005961s (±0.005448s); valid time: 0.05357s; loss: -31.4616 (±1.41746); valid loss: -17.0086\n",
      "[Epoch 20/350, Step 5820, ETA 10m 36.54s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 5900, ETA 10m 36.68s] step time: 0.005891s (±0.00512s); valid time: 0.05052s; loss: -31.7049 (±1.39505); valid loss: 1.87057\n",
      "[Epoch 21/350, Step 6000, ETA 10m 34.93s] step time: 0.005727s (±0.00512s); valid time: 0.05064s; loss: -32.2445 (±1.30416); valid loss: -17.1758\n",
      "[Epoch 21/350, Step 6100, ETA 10m 34.87s] step time: 0.006767s (±0.01244s); valid time: 0.1247s; loss: -31.6203 (±2.48285); valid loss: -19.1202 (*)\n",
      "[Epoch 22/350, Step 6200, ETA 10m 34.7s] step time: 0.005988s (±0.005711s); valid time: 0.05624s; loss: -32.4376 (±1.43721); valid loss: -15.6308\n",
      "[Epoch 22/350, Step 6300, ETA 10m 33.01s] step time: 0.005709s (±0.005146s); valid time: 0.0512s; loss: -31.9121 (±2.32389); valid loss: -17.4685\n",
      "[Epoch 22/350, Step 6400, ETA 10m 31.85s] step time: 0.006046s (±0.005151s); valid time: 0.05106s; loss: -30.714 (±7.37504); valid loss: -17.7252\n",
      "[Epoch 23/350, Step 6500, ETA 10m 31.68s] step time: 0.005993s (±0.005297s); valid time: 0.05144s; loss: -31.5992 (±1.97158); valid loss: -19.0253\n",
      "[Epoch 23/350, Step 6600, ETA 10m 30.5s] step time: 0.006026s (±0.005173s); valid time: 0.04981s; loss: -32.3961 (±1.60297); valid loss: -12.9388\n",
      "[Epoch 24/350, Step 6700, ETA 10m 30.11s] step time: 0.00588s (±0.005275s); valid time: 0.051s; loss: -32.1421 (±1.50889); valid loss: -18.5114\n",
      "[Epoch 24/350, Step 6800, ETA 10m 28.44s] step time: 0.005666s (±0.005152s); valid time: 0.05157s; loss: -31.8526 (±1.84625); valid loss: -18.2028\n",
      "[Epoch 24/350, Step 6900, ETA 10m 28.05s] step time: 0.006566s (±0.01229s); valid time: 0.1231s; loss: -32.644 (±1.3512); valid loss: -19.176 (*)\n",
      "[Epoch 25/350, Step 7000, ETA 10m 27.62s] step time: 0.005835s (±0.005375s); valid time: 0.05316s; loss: -32.0628 (±2.19524); valid loss: -17.7771\n",
      "[Epoch 25/350, Step 7100, ETA 10m 27.43s] step time: 0.006725s (±0.01374s); valid time: 0.1378s; loss: -30.6803 (±16.7413); valid loss: -19.8476 (*)\n",
      "[Epoch 25/350, Step 7200, ETA 10m 26.15s] step time: 0.005903s (±0.005223s); valid time: 0.05141s; loss: -32.0496 (±1.77086); valid loss: -18.1363\n",
      "[Epoch 26/350, Step 7300, ETA 10m 26.8s] step time: 0.006645s (±0.01271s); valid time: 0.1277s; loss: -32.3804 (±1.70988); valid loss: -20.2115 (*)\n",
      "[Epoch 26/350, Step 7400, ETA 10m 25.36s] step time: 0.00577s (±0.004906s); valid time: 0.04924s; loss: -32.2242 (±1.89509); valid loss: -5.99863\n",
      "[Epoch 26/350, Step 7500, ETA 10m 24.06s] step time: 0.005864s (±0.005034s); valid time: 0.05019s; loss: -32.6592 (±1.36793); valid loss: -12.3131\n",
      "[Epoch 27/350, Step 7600, ETA 10m 23.87s] step time: 0.006013s (±0.005402s); valid time: 0.05109s; loss: -30.8584 (±20.2976); valid loss: -19.4063\n",
      "[Epoch 27/350, Step 7700, ETA 10m 22.83s] step time: 0.006059s (±0.005213s); valid time: 0.0522s; loss: -32.6069 (±1.66214); valid loss: -18.6536\n",
      "[Epoch 27/350, Step 7800, ETA 10m 21.59s] step time: 0.005888s (±0.005188s); valid time: 0.05118s; loss: -32.9386 (±1.78765); valid loss: -20.0207\n",
      "[Epoch 28/350, Step 7900, ETA 10m 21.25s] step time: 0.005935s (±0.005543s); valid time: 0.05416s; loss: -32.605 (±3.56926); valid loss: 2.88549\n",
      "[Epoch 28/350, Step 8000, ETA 10m 20s] step time: 0.005863s (±0.005156s); valid time: 0.05143s; loss: -24.6131 (±83.4496); valid loss: -19.5947\n",
      "[Epoch 28/350, Step 8100, ETA 10m 18.74s] step time: 0.005853s (±0.005453s); valid time: 0.05294s; loss: -31.5013 (±6.89115); valid loss: -19.7646\n",
      "[Epoch 29/350, Step 8200, ETA 10m 18.5s] step time: 0.005925s (±0.006267s); valid time: 0.062s; loss: -32.8672 (±1.86769); valid loss: 24.8632\n",
      "[Epoch 29/350, Step 8300, ETA 10m 17.22s] step time: 0.005802s (±0.005559s); valid time: 0.05588s; loss: -32.7053 (±1.74057); valid loss: -19.7736\n",
      "[Epoch 29/350, Step 8400, ETA 10m 15.97s] step time: 0.005827s (±0.004981s); valid time: 0.04898s; loss: -32.1344 (±2.57907); valid loss: -18.1412\n",
      "[Epoch 30/350, Step 8500, ETA 10m 15.34s] step time: 0.005699s (±0.005254s); valid time: 0.05243s; loss: -32.3088 (±2.31212); valid loss: -18.2762\n",
      "[Epoch 30/350, Step 8600, ETA 10m 14.11s] step time: 0.005827s (±0.005145s); valid time: 0.05054s; loss: -32.8591 (±1.9503); valid loss: -17.6999\n",
      "[Epoch 30/350, Step 8700, ETA 10m 13.1s] step time: 0.006005s (±0.005494s); valid time: 0.05422s; loss: -33.0672 (±3.18495); valid loss: 36.8118\n",
      "[Epoch 30/350, Step 8730, ETA 10m 12.57s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 8800, ETA 10m 12.68s] step time: 0.005799s (±0.005237s); valid time: 0.05185s; loss: -34.1019 (±1.56137); valid loss: -19.6506\n",
      "[Epoch 31/350, Step 8900, ETA 10m 11.59s] step time: 0.005922s (±0.005145s); valid time: 0.05091s; loss: -32.5837 (±7.08436); valid loss: -16.1272\n",
      "[Epoch 31/350, Step 9000, ETA 10m 10.44s] step time: 0.005853s (±0.005076s); valid time: 0.05027s; loss: -33.03 (±2.51417); valid loss: -19.057\n",
      "[Epoch 32/350, Step 9100, ETA 10m 10.22s] step time: 0.005984s (±0.005198s); valid time: 0.05121s; loss: -33.728 (±1.88406); valid loss: 1668.92\n",
      "[Epoch 32/350, Step 9200, ETA 10m 9.186s] step time: 0.005954s (±0.00548s); valid time: 0.05404s; loss: -33.1152 (±2.19137); valid loss: -13.5066\n",
      "[Epoch 32/350, Step 9300, ETA 10m 8.013s] step time: 0.005808s (±0.005202s); valid time: 0.05198s; loss: -33.7294 (±2.13789); valid loss: -16.0323\n",
      "[Epoch 33/350, Step 9400, ETA 10m 7.771s] step time: 0.006023s (±0.005275s); valid time: 0.05228s; loss: -33.6472 (±1.7588); valid loss: -18.304\n",
      "[Epoch 33/350, Step 9500, ETA 10m 7.645s] step time: 0.006876s (±0.01299s); valid time: 0.1295s; loss: -32.8062 (±2.09091); valid loss: -20.9842 (*)\n",
      "[Epoch 33/350, Step 9600, ETA 10m 6.507s] step time: 0.005834s (±0.0052s); valid time: 0.05221s; loss: -33.4954 (±3.08992); valid loss: -13.7425\n",
      "[Epoch 34/350, Step 9700, ETA 10m 6.146s] step time: 0.00594s (±0.005078s); valid time: 0.05007s; loss: -33.491 (±1.64082); valid loss: 257.022\n",
      "[Epoch 34/350, Step 9800, ETA 10m 5.199s] step time: 0.006015s (±0.005268s); valid time: 0.05165s; loss: -34.1286 (±1.68388); valid loss: 104.818\n",
      "[Epoch 35/350, Step 9900, ETA 10m 4.716s] step time: 0.005816s (±0.005467s); valid time: 0.05467s; loss: -33.5695 (±2.27912); valid loss: -17.3701\n",
      "[Epoch 35/350, Step 10000, ETA 10m 3.68s] step time: 0.005905s (±0.005228s); valid time: 0.05166s; loss: -32.7815 (±2.81347); valid loss: -19.4991\n",
      "[Epoch 35/350, Step 10100, ETA 10m 2.669s] step time: 0.005932s (±0.005203s); valid time: 0.0506s; loss: -33.9382 (±1.6349); valid loss: -19.0023\n",
      "[Epoch 36/350, Step 10200, ETA 10m 2.253s] step time: 0.005849s (±0.005175s); valid time: 0.05127s; loss: -34.0342 (±2.09627); valid loss: -16.2062\n",
      "[Epoch 36/350, Step 10300, ETA 10m 1.475s] step time: 0.006181s (±0.005191s); valid time: 0.05089s; loss: -33.2205 (±3.31832); valid loss: 134.315\n",
      "[Epoch 36/350, Step 10400, ETA 10m 0.3555s] step time: 0.005802s (±0.005393s); valid time: 0.05368s; loss: -34.1797 (±1.6612); valid loss: -19.6541\n",
      "[Epoch 37/350, Step 10500, ETA 9m 59.92s] step time: 0.005862s (±0.005474s); valid time: 0.05412s; loss: -33.9507 (±1.92412); valid loss: -20.1531\n",
      "[Epoch 37/350, Step 10600, ETA 9m 59.06s] step time: 0.006094s (±0.00518s); valid time: 0.04995s; loss: -34.0785 (±1.90385); valid loss: -10.8912\n",
      "[Epoch 37/350, Step 10700, ETA 9m 57.97s] step time: 0.005802s (±0.005338s); valid time: 0.05333s; loss: -33.3455 (±2.65886); valid loss: -10.9473\n",
      "[Epoch 38/350, Step 10800, ETA 9m 57.51s] step time: 0.005867s (±0.005064s); valid time: 0.05002s; loss: -32.7911 (±12.8057); valid loss: -9.89405\n",
      "[Epoch 38/350, Step 10900, ETA 9m 56.5s] step time: 0.005893s (±0.006008s); valid time: 0.05843s; loss: -33.8153 (±2.05062); valid loss: 8.66944\n",
      "[Epoch 38/350, Step 11000, ETA 9m 55.42s] step time: 0.005803s (±0.005606s); valid time: 0.05572s; loss: -33.7555 (±2.32409); valid loss: -4.39633\n",
      "[Epoch 39/350, Step 11100, ETA 9m 55.51s] step time: 0.006429s (±0.01273s); valid time: 0.1275s; loss: -31.5697 (±24.373); valid loss: -21.3154 (*)\n",
      "[Epoch 39/350, Step 11200, ETA 9m 54.49s] step time: 0.005872s (±0.005173s); valid time: 0.05083s; loss: -34.2989 (±3.58598); valid loss: 1544.54\n",
      "[Epoch 39/350, Step 11300, ETA 9m 53.44s] step time: 0.005826s (±0.005341s); valid time: 0.05257s; loss: -31.9137 (±15.0668); valid loss: -19.162\n",
      "[Epoch 40/350, Step 11400, ETA 9m 53.12s] step time: 0.005972s (±0.005101s); valid time: 0.04985s; loss: -33.8795 (±2.19984); valid loss: -15.6171\n",
      "[Epoch 40/350, Step 11500, ETA 9m 51.97s] step time: 0.00569s (±0.005176s); valid time: 0.05147s; loss: -34.3825 (±1.9213); valid loss: 594.449\n",
      "[Epoch 40/350, Step 11600, ETA 9m 51.05s] step time: 0.005966s (±0.005487s); valid time: 0.05329s; loss: -34.1134 (±3.15145); valid loss: -20.9796\n",
      "[Epoch 40/350, Step 11640, ETA 9m 50.47s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 11700, ETA 9m 50.71s] step time: 0.006002s (±0.005413s); valid time: 0.05285s; loss: -34.1151 (±5.69958); valid loss: -16.5566\n",
      "[Epoch 41/350, Step 11800, ETA 9m 49.68s] step time: 0.005815s (±0.005205s); valid time: 0.05203s; loss: -34.7443 (±1.946); valid loss: -7.87169\n",
      "[Epoch 41/350, Step 11900, ETA 9m 48.68s] step time: 0.005838s (±0.005123s); valid time: 0.05018s; loss: -34.4083 (±2.02221); valid loss: -20.032\n",
      "[Epoch 42/350, Step 12000, ETA 9m 48.17s] step time: 0.005704s (±0.004961s); valid time: 0.04968s; loss: -34.487 (±1.97481); valid loss: -12.402\n",
      "[Epoch 42/350, Step 12100, ETA 9m 47.14s] step time: 0.005802s (±0.005111s); valid time: 0.04999s; loss: -35.1781 (±2.15929); valid loss: -16.9264\n",
      "[Epoch 42/350, Step 12200, ETA 9m 46.19s] step time: 0.0059s (±0.005311s); valid time: 0.05224s; loss: -34.7406 (±2.60099); valid loss: -16.7654\n",
      "[Epoch 43/350, Step 12300, ETA 9m 45.8s] step time: 0.005944s (±0.005322s); valid time: 0.05286s; loss: -32.4331 (±20.9255); valid loss: -19.4102\n",
      "[Epoch 43/350, Step 12400, ETA 9m 44.98s] step time: 0.006053s (±0.005229s); valid time: 0.05208s; loss: -34.8256 (±1.91266); valid loss: -17.9816\n",
      "[Epoch 43/350, Step 12500, ETA 9m 44.11s] step time: 0.005985s (±0.005189s); valid time: 0.05138s; loss: -34.4027 (±1.81442); valid loss: 86.5637\n",
      "[Epoch 44/350, Step 12600, ETA 9m 43.6s] step time: 0.005807s (±0.00555s); valid time: 0.0556s; loss: -35.4127 (±1.98991); valid loss: 7.79188\n",
      "[Epoch 44/350, Step 12700, ETA 9m 42.64s] step time: 0.005858s (±0.005117s); valid time: 0.05081s; loss: -34.6339 (±2.4122); valid loss: -12.4205\n",
      "[Epoch 44/350, Step 12800, ETA 9m 41.67s] step time: 0.005822s (±0.005189s); valid time: 0.05199s; loss: -33.9825 (±9.63039); valid loss: -16.4973\n",
      "[Epoch 45/350, Step 12900, ETA 9m 41.14s] step time: 0.005701s (±0.005096s); valid time: 0.05061s; loss: -32.9039 (±3.8021); valid loss: -18.1955\n",
      "[Epoch 45/350, Step 13000, ETA 9m 40.38s] step time: 0.006126s (±0.005823s); valid time: 0.05578s; loss: -34.1325 (±6.45814); valid loss: -17.1514\n",
      "[Epoch 46/350, Step 13100, ETA 9m 39.87s] step time: 0.005832s (±0.005022s); valid time: 0.04961s; loss: -34.349 (±2.64586); valid loss: -17.9885\n",
      "[Epoch 46/350, Step 13200, ETA 9m 39.06s] step time: 0.006049s (±0.005319s); valid time: 0.05156s; loss: -34.6024 (±2.46001); valid loss: -19.1554\n",
      "[Epoch 46/350, Step 13300, ETA 9m 38.2s] step time: 0.005964s (±0.005258s); valid time: 0.05209s; loss: -35.2912 (±2.36211); valid loss: -10.419\n",
      "[Epoch 47/350, Step 13400, ETA 9m 37.72s] step time: 0.005869s (±0.005211s); valid time: 0.05155s; loss: -32.9765 (±13.2619); valid loss: -14.4629\n",
      "[Epoch 47/350, Step 13500, ETA 9m 36.83s] step time: 0.005929s (±0.005446s); valid time: 0.05297s; loss: -34.307 (±6.64678); valid loss: -10.8289\n",
      "[Epoch 47/350, Step 13600, ETA 9m 35.92s] step time: 0.005878s (±0.005798s); valid time: 0.05783s; loss: -34.7661 (±2.22182); valid loss: 791.264\n",
      "[Epoch 48/350, Step 13700, ETA 9m 35.37s] step time: 0.005764s (±0.005232s); valid time: 0.05209s; loss: -33.4173 (±6.10115); valid loss: -5.51199\n",
      "[Epoch 48/350, Step 13800, ETA 9m 34.45s] step time: 0.005866s (±0.005399s); valid time: 0.0537s; loss: -34.444 (±7.24524); valid loss: 27.466\n",
      "[Epoch 48/350, Step 13900, ETA 9m 33.42s] step time: 0.005695s (±0.004958s); valid time: 0.04993s; loss: -34.299 (±2.60609); valid loss: -2.63259\n",
      "[Epoch 49/350, Step 14000, ETA 9m 33.05s] step time: 0.006017s (±0.005235s); valid time: 0.05105s; loss: -32.6961 (±16.8081); valid loss: -19.7191\n",
      "[Epoch 49/350, Step 14100, ETA 9m 32.2s] step time: 0.005962s (±0.005172s); valid time: 0.04995s; loss: -33.2298 (±5.39049); valid loss: -2.56787\n",
      "[Epoch 49/350, Step 14200, ETA 9m 31.31s] step time: 0.005879s (±0.005215s); valid time: 0.05173s; loss: -34.719 (±2.70602); valid loss: -17.2723\n",
      "[Epoch 50/350, Step 14300, ETA 9m 30.97s] step time: 0.006061s (±0.005343s); valid time: 0.05261s; loss: -34.9338 (±2.22711); valid loss: -18.4103\n",
      "[Epoch 50/350, Step 14400, ETA 9m 30.09s] step time: 0.0059s (±0.005991s); valid time: 0.05959s; loss: -34.9209 (±2.35825); valid loss: -18.6172\n",
      "[Epoch 50/350, Step 14500, ETA 9m 29.26s] step time: 0.005986s (±0.005313s); valid time: 0.05289s; loss: -35.604 (±1.90718); valid loss: 144.049\n",
      "[Epoch 50/350, Step 14550, ETA 9m 28.67s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 51/350, Step 14600, ETA 9m 28.96s] step time: 0.005944s (±0.005201s); valid time: 0.05116s; loss: -35.0927 (±2.51888); valid loss: -17.3207\n",
      "[Epoch 51/350, Step 14700, ETA 9m 27.99s] step time: 0.00575s (±0.005137s); valid time: 0.05101s; loss: -35.4035 (±2.03495); valid loss: -0.96521\n",
      "[Epoch 51/350, Step 14800, ETA 9m 27.07s] step time: 0.005805s (±0.005321s); valid time: 0.05215s; loss: -35.593 (±1.96589); valid loss: -20.2949\n",
      "[Epoch 52/350, Step 14900, ETA 9m 26.57s] step time: 0.00585s (±0.00504s); valid time: 0.04943s; loss: -35.5281 (±1.85233); valid loss: -7.60151\n",
      "[Epoch 52/350, Step 15000, ETA 9m 25.56s] step time: 0.005655s (±0.005133s); valid time: 0.05057s; loss: -35.5782 (±4.17395); valid loss: -17.494\n",
      "[Epoch 52/350, Step 15100, ETA 9m 24.77s] step time: 0.006036s (±0.005258s); valid time: 0.05222s; loss: -35.6095 (±1.90568); valid loss: -19.6565\n",
      "[Epoch 53/350, Step 15200, ETA 9m 24.29s] step time: 0.005821s (±0.006287s); valid time: 0.05661s; loss: -34.7001 (±5.38489); valid loss: -15.0733\n",
      "[Epoch 53/350, Step 15300, ETA 9m 23.34s] step time: 0.005736s (±0.005116s); valid time: 0.04967s; loss: -35.1714 (±2.13726); valid loss: 9.2099\n",
      "[Epoch 53/350, Step 15400, ETA 9m 22.46s] step time: 0.005856s (±0.005177s); valid time: 0.05142s; loss: -35.2685 (±2.50121); valid loss: -17.9241\n",
      "[Epoch 54/350, Step 15500, ETA 9m 21.93s] step time: 0.005794s (±0.005211s); valid time: 0.05101s; loss: -35.271 (±3.02503); valid loss: -17.1424\n",
      "[Epoch 54/350, Step 15600, ETA 9m 21.11s] step time: 0.005948s (±0.00521s); valid time: 0.05116s; loss: -35.434 (±2.10487); valid loss: -6.06838\n",
      "[Epoch 54/350, Step 15700, ETA 9m 20.24s] step time: 0.005855s (±0.004962s); valid time: 0.04885s; loss: -35.5305 (±1.69646); valid loss: -15.4917\n",
      "[Epoch 55/350, Step 15800, ETA 9m 19.65s] step time: 0.005688s (±0.005038s); valid time: 0.05011s; loss: -35.6927 (±2.05425); valid loss: -19.1163\n",
      "[Epoch 55/350, Step 15900, ETA 9m 18.82s] step time: 0.005934s (±0.005374s); valid time: 0.05233s; loss: -35.2891 (±1.91237); valid loss: -20.008\n",
      "[Epoch 55/350, Step 16000, ETA 9m 17.93s] step time: 0.005805s (±0.005052s); valid time: 0.05004s; loss: -35.7484 (±2.27594); valid loss: -19.8352\n",
      "[Epoch 56/350, Step 16100, ETA 9m 17.46s] step time: 0.00588s (±0.005167s); valid time: 0.05111s; loss: -33.6039 (±17.0355); valid loss: -14.6796\n",
      "[Epoch 56/350, Step 16200, ETA 9m 16.6s] step time: 0.005856s (±0.005283s); valid time: 0.05133s; loss: -35.4116 (±2.74533); valid loss: -20.4322\n",
      "[Epoch 57/350, Step 16300, ETA 9m 16.06s] step time: 0.005785s (±0.005023s); valid time: 0.04977s; loss: -35.5378 (±1.93188); valid loss: -17.5181\n",
      "[Epoch 57/350, Step 16400, ETA 9m 15.25s] step time: 0.00596s (±0.005363s); valid time: 0.05128s; loss: -35.3517 (±3.0203); valid loss: -9.14053\n",
      "[Epoch 57/350, Step 16500, ETA 9m 14.42s] step time: 0.005906s (±0.005612s); valid time: 0.05497s; loss: -35.0659 (±6.67226); valid loss: -17.5663\n",
      "[Epoch 58/350, Step 16600, ETA 9m 13.97s] step time: 0.005933s (±0.00557s); valid time: 0.05264s; loss: -35.8876 (±3.40648); valid loss: -18.1136\n",
      "[Epoch 58/350, Step 16700, ETA 9m 13.15s] step time: 0.005917s (±0.005311s); valid time: 0.05231s; loss: -35.3207 (±3.28615); valid loss: -12.6319\n",
      "[Epoch 58/350, Step 16800, ETA 9m 12.27s] step time: 0.005789s (±0.004922s); valid time: 0.04935s; loss: -34.9001 (±3.70972); valid loss: -16.1667\n",
      "[Epoch 59/350, Step 16900, ETA 9m 11.75s] step time: 0.00578s (±0.005337s); valid time: 0.05092s; loss: -32.1089 (±26.0546); valid loss: -2.96959\n",
      "[Epoch 59/350, Step 17000, ETA 9m 10.98s] step time: 0.005975s (±0.005159s); valid time: 0.05063s; loss: -35.69 (±2.30357); valid loss: 21.7972\n",
      "[Epoch 59/350, Step 17100, ETA 9m 10.11s] step time: 0.005807s (±0.005138s); valid time: 0.05011s; loss: -9.83159 (±260.722); valid loss: -11.8649\n",
      "[Epoch 60/350, Step 17200, ETA 9m 9.664s] step time: 0.005917s (±0.005291s); valid time: 0.05173s; loss: -35.2103 (±2.70108); valid loss: -9.04443\n",
      "[Epoch 60/350, Step 17300, ETA 9m 8.847s] step time: 0.005906s (±0.005381s); valid time: 0.05236s; loss: -35.8974 (±2.11393); valid loss: 559.479\n",
      "[Epoch 60/350, Step 17400, ETA 9m 8.001s] step time: 0.005841s (±0.005341s); valid time: 0.05326s; loss: -35.5885 (±3.10995); valid loss: -15.7388\n",
      "[Epoch 60/350, Step 17460, ETA 9m 7.398s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 17500, ETA 9m 7.565s] step time: 0.005957s (±0.005234s); valid time: 0.0514s; loss: -35.5923 (±2.0752); valid loss: -14.2186\n",
      "[Epoch 61/350, Step 17600, ETA 9m 6.777s] step time: 0.00596s (±0.006248s); valid time: 0.06242s; loss: -35.9294 (±2.59132); valid loss: -14.1\n",
      "[Epoch 61/350, Step 17700, ETA 9m 5.928s] step time: 0.005824s (±0.005191s); valid time: 0.0514s; loss: -28.1561 (±75.3129); valid loss: -6.31112\n",
      "[Epoch 62/350, Step 17800, ETA 9m 5.492s] step time: 0.00601s (±0.005384s); valid time: 0.05226s; loss: -35.2829 (±6.19377); valid loss: -21.0877\n",
      "[Epoch 62/350, Step 17900, ETA 9m 4.603s] step time: 0.005737s (±0.005278s); valid time: 0.05242s; loss: -35.9396 (±2.972); valid loss: -15.8585\n",
      "[Epoch 62/350, Step 18000, ETA 9m 3.795s] step time: 0.005897s (±0.005448s); valid time: 0.05359s; loss: -35.7749 (±2.09463); valid loss: -15.1658\n",
      "[Epoch 63/350, Step 18100, ETA 9m 3.33s] step time: 0.005949s (±0.005315s); valid time: 0.05185s; loss: -35.2061 (±10.6095); valid loss: -19.7009\n",
      "[Epoch 63/350, Step 18200, ETA 9m 2.51s] step time: 0.00587s (±0.005266s); valid time: 0.0516s; loss: -35.8954 (±2.54253); valid loss: -19.4593\n",
      "[Epoch 63/350, Step 18300, ETA 9m 1.769s] step time: 0.006033s (±0.005803s); valid time: 0.05727s; loss: -36.3788 (±2.03698); valid loss: -13.4563\n",
      "[Epoch 64/350, Step 18400, ETA 9m 1.285s] step time: 0.005847s (±0.005737s); valid time: 0.05272s; loss: -35.9646 (±2.36228); valid loss: -9.37635\n",
      "[Epoch 64/350, Step 18500, ETA 9m 0.4364s] step time: 0.005807s (±0.00526s); valid time: 0.05237s; loss: -32.2046 (±37.2455); valid loss: 181.138\n",
      "[Epoch 64/350, Step 18600, ETA 8m 59.67s] step time: 0.005979s (±0.005306s); valid time: 0.05152s; loss: -33.884 (±18.7693); valid loss: 0.683514\n",
      "[Epoch 65/350, Step 18700, ETA 8m 59.12s] step time: 0.005776s (±0.005103s); valid time: 0.05065s; loss: -36.4552 (±2.32278); valid loss: -0.342628\n",
      "[Epoch 65/350, Step 18800, ETA 8m 58.4s] step time: 0.006085s (±0.005292s); valid time: 0.05201s; loss: -35.2597 (±4.46647); valid loss: -17.4145\n",
      "[Epoch 65/350, Step 18900, ETA 8m 57.69s] step time: 0.006094s (±0.00551s); valid time: 0.05327s; loss: -35.7385 (±2.52257); valid loss: -15.4275\n",
      "[Epoch 66/350, Step 19000, ETA 8m 57.15s] step time: 0.005744s (±0.005051s); valid time: 0.05029s; loss: -36.1785 (±2.74576); valid loss: -14.4125\n",
      "[Epoch 66/350, Step 19100, ETA 8m 56.34s] step time: 0.005856s (±0.005183s); valid time: 0.0514s; loss: -33.8369 (±19.6444); valid loss: -5.80252\n",
      "[Epoch 66/350, Step 19200, ETA 8m 55.49s] step time: 0.00576s (±0.00612s); valid time: 0.06082s; loss: -35.8854 (±2.82866); valid loss: -18.2796\n",
      "[Epoch 67/350, Step 19300, ETA 8m 55.01s] step time: 0.005935s (±0.005167s); valid time: 0.05081s; loss: -36.6173 (±1.74962); valid loss: 55.8929\n",
      "[Epoch 67/350, Step 19400, ETA 8m 54.25s] step time: 0.005972s (±0.005261s); valid time: 0.05218s; loss: -35.9215 (±2.37296); valid loss: -8.48005\n",
      "[Epoch 68/350, Step 19500, ETA 8m 53.74s] step time: 0.005874s (±0.005283s); valid time: 0.05149s; loss: -36.5009 (±2.6148); valid loss: 82.2552\n",
      "[Epoch 68/350, Step 19600, ETA 8m 52.98s] step time: 0.005975s (±0.005471s); valid time: 0.05396s; loss: -36.487 (±2.03553); valid loss: 46.7124\n",
      "[Epoch 68/350, Step 19700, ETA 8m 52.33s] step time: 0.006202s (±0.005634s); valid time: 0.05587s; loss: -36.3327 (±3.01506); valid loss: 340.62\n",
      "[Epoch 69/350, Step 19800, ETA 8m 51.78s] step time: 0.005731s (±0.005218s); valid time: 0.05208s; loss: -36.2844 (±2.92504); valid loss: -8.98572\n",
      "[Epoch 69/350, Step 19900, ETA 8m 51.03s] step time: 0.005993s (±0.005207s); valid time: 0.05196s; loss: -36.1664 (±2.3038); valid loss: 54.0199\n",
      "[Epoch 69/350, Step 20000, ETA 8m 50.22s] step time: 0.005828s (±0.005044s); valid time: 0.05023s; loss: -35.8876 (±2.45738); valid loss: 64.4653\n",
      "[Epoch 70/350, Step 20100, ETA 8m 49.69s] step time: 0.005815s (±0.005439s); valid time: 0.05371s; loss: -36.3475 (±2.3947); valid loss: -18.9478\n",
      "[Epoch 70/350, Step 20200, ETA 8m 48.98s] step time: 0.006093s (±0.005148s); valid time: 0.05034s; loss: -36.1078 (±2.32181); valid loss: -5.4206\n",
      "[Epoch 70/350, Step 20300, ETA 8m 48.19s] step time: 0.005868s (±0.005468s); valid time: 0.05457s; loss: -36.0898 (±3.11999); valid loss: -7.16033\n",
      "[Epoch 70/350, Step 20370, ETA 8m 47.51s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 71/350, Step 20400, ETA 8m 47.71s] step time: 0.005951s (±0.005118s); valid time: 0.04943s; loss: -35.9434 (±2.68421); valid loss: -18.9737\n",
      "[Epoch 71/350, Step 20500, ETA 8m 46.91s] step time: 0.00585s (±0.005189s); valid time: 0.05121s; loss: -36.7372 (±2.17367); valid loss: -8.04509\n",
      "[Epoch 71/350, Step 20600, ETA 8m 46.15s] step time: 0.005951s (±0.005347s); valid time: 0.0529s; loss: -35.1358 (±13.4154); valid loss: -2.49871\n",
      "[Epoch 72/350, Step 20700, ETA 8m 45.66s] step time: 0.00593s (±0.005261s); valid time: 0.05191s; loss: -36.6065 (±2.55691); valid loss: -12.9854\n",
      "[Epoch 72/350, Step 20800, ETA 8m 44.85s] step time: 0.005836s (±0.005133s); valid time: 0.05048s; loss: -36.5166 (±2.24547); valid loss: -15.5834\n",
      "[Epoch 72/350, Step 20900, ETA 8m 44.06s] step time: 0.005854s (±0.005319s); valid time: 0.05326s; loss: -36.8942 (±2.42624); valid loss: 5.07549\n",
      "[Epoch 73/350, Step 21000, ETA 8m 43.61s] step time: 0.00604s (±0.00705s); valid time: 0.07064s; loss: -36.699 (±2.80859); valid loss: 74.3795\n",
      "[Epoch 73/350, Step 21100, ETA 8m 42.86s] step time: 0.005953s (±0.006041s); valid time: 0.05983s; loss: -36.8072 (±2.49693); valid loss: -11.9347\n",
      "[Epoch 73/350, Step 21200, ETA 8m 42.04s] step time: 0.005785s (±0.005693s); valid time: 0.0569s; loss: -35.8639 (±3.25665); valid loss: 307.43\n",
      "[Epoch 74/350, Step 21300, ETA 8m 41.53s] step time: 0.005923s (±0.005111s); valid time: 0.05058s; loss: -36.1393 (±4.89409); valid loss: -17.6701\n",
      "[Epoch 74/350, Step 21400, ETA 8m 40.73s] step time: 0.005825s (±0.005135s); valid time: 0.05168s; loss: -36.5554 (±2.80423); valid loss: 15.7643\n",
      "[Epoch 74/350, Step 21500, ETA 8m 40.02s] step time: 0.006056s (±0.005209s); valid time: 0.05178s; loss: -36.6981 (±1.98213); valid loss: 7.69001\n",
      "[Epoch 75/350, Step 21600, ETA 8m 39.53s] step time: 0.005866s (±0.005213s); valid time: 0.052s; loss: -36.7085 (±2.67952); valid loss: -11.546\n",
      "[Epoch 75/350, Step 21700, ETA 8m 38.81s] step time: 0.006045s (±0.005628s); valid time: 0.05582s; loss: -36.9082 (±2.81237); valid loss: -12.0635\n",
      "[Epoch 75/350, Step 21800, ETA 8m 38.07s] step time: 0.005968s (±0.005309s); valid time: 0.05139s; loss: -36.4251 (±2.58201); valid loss: -19.5738\n",
      "[Epoch 76/350, Step 21900, ETA 8m 37.59s] step time: 0.005919s (±0.005949s); valid time: 0.05765s; loss: -36.259 (±2.82408); valid loss: -10.4214\n",
      "[Epoch 76/350, Step 22000, ETA 8m 36.81s] step time: 0.005869s (±0.00534s); valid time: 0.05299s; loss: -35.6802 (±4.89409); valid loss: -7.84524\n",
      "[Epoch 76/350, Step 22100, ETA 8m 36.08s] step time: 0.006008s (±0.005305s); valid time: 0.05177s; loss: -28.1722 (±83.3692); valid loss: -19.2347\n",
      "[Epoch 77/350, Step 22200, ETA 8m 35.57s] step time: 0.00587s (±0.005191s); valid time: 0.05188s; loss: -28.8899 (±54.3523); valid loss: -16.3487\n",
      "[Epoch 77/350, Step 22300, ETA 8m 34.84s] step time: 0.005996s (±0.005605s); valid time: 0.05443s; loss: -36.7289 (±2.29748); valid loss: -20.617\n",
      "[Epoch 77/350, Step 22400, ETA 8m 34.15s] step time: 0.006103s (±0.005871s); valid time: 0.05748s; loss: -35.3739 (±11.476); valid loss: -14.484\n",
      "[Epoch 78/350, Step 22500, ETA 8m 33.69s] step time: 0.005983s (±0.005846s); valid time: 0.05847s; loss: -36.5773 (±3.16978); valid loss: -5.45045\n",
      "[Epoch 78/350, Step 22600, ETA 8m 32.97s] step time: 0.006022s (±0.006778s); valid time: 0.05976s; loss: -35.6723 (±8.1963); valid loss: -14.9045\n",
      "[Epoch 79/350, Step 22700, ETA 8m 32.47s] step time: 0.005945s (±0.005552s); valid time: 0.05394s; loss: -36.5839 (±2.5531); valid loss: -14.3731\n",
      "[Epoch 79/350, Step 22800, ETA 8m 31.72s] step time: 0.005925s (±0.005714s); valid time: 0.05187s; loss: -34.2286 (±22.9582); valid loss: -12.5173\n",
      "[Epoch 79/350, Step 22900, ETA 8m 31s] step time: 0.005992s (±0.005192s); valid time: 0.05096s; loss: -32.6374 (±39.4956); valid loss: -1.21259\n",
      "[Epoch 80/350, Step 23000, ETA 8m 30.46s] step time: 0.005803s (±0.005273s); valid time: 0.05167s; loss: -36.7498 (±2.1776); valid loss: -12.8212\n",
      "[Epoch 80/350, Step 23100, ETA 8m 29.67s] step time: 0.005824s (±0.00515s); valid time: 0.0509s; loss: -36.7922 (±2.16178); valid loss: -9.71814\n",
      "[Epoch 80/350, Step 23200, ETA 8m 28.88s] step time: 0.005802s (±0.005007s); valid time: 0.04952s; loss: -36.5878 (±2.07197); valid loss: -13.1056\n",
      "[Epoch 80/350, Step 23280, ETA 8m 28.1s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 23300, ETA 8m 28.35s] step time: 0.005857s (±0.006174s); valid time: 0.06165s; loss: -35.8126 (±6.65991); valid loss: -16.201\n",
      "[Epoch 81/350, Step 23400, ETA 8m 27.59s] step time: 0.005898s (±0.005045s); valid time: 0.04966s; loss: 64.6018 (±990.349); valid loss: -18.6548\n",
      "[Epoch 81/350, Step 23500, ETA 8m 26.89s] step time: 0.006062s (±0.005187s); valid time: 0.05145s; loss: -37.1053 (±2.4439); valid loss: -16.9798\n",
      "[Epoch 82/350, Step 23600, ETA 8m 26.33s] step time: 0.005808s (±0.005301s); valid time: 0.05247s; loss: -17.4139 (±188.608); valid loss: 648.455\n",
      "[Epoch 82/350, Step 23700, ETA 8m 25.56s] step time: 0.005836s (±0.005155s); valid time: 0.0508s; loss: -36.8483 (±2.94554); valid loss: 195.637\n",
      "[Epoch 82/350, Step 23800, ETA 8m 24.79s] step time: 0.005864s (±0.005905s); valid time: 0.0588s; loss: -36.6684 (±3.94268); valid loss: -18.6388\n",
      "[Epoch 83/350, Step 23900, ETA 8m 24.29s] step time: 0.005954s (±0.00523s); valid time: 0.04997s; loss: -37.192 (±2.08141); valid loss: -12.8623\n",
      "[Epoch 83/350, Step 24000, ETA 8m 23.5s] step time: 0.005803s (±0.005279s); valid time: 0.05224s; loss: -35.5921 (±8.30545); valid loss: -7.31679\n",
      "[Epoch 83/350, Step 24100, ETA 8m 22.69s] step time: 0.005653s (±0.004937s); valid time: 0.04954s; loss: -36.9706 (±2.65464); valid loss: -15.03\n",
      "[Epoch 84/350, Step 24200, ETA 8m 22.21s] step time: 0.005993s (±0.006001s); valid time: 0.05863s; loss: -37.0274 (±2.53225); valid loss: 110.066\n",
      "[Epoch 84/350, Step 24300, ETA 8m 21.48s] step time: 0.005963s (±0.005189s); valid time: 0.05145s; loss: -31.6712 (±47.1946); valid loss: -19.9313\n",
      "[Epoch 84/350, Step 24400, ETA 8m 20.7s] step time: 0.005824s (±0.00503s); valid time: 0.04988s; loss: -36.773 (±2.87645); valid loss: 11.9912\n",
      "[Epoch 85/350, Step 24500, ETA 8m 20.15s] step time: 0.005857s (±0.005246s); valid time: 0.05153s; loss: -36.9872 (±2.55576); valid loss: -4.96483\n",
      "[Epoch 85/350, Step 24600, ETA 8m 19.4s] step time: 0.005872s (±0.006245s); valid time: 0.06243s; loss: -37.2758 (±2.18882); valid loss: 20.3238\n",
      "[Epoch 85/350, Step 24700, ETA 8m 18.6s] step time: 0.005742s (±0.005094s); valid time: 0.05011s; loss: -36.1851 (±7.55688); valid loss: 259.229\n",
      "[Epoch 86/350, Step 24800, ETA 8m 18.03s] step time: 0.005703s (±0.005019s); valid time: 0.04998s; loss: -36.5654 (±2.80674); valid loss: 4.83407\n",
      "[Epoch 86/350, Step 24900, ETA 8m 17.22s] step time: 0.005678s (±0.005133s); valid time: 0.05129s; loss: -37.1156 (±1.91693); valid loss: 100.374\n",
      "[Epoch 86/350, Step 25000, ETA 8m 16.44s] step time: 0.005817s (±0.005065s); valid time: 0.05018s; loss: -35.3846 (±17.2234); valid loss: -16.5653\n",
      "[Epoch 87/350, Step 25100, ETA 8m 15.93s] step time: 0.005898s (±0.00541s); valid time: 0.05367s; loss: -37.3808 (±1.68826); valid loss: -12.7002\n",
      "[Epoch 87/350, Step 25200, ETA 8m 15.16s] step time: 0.005805s (±0.005095s); valid time: 0.05108s; loss: -34.9228 (±16.4667); valid loss: -20.5085\n",
      "[Epoch 87/350, Step 25300, ETA 8m 14.42s] step time: 0.005896s (±0.005127s); valid time: 0.05007s; loss: -36.4863 (±3.7595); valid loss: 770.645\n",
      "[Epoch 88/350, Step 25400, ETA 8m 13.88s] step time: 0.005897s (±0.005975s); valid time: 0.05918s; loss: -37.1661 (±2.10857); valid loss: -8.5609\n",
      "[Epoch 88/350, Step 25500, ETA 8m 13.1s] step time: 0.005743s (±0.00514s); valid time: 0.0512s; loss: -36.8999 (±2.64372); valid loss: -15.3822\n",
      "[Epoch 88/350, Step 25600, ETA 8m 12.36s] step time: 0.0059s (±0.005349s); valid time: 0.05252s; loss: -37.0151 (±2.07443); valid loss: -7.50569\n",
      "[Epoch 89/350, Step 25700, ETA 8m 11.8s] step time: 0.005766s (±0.005125s); valid time: 0.05088s; loss: -37.3416 (±2.43457); valid loss: 76.5322\n",
      "[Epoch 89/350, Step 25800, ETA 8m 11.09s] step time: 0.005986s (±0.005186s); valid time: 0.05123s; loss: -35.4212 (±7.73155); valid loss: 2483.96\n",
      "[Epoch 90/350, Step 25900, ETA 8m 10.61s] step time: 0.006063s (±0.005816s); valid time: 0.05652s; loss: -36.7674 (±4.10267); valid loss: -16.1054\n",
      "[Epoch 90/350, Step 26000, ETA 8m 9.825s] step time: 0.005753s (±0.005217s); valid time: 0.05112s; loss: -36.2882 (±6.67914); valid loss: -2.55045\n",
      "[Epoch 90/350, Step 26100, ETA 8m 9.128s] step time: 0.006039s (±0.005348s); valid time: 0.05249s; loss: -36.81 (±2.97367); valid loss: -11.2946\n",
      "[Epoch 90/350, Step 26190, ETA 8m 8.279s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 26200, ETA 8m 8.581s] step time: 0.005786s (±0.00608s); valid time: 0.0602s; loss: -36.8637 (±3.73937); valid loss: -9.22161\n",
      "[Epoch 91/350, Step 26300, ETA 8m 7.819s] step time: 0.005806s (±0.005244s); valid time: 0.05227s; loss: -37.5235 (±1.72567); valid loss: -3.01311\n",
      "[Epoch 91/350, Step 26400, ETA 8m 7.114s] step time: 0.006013s (±0.005327s); valid time: 0.05268s; loss: -36.724 (±4.35949); valid loss: -17.0381\n",
      "[Epoch 92/350, Step 26500, ETA 8m 6.563s] step time: 0.005831s (±0.005199s); valid time: 0.05171s; loss: -37.2082 (±2.73083); valid loss: 21.38\n",
      "[Epoch 92/350, Step 26600, ETA 8m 5.855s] step time: 0.005992s (±0.005164s); valid time: 0.0507s; loss: -37.1128 (±2.96674); valid loss: -12.6469\n",
      "[Epoch 92/350, Step 26700, ETA 8m 5.141s] step time: 0.005968s (±0.005493s); valid time: 0.05405s; loss: -37.0674 (±2.33018); valid loss: 248.117\n",
      "[Epoch 93/350, Step 26800, ETA 8m 4.579s] step time: 0.005781s (±0.005109s); valid time: 0.05048s; loss: -37.052 (±4.4991); valid loss: 45.705\n",
      "[Epoch 93/350, Step 26900, ETA 8m 3.837s] step time: 0.005872s (±0.005316s); valid time: 0.05024s; loss: -36.4628 (±4.93273); valid loss: -6.53561\n",
      "[Epoch 93/350, Step 27000, ETA 8m 3.065s] step time: 0.00575s (±0.005276s); valid time: 0.05265s; loss: -37.4046 (±2.35455); valid loss: 34.4827\n",
      "[Epoch 94/350, Step 27100, ETA 8m 2.524s] step time: 0.005894s (±0.004927s); valid time: 0.0489s; loss: -37.3531 (±2.23097); valid loss: 145.725\n",
      "[Epoch 94/350, Step 27200, ETA 8m 1.797s] step time: 0.005914s (±0.005187s); valid time: 0.05072s; loss: -37.2309 (±2.55255); valid loss: -4.2315\n",
      "[Epoch 94/350, Step 27300, ETA 8m 1.063s] step time: 0.005875s (±0.005177s); valid time: 0.05116s; loss: -37.1582 (±2.99348); valid loss: 25.591\n",
      "[Epoch 95/350, Step 27400, ETA 8m 0.4997s] step time: 0.005824s (±0.004996s); valid time: 0.04956s; loss: -37.4793 (±2.14373); valid loss: 4.59306\n",
      "[Epoch 95/350, Step 27500, ETA 7m 59.8s] step time: 0.006019s (±0.005603s); valid time: 0.05446s; loss: -26.3751 (±106.513); valid loss: 1.26187\n",
      "[Epoch 95/350, Step 27600, ETA 7m 59.04s] step time: 0.005772s (±0.005132s); valid time: 0.05097s; loss: -36.0817 (±7.40563); valid loss: 8968.81\n",
      "[Epoch 96/350, Step 27700, ETA 7m 58.49s] step time: 0.005788s (±0.004977s); valid time: 0.04965s; loss: -36.7282 (±2.48328); valid loss: -17.4242\n",
      "[Epoch 96/350, Step 27800, ETA 7m 57.73s] step time: 0.005771s (±0.00507s); valid time: 0.05064s; loss: -37.2421 (±2.04358); valid loss: -10.8736\n",
      "[Epoch 96/350, Step 27900, ETA 7m 57.02s] step time: 0.005958s (±0.005435s); valid time: 0.05236s; loss: -36.7527 (±7.48519); valid loss: -14.2912\n",
      "[Epoch 97/350, Step 28000, ETA 7m 56.49s] step time: 0.005848s (±0.005725s); valid time: 0.0565s; loss: -36.7765 (±7.45177); valid loss: -2.66362\n",
      "[Epoch 97/350, Step 28100, ETA 7m 55.7s] step time: 0.005687s (±0.005157s); valid time: 0.05081s; loss: -37.334 (±2.58769); valid loss: -16.7577\n",
      "[Epoch 97/350, Step 28200, ETA 7m 55.01s] step time: 0.006008s (±0.005416s); valid time: 0.05356s; loss: -36.1987 (±9.09434); valid loss: -13.6756\n",
      "[Epoch 98/350, Step 28300, ETA 7m 54.5s] step time: 0.00599s (±0.005737s); valid time: 0.0551s; loss: -37.0108 (±3.02195); valid loss: 45.4358\n",
      "[Epoch 98/350, Step 28400, ETA 7m 53.81s] step time: 0.006041s (±0.00548s); valid time: 0.05347s; loss: -37.2182 (±3.47306); valid loss: -14.7049\n",
      "[Epoch 98/350, Step 28500, ETA 7m 53.07s] step time: 0.00583s (±0.005235s); valid time: 0.05146s; loss: -36.8696 (±2.92244); valid loss: 335.492\n",
      "[Epoch 99/350, Step 28600, ETA 7m 52.51s] step time: 0.00584s (±0.006211s); valid time: 0.06063s; loss: -37.2145 (±2.24278); valid loss: -10.9275\n",
      "[Epoch 99/350, Step 28700, ETA 7m 51.77s] step time: 0.00583s (±0.005107s); valid time: 0.0507s; loss: -37.3455 (±2.58749); valid loss: 42.5051\n",
      "[Epoch 99/350, Step 28800, ETA 7m 51.05s] step time: 0.005913s (±0.005162s); valid time: 0.05144s; loss: -36.0562 (±8.15972); valid loss: 9657.65\n",
      "[Epoch 100/350, Step 28900, ETA 7m 50.46s] step time: 0.005738s (±0.005207s); valid time: 0.05207s; loss: -36.4704 (±6.36738); valid loss: 423.641\n",
      "[Epoch 100/350, Step 29000, ETA 7m 49.74s] step time: 0.005921s (±0.005209s); valid time: 0.05115s; loss: -36.1948 (±4.42696); valid loss: 175.769\n",
      "[Epoch 100/350, Step 29100, ETA 7m 49.08s] step time: 0.006124s (±0.005308s); valid time: 0.05195s; loss: -37.2455 (±4.77052); valid loss: 10.6963\n",
      "[Epoch 100/350, Step 29100, ETA 7m 49.08s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 101/350, Step 29200, ETA 7m 48.51s] step time: 0.005815s (±0.00522s); valid time: 0.0523s; loss: -34.3803 (±33.2177); valid loss: 287.369\n",
      "[Epoch 101/350, Step 29300, ETA 7m 47.81s] step time: 0.005983s (±0.005197s); valid time: 0.05075s; loss: -36.8137 (±4.68799); valid loss: -14.1311\n",
      "[Epoch 102/350, Step 29400, ETA 7m 47.24s] step time: 0.00583s (±0.006299s); valid time: 0.05898s; loss: -35.943 (±9.29719); valid loss: 12.9311\n",
      "[Epoch 102/350, Step 29500, ETA 7m 46.48s] step time: 0.005735s (±0.005144s); valid time: 0.0506s; loss: -36.9749 (±3.77047); valid loss: 100.725\n",
      "[Epoch 102/350, Step 29600, ETA 7m 45.78s] step time: 0.005995s (±0.005314s); valid time: 0.05257s; loss: -36.9532 (±5.60251); valid loss: 18.3851\n",
      "[Epoch 103/350, Step 29700, ETA 7m 45.24s] step time: 0.005889s (±0.005187s); valid time: 0.05159s; loss: -37.4211 (±3.76832); valid loss: 49.6027\n",
      "[Epoch 103/350, Step 29800, ETA 7m 44.52s] step time: 0.005931s (±0.005379s); valid time: 0.05362s; loss: -37.3375 (±3.03357); valid loss: 8518.73\n",
      "[Epoch 103/350, Step 29900, ETA 7m 43.85s] step time: 0.006063s (±0.00533s); valid time: 0.05263s; loss: -36.2794 (±6.92011); valid loss: 10.212\n",
      "[Epoch 104/350, Step 30000, ETA 7m 43.29s] step time: 0.005848s (±0.005299s); valid time: 0.05203s; loss: -37.4446 (±2.84109); valid loss: 11.9627\n",
      "[Epoch 104/350, Step 30100, ETA 7m 42.6s] step time: 0.00601s (±0.00586s); valid time: 0.05791s; loss: -37.1153 (±2.97891); valid loss: 29.6292\n",
      "[Epoch 104/350, Step 30200, ETA 7m 41.9s] step time: 0.005958s (±0.006189s); valid time: 0.06175s; loss: -37.4473 (±2.55435); valid loss: -3.72805\n",
      "[Epoch 105/350, Step 30300, ETA 7m 41.35s] step time: 0.005928s (±0.005291s); valid time: 0.05216s; loss: -37.5367 (±2.52311); valid loss: -8.29825\n",
      "[Epoch 105/350, Step 30400, ETA 7m 40.63s] step time: 0.005859s (±0.005379s); valid time: 0.05332s; loss: -37.4081 (±2.82265); valid loss: 576.357\n",
      "[Epoch 105/350, Step 30500, ETA 7m 39.87s] step time: 0.005742s (±0.00515s); valid time: 0.05134s; loss: -36.2984 (±3.75875); valid loss: -15.5077\n",
      "[Epoch 106/350, Step 30600, ETA 7m 39.3s] step time: 0.005808s (±0.005206s); valid time: 0.05173s; loss: -37.5999 (±2.83728); valid loss: 1.52087\n",
      "[Epoch 106/350, Step 30700, ETA 7m 38.64s] step time: 0.006152s (±0.005048s); valid time: 0.04899s; loss: -37.2397 (±2.73914); valid loss: 847.67\n",
      "[Epoch 106/350, Step 30800, ETA 7m 37.9s] step time: 0.005794s (±0.005163s); valid time: 0.05098s; loss: -37.2174 (±2.51952); valid loss: -15.4917\n",
      "[Epoch 107/350, Step 30900, ETA 7m 37.34s] step time: 0.005872s (±0.006146s); valid time: 0.06017s; loss: -35.5195 (±16.516); valid loss: 578.717\n",
      "[Epoch 107/350, Step 31000, ETA 7m 36.62s] step time: 0.005884s (±0.005526s); valid time: 0.05206s; loss: -36.9222 (±3.56712); valid loss: -12.1287\n",
      "[Epoch 107/350, Step 31100, ETA 7m 35.91s] step time: 0.005916s (±0.005252s); valid time: 0.05194s; loss: -36.7902 (±7.6073); valid loss: -13.0525\n",
      "[Epoch 108/350, Step 31200, ETA 7m 35.37s] step time: 0.005918s (±0.005226s); valid time: 0.05155s; loss: -36.8667 (±6.74638); valid loss: -17.148\n",
      "[Epoch 108/350, Step 31300, ETA 7m 34.63s] step time: 0.005809s (±0.005221s); valid time: 0.05205s; loss: -37.4287 (±2.66946); valid loss: 27.0293\n",
      "[Epoch 108/350, Step 31400, ETA 7m 33.92s] step time: 0.005894s (±0.00547s); valid time: 0.05406s; loss: -37.2683 (±2.49469); valid loss: 326.614\n",
      "[Epoch 109/350, Step 31500, ETA 7m 33.36s] step time: 0.00582s (±0.005221s); valid time: 0.05127s; loss: -24.7467 (±111.874); valid loss: 39.9704\n",
      "[Epoch 109/350, Step 31600, ETA 7m 32.6s] step time: 0.00571s (±0.00507s); valid time: 0.05091s; loss: -37.4762 (±2.56241); valid loss: 315.982\n",
      "[Epoch 109/350, Step 31700, ETA 7m 31.88s] step time: 0.005841s (±0.005254s); valid time: 0.05145s; loss: -37.433 (±2.30304); valid loss: 16.4713\n",
      "[Epoch 110/350, Step 31800, ETA 7m 31.31s] step time: 0.005856s (±0.005958s); valid time: 0.05384s; loss: -36.4883 (±8.77484); valid loss: 49.7954\n",
      "[Epoch 110/350, Step 31900, ETA 7m 30.6s] step time: 0.005877s (±0.005333s); valid time: 0.05312s; loss: -37.7812 (±3.4325); valid loss: 349.9\n",
      "[Epoch 110/350, Step 32000, ETA 7m 29.88s] step time: 0.005858s (±0.005281s); valid time: 0.05212s; loss: -36.305 (±10.693); valid loss: 177.921\n",
      "[Epoch 110/350, Step 32010, ETA 7m 29.79s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 32100, ETA 7m 29.3s] step time: 0.005779s (±0.005136s); valid time: 0.051s; loss: -37.5376 (±3.34646); valid loss: 174.834\n",
      "[Epoch 111/350, Step 32200, ETA 7m 28.57s] step time: 0.00581s (±0.005126s); valid time: 0.05096s; loss: -36.9263 (±4.57676); valid loss: 82.4562\n",
      "[Epoch 111/350, Step 32300, ETA 7m 27.87s] step time: 0.005927s (±0.005147s); valid time: 0.04987s; loss: -37.435 (±3.7988); valid loss: 179.175\n",
      "[Epoch 112/350, Step 32400, ETA 7m 27.3s] step time: 0.005869s (±0.005007s); valid time: 0.04901s; loss: -36.4778 (±8.84813); valid loss: 22.4737\n",
      "[Epoch 112/350, Step 32500, ETA 7m 26.58s] step time: 0.005843s (±0.005367s); valid time: 0.05269s; loss: -37.539 (±2.87775); valid loss: 34.834\n",
      "[Epoch 113/350, Step 32600, ETA 7m 26.04s] step time: 0.005966s (±0.005313s); valid time: 0.05247s; loss: -37.0311 (±4.38218); valid loss: -16.245\n",
      "[Epoch 113/350, Step 32700, ETA 7m 25.29s] step time: 0.005671s (±0.005146s); valid time: 0.05064s; loss: -36.6797 (±9.35736); valid loss: 6393.44\n",
      "[Epoch 113/350, Step 32800, ETA 7m 24.62s] step time: 0.00607s (±0.005446s); valid time: 0.0534s; loss: -37.1592 (±4.0668); valid loss: 48.1567\n",
      "[Epoch 114/350, Step 32900, ETA 7m 24.03s] step time: 0.005782s (±0.00517s); valid time: 0.05156s; loss: -37.6276 (±2.21813); valid loss: 16.0148\n",
      "[Epoch 114/350, Step 33000, ETA 7m 23.37s] step time: 0.006082s (±0.005622s); valid time: 0.05585s; loss: -37.2544 (±4.43737); valid loss: 15.8107\n",
      "[Epoch 114/350, Step 33100, ETA 7m 22.72s] step time: 0.006165s (±0.005438s); valid time: 0.05393s; loss: -37.587 (±2.48333); valid loss: 14.7833\n",
      "[Epoch 115/350, Step 33200, ETA 7m 22.16s] step time: 0.005907s (±0.005281s); valid time: 0.05213s; loss: -37.8774 (±2.64617); valid loss: 106.436\n",
      "[Epoch 115/350, Step 33300, ETA 7m 21.44s] step time: 0.005852s (±0.005385s); valid time: 0.05353s; loss: -37.6396 (±2.58721); valid loss: -17.6222\n",
      "[Epoch 115/350, Step 33400, ETA 7m 20.74s] step time: 0.005932s (±0.005371s); valid time: 0.0528s; loss: -36.5074 (±9.57477); valid loss: 59.8436\n",
      "[Epoch 116/350, Step 33500, ETA 7m 20.18s] step time: 0.005861s (±0.005131s); valid time: 0.05124s; loss: -36.9788 (±4.06014); valid loss: 9.75257\n",
      "[Epoch 116/350, Step 33600, ETA 7m 19.48s] step time: 0.005899s (±0.0054s); valid time: 0.05349s; loss: -37.5665 (±3.82204); valid loss: 881.422\n",
      "[Epoch 116/350, Step 33700, ETA 7m 18.78s] step time: 0.005904s (±0.00546s); valid time: 0.05427s; loss: -32.9491 (±46.9902); valid loss: 408.227\n",
      "[Epoch 117/350, Step 33800, ETA 7m 18.24s] step time: 0.006028s (±0.005385s); valid time: 0.05148s; loss: -29.489 (±72.5394); valid loss: 182.738\n",
      "[Epoch 117/350, Step 33900, ETA 7m 17.55s] step time: 0.005958s (±0.005355s); valid time: 0.05301s; loss: -37.5321 (±2.58003); valid loss: -14.2389\n",
      "[Epoch 117/350, Step 34000, ETA 7m 16.88s] step time: 0.006049s (±0.005407s); valid time: 0.05273s; loss: -37.6772 (±3.1986); valid loss: 589.893\n",
      "[Epoch 118/350, Step 34100, ETA 7m 16.35s] step time: 0.00607s (±0.005324s); valid time: 0.05278s; loss: -37.0062 (±6.22913); valid loss: 72.8998\n",
      "[Epoch 118/350, Step 34200, ETA 7m 15.66s] step time: 0.005982s (±0.005173s); valid time: 0.05127s; loss: 312.418 (±3050.37); valid loss: -1.91786\n",
      "[Epoch 118/350, Step 34300, ETA 7m 14.98s] step time: 0.005989s (±0.005062s); valid time: 0.05063s; loss: -37.4024 (±8.46703); valid loss: 53.5988\n",
      "[Epoch 119/350, Step 34400, ETA 7m 14.45s] step time: 0.006002s (±0.005487s); valid time: 0.05365s; loss: -37.6282 (±2.68025); valid loss: 321.678\n",
      "[Epoch 119/350, Step 34500, ETA 7m 13.78s] step time: 0.00609s (±0.005563s); valid time: 0.05435s; loss: -37.3547 (±4.77494); valid loss: 1073.89\n",
      "[Epoch 119/350, Step 34600, ETA 7m 13.09s] step time: 0.005955s (±0.005054s); valid time: 0.04984s; loss: -30.7114 (±62.6122); valid loss: 123.466\n",
      "[Epoch 120/350, Step 34700, ETA 7m 12.57s] step time: 0.006045s (±0.0054s); valid time: 0.05337s; loss: -37.9222 (±2.23699); valid loss: 150.614\n",
      "[Epoch 120/350, Step 34800, ETA 7m 11.86s] step time: 0.00585s (±0.00534s); valid time: 0.05298s; loss: -35.466 (±21.1359); valid loss: 236.684\n",
      "[Epoch 120/350, Step 34900, ETA 7m 11.18s] step time: 0.006022s (±0.005575s); valid time: 0.05403s; loss: -37.6148 (±3.29068); valid loss: 52.7129\n",
      "[Epoch 120/350, Step 34920, ETA 7m 11.02s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 35000, ETA 7m 10.65s] step time: 0.006053s (±0.005628s); valid time: 0.0556s; loss: -37.5708 (±5.45721); valid loss: 1761.9\n",
      "[Epoch 121/350, Step 35100, ETA 7m 9.956s] step time: 0.005923s (±0.005736s); valid time: 0.0538s; loss: -31.0922 (±62.9709); valid loss: 630.388\n",
      "[Epoch 121/350, Step 35200, ETA 7m 9.291s] step time: 0.006076s (±0.005416s); valid time: 0.05377s; loss: -5.69516 (±317.896); valid loss: 149.291\n",
      "[Epoch 122/350, Step 35300, ETA 7m 8.751s] step time: 0.006037s (±0.00531s); valid time: 0.05163s; loss: -37.4715 (±4.21989); valid loss: 499.415\n",
      "[Epoch 122/350, Step 35400, ETA 7m 8.063s] step time: 0.005962s (±0.00569s); valid time: 0.05214s; loss: -36.8827 (±9.49861); valid loss: 95.7433\n",
      "[Epoch 122/350, Step 35500, ETA 7m 7.362s] step time: 0.005882s (±0.00506s); valid time: 0.05041s; loss: -29.1841 (±62.2455); valid loss: 115.196\n",
      "[Epoch 123/350, Step 35600, ETA 7m 6.809s] step time: 0.006012s (±0.005476s); valid time: 0.05276s; loss: -38.2732 (±2.26068); valid loss: 399.594\n",
      "[Epoch 123/350, Step 35700, ETA 7m 6.106s] step time: 0.005879s (±0.00516s); valid time: 0.05137s; loss: -35.2982 (±19.17); valid loss: 3539.56\n",
      "[Epoch 124/350, Step 35800, ETA 7m 5.576s] step time: 0.006149s (±0.005277s); valid time: 0.05088s; loss: -31.0055 (±69.9762); valid loss: -1.13765\n",
      "[Epoch 124/350, Step 35900, ETA 7m 4.875s] step time: 0.005894s (±0.005626s); valid time: 0.05557s; loss: -37.8733 (±4.4094); valid loss: 65.8824\n",
      "[Epoch 124/350, Step 36000, ETA 7m 4.175s] step time: 0.005899s (±0.00567s); valid time: 0.05632s; loss: -36.5679 (±8.05693); valid loss: 19.4971\n",
      "[Epoch 125/350, Step 36100, ETA 7m 3.593s] step time: 0.005802s (±0.005277s); valid time: 0.05207s; loss: -37.8398 (±1.79729); valid loss: 110.904\n",
      "[Epoch 125/350, Step 36200, ETA 7m 2.878s] step time: 0.005808s (±0.005001s); valid time: 0.05033s; loss: -8.02605 (±231.655); valid loss: 120.448\n",
      "[Epoch 125/350, Step 36300, ETA 7m 2.182s] step time: 0.005906s (±0.005257s); valid time: 0.05208s; loss: -37.0607 (±7.26095); valid loss: 4468.27\n",
      "[Epoch 126/350, Step 36400, ETA 7m 1.622s] step time: 0.005958s (±0.005566s); valid time: 0.05323s; loss: -31.1095 (±63.0125); valid loss: 138.541\n",
      "[Epoch 126/350, Step 36500, ETA 7m 0.886s] step time: 0.005692s (±0.005033s); valid time: 0.05055s; loss: -37.2101 (±6.96115); valid loss: 1670.29\n",
      "[Epoch 126/350, Step 36600, ETA 7m 0.1884s] step time: 0.0059s (±0.005093s); valid time: 0.04989s; loss: -37.8109 (±2.32538); valid loss: 455.629\n",
      "[Epoch 127/350, Step 36700, ETA 6m 59.62s] step time: 0.005854s (±0.005835s); valid time: 0.05827s; loss: -35.966 (±10.6895); valid loss: -5.35404\n",
      "[Epoch 127/350, Step 36800, ETA 6m 58.92s] step time: 0.005901s (±0.005164s); valid time: 0.05125s; loss: -36.5588 (±12.0365); valid loss: 3.55129\n",
      "[Epoch 127/350, Step 36900, ETA 6m 58.24s] step time: 0.006017s (±0.005265s); valid time: 0.05164s; loss: -37.7732 (±4.42394); valid loss: 365.39\n",
      "[Epoch 128/350, Step 37000, ETA 6m 57.65s] step time: 0.005766s (±0.005063s); valid time: 0.0502s; loss: -37.8893 (±2.03345); valid loss: 346.307\n",
      "[Epoch 128/350, Step 37100, ETA 6m 56.96s] step time: 0.005943s (±0.005675s); valid time: 0.05647s; loss: -37.9946 (±1.93353); valid loss: 612.186\n",
      "[Epoch 128/350, Step 37200, ETA 6m 56.28s] step time: 0.006015s (±0.005396s); valid time: 0.05211s; loss: -37.5386 (±3.19973); valid loss: -11.7509\n",
      "[Epoch 129/350, Step 37300, ETA 6m 55.69s] step time: 0.005816s (±0.005018s); valid time: 0.04877s; loss: -36.8128 (±5.98571); valid loss: -13.3555\n",
      "[Epoch 129/350, Step 37400, ETA 6m 54.97s] step time: 0.005734s (±0.005475s); valid time: 0.05366s; loss: -37.5247 (±3.05855); valid loss: 5447.2\n",
      "[Epoch 129/350, Step 37500, ETA 6m 54.25s] step time: 0.005779s (±0.005216s); valid time: 0.05157s; loss: -38.1166 (±2.54914); valid loss: 22.7297\n",
      "[Epoch 130/350, Step 37600, ETA 6m 53.7s] step time: 0.006s (±0.005055s); valid time: 0.04988s; loss: -37.7088 (±2.69104); valid loss: 99.9405\n",
      "[Epoch 130/350, Step 37700, ETA 6m 53.02s] step time: 0.005981s (±0.005398s); valid time: 0.05255s; loss: -37.1662 (±4.48838); valid loss: 14.1616\n",
      "[Epoch 130/350, Step 37800, ETA 6m 52.36s] step time: 0.0061s (±0.005323s); valid time: 0.0523s; loss: -37.9352 (±2.06019); valid loss: -4.9234\n",
      "[Epoch 130/350, Step 37830, ETA 6m 52.15s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 37900, ETA 6m 51.78s] step time: 0.005768s (±0.005103s); valid time: 0.05093s; loss: -37.897 (±2.52867); valid loss: 351.609\n",
      "[Epoch 131/350, Step 38000, ETA 6m 51.08s] step time: 0.005889s (±0.005686s); valid time: 0.05623s; loss: -33.6263 (±44.0584); valid loss: 434.488\n",
      "[Epoch 131/350, Step 38100, ETA 6m 50.36s] step time: 0.005738s (±0.00532s); valid time: 0.05307s; loss: -35.8555 (±15.9923); valid loss: -10.5247\n",
      "[Epoch 132/350, Step 38200, ETA 6m 49.79s] step time: 0.005949s (±0.005243s); valid time: 0.05159s; loss: -37.2887 (±5.92919); valid loss: 28.7499\n",
      "[Epoch 132/350, Step 38300, ETA 6m 49.1s] step time: 0.005904s (±0.00618s); valid time: 0.06124s; loss: -37.8676 (±2.20898); valid loss: 67.3594\n",
      "[Epoch 132/350, Step 38400, ETA 6m 48.38s] step time: 0.005739s (±0.005376s); valid time: 0.05397s; loss: -37.683 (±4.40699); valid loss: 17.6145\n",
      "[Epoch 133/350, Step 38500, ETA 6m 47.82s] step time: 0.006035s (±0.005205s); valid time: 0.05124s; loss: -28.0137 (±69.0657); valid loss: 63.0987\n",
      "[Epoch 133/350, Step 38600, ETA 6m 47.11s] step time: 0.005731s (±0.005318s); valid time: 0.05326s; loss: -37.9024 (±2.4167); valid loss: 20.688\n",
      "[Epoch 133/350, Step 38700, ETA 6m 46.43s] step time: 0.00597s (±0.005435s); valid time: 0.05375s; loss: -37.5423 (±2.98187); valid loss: 609.584\n",
      "[Epoch 134/350, Step 38800, ETA 6m 45.86s] step time: 0.006002s (±0.005331s); valid time: 0.05231s; loss: -36.8068 (±8.39944); valid loss: 0.606063\n",
      "[Epoch 134/350, Step 38900, ETA 6m 45.16s] step time: 0.005798s (±0.005223s); valid time: 0.05194s; loss: -37.539 (±3.48796); valid loss: -3.17747\n",
      "[Epoch 135/350, Step 39000, ETA 6m 44.59s] step time: 0.006004s (±0.005299s); valid time: 0.05227s; loss: -31.976 (±59.6757); valid loss: 324.301\n",
      "[Epoch 135/350, Step 39100, ETA 6m 43.91s] step time: 0.005952s (±0.005883s); valid time: 0.05821s; loss: -37.5348 (±3.24773); valid loss: -7.10606\n",
      "[Epoch 135/350, Step 39200, ETA 6m 43.21s] step time: 0.005844s (±0.005376s); valid time: 0.05303s; loss: -37.8783 (±3.41992); valid loss: 176.592\n",
      "[Epoch 136/350, Step 39300, ETA 6m 42.66s] step time: 0.006058s (±0.005353s); valid time: 0.05116s; loss: -37.7236 (±2.93596); valid loss: 7.74203\n",
      "[Epoch 136/350, Step 39400, ETA 6m 41.96s] step time: 0.005896s (±0.00539s); valid time: 0.05275s; loss: -36.9292 (±7.27241); valid loss: 557.713\n",
      "[Epoch 136/350, Step 39500, ETA 6m 41.28s] step time: 0.005941s (±0.005362s); valid time: 0.053s; loss: -35.431 (±25.0727); valid loss: -9.91196\n",
      "[Epoch 137/350, Step 39600, ETA 6m 40.7s] step time: 0.005832s (±0.005372s); valid time: 0.05343s; loss: -36.7167 (±5.38822); valid loss: -19.759\n",
      "[Epoch 137/350, Step 39700, ETA 6m 40.01s] step time: 0.00589s (±0.005438s); valid time: 0.05394s; loss: -37.5895 (±5.92498); valid loss: 156.078\n",
      "[Epoch 137/350, Step 39800, ETA 6m 39.32s] step time: 0.005901s (±0.005159s); valid time: 0.05083s; loss: -37.724 (±4.35662); valid loss: -6.06974\n",
      "[Epoch 138/350, Step 39900, ETA 6m 38.72s] step time: 0.005792s (±0.005196s); valid time: 0.05147s; loss: -38.0381 (±2.73773); valid loss: 44.7377\n",
      "[Epoch 138/350, Step 40000, ETA 6m 38.01s] step time: 0.005721s (±0.005593s); valid time: 0.05309s; loss: -37.9951 (±2.52189); valid loss: 602.168\n",
      "[Epoch 138/350, Step 40100, ETA 6m 37.32s] step time: 0.005896s (±0.005077s); valid time: 0.05008s; loss: -37.6575 (±2.60871); valid loss: -6.79236\n",
      "[Epoch 139/350, Step 40200, ETA 6m 36.72s] step time: 0.005754s (±0.005s); valid time: 0.04977s; loss: -37.5393 (±5.49322); valid loss: 4.22492\n",
      "[Epoch 139/350, Step 40300, ETA 6m 36.02s] step time: 0.005864s (±0.005187s); valid time: 0.05158s; loss: -37.0426 (±8.468); valid loss: 15.1442\n",
      "[Epoch 139/350, Step 40400, ETA 6m 35.37s] step time: 0.006135s (±0.005288s); valid time: 0.0519s; loss: -37.3775 (±3.02707); valid loss: -0.325527\n",
      "[Epoch 140/350, Step 40500, ETA 6m 34.81s] step time: 0.006009s (±0.005202s); valid time: 0.05133s; loss: -38.369 (±2.73216); valid loss: 31.8132\n",
      "[Epoch 140/350, Step 40600, ETA 6m 34.13s] step time: 0.005957s (±0.005287s); valid time: 0.05195s; loss: -37.5148 (±3.42352); valid loss: 14.7373\n",
      "[Epoch 140/350, Step 40700, ETA 6m 33.43s] step time: 0.005818s (±0.005997s); valid time: 0.05742s; loss: -37.865 (±2.31637); valid loss: -13.6516\n",
      "[Epoch 140/350, Step 40740, ETA 6m 33.13s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 40800, ETA 6m 32.85s] step time: 0.00593s (±0.005089s); valid time: 0.04987s; loss: -34.5461 (±33.5548); valid loss: 169.317\n",
      "[Epoch 141/350, Step 40900, ETA 6m 32.2s] step time: 0.006144s (±0.005652s); valid time: 0.05637s; loss: -38.1395 (±2.46384); valid loss: 118.952\n",
      "[Epoch 141/350, Step 41000, ETA 6m 31.52s] step time: 0.005888s (±0.005167s); valid time: 0.05151s; loss: -37.745 (±5.27853); valid loss: 4.61174\n",
      "[Epoch 142/350, Step 41100, ETA 6m 30.95s] step time: 0.005864s (±0.005268s); valid time: 0.05197s; loss: -37.2816 (±4.31057); valid loss: -3.98881\n",
      "[Epoch 142/350, Step 41200, ETA 6m 30.27s] step time: 0.006007s (±0.005898s); valid time: 0.05826s; loss: -38.0207 (±2.46957); valid loss: 7.91775\n",
      "[Epoch 142/350, Step 41300, ETA 6m 29.59s] step time: 0.005934s (±0.005863s); valid time: 0.05854s; loss: -37.5726 (±3.48445); valid loss: 126.3\n",
      "[Epoch 143/350, Step 41400, ETA 6m 29.01s] step time: 0.005783s (±0.005354s); valid time: 0.05371s; loss: -35.4178 (±24.0723); valid loss: 20.1552\n",
      "[Epoch 143/350, Step 41500, ETA 6m 28.32s] step time: 0.00586s (±0.005333s); valid time: 0.05243s; loss: -37.3618 (±6.66303); valid loss: 6.58209\n",
      "[Epoch 143/350, Step 41600, ETA 6m 27.63s] step time: 0.005889s (±0.005144s); valid time: 0.05137s; loss: -37.6272 (±3.79714); valid loss: 932.043\n",
      "[Epoch 144/350, Step 41700, ETA 6m 27.03s] step time: 0.005814s (±0.005884s); valid time: 0.05819s; loss: -37.8339 (±3.21531); valid loss: 261.505\n",
      "[Epoch 144/350, Step 41800, ETA 6m 26.32s] step time: 0.005699s (±0.005075s); valid time: 0.05052s; loss: -34.2571 (±40.4945); valid loss: 436.013\n",
      "[Epoch 144/350, Step 41900, ETA 6m 25.64s] step time: 0.005919s (±0.005603s); valid time: 0.05561s; loss: -32.2594 (±55.6773); valid loss: 19.6057\n",
      "[Epoch 145/350, Step 42000, ETA 6m 25.08s] step time: 0.006068s (±0.005162s); valid time: 0.05045s; loss: -35.5824 (±20.241); valid loss: 23.8831\n",
      "[Epoch 145/350, Step 42100, ETA 6m 24.4s] step time: 0.00593s (±0.005345s); valid time: 0.05231s; loss: -38.0525 (±2.51767); valid loss: -3.72064\n",
      "[Epoch 146/350, Step 42200, ETA 6m 23.83s] step time: 0.006006s (±0.005878s); valid time: 0.05771s; loss: -36.2882 (±17.2474); valid loss: 1.68184\n",
      "[Epoch 146/350, Step 42300, ETA 6m 23.15s] step time: 0.005924s (±0.005183s); valid time: 0.05094s; loss: -37.3157 (±5.46799); valid loss: -10.1266\n",
      "[Epoch 146/350, Step 42400, ETA 6m 22.44s] step time: 0.005737s (±0.005271s); valid time: 0.05268s; loss: -38.1436 (±2.58373); valid loss: -0.834683\n",
      "[Epoch 147/350, Step 42500, ETA 6m 21.85s] step time: 0.005881s (±0.005907s); valid time: 0.05843s; loss: -38.0099 (±2.74814); valid loss: 82.6187\n",
      "[Epoch 147/350, Step 42600, ETA 6m 21.19s] step time: 0.006024s (±0.006163s); valid time: 0.05353s; loss: -37.4535 (±6.94713); valid loss: 97.6393\n",
      "[Epoch 147/350, Step 42700, ETA 6m 20.48s] step time: 0.005727s (±0.005099s); valid time: 0.05082s; loss: -37.9071 (±3.23075); valid loss: 14.7188\n",
      "[Epoch 148/350, Step 42800, ETA 6m 19.9s] step time: 0.005931s (±0.005237s); valid time: 0.05051s; loss: -37.6352 (±3.31302); valid loss: 94.657\n",
      "[Epoch 148/350, Step 42900, ETA 6m 19.21s] step time: 0.005864s (±0.005094s); valid time: 0.05097s; loss: -38.0179 (±3.11743); valid loss: 68.0727\n",
      "[Epoch 148/350, Step 43000, ETA 6m 18.56s] step time: 0.006143s (±0.005258s); valid time: 0.05166s; loss: -37.5642 (±3.84094); valid loss: 0.203863\n",
      "[Epoch 149/350, Step 43100, ETA 6m 17.98s] step time: 0.005835s (±0.005217s); valid time: 0.05165s; loss: -37.7118 (±2.76943); valid loss: 372.816\n",
      "[Epoch 149/350, Step 43200, ETA 6m 17.27s] step time: 0.005743s (±0.005118s); valid time: 0.05105s; loss: -38.119 (±2.44413); valid loss: 3.20167\n",
      "[Epoch 149/350, Step 43300, ETA 6m 16.61s] step time: 0.006009s (±0.005575s); valid time: 0.05546s; loss: -37.6007 (±4.29414); valid loss: -6.29692\n",
      "[Epoch 150/350, Step 43400, ETA 6m 16.02s] step time: 0.005901s (±0.005391s); valid time: 0.05323s; loss: -36.4224 (±15.0948); valid loss: -9.71019\n",
      "[Epoch 150/350, Step 43500, ETA 6m 15.32s] step time: 0.005815s (±0.005211s); valid time: 0.05182s; loss: -37.4751 (±6.05489); valid loss: 1779.69\n",
      "[Epoch 150/350, Step 43600, ETA 6m 14.67s] step time: 0.006143s (±0.005587s); valid time: 0.05318s; loss: -36.367 (±13.0915); valid loss: 41.9268\n",
      "[Epoch 150/350, Step 43650, ETA 6m 14.3s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 151/350, Step 43700, ETA 6m 14.09s] step time: 0.005944s (±0.005535s); valid time: 0.05455s; loss: -36.6306 (±13.3061); valid loss: 73.9615\n",
      "[Epoch 151/350, Step 43800, ETA 6m 13.42s] step time: 0.00598s (±0.005374s); valid time: 0.05221s; loss: -37.1601 (±5.26082); valid loss: 10249.1\n",
      "[Epoch 151/350, Step 43900, ETA 6m 12.74s] step time: 0.005885s (±0.005355s); valid time: 0.05282s; loss: -38.0177 (±2.73296); valid loss: 52.2139\n",
      "[Epoch 152/350, Step 44000, ETA 6m 12.14s] step time: 0.005828s (±0.00551s); valid time: 0.05468s; loss: -37.9139 (±3.30021); valid loss: -4.93895\n",
      "[Epoch 152/350, Step 44100, ETA 6m 11.48s] step time: 0.006061s (±0.00519s); valid time: 0.05148s; loss: -38.1342 (±3.29456); valid loss: 56.6006\n",
      "[Epoch 152/350, Step 44200, ETA 6m 10.8s] step time: 0.005911s (±0.005033s); valid time: 0.04896s; loss: -37.6742 (±4.05368); valid loss: 154.221\n",
      "[Epoch 153/350, Step 44300, ETA 6m 10.19s] step time: 0.005721s (±0.005136s); valid time: 0.05115s; loss: -37.8926 (±2.91581); valid loss: 285.146\n",
      "[Epoch 153/350, Step 44400, ETA 6m 9.5s] step time: 0.00582s (±0.005377s); valid time: 0.05351s; loss: -38.1713 (±2.32818); valid loss: 224.36\n",
      "[Epoch 153/350, Step 44500, ETA 6m 8.801s] step time: 0.005758s (±0.005001s); valid time: 0.04902s; loss: -36.7517 (±13.5034); valid loss: 3180.01\n",
      "[Epoch 154/350, Step 44600, ETA 6m 8.19s] step time: 0.00573s (±0.00515s); valid time: 0.05089s; loss: -37.8291 (±3.0733); valid loss: 133.547\n",
      "[Epoch 154/350, Step 44700, ETA 6m 7.509s] step time: 0.005893s (±0.005205s); valid time: 0.05108s; loss: -32.8471 (±39.1892); valid loss: 44.8491\n",
      "[Epoch 154/350, Step 44800, ETA 6m 6.804s] step time: 0.005697s (±0.004955s); valid time: 0.04939s; loss: -32.2285 (±48.7198); valid loss: 40.0237\n",
      "[Epoch 155/350, Step 44900, ETA 6m 6.225s] step time: 0.005973s (±0.005426s); valid time: 0.05359s; loss: -37.0507 (±8.2213); valid loss: 4.60932\n",
      "[Epoch 155/350, Step 45000, ETA 6m 5.61s] step time: 0.006394s (±0.005432s); valid time: 0.05338s; loss: -37.8145 (±3.84012); valid loss: 98.5082\n",
      "[Epoch 155/350, Step 45100, ETA 6m 4.944s] step time: 0.006001s (±0.005568s); valid time: 0.05479s; loss: -38.0326 (±2.69555); valid loss: -4.70541\n",
      "[Epoch 156/350, Step 45200, ETA 6m 4.387s] step time: 0.006173s (±0.005777s); valid time: 0.05633s; loss: -37.5577 (±3.83752); valid loss: 453.484\n",
      "[Epoch 156/350, Step 45300, ETA 6m 3.708s] step time: 0.005905s (±0.005951s); valid time: 0.05894s; loss: -37.3146 (±6.91665); valid loss: 1717.65\n",
      "[Epoch 157/350, Step 45400, ETA 6m 3.107s] step time: 0.005823s (±0.005404s); valid time: 0.05423s; loss: -38.1627 (±3.63662); valid loss: 1182.75\n",
      "[Epoch 157/350, Step 45500, ETA 6m 2.436s] step time: 0.005954s (±0.005369s); valid time: 0.05298s; loss: -37.2142 (±5.49579); valid loss: 29.491\n",
      "[Epoch 157/350, Step 45600, ETA 6m 1.749s] step time: 0.00584s (±0.005276s); valid time: 0.05293s; loss: -37.6485 (±3.22743); valid loss: -7.20743\n",
      "[Epoch 158/350, Step 45700, ETA 6m 1.163s] step time: 0.005909s (±0.005288s); valid time: 0.05244s; loss: -37.3598 (±10.8089); valid loss: 150.607\n",
      "[Epoch 158/350, Step 45800, ETA 6m 0.4884s] step time: 0.005934s (±0.005201s); valid time: 0.05122s; loss: -37.8885 (±4.60449); valid loss: 9.43247\n",
      "[Epoch 158/350, Step 45900, ETA 5m 59.81s] step time: 0.005889s (±0.005302s); valid time: 0.05215s; loss: -37.7482 (±4.21543); valid loss: -7.80735\n",
      "[Epoch 159/350, Step 46000, ETA 5m 59.22s] step time: 0.005896s (±0.005216s); valid time: 0.05037s; loss: -28.6936 (±91.34); valid loss: 113.851\n",
      "[Epoch 159/350, Step 46100, ETA 5m 58.54s] step time: 0.005916s (±0.005263s); valid time: 0.0506s; loss: -38.1736 (±3.82579); valid loss: 42.5787\n",
      "[Epoch 159/350, Step 46200, ETA 5m 57.87s] step time: 0.005942s (±0.005172s); valid time: 0.05163s; loss: -37.6506 (±3.77582); valid loss: 2520.85\n",
      "[Epoch 160/350, Step 46300, ETA 5m 57.28s] step time: 0.005877s (±0.005244s); valid time: 0.05062s; loss: -37.7431 (±3.70741); valid loss: 1075.41\n",
      "[Epoch 160/350, Step 46400, ETA 5m 56.59s] step time: 0.005824s (±0.005148s); valid time: 0.05155s; loss: -37.6711 (±3.43792); valid loss: 67.1852\n",
      "[Epoch 160/350, Step 46500, ETA 5m 55.93s] step time: 0.006036s (±0.005419s); valid time: 0.05294s; loss: -38.46 (±2.01086); valid loss: 5.62866\n",
      "[Epoch 160/350, Step 46560, ETA 5m 55.5s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 46600, ETA 5m 55.38s] step time: 0.006224s (±0.005466s); valid time: 0.05212s; loss: -37.3553 (±4.04988); valid loss: 93.5424\n",
      "[Epoch 161/350, Step 46700, ETA 5m 54.7s] step time: 0.005898s (±0.005298s); valid time: 0.05279s; loss: -37.8566 (±3.04504); valid loss: 16.5189\n",
      "[Epoch 161/350, Step 46800, ETA 5m 54.06s] step time: 0.006156s (±0.005275s); valid time: 0.05092s; loss: -18.1328 (±186.701); valid loss: 22.5333\n",
      "[Epoch 162/350, Step 46900, ETA 5m 53.47s] step time: 0.005942s (±0.005329s); valid time: 0.05239s; loss: -38.78 (±2.14667); valid loss: -8.59816\n",
      "[Epoch 162/350, Step 47000, ETA 5m 52.8s] step time: 0.005945s (±0.005264s); valid time: 0.05241s; loss: -37.2833 (±4.12746); valid loss: 32.9175\n",
      "[Epoch 162/350, Step 47100, ETA 5m 52.13s] step time: 0.005978s (±0.005268s); valid time: 0.05129s; loss: -37.2241 (±7.23521); valid loss: 77.4145\n",
      "[Epoch 163/350, Step 47200, ETA 5m 51.56s] step time: 0.006103s (±0.005297s); valid time: 0.05163s; loss: -38.1993 (±2.50598); valid loss: 437.723\n",
      "[Epoch 163/350, Step 47300, ETA 5m 50.89s] step time: 0.005963s (±0.005342s); valid time: 0.05353s; loss: -37.7112 (±5.00767); valid loss: 72.9422\n",
      "[Epoch 163/350, Step 47400, ETA 5m 50.21s] step time: 0.005884s (±0.005417s); valid time: 0.0535s; loss: -38.1006 (±2.78194); valid loss: -0.774927\n",
      "[Epoch 164/350, Step 47500, ETA 5m 49.61s] step time: 0.005778s (±0.005262s); valid time: 0.05287s; loss: -37.5083 (±4.04235); valid loss: 512.532\n",
      "[Epoch 164/350, Step 47600, ETA 5m 48.95s] step time: 0.006009s (±0.006153s); valid time: 0.06064s; loss: -38.1712 (±2.57329); valid loss: 3.68667\n",
      "[Epoch 164/350, Step 47700, ETA 5m 48.3s] step time: 0.006153s (±0.005395s); valid time: 0.05266s; loss: -38.0676 (±4.13649); valid loss: 71.0517\n",
      "[Epoch 165/350, Step 47800, ETA 5m 47.7s] step time: 0.005878s (±0.005137s); valid time: 0.05102s; loss: -36.804 (±7.08688); valid loss: 27.941\n",
      "[Epoch 165/350, Step 47900, ETA 5m 47.03s] step time: 0.005925s (±0.005382s); valid time: 0.05328s; loss: -38.0714 (±2.35726); valid loss: 856.55\n",
      "[Epoch 165/350, Step 48000, ETA 5m 46.37s] step time: 0.005997s (±0.005158s); valid time: 0.0512s; loss: -38.6942 (±1.83497); valid loss: 46.3014\n",
      "[Epoch 166/350, Step 48100, ETA 5m 45.78s] step time: 0.005959s (±0.005392s); valid time: 0.05355s; loss: -38.1431 (±2.86368); valid loss: 169.12\n",
      "[Epoch 166/350, Step 48200, ETA 5m 45.12s] step time: 0.006112s (±0.005698s); valid time: 0.05543s; loss: -37.5397 (±7.51877); valid loss: 2174.9\n",
      "[Epoch 166/350, Step 48300, ETA 5m 44.45s] step time: 0.005905s (±0.005944s); valid time: 0.0592s; loss: -38.0523 (±2.88996); valid loss: -2.21364\n",
      "[Epoch 167/350, Step 48400, ETA 5m 43.84s] step time: 0.005816s (±0.004958s); valid time: 0.04939s; loss: -37.9628 (±3.91148); valid loss: 41.6356\n",
      "[Epoch 167/350, Step 48500, ETA 5m 43.19s] step time: 0.006128s (±0.005459s); valid time: 0.05189s; loss: -37.5185 (±3.629); valid loss: 105.97\n",
      "[Epoch 168/350, Step 48600, ETA 5m 42.58s] step time: 0.005721s (±0.005162s); valid time: 0.05121s; loss: -29.0659 (±86.7051); valid loss: 31.4787\n",
      "[Epoch 168/350, Step 48700, ETA 5m 41.92s] step time: 0.006066s (±0.005437s); valid time: 0.05379s; loss: -34.8559 (±20.6617); valid loss: 20.0798\n",
      "[Epoch 168/350, Step 48800, ETA 5m 41.23s] step time: 0.005797s (±0.005189s); valid time: 0.05145s; loss: -38.1601 (±4.98015); valid loss: 0.8675\n",
      "[Epoch 169/350, Step 48900, ETA 5m 40.62s] step time: 0.005811s (±0.005359s); valid time: 0.0527s; loss: -37.9927 (±3.32289); valid loss: 8.14963\n",
      "[Epoch 169/350, Step 49000, ETA 5m 39.94s] step time: 0.005845s (±0.005696s); valid time: 0.05622s; loss: -38.0577 (±2.64142); valid loss: 122.091\n",
      "[Epoch 169/350, Step 49100, ETA 5m 39.26s] step time: 0.005777s (±0.006124s); valid time: 0.06083s; loss: -37.9508 (±3.15935); valid loss: 238.297\n",
      "[Epoch 170/350, Step 49200, ETA 5m 38.64s] step time: 0.005703s (±0.005724s); valid time: 0.05698s; loss: -37.383 (±5.03367); valid loss: 10.4639\n",
      "[Epoch 170/350, Step 49300, ETA 5m 37.96s] step time: 0.005917s (±0.005088s); valid time: 0.05023s; loss: -38.1002 (±2.83913); valid loss: 8.99544\n",
      "[Epoch 170/350, Step 49400, ETA 5m 37.28s] step time: 0.005774s (±0.005018s); valid time: 0.04974s; loss: -38.2886 (±2.64076); valid loss: 8.37236\n",
      "[Epoch 170/350, Step 49470, ETA 5m 36.78s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 171/350, Step 49500, ETA 5m 36.69s] step time: 0.005991s (±0.005428s); valid time: 0.05107s; loss: -37.1569 (±5.60829); valid loss: 113.793\n",
      "[Epoch 171/350, Step 49600, ETA 5m 36.02s] step time: 0.005889s (±0.005403s); valid time: 0.05338s; loss: -38.0556 (±2.88894); valid loss: 61.0854\n",
      "[Epoch 171/350, Step 49700, ETA 5m 35.34s] step time: 0.005861s (±0.005201s); valid time: 0.05129s; loss: -25.1686 (±127.81); valid loss: 137.008\n",
      "[Epoch 172/350, Step 49800, ETA 5m 34.75s] step time: 0.005963s (±0.005489s); valid time: 0.05092s; loss: -38.6164 (±2.00741); valid loss: 46.1176\n",
      "[Epoch 172/350, Step 49900, ETA 5m 34.08s] step time: 0.005953s (±0.005314s); valid time: 0.05018s; loss: -20.2762 (±172.946); valid loss: 11.4769\n",
      "[Epoch 172/350, Step 50000, ETA 5m 33.42s] step time: 0.005995s (±0.00528s); valid time: 0.05191s; loss: -29.6213 (±83.6882); valid loss: 97.7912\n",
      "[Epoch 173/350, Step 50100, ETA 5m 32.83s] step time: 0.005967s (±0.004985s); valid time: 0.04964s; loss: -36.017 (±11.5647); valid loss: 52.1644\n",
      "[Epoch 173/350, Step 50200, ETA 5m 32.14s] step time: 0.005714s (±0.005085s); valid time: 0.05031s; loss: -38.4712 (±2.69765); valid loss: 381.776\n",
      "[Epoch 173/350, Step 50300, ETA 5m 31.48s] step time: 0.006009s (±0.005413s); valid time: 0.05352s; loss: -38.1613 (±2.88936); valid loss: 52.6807\n",
      "[Epoch 174/350, Step 50400, ETA 5m 30.88s] step time: 0.00596s (±0.005481s); valid time: 0.05345s; loss: -37.5102 (±4.32041); valid loss: 25.5841\n",
      "[Epoch 174/350, Step 50500, ETA 5m 30.19s] step time: 0.005733s (±0.004976s); valid time: 0.04992s; loss: -36.785 (±9.10954); valid loss: 151.946\n",
      "[Epoch 174/350, Step 50600, ETA 5m 29.55s] step time: 0.00611s (±0.005413s); valid time: 0.05297s; loss: -38.0959 (±3.08864); valid loss: 9.85732\n",
      "[Epoch 175/350, Step 50700, ETA 5m 28.94s] step time: 0.005722s (±0.005708s); valid time: 0.0571s; loss: -27.9448 (±104.938); valid loss: 9.17196\n",
      "[Epoch 175/350, Step 50800, ETA 5m 28.27s] step time: 0.005937s (±0.005333s); valid time: 0.05329s; loss: -36.4743 (±15.6066); valid loss: 1.73861\n",
      "[Epoch 175/350, Step 50900, ETA 5m 27.58s] step time: 0.005803s (±0.005094s); valid time: 0.05041s; loss: -38.1112 (±3.71928); valid loss: 3080.99\n",
      "[Epoch 176/350, Step 51000, ETA 5m 26.98s] step time: 0.00579s (±0.005067s); valid time: 0.05052s; loss: -37.8604 (±2.94448); valid loss: 2221.18\n",
      "[Epoch 176/350, Step 51100, ETA 5m 26.31s] step time: 0.00589s (±0.00508s); valid time: 0.05012s; loss: -35.3257 (±27.8511); valid loss: 49.3345\n",
      "[Epoch 176/350, Step 51200, ETA 5m 25.62s] step time: 0.005763s (±0.005145s); valid time: 0.05022s; loss: -37.9901 (±3.27606); valid loss: 353.145\n",
      "[Epoch 177/350, Step 51300, ETA 5m 25.01s] step time: 0.005777s (±0.00505s); valid time: 0.04957s; loss: -36.6609 (±11.0862); valid loss: 2735.81\n",
      "[Epoch 177/350, Step 51400, ETA 5m 24.33s] step time: 0.005901s (±0.005409s); valid time: 0.05406s; loss: -36.7087 (±9.52035); valid loss: 4.55402\n",
      "[Epoch 177/350, Step 51500, ETA 5m 23.66s] step time: 0.005893s (±0.005988s); valid time: 0.05983s; loss: -38.0992 (±3.2636); valid loss: 262.319\n",
      "[Epoch 178/350, Step 51600, ETA 5m 23.08s] step time: 0.006071s (±0.005174s); valid time: 0.04957s; loss: -38.28 (±2.48807); valid loss: 76.0467\n",
      "[Epoch 178/350, Step 51700, ETA 5m 22.41s] step time: 0.005851s (±0.005395s); valid time: 0.05343s; loss: 66.5091 (±1038.22); valid loss: 139.145\n",
      "[Epoch 179/350, Step 51800, ETA 5m 21.8s] step time: 0.005871s (±0.005994s); valid time: 0.05979s; loss: -37.8301 (±3.52261); valid loss: 150.397\n",
      "[Epoch 179/350, Step 51900, ETA 5m 21.12s] step time: 0.00577s (±0.005004s); valid time: 0.04974s; loss: -38.5329 (±2.16743); valid loss: 390.18\n",
      "[Epoch 179/350, Step 52000, ETA 5m 20.47s] step time: 0.006095s (±0.005825s); valid time: 0.05625s; loss: -35.6959 (±21.398); valid loss: 72.5467\n",
      "[Epoch 180/350, Step 52100, ETA 5m 19.85s] step time: 0.005721s (±0.005235s); valid time: 0.05217s; loss: -37.7319 (±6.55279); valid loss: 19.8481\n",
      "[Epoch 180/350, Step 52200, ETA 5m 19.19s] step time: 0.006018s (±0.005096s); valid time: 0.04987s; loss: -38.2951 (±2.92485); valid loss: 2647.45\n",
      "[Epoch 180/350, Step 52300, ETA 5m 18.5s] step time: 0.005733s (±0.005078s); valid time: 0.04983s; loss: -37.2538 (±5.56977); valid loss: 109.256\n",
      "[Epoch 180/350, Step 52380, ETA 5m 17.91s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 52400, ETA 5m 17.88s] step time: 0.005731s (±0.005927s); valid time: 0.05848s; loss: -37.8797 (±4.13149); valid loss: 1601.54\n",
      "[Epoch 181/350, Step 52500, ETA 5m 17.23s] step time: 0.00604s (±0.005406s); valid time: 0.05214s; loss: -37.8494 (±3.58576); valid loss: 701.516\n",
      "[Epoch 181/350, Step 52600, ETA 5m 16.54s] step time: 0.005674s (±0.005172s); valid time: 0.05038s; loss: -38.2096 (±2.59171); valid loss: 24.336\n",
      "[Epoch 182/350, Step 52700, ETA 5m 15.94s] step time: 0.005981s (±0.005082s); valid time: 0.05007s; loss: -37.1783 (±11.7101); valid loss: 662.311\n",
      "[Epoch 182/350, Step 52800, ETA 5m 15.28s] step time: 0.005949s (±0.005176s); valid time: 0.05021s; loss: -37.8167 (±3.06287); valid loss: 168.035\n",
      "[Epoch 182/350, Step 52900, ETA 5m 14.59s] step time: 0.005752s (±0.005103s); valid time: 0.05073s; loss: -38.3381 (±2.60559); valid loss: 126.19\n",
      "[Epoch 183/350, Step 53000, ETA 5m 13.99s] step time: 0.005951s (±0.005181s); valid time: 0.0508s; loss: -38.2605 (±2.49181); valid loss: 9.48598\n",
      "[Epoch 183/350, Step 53100, ETA 5m 13.32s] step time: 0.005859s (±0.005145s); valid time: 0.0503s; loss: -37.1862 (±10.0834); valid loss: 36.1875\n",
      "[Epoch 183/350, Step 53200, ETA 5m 12.67s] step time: 0.006064s (±0.006105s); valid time: 0.05992s; loss: -38.2156 (±3.01066); valid loss: 10995.4\n",
      "[Epoch 184/350, Step 53300, ETA 5m 12.06s] step time: 0.005894s (±0.005138s); valid time: 0.05118s; loss: -37.3368 (±6.90622); valid loss: 1015.72\n",
      "[Epoch 184/350, Step 53400, ETA 5m 11.38s] step time: 0.005787s (±0.005074s); valid time: 0.05042s; loss: -38.3442 (±2.65769); valid loss: 3146.03\n",
      "[Epoch 184/350, Step 53500, ETA 5m 10.72s] step time: 0.005905s (±0.005264s); valid time: 0.05124s; loss: -37.4495 (±5.16514); valid loss: 41.1803\n",
      "[Epoch 185/350, Step 53600, ETA 5m 10.13s] step time: 0.006174s (±0.005348s); valid time: 0.05203s; loss: -34.2095 (±41.3218); valid loss: -8.28921\n",
      "[Epoch 185/350, Step 53700, ETA 5m 9.455s] step time: 0.005778s (±0.005175s); valid time: 0.05084s; loss: -37.236 (±10.5396); valid loss: 392.187\n",
      "[Epoch 185/350, Step 53800, ETA 5m 8.78s] step time: 0.00583s (±0.005098s); valid time: 0.04952s; loss: -37.9138 (±3.22265); valid loss: 291.233\n",
      "[Epoch 186/350, Step 53900, ETA 5m 8.176s] step time: 0.005795s (±0.004984s); valid time: 0.04936s; loss: -38.4774 (±2.62293); valid loss: 11.8076\n",
      "[Epoch 186/350, Step 54000, ETA 5m 7.493s] step time: 0.005716s (±0.005149s); valid time: 0.05154s; loss: -38.1805 (±2.60743); valid loss: 154.837\n",
      "[Epoch 186/350, Step 54100, ETA 5m 6.827s] step time: 0.005924s (±0.005183s); valid time: 0.0513s; loss: -38.1075 (±3.24854); valid loss: 3.89069\n",
      "[Epoch 187/350, Step 54200, ETA 5m 6.212s] step time: 0.005772s (±0.005844s); valid time: 0.05837s; loss: -38.4738 (±3.05615); valid loss: 23.4061\n",
      "[Epoch 187/350, Step 54300, ETA 5m 5.533s] step time: 0.00577s (±0.005063s); valid time: 0.05028s; loss: -37.9416 (±3.87851); valid loss: 930.865\n",
      "[Epoch 187/350, Step 54400, ETA 5m 4.869s] step time: 0.005941s (±0.005675s); valid time: 0.05185s; loss: -35.9286 (±14.3805); valid loss: 49.7721\n",
      "[Epoch 188/350, Step 54500, ETA 5m 4.249s] step time: 0.00575s (±0.005343s); valid time: 0.05096s; loss: -38.14 (±2.31265); valid loss: 31.4661\n",
      "[Epoch 188/350, Step 54600, ETA 5m 3.587s] step time: 0.005956s (±0.005385s); valid time: 0.05217s; loss: -36.8444 (±6.99616); valid loss: 62.515\n",
      "[Epoch 188/350, Step 54700, ETA 5m 2.91s] step time: 0.005787s (±0.005114s); valid time: 0.04983s; loss: -38.5468 (±3.14573); valid loss: 3.04021\n",
      "[Epoch 189/350, Step 54800, ETA 5m 2.312s] step time: 0.005991s (±0.005919s); valid time: 0.0538s; loss: -36.7949 (±11.1332); valid loss: -6.34582\n",
      "[Epoch 189/350, Step 54900, ETA 5m 1.66s] step time: 0.006061s (±0.005055s); valid time: 0.0501s; loss: -38.0772 (±2.81257); valid loss: 81.0555\n",
      "[Epoch 190/350, Step 55000, ETA 5m 1.041s] step time: 0.005762s (±0.00587s); valid time: 0.05829s; loss: -37.6786 (±4.71071); valid loss: 144.094\n",
      "[Epoch 190/350, Step 55100, ETA 5m 0.3611s] step time: 0.005749s (±0.005335s); valid time: 0.05212s; loss: -38.0946 (±3.2544); valid loss: 897.203\n",
      "[Epoch 190/350, Step 55200, ETA 4m 59.68s] step time: 0.005753s (±0.005062s); valid time: 0.04932s; loss: -36.9045 (±12.2146); valid loss: 18.7655\n",
      "[Epoch 190/350, Step 55290, ETA 4m 59.03s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 55300, ETA 4m 59.06s] step time: 0.005694s (±0.005108s); valid time: 0.05056s; loss: -38.2641 (±2.55956); valid loss: 152.672\n",
      "[Epoch 191/350, Step 55400, ETA 4m 58.4s] step time: 0.005918s (±0.005032s); valid time: 0.04946s; loss: -38.302 (±2.60069); valid loss: 7.60982\n",
      "[Epoch 191/350, Step 55500, ETA 4m 57.72s] step time: 0.005807s (±0.00509s); valid time: 0.0497s; loss: -38.1487 (±3.2856); valid loss: 20.6352\n",
      "[Epoch 192/350, Step 55600, ETA 4m 57.1s] step time: 0.005697s (±0.005004s); valid time: 0.04983s; loss: -38.0906 (±3.44938); valid loss: 86.0098\n",
      "[Epoch 192/350, Step 55700, ETA 4m 56.44s] step time: 0.005945s (±0.005227s); valid time: 0.05115s; loss: -37.6645 (±3.60362); valid loss: 59.0837\n",
      "[Epoch 192/350, Step 55800, ETA 4m 55.76s] step time: 0.005788s (±0.00509s); valid time: 0.05067s; loss: -38.0368 (±3.255); valid loss: 103.126\n",
      "[Epoch 193/350, Step 55900, ETA 4m 55.16s] step time: 0.005972s (±0.005772s); valid time: 0.05665s; loss: -38.0378 (±3.72016); valid loss: 5074.9\n",
      "[Epoch 193/350, Step 56000, ETA 4m 54.49s] step time: 0.005861s (±0.005385s); valid time: 0.05106s; loss: -36.5853 (±11.7971); valid loss: 61.5747\n",
      "[Epoch 193/350, Step 56100, ETA 4m 53.81s] step time: 0.005729s (±0.005066s); valid time: 0.05072s; loss: -38.2232 (±2.4862); valid loss: 14.1784\n",
      "[Epoch 194/350, Step 56200, ETA 4m 53.22s] step time: 0.006026s (±0.005566s); valid time: 0.05254s; loss: -38.6516 (±2.24098); valid loss: 301.141\n",
      "[Epoch 194/350, Step 56300, ETA 4m 52.55s] step time: 0.005882s (±0.005483s); valid time: 0.05411s; loss: -37.3515 (±7.15947); valid loss: 15.2741\n",
      "[Epoch 194/350, Step 56400, ETA 4m 51.87s] step time: 0.00576s (±0.005154s); valid time: 0.04995s; loss: -37.6269 (±3.4917); valid loss: 135.087\n",
      "[Epoch 195/350, Step 56500, ETA 4m 51.29s] step time: 0.006109s (±0.005266s); valid time: 0.0515s; loss: -38.4299 (±2.54225); valid loss: 17.7765\n",
      "[Epoch 195/350, Step 56600, ETA 4m 50.61s] step time: 0.005767s (±0.005043s); valid time: 0.0501s; loss: -36.5527 (±13.664); valid loss: 56.798\n",
      "[Epoch 195/350, Step 56700, ETA 4m 49.96s] step time: 0.005976s (±0.005315s); valid time: 0.0517s; loss: -38.1937 (±2.95657); valid loss: 217.493\n",
      "[Epoch 196/350, Step 56800, ETA 4m 49.37s] step time: 0.006137s (±0.005999s); valid time: 0.05906s; loss: -38.122 (±3.31152); valid loss: 1257.73\n",
      "[Epoch 196/350, Step 56900, ETA 4m 48.69s] step time: 0.005771s (±0.005118s); valid time: 0.05059s; loss: -32.1558 (±46.9603); valid loss: 106.355\n",
      "[Epoch 196/350, Step 57000, ETA 4m 48.03s] step time: 0.005882s (±0.005183s); valid time: 0.05095s; loss: -38.1298 (±3.85246); valid loss: 13.9505\n",
      "[Epoch 197/350, Step 57100, ETA 4m 47.41s] step time: 0.005742s (±0.004945s); valid time: 0.04917s; loss: -37.4566 (±8.80235); valid loss: 85.695\n",
      "[Epoch 197/350, Step 57200, ETA 4m 46.73s] step time: 0.005736s (±0.005202s); valid time: 0.05096s; loss: -37.8321 (±4.4601); valid loss: 1155.55\n",
      "[Epoch 197/350, Step 57300, ETA 4m 46.06s] step time: 0.005852s (±0.005096s); valid time: 0.0507s; loss: -37.5766 (±6.425); valid loss: 24.4987\n",
      "[Epoch 198/350, Step 57400, ETA 4m 45.45s] step time: 0.005801s (±0.005214s); valid time: 0.05203s; loss: -38.4794 (±2.61711); valid loss: 1063.67\n",
      "[Epoch 198/350, Step 57500, ETA 4m 44.78s] step time: 0.005855s (±0.005181s); valid time: 0.05044s; loss: -37.8992 (±3.46434); valid loss: 149.892\n",
      "[Epoch 198/350, Step 57600, ETA 4m 44.17s] step time: 0.006552s (±0.00597s); valid time: 0.05806s; loss: -37.0027 (±8.40564); valid loss: 256.149\n",
      "[Epoch 199/350, Step 57700, ETA 4m 43.56s] step time: 0.005839s (±0.005158s); valid time: 0.05117s; loss: -38.1925 (±3.49282); valid loss: 214.446\n",
      "[Epoch 199/350, Step 57800, ETA 4m 42.9s] step time: 0.00601s (±0.005516s); valid time: 0.05419s; loss: -38.0517 (±2.67337); valid loss: 160.26\n",
      "[Epoch 199/350, Step 57900, ETA 4m 42.24s] step time: 0.005931s (±0.005202s); valid time: 0.05054s; loss: -29.2298 (±88.0778); valid loss: 53.8141\n",
      "[Epoch 200/350, Step 58000, ETA 4m 41.63s] step time: 0.005877s (±0.005043s); valid time: 0.05044s; loss: -38.2348 (±2.82732); valid loss: 228.79\n",
      "[Epoch 200/350, Step 58100, ETA 4m 40.97s] step time: 0.005962s (±0.005317s); valid time: 0.05191s; loss: -37.0082 (±8.8592); valid loss: 7.4517\n",
      "[Epoch 200/350, Step 58200, ETA 4m 40.32s] step time: 0.005985s (±0.005947s); valid time: 0.0595s; loss: -37.5075 (±7.46072); valid loss: 40.5498\n",
      "[Epoch 200/350, Step 58200, ETA 4m 40.32s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 201/350, Step 58300, ETA 4m 39.68s] step time: 0.005667s (±0.004999s); valid time: 0.04909s; loss: -37.5283 (±6.17729); valid loss: 401.364\n",
      "[Epoch 201/350, Step 58400, ETA 4m 39.02s] step time: 0.005798s (±0.005572s); valid time: 0.05531s; loss: -37.9083 (±4.04852); valid loss: 8.65055\n",
      "[Epoch 202/350, Step 58500, ETA 4m 38.4s] step time: 0.00589s (±0.00505s); valid time: 0.05042s; loss: -37.7709 (±4.30728); valid loss: 96.3339\n",
      "[Epoch 202/350, Step 58600, ETA 4m 37.74s] step time: 0.005949s (±0.005188s); valid time: 0.05033s; loss: -35.0245 (±33.2063); valid loss: 38.2818\n",
      "[Epoch 202/350, Step 58700, ETA 4m 37.08s] step time: 0.005823s (±0.0052s); valid time: 0.05076s; loss: -37.2527 (±8.97512); valid loss: -5.88693\n",
      "[Epoch 203/350, Step 58800, ETA 4m 36.45s] step time: 0.005691s (±0.005094s); valid time: 0.05069s; loss: -38.6328 (±2.28809); valid loss: 599.837\n",
      "[Epoch 203/350, Step 58900, ETA 4m 35.78s] step time: 0.005814s (±0.005142s); valid time: 0.05062s; loss: -38.0385 (±3.09051); valid loss: 161.018\n",
      "[Epoch 203/350, Step 59000, ETA 4m 35.11s] step time: 0.005759s (±0.005927s); valid time: 0.05866s; loss: -37.6961 (±3.6898); valid loss: 225.706\n",
      "[Epoch 204/350, Step 59100, ETA 4m 34.49s] step time: 0.005774s (±0.005201s); valid time: 0.05201s; loss: -37.9696 (±3.21757); valid loss: 72.6469\n",
      "[Epoch 204/350, Step 59200, ETA 4m 33.83s] step time: 0.005922s (±0.005376s); valid time: 0.05322s; loss: -38.0826 (±2.88415); valid loss: 49.3185\n",
      "[Epoch 204/350, Step 59300, ETA 4m 33.18s] step time: 0.006046s (±0.005746s); valid time: 0.0567s; loss: -38.3228 (±2.75104); valid loss: 223.62\n",
      "[Epoch 205/350, Step 59400, ETA 4m 32.57s] step time: 0.005968s (±0.005333s); valid time: 0.05272s; loss: -37.5423 (±7.89408); valid loss: 5.12389\n",
      "[Epoch 205/350, Step 59500, ETA 4m 31.91s] step time: 0.00594s (±0.005324s); valid time: 0.05256s; loss: 219.402 (±2562.77); valid loss: 136.885\n",
      "[Epoch 205/350, Step 59600, ETA 4m 31.24s] step time: 0.005788s (±0.005398s); valid time: 0.05359s; loss: -37.2468 (±8.14584); valid loss: 56.0676\n",
      "[Epoch 206/350, Step 59700, ETA 4m 30.64s] step time: 0.006054s (±0.005321s); valid time: 0.05037s; loss: -38.1528 (±2.57592); valid loss: 19.1237\n",
      "[Epoch 206/350, Step 59800, ETA 4m 29.99s] step time: 0.006089s (±0.005438s); valid time: 0.0541s; loss: -38.5429 (±2.17561); valid loss: 9.96493\n",
      "[Epoch 206/350, Step 59900, ETA 4m 29.33s] step time: 0.005956s (±0.005285s); valid time: 0.05171s; loss: -37.4521 (±7.17572); valid loss: 162.041\n",
      "[Epoch 207/350, Step 60000, ETA 4m 28.73s] step time: 0.005889s (±0.005408s); valid time: 0.05259s; loss: -36.3322 (±7.15196); valid loss: -5.27891\n",
      "[Epoch 207/350, Step 60100, ETA 4m 28.08s] step time: 0.00605s (±0.005213s); valid time: 0.05126s; loss: -38.4508 (±2.91381); valid loss: 21.0949\n",
      "[Epoch 207/350, Step 60200, ETA 4m 27.42s] step time: 0.005952s (±0.00525s); valid time: 0.05101s; loss: -37.6814 (±6.75225); valid loss: 96.056\n",
      "[Epoch 208/350, Step 60300, ETA 4m 26.81s] step time: 0.005948s (±0.005243s); valid time: 0.05131s; loss: -38.6722 (±2.4543); valid loss: 178.063\n",
      "[Epoch 208/350, Step 60400, ETA 4m 26.14s] step time: 0.005828s (±0.005273s); valid time: 0.05197s; loss: -37.9176 (±3.23073); valid loss: 49.9196\n",
      "[Epoch 208/350, Step 60500, ETA 4m 25.49s] step time: 0.005912s (±0.005459s); valid time: 0.05483s; loss: -37.527 (±3.91944); valid loss: 85.975\n",
      "[Epoch 209/350, Step 60600, ETA 4m 24.88s] step time: 0.005854s (±0.005133s); valid time: 0.05111s; loss: -37.1739 (±6.77132); valid loss: -6.48201\n",
      "[Epoch 209/350, Step 60700, ETA 4m 24.21s] step time: 0.005816s (±0.005086s); valid time: 0.05086s; loss: -7.74695 (±295.56); valid loss: 256.065\n",
      "[Epoch 209/350, Step 60800, ETA 4m 23.56s] step time: 0.006023s (±0.005195s); valid time: 0.05153s; loss: -38.0587 (±3.33491); valid loss: -7.12089\n",
      "[Epoch 210/350, Step 60900, ETA 4m 22.95s] step time: 0.00597s (±0.005494s); valid time: 0.04977s; loss: -38.5684 (±2.36474); valid loss: 1.36972\n",
      "[Epoch 210/350, Step 61000, ETA 4m 22.28s] step time: 0.005738s (±0.005368s); valid time: 0.05295s; loss: -37.6334 (±5.15804); valid loss: 91.5762\n",
      "[Epoch 210/350, Step 61100, ETA 4m 21.62s] step time: 0.005876s (±0.005101s); valid time: 0.05055s; loss: -37.9871 (±3.04231); valid loss: -4.45355\n",
      "[Epoch 210/350, Step 61110, ETA 4m 21.55s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 61200, ETA 4m 20.99s] step time: 0.00578s (±0.005175s); valid time: 0.05108s; loss: -38.3791 (±2.72983); valid loss: 26.431\n",
      "[Epoch 211/350, Step 61300, ETA 4m 20.34s] step time: 0.00598s (±0.005608s); valid time: 0.05137s; loss: -37.9978 (±3.7234); valid loss: 14.2881\n",
      "[Epoch 211/350, Step 61400, ETA 4m 19.67s] step time: 0.005824s (±0.0051s); valid time: 0.05098s; loss: -37.7965 (±4.11246); valid loss: 15.1957\n",
      "[Epoch 212/350, Step 61500, ETA 4m 19.05s] step time: 0.00573s (±0.004904s); valid time: 0.04875s; loss: -38.1363 (±3.47709); valid loss: 201.148\n",
      "[Epoch 212/350, Step 61600, ETA 4m 18.39s] step time: 0.005977s (±0.005481s); valid time: 0.05336s; loss: -37.9862 (±2.7134); valid loss: 8.57574\n",
      "[Epoch 213/350, Step 61700, ETA 4m 17.77s] step time: 0.005819s (±0.006204s); valid time: 0.06221s; loss: -38.5165 (±2.42023); valid loss: 1497.02\n",
      "[Epoch 213/350, Step 61800, ETA 4m 17.11s] step time: 0.005794s (±0.005219s); valid time: 0.05185s; loss: -37.1148 (±14.7558); valid loss: 12.9312\n",
      "[Epoch 213/350, Step 61900, ETA 4m 16.44s] step time: 0.005841s (±0.00518s); valid time: 0.05043s; loss: -37.3964 (±7.90999); valid loss: 5.23907\n",
      "[Epoch 214/350, Step 62000, ETA 4m 15.82s] step time: 0.005765s (±0.005183s); valid time: 0.0517s; loss: -37.728 (±4.03493); valid loss: 5.52313\n",
      "[Epoch 214/350, Step 62100, ETA 4m 15.16s] step time: 0.005942s (±0.005448s); valid time: 0.05332s; loss: -38.4293 (±2.90244); valid loss: 130.924\n",
      "[Epoch 214/350, Step 62200, ETA 4m 14.5s] step time: 0.005889s (±0.005146s); valid time: 0.05058s; loss: -37.4661 (±2.95075); valid loss: 98.5176\n",
      "[Epoch 215/350, Step 62300, ETA 4m 13.87s] step time: 0.005706s (±0.005138s); valid time: 0.05137s; loss: -29.9926 (±81.0715); valid loss: 122.968\n",
      "[Epoch 215/350, Step 62400, ETA 4m 13.21s] step time: 0.005808s (±0.005096s); valid time: 0.05019s; loss: -36.8233 (±12.3146); valid loss: -4.55872\n",
      "[Epoch 215/350, Step 62500, ETA 4m 12.54s] step time: 0.005731s (±0.005121s); valid time: 0.05102s; loss: -38.3234 (±2.72095); valid loss: 338.677\n",
      "[Epoch 216/350, Step 62600, ETA 4m 11.92s] step time: 0.005824s (±0.005174s); valid time: 0.05109s; loss: -38.1379 (±3.54627); valid loss: 1299.11\n",
      "[Epoch 216/350, Step 62700, ETA 4m 11.25s] step time: 0.005776s (±0.005037s); valid time: 0.04988s; loss: -38.0114 (±3.14084); valid loss: -0.501454\n",
      "[Epoch 216/350, Step 62800, ETA 4m 10.58s] step time: 0.005734s (±0.005185s); valid time: 0.05134s; loss: -36.6366 (±15.185); valid loss: 89.0373\n",
      "[Epoch 217/350, Step 62900, ETA 4m 9.966s] step time: 0.005871s (±0.005215s); valid time: 0.05081s; loss: -22.2468 (±127.739); valid loss: 120.585\n",
      "[Epoch 217/350, Step 63000, ETA 4m 9.332s] step time: 0.006267s (±0.005215s); valid time: 0.05069s; loss: -38.6709 (±2.17266); valid loss: 239.363\n",
      "[Epoch 217/350, Step 63100, ETA 4m 8.663s] step time: 0.005729s (±0.005205s); valid time: 0.0511s; loss: -38.5083 (±2.17201); valid loss: 61.6331\n",
      "[Epoch 218/350, Step 63200, ETA 4m 8.047s] step time: 0.005837s (±0.005097s); valid time: 0.05038s; loss: -38.3539 (±2.14262); valid loss: 6.07707\n",
      "[Epoch 218/350, Step 63300, ETA 4m 7.378s] step time: 0.005747s (±0.005438s); valid time: 0.05428s; loss: -37.0469 (±11.3013); valid loss: 5.83977\n",
      "[Epoch 218/350, Step 63400, ETA 4m 6.713s] step time: 0.005785s (±0.004996s); valid time: 0.04933s; loss: -38.0307 (±3.06178); valid loss: 132.941\n",
      "[Epoch 219/350, Step 63500, ETA 4m 6.096s] step time: 0.005872s (±0.005128s); valid time: 0.05098s; loss: -33.7285 (±39.6126); valid loss: 839.153\n",
      "[Epoch 219/350, Step 63600, ETA 4m 5.43s] step time: 0.005767s (±0.005392s); valid time: 0.05344s; loss: -36.298 (±16.4319); valid loss: -0.227619\n",
      "[Epoch 219/350, Step 63700, ETA 4m 4.764s] step time: 0.005768s (±0.005038s); valid time: 0.04976s; loss: -38.1608 (±3.08475); valid loss: 153.501\n",
      "[Epoch 220/350, Step 63800, ETA 4m 4.142s] step time: 0.005754s (±0.00592s); valid time: 0.05952s; loss: -38.5474 (±2.30772); valid loss: 2.48688\n",
      "[Epoch 220/350, Step 63900, ETA 4m 3.48s] step time: 0.005839s (±0.005171s); valid time: 0.05111s; loss: -37.2668 (±5.64557); valid loss: 165.419\n",
      "[Epoch 220/350, Step 64000, ETA 4m 2.827s] step time: 0.005967s (±0.005301s); valid time: 0.05163s; loss: -38.5034 (±1.95726); valid loss: 192.914\n",
      "[Epoch 220/350, Step 64020, ETA 4m 2.688s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 64100, ETA 4m 2.207s] step time: 0.005781s (±0.005302s); valid time: 0.05268s; loss: -36.9703 (±11.8345); valid loss: 224.418\n",
      "[Epoch 221/350, Step 64200, ETA 4m 1.545s] step time: 0.005828s (±0.005138s); valid time: 0.05104s; loss: -38.2532 (±2.76746); valid loss: 31.6298\n",
      "[Epoch 221/350, Step 64300, ETA 4m 0.8806s] step time: 0.005783s (±0.005157s); valid time: 0.04948s; loss: -29.931 (±82.6134); valid loss: 1854.02\n",
      "[Epoch 222/350, Step 64400, ETA 4m 0.2502s] step time: 0.00567s (±0.005025s); valid time: 0.05003s; loss: -37.8983 (±3.46258); valid loss: 24.6477\n",
      "[Epoch 222/350, Step 64500, ETA 3m 59.59s] step time: 0.005842s (±0.005266s); valid time: 0.05157s; loss: -37.9878 (±2.83957); valid loss: 766.518\n",
      "[Epoch 222/350, Step 64600, ETA 3m 58.93s] step time: 0.005904s (±0.005278s); valid time: 0.05186s; loss: -36.7512 (±15.2002); valid loss: 10.7278\n",
      "[Epoch 223/350, Step 64700, ETA 3m 58.31s] step time: 0.005649s (±0.005147s); valid time: 0.04899s; loss: -38.1153 (±2.91489); valid loss: 3985.27\n",
      "[Epoch 223/350, Step 64800, ETA 3m 57.65s] step time: 0.005846s (±0.005123s); valid time: 0.05097s; loss: -38.0983 (±4.39909); valid loss: 111.54\n",
      "[Epoch 224/350, Step 64900, ETA 3m 57.02s] step time: 0.005745s (±0.005197s); valid time: 0.05164s; loss: -37.3945 (±7.60256); valid loss: 6.7443\n",
      "[Epoch 224/350, Step 65000, ETA 3m 56.37s] step time: 0.005958s (±0.005203s); valid time: 0.05104s; loss: -38.3459 (±2.58639); valid loss: 62.052\n",
      "[Epoch 224/350, Step 65100, ETA 3m 55.71s] step time: 0.005903s (±0.00524s); valid time: 0.05018s; loss: -37.7185 (±4.42815); valid loss: -3.37153\n",
      "[Epoch 225/350, Step 65200, ETA 3m 55.09s] step time: 0.005722s (±0.005097s); valid time: 0.05052s; loss: -37.9436 (±4.46447); valid loss: 19.2352\n",
      "[Epoch 225/350, Step 65300, ETA 3m 54.43s] step time: 0.005861s (±0.005157s); valid time: 0.05034s; loss: -37.1043 (±5.90376); valid loss: 11.0596\n",
      "[Epoch 225/350, Step 65400, ETA 3m 53.76s] step time: 0.005713s (±0.005238s); valid time: 0.05185s; loss: -36.6511 (±15.5186); valid loss: 54.4424\n",
      "[Epoch 226/350, Step 65500, ETA 3m 53.14s] step time: 0.005914s (±0.00582s); valid time: 0.0548s; loss: -37.4342 (±10.8903); valid loss: 13.577\n",
      "[Epoch 226/350, Step 65600, ETA 3m 52.48s] step time: 0.005867s (±0.005398s); valid time: 0.05322s; loss: -38.09 (±3.22598); valid loss: 142.342\n",
      "[Epoch 226/350, Step 65700, ETA 3m 51.82s] step time: 0.005706s (±0.005084s); valid time: 0.05052s; loss: -38.3288 (±3.12569); valid loss: 464.31\n",
      "[Epoch 227/350, Step 65800, ETA 3m 51.21s] step time: 0.006142s (±0.005209s); valid time: 0.05031s; loss: -37.5745 (±5.37745); valid loss: 34.2533\n",
      "[Epoch 227/350, Step 65900, ETA 3m 50.56s] step time: 0.005898s (±0.005425s); valid time: 0.0535s; loss: -38.1792 (±2.92151); valid loss: -7.67578\n",
      "[Epoch 227/350, Step 66000, ETA 3m 49.89s] step time: 0.005796s (±0.005238s); valid time: 0.05139s; loss: -37.8725 (±3.66743); valid loss: 27.6587\n",
      "[Epoch 228/350, Step 66100, ETA 3m 49.29s] step time: 0.006093s (±0.00542s); valid time: 0.05366s; loss: -37.3337 (±9.11147); valid loss: 96.0693\n",
      "[Epoch 228/350, Step 66200, ETA 3m 48.64s] step time: 0.005983s (±0.005222s); valid time: 0.05153s; loss: -37.7001 (±7.71373); valid loss: 237.631\n",
      "[Epoch 228/350, Step 66300, ETA 3m 47.98s] step time: 0.005877s (±0.004922s); valid time: 0.0485s; loss: -38.2419 (±2.70559); valid loss: 43.5565\n",
      "[Epoch 229/350, Step 66400, ETA 3m 47.36s] step time: 0.00586s (±0.005338s); valid time: 0.0516s; loss: -38.0007 (±3.42449); valid loss: 30.6336\n",
      "[Epoch 229/350, Step 66500, ETA 3m 46.71s] step time: 0.005912s (±0.005352s); valid time: 0.05283s; loss: -38.1971 (±2.918); valid loss: -0.415023\n",
      "[Epoch 229/350, Step 66600, ETA 3m 46.05s] step time: 0.005902s (±0.005038s); valid time: 0.04966s; loss: -37.995 (±2.77636); valid loss: 12815.9\n",
      "[Epoch 230/350, Step 66700, ETA 3m 45.43s] step time: 0.005907s (±0.005394s); valid time: 0.05396s; loss: -38.4224 (±2.9843); valid loss: 16132.2\n",
      "[Epoch 230/350, Step 66800, ETA 3m 44.77s] step time: 0.005853s (±0.005093s); valid time: 0.05033s; loss: -37.7996 (±3.0974); valid loss: 17.6666\n",
      "[Epoch 230/350, Step 66900, ETA 3m 44.11s] step time: 0.005785s (±0.00499s); valid time: 0.0495s; loss: -38.2472 (±2.89683); valid loss: 49.9308\n",
      "[Epoch 230/350, Step 66930, ETA 3m 43.9s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 67000, ETA 3m 43.48s] step time: 0.005636s (±0.005095s); valid time: 0.0509s; loss: 0.586918 (±350.742); valid loss: 105.663\n",
      "[Epoch 231/350, Step 67100, ETA 3m 42.83s] step time: 0.00583s (±0.005141s); valid time: 0.0501s; loss: -38.0319 (±4.05745); valid loss: 201.672\n",
      "[Epoch 231/350, Step 67200, ETA 3m 42.18s] step time: 0.006001s (±0.005276s); valid time: 0.05142s; loss: -38.0294 (±2.89724); valid loss: 31.0259\n",
      "[Epoch 232/350, Step 67300, ETA 3m 41.55s] step time: 0.005833s (±0.005107s); valid time: 0.05022s; loss: -18.1946 (±199.739); valid loss: 139.985\n",
      "[Epoch 232/350, Step 67400, ETA 3m 40.9s] step time: 0.00602s (±0.004911s); valid time: 0.04825s; loss: -37.8257 (±4.06535); valid loss: 43.2113\n",
      "[Epoch 232/350, Step 67500, ETA 3m 40.25s] step time: 0.005993s (±0.005249s); valid time: 0.05109s; loss: -38.12 (±2.67429); valid loss: 8.54832\n",
      "[Epoch 233/350, Step 67600, ETA 3m 39.63s] step time: 0.005811s (±0.005254s); valid time: 0.05188s; loss: -38.3856 (±2.7546); valid loss: 978.468\n",
      "[Epoch 233/350, Step 67700, ETA 3m 38.97s] step time: 0.005819s (±0.005234s); valid time: 0.05164s; loss: -37.969 (±3.09613); valid loss: 0.00108328\n",
      "[Epoch 233/350, Step 67800, ETA 3m 38.31s] step time: 0.005851s (±0.006462s); valid time: 0.06487s; loss: -38.0677 (±3.60411); valid loss: 62.9681\n",
      "[Epoch 234/350, Step 67900, ETA 3m 37.69s] step time: 0.005785s (±0.005067s); valid time: 0.04964s; loss: -38.1392 (±2.86462); valid loss: 342.589\n",
      "[Epoch 234/350, Step 68000, ETA 3m 37.03s] step time: 0.005874s (±0.005188s); valid time: 0.05134s; loss: -38.0499 (±4.45499); valid loss: 57.689\n",
      "[Epoch 235/350, Step 68100, ETA 3m 36.42s] step time: 0.005915s (±0.005223s); valid time: 0.05192s; loss: -37.7192 (±3.8874); valid loss: 160.575\n",
      "[Epoch 235/350, Step 68200, ETA 3m 35.76s] step time: 0.005926s (±0.005093s); valid time: 0.05001s; loss: -38.3301 (±3.57392); valid loss: 3571.61\n",
      "[Epoch 235/350, Step 68300, ETA 3m 35.1s] step time: 0.005751s (±0.005193s); valid time: 0.05096s; loss: -37.7796 (±3.8966); valid loss: 17.3476\n",
      "[Epoch 236/350, Step 68400, ETA 3m 34.47s] step time: 0.005784s (±0.005288s); valid time: 0.0524s; loss: -37.4198 (±6.2797); valid loss: 14.7811\n",
      "[Epoch 236/350, Step 68500, ETA 3m 33.82s] step time: 0.005917s (±0.005577s); valid time: 0.05506s; loss: -36.8166 (±10.7909); valid loss: 171.802\n",
      "[Epoch 236/350, Step 68600, ETA 3m 33.16s] step time: 0.005777s (±0.0052s); valid time: 0.05193s; loss: -38.2257 (±4.02327); valid loss: 118.698\n",
      "[Epoch 237/350, Step 68700, ETA 3m 32.54s] step time: 0.005925s (±0.005302s); valid time: 0.05222s; loss: -38.0066 (±3.10327); valid loss: 55.147\n",
      "[Epoch 237/350, Step 68800, ETA 3m 31.9s] step time: 0.006073s (±0.005286s); valid time: 0.05187s; loss: -37.5696 (±4.72523); valid loss: 1667.9\n",
      "[Epoch 237/350, Step 68900, ETA 3m 31.23s] step time: 0.005718s (±0.00514s); valid time: 0.0505s; loss: -37.7572 (±6.78516); valid loss: 4.5805\n",
      "[Epoch 238/350, Step 69000, ETA 3m 30.61s] step time: 0.005815s (±0.005197s); valid time: 0.05078s; loss: -38.3647 (±2.64275); valid loss: 60.1841\n",
      "[Epoch 238/350, Step 69100, ETA 3m 29.96s] step time: 0.005901s (±0.005303s); valid time: 0.05231s; loss: -19.4409 (±185.414); valid loss: 29.8455\n",
      "[Epoch 238/350, Step 69200, ETA 3m 29.29s] step time: 0.005714s (±0.005229s); valid time: 0.05097s; loss: 11.4792 (±474.626); valid loss: 42.5662\n",
      "[Epoch 239/350, Step 69300, ETA 3m 28.67s] step time: 0.005894s (±0.00524s); valid time: 0.05068s; loss: -37.4632 (±4.68746); valid loss: 326.503\n",
      "[Epoch 239/350, Step 69400, ETA 3m 28.01s] step time: 0.005796s (±0.005166s); valid time: 0.05018s; loss: -38.0952 (±3.04971); valid loss: 7.59873\n",
      "[Epoch 239/350, Step 69500, ETA 3m 27.38s] step time: 0.006278s (±0.005712s); valid time: 0.05547s; loss: -38.2007 (±3.1877); valid loss: 5.05915\n",
      "[Epoch 240/350, Step 69600, ETA 3m 26.76s] step time: 0.005953s (±0.00521s); valid time: 0.05069s; loss: -35.0146 (±28.183); valid loss: 1105.24\n",
      "[Epoch 240/350, Step 69700, ETA 3m 26.1s] step time: 0.005728s (±0.005141s); valid time: 0.05066s; loss: -37.4052 (±11.9012); valid loss: 63.4303\n",
      "[Epoch 240/350, Step 69800, ETA 3m 25.45s] step time: 0.005976s (±0.005378s); valid time: 0.0526s; loss: -37.9899 (±3.87838); valid loss: 85.8229\n",
      "[Epoch 240/350, Step 69840, ETA 3m 25.18s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 69900, ETA 3m 24.83s] step time: 0.006011s (±0.005478s); valid time: 0.05332s; loss: -34.0667 (±41.2076); valid loss: 2.75099\n",
      "[Epoch 241/350, Step 70000, ETA 3m 24.18s] step time: 0.005826s (±0.005363s); valid time: 0.05289s; loss: -35.5896 (±17.7894); valid loss: 96.0512\n",
      "[Epoch 241/350, Step 70100, ETA 3m 23.52s] step time: 0.005917s (±0.005346s); valid time: 0.05323s; loss: -38.2735 (±2.31438); valid loss: -6.08505\n",
      "[Epoch 242/350, Step 70200, ETA 3m 22.9s] step time: 0.005758s (±0.005648s); valid time: 0.05644s; loss: -38.2024 (±2.74985); valid loss: 4.69618\n",
      "[Epoch 242/350, Step 70300, ETA 3m 22.25s] step time: 0.006049s (±0.005463s); valid time: 0.05352s; loss: -37.6164 (±4.27724); valid loss: 466.333\n",
      "[Epoch 242/350, Step 70400, ETA 3m 21.6s] step time: 0.006014s (±0.005073s); valid time: 0.04998s; loss: -38.3755 (±2.61447); valid loss: 161.646\n",
      "[Epoch 243/350, Step 70500, ETA 3m 20.98s] step time: 0.005778s (±0.005082s); valid time: 0.05068s; loss: -38.2248 (±3.52371); valid loss: 1.6286\n",
      "[Epoch 243/350, Step 70600, ETA 3m 20.32s] step time: 0.005908s (±0.005233s); valid time: 0.05192s; loss: -38.328 (±2.39223); valid loss: 11.6184\n",
      "[Epoch 243/350, Step 70700, ETA 3m 19.68s] step time: 0.006009s (±0.005551s); valid time: 0.05511s; loss: -38.2967 (±2.50398); valid loss: 189.454\n",
      "[Epoch 244/350, Step 70800, ETA 3m 19.05s] step time: 0.00573s (±0.005178s); valid time: 0.05055s; loss: -36.5202 (±9.50407); valid loss: 1102.24\n",
      "[Epoch 244/350, Step 70900, ETA 3m 18.39s] step time: 0.005759s (±0.005526s); valid time: 0.0546s; loss: -38.453 (±2.71892); valid loss: 121.705\n",
      "[Epoch 244/350, Step 71000, ETA 3m 17.73s] step time: 0.005693s (±0.004968s); valid time: 0.04959s; loss: -38.2079 (±5.83181); valid loss: 22.3152\n",
      "[Epoch 245/350, Step 71100, ETA 3m 17.11s] step time: 0.005873s (±0.00517s); valid time: 0.05053s; loss: -38.5201 (±2.44057); valid loss: 51.7251\n",
      "[Epoch 245/350, Step 71200, ETA 3m 16.46s] step time: 0.005974s (±0.005024s); valid time: 0.04955s; loss: -38.1808 (±2.9666); valid loss: 317.447\n",
      "[Epoch 246/350, Step 71300, ETA 3m 15.83s] step time: 0.005887s (±0.005325s); valid time: 0.05296s; loss: -37.6334 (±3.6577); valid loss: 172.462\n",
      "[Epoch 246/350, Step 71400, ETA 3m 15.18s] step time: 0.005903s (±0.005236s); valid time: 0.05158s; loss: -38.6974 (±2.07902); valid loss: 143.598\n",
      "[Epoch 246/350, Step 71500, ETA 3m 14.53s] step time: 0.005879s (±0.005274s); valid time: 0.05131s; loss: -37.977 (±3.19787); valid loss: 26.153\n",
      "[Epoch 247/350, Step 71600, ETA 3m 13.9s] step time: 0.005791s (±0.005221s); valid time: 0.0519s; loss: -34.65 (±34.6595); valid loss: 26.3537\n",
      "[Epoch 247/350, Step 71700, ETA 3m 13.25s] step time: 0.005869s (±0.005076s); valid time: 0.04952s; loss: -37.5868 (±4.91581); valid loss: 52.9798\n",
      "[Epoch 247/350, Step 71800, ETA 3m 12.59s] step time: 0.005775s (±0.005148s); valid time: 0.05025s; loss: -38.0382 (±2.86269); valid loss: 321.846\n",
      "[Epoch 248/350, Step 71900, ETA 3m 11.97s] step time: 0.005897s (±0.005269s); valid time: 0.05188s; loss: -37.0421 (±9.73075); valid loss: 627.888\n",
      "[Epoch 248/350, Step 72000, ETA 3m 11.32s] step time: 0.005869s (±0.005345s); valid time: 0.05294s; loss: -36.5459 (±15.2898); valid loss: 21.7789\n",
      "[Epoch 248/350, Step 72100, ETA 3m 10.66s] step time: 0.005799s (±0.005088s); valid time: 0.05065s; loss: -37.6665 (±3.39103); valid loss: 4.38152\n",
      "[Epoch 249/350, Step 72200, ETA 3m 10.05s] step time: 0.0061s (±0.005554s); valid time: 0.05533s; loss: -38.3002 (±3.1172); valid loss: 129.58\n",
      "[Epoch 249/350, Step 72300, ETA 3m 9.397s] step time: 0.005985s (±0.005266s); valid time: 0.05216s; loss: -38.4444 (±2.37535); valid loss: 98.5879\n",
      "[Epoch 249/350, Step 72400, ETA 3m 8.747s] step time: 0.00592s (±0.005868s); valid time: 0.05835s; loss: -37.389 (±6.78851); valid loss: 94.1838\n",
      "[Epoch 250/350, Step 72500, ETA 3m 8.123s] step time: 0.005864s (±0.005149s); valid time: 0.05047s; loss: -24.073 (±137.969); valid loss: 41.3228\n",
      "[Epoch 250/350, Step 72600, ETA 3m 7.469s] step time: 0.005865s (±0.005745s); valid time: 0.05758s; loss: -37.9371 (±3.40804); valid loss: 1897.95\n",
      "[Epoch 250/350, Step 72700, ETA 3m 6.822s] step time: 0.005999s (±0.005146s); valid time: 0.05077s; loss: -38.2119 (±2.69058); valid loss: 18.0962\n",
      "[Epoch 250/350, Step 72750, ETA 3m 6.485s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 251/350, Step 72800, ETA 3m 6.201s] step time: 0.005968s (±0.005209s); valid time: 0.05074s; loss: -38.1725 (±2.90934); valid loss: 75.7793\n",
      "[Epoch 251/350, Step 72900, ETA 3m 5.547s] step time: 0.005844s (±0.00515s); valid time: 0.05135s; loss: -37.5996 (±7.613); valid loss: 8586.18\n",
      "[Epoch 251/350, Step 73000, ETA 3m 4.896s] step time: 0.00591s (±0.005207s); valid time: 0.05135s; loss: -37.8598 (±3.67673); valid loss: 10501.4\n",
      "[Epoch 252/350, Step 73100, ETA 3m 4.273s] step time: 0.005914s (±0.005126s); valid time: 0.05055s; loss: -37.755 (±3.86564); valid loss: 28.0196\n",
      "[Epoch 252/350, Step 73200, ETA 3m 3.615s] step time: 0.005736s (±0.005424s); valid time: 0.05366s; loss: -37.6066 (±9.56786); valid loss: 570.58\n",
      "[Epoch 252/350, Step 73300, ETA 3m 2.965s] step time: 0.005931s (±0.005247s); valid time: 0.05151s; loss: -38.5554 (±2.30791); valid loss: 33.3567\n",
      "[Epoch 253/350, Step 73400, ETA 3m 2.341s] step time: 0.005836s (±0.006145s); valid time: 0.06048s; loss: -38.4509 (±2.50983); valid loss: 777.196\n",
      "[Epoch 253/350, Step 73500, ETA 3m 1.696s] step time: 0.006052s (±0.005346s); valid time: 0.05287s; loss: -36.7395 (±16.0118); valid loss: 65.3799\n",
      "[Epoch 253/350, Step 73600, ETA 3m 1.044s] step time: 0.005876s (±0.005255s); valid time: 0.05205s; loss: -37.9828 (±2.99382); valid loss: 764.291\n",
      "[Epoch 254/350, Step 73700, ETA 3m 0.4135s] step time: 0.005755s (±0.004981s); valid time: 0.04901s; loss: -35.0177 (±28.5657); valid loss: 937.45\n",
      "[Epoch 254/350, Step 73800, ETA 2m 59.77s] step time: 0.006083s (±0.005292s); valid time: 0.0512s; loss: -38.0001 (±4.06913); valid loss: 39.4696\n",
      "[Epoch 254/350, Step 73900, ETA 2m 59.12s] step time: 0.005853s (±0.00535s); valid time: 0.05032s; loss: -38.3365 (±2.14993); valid loss: 399.353\n",
      "[Epoch 255/350, Step 74000, ETA 2m 58.49s] step time: 0.00579s (±0.005561s); valid time: 0.05494s; loss: -33.7987 (±43.8811); valid loss: 329.934\n",
      "[Epoch 255/350, Step 74100, ETA 2m 57.84s] step time: 0.005998s (±0.005293s); valid time: 0.051s; loss: -38.6399 (±2.32278); valid loss: 2832.77\n",
      "[Epoch 255/350, Step 74200, ETA 2m 57.19s] step time: 0.005927s (±0.005882s); valid time: 0.05905s; loss: -29.1006 (±85.8802); valid loss: 22.8805\n",
      "[Epoch 256/350, Step 74300, ETA 2m 56.57s] step time: 0.006034s (±0.005322s); valid time: 0.05111s; loss: -37.2061 (±9.90116); valid loss: 42.9508\n",
      "[Epoch 256/350, Step 74400, ETA 2m 55.92s] step time: 0.005864s (±0.005276s); valid time: 0.05173s; loss: -37.7178 (±6.07877); valid loss: 120.481\n",
      "[Epoch 257/350, Step 74500, ETA 2m 55.29s] step time: 0.005872s (±0.005257s); valid time: 0.05172s; loss: -37.8512 (±3.55775); valid loss: 18.3161\n",
      "[Epoch 257/350, Step 74600, ETA 2m 54.64s] step time: 0.005869s (±0.00523s); valid time: 0.05188s; loss: -37.8354 (±4.28023); valid loss: 1554.29\n",
      "[Epoch 257/350, Step 74700, ETA 2m 53.99s] step time: 0.005953s (±0.00513s); valid time: 0.05106s; loss: -36.1216 (±20.2185); valid loss: 209.794\n",
      "[Epoch 258/350, Step 74800, ETA 2m 53.37s] step time: 0.005879s (±0.005321s); valid time: 0.05239s; loss: -38.1756 (±2.58884); valid loss: 115.675\n",
      "[Epoch 258/350, Step 74900, ETA 2m 52.72s] step time: 0.005922s (±0.005058s); valid time: 0.04989s; loss: -36.8377 (±10.483); valid loss: 82.505\n",
      "[Epoch 258/350, Step 75000, ETA 2m 52.06s] step time: 0.005714s (±0.005747s); valid time: 0.05077s; loss: -38.3473 (±2.84091); valid loss: 2.88931\n",
      "[Epoch 259/350, Step 75100, ETA 2m 51.43s] step time: 0.00589s (±0.005194s); valid time: 0.05226s; loss: -38.0447 (±3.14628); valid loss: 98.8676\n",
      "[Epoch 259/350, Step 75200, ETA 2m 50.78s] step time: 0.005933s (±0.005371s); valid time: 0.05255s; loss: 7.48659 (±428.206); valid loss: 7.9558\n",
      "[Epoch 259/350, Step 75300, ETA 2m 50.13s] step time: 0.005862s (±0.005649s); valid time: 0.05338s; loss: -38.2747 (±2.60943); valid loss: 1430.79\n",
      "[Epoch 260/350, Step 75400, ETA 2m 49.51s] step time: 0.005829s (±0.005383s); valid time: 0.05371s; loss: -37.576 (±5.15652); valid loss: -0.129509\n",
      "[Epoch 260/350, Step 75500, ETA 2m 48.86s] step time: 0.005931s (±0.00526s); valid time: 0.05182s; loss: -38.4036 (±2.56196); valid loss: 16.2345\n",
      "[Epoch 260/350, Step 75600, ETA 2m 48.21s] step time: 0.006032s (±0.005294s); valid time: 0.05137s; loss: -35.4474 (±22.8149); valid loss: 1465.51\n",
      "[Epoch 260/350, Step 75660, ETA 2m 47.81s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 75700, ETA 2m 47.59s] step time: 0.005879s (±0.005092s); valid time: 0.04991s; loss: -37.5465 (±4.80155); valid loss: 193.6\n",
      "[Epoch 261/350, Step 75800, ETA 2m 46.93s] step time: 0.005885s (±0.005283s); valid time: 0.05152s; loss: -38.3527 (±3.32918); valid loss: 57.5429\n",
      "[Epoch 261/350, Step 75900, ETA 2m 46.28s] step time: 0.00584s (±0.004924s); valid time: 0.04774s; loss: -37.8264 (±3.3387); valid loss: 8.15215\n",
      "[Epoch 262/350, Step 76000, ETA 2m 45.66s] step time: 0.005758s (±0.005119s); valid time: 0.05023s; loss: -37.9655 (±2.72343); valid loss: 138.168\n",
      "[Epoch 262/350, Step 76100, ETA 2m 45s] step time: 0.005749s (±0.00551s); valid time: 0.05417s; loss: -38.311 (±4.26795); valid loss: 10.649\n",
      "[Epoch 262/350, Step 76200, ETA 2m 44.35s] step time: 0.005874s (±0.005252s); valid time: 0.05032s; loss: -38.1642 (±2.52323); valid loss: 13.5658\n",
      "[Epoch 263/350, Step 76300, ETA 2m 43.73s] step time: 0.005933s (±0.005357s); valid time: 0.05234s; loss: -36.8643 (±12.8092); valid loss: 83.157\n",
      "[Epoch 263/350, Step 76400, ETA 2m 43.07s] step time: 0.005753s (±0.005199s); valid time: 0.05102s; loss: -37.3837 (±9.78175); valid loss: 197.69\n",
      "[Epoch 263/350, Step 76500, ETA 2m 42.42s] step time: 0.005892s (±0.005131s); valid time: 0.05005s; loss: -38.3373 (±2.81652); valid loss: 29.8385\n",
      "[Epoch 264/350, Step 76600, ETA 2m 41.79s] step time: 0.005831s (±0.005141s); valid time: 0.05137s; loss: -37.8108 (±3.73848); valid loss: 593.876\n",
      "[Epoch 264/350, Step 76700, ETA 2m 41.14s] step time: 0.005833s (±0.005352s); valid time: 0.05235s; loss: -37.9517 (±7.10343); valid loss: 4.69096\n",
      "[Epoch 264/350, Step 76800, ETA 2m 40.5s] step time: 0.006093s (±0.005309s); valid time: 0.05263s; loss: -38.1893 (±2.68785); valid loss: 1561.44\n",
      "[Epoch 265/350, Step 76900, ETA 2m 39.87s] step time: 0.005862s (±0.005383s); valid time: 0.05344s; loss: -38.0685 (±3.2666); valid loss: 129.58\n",
      "[Epoch 265/350, Step 77000, ETA 2m 39.23s] step time: 0.005953s (±0.005231s); valid time: 0.05236s; loss: -38.1025 (±3.85998); valid loss: 16.1421\n",
      "[Epoch 265/350, Step 77100, ETA 2m 38.58s] step time: 0.006044s (±0.005343s); valid time: 0.05232s; loss: -37.0166 (±8.28579); valid loss: 263.607\n",
      "[Epoch 266/350, Step 77200, ETA 2m 37.95s] step time: 0.005775s (±0.00533s); valid time: 0.05289s; loss: -37.7365 (±5.75844); valid loss: 1268.07\n",
      "[Epoch 266/350, Step 77300, ETA 2m 37.3s] step time: 0.005924s (±0.005812s); valid time: 0.05721s; loss: -36.9542 (±14.7011); valid loss: 44.4138\n",
      "[Epoch 266/350, Step 77400, ETA 2m 36.65s] step time: 0.005833s (±0.005166s); valid time: 0.051s; loss: -35.0204 (±25.4101); valid loss: 12.7185\n",
      "[Epoch 267/350, Step 77500, ETA 2m 36.03s] step time: 0.006027s (±0.005506s); valid time: 0.05289s; loss: -23.5494 (±146.217); valid loss: 118.678\n",
      "[Epoch 267/350, Step 77600, ETA 2m 35.38s] step time: 0.005802s (±0.005s); valid time: 0.04966s; loss: -33.2162 (±48.6776); valid loss: 303.285\n",
      "[Epoch 268/350, Step 77700, ETA 2m 34.75s] step time: 0.005764s (±0.00635s); valid time: 0.06296s; loss: -37.9383 (±3.64889); valid loss: 381.809\n",
      "[Epoch 268/350, Step 77800, ETA 2m 34.09s] step time: 0.005795s (±0.005405s); valid time: 0.05417s; loss: -38.4684 (±2.80837); valid loss: 108.378\n",
      "[Epoch 268/350, Step 77900, ETA 2m 33.44s] step time: 0.005881s (±0.005081s); valid time: 0.04922s; loss: -37.7121 (±6.68444); valid loss: 9.1544\n",
      "[Epoch 269/350, Step 78000, ETA 2m 32.81s] step time: 0.005794s (±0.005332s); valid time: 0.05316s; loss: -37.6434 (±3.51869); valid loss: 93.905\n",
      "[Epoch 269/350, Step 78100, ETA 2m 32.17s] step time: 0.005956s (±0.005422s); valid time: 0.05279s; loss: -38.3352 (±2.72419); valid loss: -9.44895\n",
      "[Epoch 269/350, Step 78200, ETA 2m 31.52s] step time: 0.006067s (±0.005905s); valid time: 0.05849s; loss: -36.9026 (±12.2828); valid loss: 38.8513\n",
      "[Epoch 270/350, Step 78300, ETA 2m 30.89s] step time: 0.005775s (±0.005006s); valid time: 0.04987s; loss: -37.2467 (±7.6936); valid loss: 196.953\n",
      "[Epoch 270/350, Step 78400, ETA 2m 30.24s] step time: 0.005935s (±0.00515s); valid time: 0.05078s; loss: -38.189 (±3.16383); valid loss: 92.2603\n",
      "[Epoch 270/350, Step 78500, ETA 2m 29.6s] step time: 0.005895s (±0.005355s); valid time: 0.05319s; loss: -38.3151 (±2.61079); valid loss: 24.3769\n",
      "[Epoch 270/350, Step 78570, ETA 2m 29.13s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 271/350, Step 78600, ETA 2m 28.97s] step time: 0.005918s (±0.005298s); valid time: 0.05102s; loss: -37.8614 (±2.78898); valid loss: 20.9772\n",
      "[Epoch 271/350, Step 78700, ETA 2m 28.32s] step time: 0.005859s (±0.005083s); valid time: 0.05034s; loss: -37.2023 (±8.86847); valid loss: 58.3883\n",
      "[Epoch 271/350, Step 78800, ETA 2m 27.67s] step time: 0.005725s (±0.005314s); valid time: 0.05228s; loss: -38.4146 (±2.8448); valid loss: 489.491\n",
      "[Epoch 272/350, Step 78900, ETA 2m 27.04s] step time: 0.006088s (±0.006646s); valid time: 0.06566s; loss: -37.2666 (±6.47641); valid loss: -0.827032\n",
      "[Epoch 272/350, Step 79000, ETA 2m 26.4s] step time: 0.005977s (±0.005514s); valid time: 0.05438s; loss: -38.3135 (±2.854); valid loss: 602.367\n",
      "[Epoch 272/350, Step 79100, ETA 2m 25.75s] step time: 0.005789s (±0.005165s); valid time: 0.05165s; loss: -37.5211 (±6.94659); valid loss: 14.4151\n",
      "[Epoch 273/350, Step 79200, ETA 2m 25.12s] step time: 0.006006s (±0.005558s); valid time: 0.05316s; loss: -38.2307 (±2.8964); valid loss: 5338.02\n",
      "[Epoch 273/350, Step 79300, ETA 2m 24.47s] step time: 0.005924s (±0.006004s); valid time: 0.05877s; loss: -38.1217 (±3.11027); valid loss: 44.2256\n",
      "[Epoch 273/350, Step 79400, ETA 2m 23.82s] step time: 0.005797s (±0.005021s); valid time: 0.05003s; loss: -12.8403 (±253.387); valid loss: 25.573\n",
      "[Epoch 274/350, Step 79500, ETA 2m 23.19s] step time: 0.005884s (±0.005208s); valid time: 0.05104s; loss: -37.1734 (±9.76143); valid loss: 162.401\n",
      "[Epoch 274/350, Step 79600, ETA 2m 22.54s] step time: 0.005763s (±0.005307s); valid time: 0.05253s; loss: -38.658 (±2.22093); valid loss: 32.8701\n",
      "[Epoch 274/350, Step 79700, ETA 2m 21.9s] step time: 0.005997s (±0.005201s); valid time: 0.05158s; loss: -37.6597 (±4.23946); valid loss: 1567.71\n",
      "[Epoch 275/350, Step 79800, ETA 2m 21.27s] step time: 0.005749s (±0.005046s); valid time: 0.0498s; loss: -36.9103 (±5.92962); valid loss: 127.319\n",
      "[Epoch 275/350, Step 79900, ETA 2m 20.61s] step time: 0.005741s (±0.005063s); valid time: 0.0506s; loss: -38.2646 (±2.52004); valid loss: 105.342\n",
      "[Epoch 275/350, Step 80000, ETA 2m 19.97s] step time: 0.005936s (±0.005325s); valid time: 0.05195s; loss: -38.5135 (±2.20057); valid loss: 9.62142\n",
      "[Epoch 276/350, Step 80100, ETA 2m 19.34s] step time: 0.005783s (±0.005999s); valid time: 0.06017s; loss: -37.9262 (±5.059); valid loss: 583.286\n",
      "[Epoch 276/350, Step 80200, ETA 2m 18.69s] step time: 0.005813s (±0.005193s); valid time: 0.05136s; loss: -36.228 (±16.4174); valid loss: -1.87134\n",
      "[Epoch 276/350, Step 80300, ETA 2m 18.04s] step time: 0.005933s (±0.004999s); valid time: 0.04945s; loss: -38.3957 (±2.31826); valid loss: 111.493\n",
      "[Epoch 277/350, Step 80400, ETA 2m 17.41s] step time: 0.005707s (±0.005259s); valid time: 0.05091s; loss: -38.0534 (±5.30005); valid loss: 881.627\n",
      "[Epoch 277/350, Step 80500, ETA 2m 16.76s] step time: 0.005903s (±0.005109s); valid time: 0.05075s; loss: -37.8563 (±3.92397); valid loss: 335.485\n",
      "[Epoch 277/350, Step 80600, ETA 2m 16.11s] step time: 0.00584s (±0.005219s); valid time: 0.05056s; loss: -37.0393 (±9.60693); valid loss: 140.207\n",
      "[Epoch 278/350, Step 80700, ETA 2m 15.48s] step time: 0.005904s (±0.006011s); valid time: 0.05931s; loss: -35.3347 (±21.0487); valid loss: 1258.64\n",
      "[Epoch 278/350, Step 80800, ETA 2m 14.84s] step time: 0.006201s (±0.005539s); valid time: 0.05311s; loss: -37.7662 (±4.10674); valid loss: 33.2942\n",
      "[Epoch 279/350, Step 80900, ETA 2m 14.21s] step time: 0.005873s (±0.005403s); valid time: 0.05246s; loss: -37.2905 (±10.9206); valid loss: 60.1366\n",
      "[Epoch 279/350, Step 81000, ETA 2m 13.56s] step time: 0.005764s (±0.005052s); valid time: 0.04944s; loss: -38.7271 (±1.95682); valid loss: 19.9325\n",
      "[Epoch 279/350, Step 81100, ETA 2m 12.91s] step time: 0.005747s (±0.005473s); valid time: 0.05404s; loss: -36.0842 (±12.781); valid loss: 126.43\n",
      "[Epoch 280/350, Step 81200, ETA 2m 12.28s] step time: 0.005759s (±0.005138s); valid time: 0.05107s; loss: -37.1354 (±5.38914); valid loss: 435.645\n",
      "[Epoch 280/350, Step 81300, ETA 2m 11.63s] step time: 0.005967s (±0.005191s); valid time: 0.05066s; loss: -38.2492 (±2.69883); valid loss: 347.461\n",
      "[Epoch 280/350, Step 81400, ETA 2m 10.98s] step time: 0.005847s (±0.005621s); valid time: 0.05447s; loss: -37.9118 (±3.92651); valid loss: 280.876\n",
      "[Epoch 280/350, Step 81480, ETA 2m 10.46s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 81500, ETA 2m 10.36s] step time: 0.005917s (±0.006052s); valid time: 0.06058s; loss: -38.6384 (±1.8652); valid loss: 0.641047\n",
      "[Epoch 281/350, Step 81600, ETA 2m 9.71s] step time: 0.005994s (±0.00575s); valid time: 0.0568s; loss: -36.5317 (±15.335); valid loss: 306.728\n",
      "[Epoch 281/350, Step 81700, ETA 2m 9.068s] step time: 0.006081s (±0.006241s); valid time: 0.06167s; loss: -37.7878 (±3.45959); valid loss: 313.126\n",
      "[Epoch 282/350, Step 81800, ETA 2m 8.439s] step time: 0.005896s (±0.005189s); valid time: 0.05124s; loss: -38.2329 (±3.1366); valid loss: 47.383\n",
      "[Epoch 282/350, Step 81900, ETA 2m 7.795s] step time: 0.006036s (±0.005311s); valid time: 0.05229s; loss: -37.404 (±6.26187); valid loss: 67.0263\n",
      "[Epoch 282/350, Step 82000, ETA 2m 7.147s] step time: 0.005851s (±0.005094s); valid time: 0.0507s; loss: -34.6406 (±35.6104); valid loss: 115.391\n",
      "[Epoch 283/350, Step 82100, ETA 2m 6.515s] step time: 0.005822s (±0.005224s); valid time: 0.05163s; loss: -37.8969 (±5.89213); valid loss: 452.087\n",
      "[Epoch 283/350, Step 82200, ETA 2m 5.872s] step time: 0.006064s (±0.005593s); valid time: 0.05511s; loss: -38.124 (±3.44811); valid loss: 371.261\n",
      "[Epoch 283/350, Step 82300, ETA 2m 5.224s] step time: 0.005832s (±0.005623s); valid time: 0.05621s; loss: -37.9031 (±3.20226); valid loss: 12.1879\n",
      "[Epoch 284/350, Step 82400, ETA 2m 4.602s] step time: 0.006279s (±0.005483s); valid time: 0.05339s; loss: 40.1206 (±779.952); valid loss: 170.363\n",
      "[Epoch 284/350, Step 82500, ETA 2m 3.957s] step time: 0.005939s (±0.00522s); valid time: 0.05111s; loss: -30.2006 (±76.4595); valid loss: 74.7681\n",
      "[Epoch 284/350, Step 82600, ETA 2m 3.313s] step time: 0.006034s (±0.00547s); valid time: 0.0519s; loss: -38.1662 (±3.30445); valid loss: 103.692\n",
      "[Epoch 285/350, Step 82700, ETA 2m 2.686s] step time: 0.005932s (±0.005487s); valid time: 0.05213s; loss: -36.3872 (±19.9723); valid loss: 26.6309\n",
      "[Epoch 285/350, Step 82800, ETA 2m 2.043s] step time: 0.006076s (±0.006286s); valid time: 0.06249s; loss: -38.2633 (±2.77594); valid loss: 2.63283\n",
      "[Epoch 285/350, Step 82900, ETA 2m 1.404s] step time: 0.006232s (±0.005232s); valid time: 0.05198s; loss: -37.9934 (±3.27838); valid loss: 178.037\n",
      "[Epoch 286/350, Step 83000, ETA 2m 0.7741s] step time: 0.005853s (±0.005227s); valid time: 0.05184s; loss: -36.9081 (±12.6141); valid loss: 6.12707\n",
      "[Epoch 286/350, Step 83100, ETA 2m 0.1252s] step time: 0.005792s (±0.005311s); valid time: 0.05249s; loss: -38.3113 (±2.48911); valid loss: 118.963\n",
      "[Epoch 286/350, Step 83200, ETA 1m 59.49s] step time: 0.006232s (±0.005405s); valid time: 0.05296s; loss: -38.0013 (±5.25489); valid loss: 152.548\n",
      "[Epoch 287/350, Step 83300, ETA 1m 58.86s] step time: 0.005848s (±0.005114s); valid time: 0.05016s; loss: -38.2292 (±2.4715); valid loss: 19.709\n",
      "[Epoch 287/350, Step 83400, ETA 1m 58.21s] step time: 0.005893s (±0.005243s); valid time: 0.05194s; loss: -37.5107 (±4.42439); valid loss: 917.315\n",
      "[Epoch 287/350, Step 83500, ETA 1m 57.57s] step time: 0.006021s (±0.005675s); valid time: 0.05593s; loss: -35.7507 (±24.483); valid loss: 27.9938\n",
      "[Epoch 288/350, Step 83600, ETA 1m 56.93s] step time: 0.005911s (±0.005272s); valid time: 0.05184s; loss: -37.9195 (±5.01545); valid loss: 44.7812\n",
      "[Epoch 288/350, Step 83700, ETA 1m 56.29s] step time: 0.005889s (±0.005242s); valid time: 0.0514s; loss: -38.2504 (±3.07716); valid loss: 142.301\n",
      "[Epoch 288/350, Step 83800, ETA 1m 55.64s] step time: 0.005985s (±0.005344s); valid time: 0.0524s; loss: -37.6912 (±3.86241); valid loss: 67.5798\n",
      "[Epoch 289/350, Step 83900, ETA 1m 55.01s] step time: 0.005984s (±0.006275s); valid time: 0.06229s; loss: -35.5644 (±30.0402); valid loss: 39.6314\n",
      "[Epoch 289/350, Step 84000, ETA 1m 54.37s] step time: 0.006159s (±0.005393s); valid time: 0.05296s; loss: -38.0002 (±3.69114); valid loss: -2.47267\n",
      "[Epoch 290/350, Step 84100, ETA 1m 53.74s] step time: 0.005831s (±0.005324s); valid time: 0.05166s; loss: -37.8416 (±3.6328); valid loss: 2096.57\n",
      "[Epoch 290/350, Step 84200, ETA 1m 53.09s] step time: 0.005896s (±0.005428s); valid time: 0.05388s; loss: -37.2577 (±8.25206); valid loss: 39.3208\n",
      "[Epoch 290/350, Step 84300, ETA 1m 52.45s] step time: 0.005971s (±0.005288s); valid time: 0.05081s; loss: -38.2218 (±2.65383); valid loss: 111.742\n",
      "[Epoch 290/350, Step 84390, ETA 1m 51.86s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 84400, ETA 1m 51.82s] step time: 0.005862s (±0.005183s); valid time: 0.05175s; loss: -36.4543 (±15.9667); valid loss: 198.394\n",
      "[Epoch 291/350, Step 84500, ETA 1m 51.17s] step time: 0.005966s (±0.0054s); valid time: 0.05342s; loss: -38.0254 (±3.5314); valid loss: 67.0634\n",
      "[Epoch 291/350, Step 84600, ETA 1m 50.53s] step time: 0.006027s (±0.005379s); valid time: 0.05235s; loss: -38.4381 (±2.03974); valid loss: 476.609\n",
      "[Epoch 292/350, Step 84700, ETA 1m 49.9s] step time: 0.006016s (±0.005349s); valid time: 0.05242s; loss: -38.0447 (±4.03945); valid loss: 33.4933\n",
      "[Epoch 292/350, Step 84800, ETA 1m 49.25s] step time: 0.005885s (±0.005485s); valid time: 0.05435s; loss: -32.7528 (±50.2373); valid loss: 936.961\n",
      "[Epoch 292/350, Step 84900, ETA 1m 48.61s] step time: 0.005979s (±0.005463s); valid time: 0.05407s; loss: -38.2355 (±3.18796); valid loss: 94.4052\n",
      "[Epoch 293/350, Step 85000, ETA 1m 47.98s] step time: 0.006149s (±0.005862s); valid time: 0.05344s; loss: -37.6138 (±4.81555); valid loss: 763.32\n",
      "[Epoch 293/350, Step 85100, ETA 1m 47.34s] step time: 0.006006s (±0.005477s); valid time: 0.05373s; loss: -38.2218 (±2.79016); valid loss: 20.454\n",
      "[Epoch 293/350, Step 85200, ETA 1m 46.7s] step time: 0.005939s (±0.005323s); valid time: 0.05315s; loss: -21.0401 (±169.854); valid loss: 1102.82\n",
      "[Epoch 294/350, Step 85300, ETA 1m 46.06s] step time: 0.005927s (±0.005517s); valid time: 0.0544s; loss: -38.1663 (±3.09503); valid loss: 174.676\n",
      "[Epoch 294/350, Step 85400, ETA 1m 45.42s] step time: 0.006007s (±0.005313s); valid time: 0.05192s; loss: -38.1243 (±2.92541); valid loss: 20.9985\n",
      "[Epoch 294/350, Step 85500, ETA 1m 44.78s] step time: 0.006042s (±0.005484s); valid time: 0.05371s; loss: -38.6037 (±2.37315); valid loss: 4426.32\n",
      "[Epoch 295/350, Step 85600, ETA 1m 44.15s] step time: 0.006163s (±0.005316s); valid time: 0.0516s; loss: -38.1756 (±2.76828); valid loss: 26.809\n",
      "[Epoch 295/350, Step 85700, ETA 1m 43.5s] step time: 0.005823s (±0.005172s); valid time: 0.05163s; loss: -38.0171 (±4.52871); valid loss: 10.3084\n",
      "[Epoch 295/350, Step 85800, ETA 1m 42.86s] step time: 0.005928s (±0.005364s); valid time: 0.05293s; loss: -37.2825 (±4.25952); valid loss: 1161.75\n",
      "[Epoch 296/350, Step 85900, ETA 1m 42.23s] step time: 0.006012s (±0.005159s); valid time: 0.04973s; loss: -35.5023 (±26.3485); valid loss: 389.894\n",
      "[Epoch 296/350, Step 86000, ETA 1m 41.58s] step time: 0.005983s (±0.005416s); valid time: 0.05276s; loss: -38.6266 (±2.26458); valid loss: 176.518\n",
      "[Epoch 296/350, Step 86100, ETA 1m 40.94s] step time: 0.00588s (±0.005323s); valid time: 0.05201s; loss: -35.8817 (±13.8836); valid loss: -4.54085\n",
      "[Epoch 297/350, Step 86200, ETA 1m 40.31s] step time: 0.006016s (±0.005689s); valid time: 0.05505s; loss: -37.6846 (±5.03089); valid loss: 49.349\n",
      "[Epoch 297/350, Step 86300, ETA 1m 39.66s] step time: 0.005959s (±0.005084s); valid time: 0.05007s; loss: -37.6609 (±8.40467); valid loss: 15.8379\n",
      "[Epoch 297/350, Step 86400, ETA 1m 39.02s] step time: 0.005892s (±0.005129s); valid time: 0.05077s; loss: -36.524 (±17.7959); valid loss: 157.299\n",
      "[Epoch 298/350, Step 86500, ETA 1m 38.38s] step time: 0.005848s (±0.005558s); valid time: 0.05391s; loss: -38.0194 (±2.54396); valid loss: 267.192\n",
      "[Epoch 298/350, Step 86600, ETA 1m 37.74s] step time: 0.005873s (±0.005105s); valid time: 0.05054s; loss: -38.3735 (±3.05699); valid loss: 835.65\n",
      "[Epoch 298/350, Step 86700, ETA 1m 37.09s] step time: 0.005951s (±0.005584s); valid time: 0.05515s; loss: -38.3613 (±2.42561); valid loss: 15.0609\n",
      "[Epoch 299/350, Step 86800, ETA 1m 36.46s] step time: 0.005859s (±0.005222s); valid time: 0.05098s; loss: -37.3826 (±4.91428); valid loss: 142.184\n",
      "[Epoch 299/350, Step 86900, ETA 1m 35.81s] step time: 0.00598s (±0.006312s); valid time: 0.06203s; loss: -38.6565 (±2.10744); valid loss: 639.16\n",
      "[Epoch 299/350, Step 87000, ETA 1m 35.17s] step time: 0.005796s (±0.005312s); valid time: 0.05333s; loss: -37.9603 (±3.55784); valid loss: 14.5857\n",
      "[Epoch 300/350, Step 87100, ETA 1m 34.53s] step time: 0.005997s (±0.005185s); valid time: 0.05042s; loss: -38.4725 (±2.20821); valid loss: 1351.38\n",
      "[Epoch 300/350, Step 87200, ETA 1m 33.89s] step time: 0.005978s (±0.005167s); valid time: 0.05066s; loss: -38.1342 (±2.85424); valid loss: -7.77873\n",
      "[Epoch 300/350, Step 87300, ETA 1m 33.24s] step time: 0.005868s (±0.005319s); valid time: 0.05278s; loss: -38.2994 (±2.7844); valid loss: 1488.25\n",
      "[Epoch 300/350, Step 87300, ETA 1m 33.24s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 301/350, Step 87400, ETA 1m 32.61s] step time: 0.005918s (±0.005069s); valid time: 0.04971s; loss: -38.2063 (±2.91788); valid loss: 14.6264\n",
      "[Epoch 301/350, Step 87500, ETA 1m 31.97s] step time: 0.005947s (±0.006295s); valid time: 0.06156s; loss: -38.1785 (±2.85863); valid loss: 12.6217\n",
      "[Epoch 302/350, Step 87600, ETA 1m 31.33s] step time: 0.005998s (±0.00549s); valid time: 0.05395s; loss: -37.1195 (±7.21358); valid loss: 50.9375\n",
      "[Epoch 302/350, Step 87700, ETA 1m 30.69s] step time: 0.005843s (±0.005504s); valid time: 0.05211s; loss: -38.3968 (±2.69086); valid loss: 62.3829\n",
      "[Epoch 302/350, Step 87800, ETA 1m 30.04s] step time: 0.006016s (±0.00515s); valid time: 0.05125s; loss: -38.0077 (±2.8375); valid loss: 67.5495\n",
      "[Epoch 303/350, Step 87900, ETA 1m 29.41s] step time: 0.005914s (±0.005449s); valid time: 0.05399s; loss: -32.3535 (±57.2875); valid loss: 90.3781\n",
      "[Epoch 303/350, Step 88000, ETA 1m 28.77s] step time: 0.005833s (±0.005444s); valid time: 0.05281s; loss: -37.6127 (±4.80008); valid loss: 66.6477\n",
      "[Epoch 303/350, Step 88100, ETA 1m 28.12s] step time: 0.005908s (±0.005078s); valid time: 0.05003s; loss: -37.6667 (±5.40536); valid loss: 3.14046\n",
      "[Epoch 304/350, Step 88200, ETA 1m 27.48s] step time: 0.005716s (±0.005099s); valid time: 0.05125s; loss: -38.2252 (±2.71406); valid loss: 206.632\n",
      "[Epoch 304/350, Step 88300, ETA 1m 26.84s] step time: 0.005887s (±0.005056s); valid time: 0.04945s; loss: -36.505 (±12.0216); valid loss: 72.2416\n",
      "[Epoch 304/350, Step 88400, ETA 1m 26.19s] step time: 0.005882s (±0.005163s); valid time: 0.05041s; loss: -38.5156 (±2.20409); valid loss: 2.22075\n",
      "[Epoch 305/350, Step 88500, ETA 1m 25.56s] step time: 0.005839s (±0.005151s); valid time: 0.05056s; loss: -37.9515 (±2.95347); valid loss: 2.42371\n",
      "[Epoch 305/350, Step 88600, ETA 1m 24.91s] step time: 0.005785s (±0.00512s); valid time: 0.05121s; loss: -38.3417 (±2.59589); valid loss: 655.339\n",
      "[Epoch 305/350, Step 88700, ETA 1m 24.27s] step time: 0.006047s (±0.005293s); valid time: 0.05201s; loss: -25.4438 (±123.282); valid loss: 177\n",
      "[Epoch 306/350, Step 88800, ETA 1m 23.64s] step time: 0.006082s (±0.005935s); valid time: 0.05788s; loss: -38.3144 (±2.56581); valid loss: 249.616\n",
      "[Epoch 306/350, Step 88900, ETA 1m 22.99s] step time: 0.005849s (±0.005465s); valid time: 0.05096s; loss: -25.0311 (±124.778); valid loss: 6.7141\n",
      "[Epoch 306/350, Step 89000, ETA 1m 22.35s] step time: 0.005919s (±0.005125s); valid time: 0.0504s; loss: -38.4495 (±2.28287); valid loss: 277.409\n",
      "[Epoch 307/350, Step 89100, ETA 1m 21.71s] step time: 0.005815s (±0.005537s); valid time: 0.05439s; loss: -31.1739 (±72.8908); valid loss: 61.8487\n",
      "[Epoch 307/350, Step 89200, ETA 1m 21.07s] step time: 0.005935s (±0.005397s); valid time: 0.05338s; loss: -38.41 (±2.66754); valid loss: -6.91903\n",
      "[Epoch 307/350, Step 89300, ETA 1m 20.42s] step time: 0.005939s (±0.005444s); valid time: 0.05353s; loss: -37.4025 (±6.88628); valid loss: 44.6944\n",
      "[Epoch 308/350, Step 89400, ETA 1m 19.79s] step time: 0.005789s (±0.00521s); valid time: 0.0515s; loss: -36.0674 (±13.2422); valid loss: 86.7668\n",
      "[Epoch 308/350, Step 89500, ETA 1m 19.15s] step time: 0.006032s (±0.005207s); valid time: 0.0512s; loss: -38.1886 (±2.66023); valid loss: 224.27\n",
      "[Epoch 308/350, Step 89600, ETA 1m 18.5s] step time: 0.005803s (±0.005378s); valid time: 0.05224s; loss: -37.816 (±4.28375); valid loss: 51.0624\n",
      "[Epoch 309/350, Step 89700, ETA 1m 17.86s] step time: 0.005821s (±0.005199s); valid time: 0.05093s; loss: -38.4007 (±2.76801); valid loss: 28.3788\n",
      "[Epoch 309/350, Step 89800, ETA 1m 17.22s] step time: 0.005989s (±0.005256s); valid time: 0.05146s; loss: -14.1963 (±238.91); valid loss: 57.9678\n",
      "[Epoch 309/350, Step 89900, ETA 1m 16.58s] step time: 0.005796s (±0.005684s); valid time: 0.05697s; loss: -38.1346 (±2.91309); valid loss: 361.706\n",
      "[Epoch 310/350, Step 90000, ETA 1m 15.94s] step time: 0.005805s (±0.005239s); valid time: 0.05193s; loss: -37.053 (±10.0952); valid loss: 202.995\n",
      "[Epoch 310/350, Step 90100, ETA 1m 15.3s] step time: 0.005949s (±0.005377s); valid time: 0.05195s; loss: -37.9059 (±3.78099); valid loss: 321.115\n",
      "[Epoch 310/350, Step 90200, ETA 1m 14.65s] step time: 0.005911s (±0.005239s); valid time: 0.05224s; loss: -35.5575 (±18.3778); valid loss: 127.678\n",
      "[Epoch 310/350, Step 90210, ETA 1m 14.59s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 90300, ETA 1m 14.02s] step time: 0.005796s (±0.005168s); valid time: 0.05099s; loss: -38.5712 (±2.45378); valid loss: 143.531\n",
      "[Epoch 311/350, Step 90400, ETA 1m 13.37s] step time: 0.006007s (±0.00598s); valid time: 0.05806s; loss: -37.7945 (±3.64507); valid loss: 176.913\n",
      "[Epoch 311/350, Step 90500, ETA 1m 12.73s] step time: 0.005684s (±0.004928s); valid time: 0.04936s; loss: -38.1772 (±3.7165); valid loss: 73.9591\n",
      "[Epoch 312/350, Step 90600, ETA 1m 12.09s] step time: 0.005814s (±0.005214s); valid time: 0.05128s; loss: -37.4916 (±6.92862); valid loss: 48.0541\n",
      "[Epoch 312/350, Step 90700, ETA 1m 11.45s] step time: 0.005882s (±0.005178s); valid time: 0.05073s; loss: -38.0724 (±2.33856); valid loss: 220.999\n",
      "[Epoch 313/350, Step 90800, ETA 1m 10.81s] step time: 0.005899s (±0.005668s); valid time: 0.05265s; loss: -38.2795 (±2.89357); valid loss: 10.8386\n",
      "[Epoch 313/350, Step 90900, ETA 1m 10.17s] step time: 0.005873s (±0.005386s); valid time: 0.05373s; loss: 242.538 (±2773.31); valid loss: 60.8958\n",
      "[Epoch 313/350, Step 91000, ETA 1m 9.521s] step time: 0.005848s (±0.005426s); valid time: 0.0541s; loss: -38.1065 (±2.68859); valid loss: 213.064\n",
      "[Epoch 314/350, Step 91100, ETA 1m 8.887s] step time: 0.006006s (±0.005239s); valid time: 0.05122s; loss: 25.7863 (±635.468); valid loss: -3.21078\n",
      "[Epoch 314/350, Step 91200, ETA 1m 8.243s] step time: 0.005893s (±0.005824s); valid time: 0.05791s; loss: -38.2153 (±2.55761); valid loss: 1016.34\n",
      "[Epoch 314/350, Step 91300, ETA 1m 7.6s] step time: 0.005967s (±0.005469s); valid time: 0.05148s; loss: -38.1404 (±2.69856); valid loss: 74.3071\n",
      "[Epoch 315/350, Step 91400, ETA 1m 6.964s] step time: 0.005899s (±0.005417s); valid time: 0.05422s; loss: -36.1373 (±15.2015); valid loss: -0.631982\n",
      "[Epoch 315/350, Step 91500, ETA 1m 6.319s] step time: 0.00585s (±0.005353s); valid time: 0.05286s; loss: -37.5321 (±4.30821); valid loss: 299.866\n",
      "[Epoch 315/350, Step 91600, ETA 1m 5.676s] step time: 0.005934s (±0.005584s); valid time: 0.05421s; loss: -38.4017 (±2.90679); valid loss: 19.0716\n",
      "[Epoch 316/350, Step 91700, ETA 1m 5.041s] step time: 0.005957s (±0.005206s); valid time: 0.05153s; loss: 231.352 (±2675.11); valid loss: 112.403\n",
      "[Epoch 316/350, Step 91800, ETA 1m 4.396s] step time: 0.005776s (±0.005485s); valid time: 0.05166s; loss: -37.7654 (±3.94528); valid loss: 2286.98\n",
      "[Epoch 316/350, Step 91900, ETA 1m 3.751s] step time: 0.005832s (±0.005303s); valid time: 0.05156s; loss: -37.9792 (±4.23384); valid loss: 637.203\n",
      "[Epoch 317/350, Step 92000, ETA 1m 3.116s] step time: 0.005953s (±0.007455s); valid time: 0.07466s; loss: -38.4604 (±2.77504); valid loss: 31.1971\n",
      "[Epoch 317/350, Step 92100, ETA 1m 2.471s] step time: 0.005757s (±0.005762s); valid time: 0.05639s; loss: -37.9519 (±3.44812); valid loss: 25.5102\n",
      "[Epoch 317/350, Step 92200, ETA 1m 1.827s] step time: 0.005874s (±0.005722s); valid time: 0.05678s; loss: -37.9819 (±3.25224); valid loss: 35.2139\n",
      "[Epoch 318/350, Step 92300, ETA 1m 1.189s] step time: 0.005798s (±0.005157s); valid time: 0.05152s; loss: -37.7404 (±4.22626); valid loss: -5.62731\n",
      "[Epoch 318/350, Step 92400, ETA 1m 0.5459s] step time: 0.00589s (±0.005183s); valid time: 0.05119s; loss: -37.6846 (±3.49541); valid loss: 96.8653\n",
      "[Epoch 318/350, Step 92500, ETA 59.9s] step time: 0.005816s (±0.005161s); valid time: 0.05029s; loss: -38.0398 (±4.98349); valid loss: 48.9322\n",
      "[Epoch 319/350, Step 92600, ETA 59.27s] step time: 0.005894s (±0.005311s); valid time: 0.05241s; loss: -38.0235 (±3.76149); valid loss: 153.899\n",
      "[Epoch 319/350, Step 92700, ETA 58.62s] step time: 0.005864s (±0.005079s); valid time: 0.04998s; loss: -37.9816 (±4.34779); valid loss: 397.093\n",
      "[Epoch 319/350, Step 92800, ETA 57.98s] step time: 0.00589s (±0.005153s); valid time: 0.05048s; loss: -38.0187 (±3.5263); valid loss: 300.521\n",
      "[Epoch 320/350, Step 92900, ETA 57.34s] step time: 0.005816s (±0.005285s); valid time: 0.05227s; loss: -38.2593 (±3.16688); valid loss: 3487.96\n",
      "[Epoch 320/350, Step 93000, ETA 56.7s] step time: 0.005836s (±0.005406s); valid time: 0.05294s; loss: -38.1855 (±3.14941); valid loss: 126.489\n",
      "[Epoch 320/350, Step 93100, ETA 56.05s] step time: 0.005786s (±0.005179s); valid time: 0.05063s; loss: -31.5875 (±61.9432); valid loss: 21.04\n",
      "[Epoch 320/350, Step 93120, ETA 55.92s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 93200, ETA 55.42s] step time: 0.005711s (±0.005106s); valid time: 0.05102s; loss: -38.6067 (±2.44388); valid loss: 1386.95\n",
      "[Epoch 321/350, Step 93300, ETA 54.77s] step time: 0.006074s (±0.005408s); valid time: 0.0534s; loss: -36.8397 (±9.83487); valid loss: 167.978\n",
      "[Epoch 321/350, Step 93400, ETA 54.13s] step time: 0.005805s (±0.005145s); valid time: 0.05063s; loss: -38.3107 (±3.14332); valid loss: 92.0871\n",
      "[Epoch 322/350, Step 93500, ETA 53.49s] step time: 0.005888s (±0.005666s); valid time: 0.05554s; loss: -37.7285 (±3.69666); valid loss: 154.98\n",
      "[Epoch 322/350, Step 93600, ETA 52.85s] step time: 0.005621s (±0.005059s); valid time: 0.05021s; loss: -36.0495 (±24.1897); valid loss: 518.496\n",
      "[Epoch 322/350, Step 93700, ETA 52.21s] step time: 0.005994s (±0.005498s); valid time: 0.05323s; loss: -37.8694 (±3.10944); valid loss: 353.4\n",
      "[Epoch 323/350, Step 93800, ETA 51.57s] step time: 0.005913s (±0.0051s); valid time: 0.0499s; loss: -38.6753 (±2.16702); valid loss: -0.966195\n",
      "[Epoch 323/350, Step 93900, ETA 50.93s] step time: 0.005835s (±0.005483s); valid time: 0.05296s; loss: -38.2679 (±2.72893); valid loss: 154.668\n",
      "[Epoch 324/350, Step 94000, ETA 50.29s] step time: 0.005952s (±0.005103s); valid time: 0.04996s; loss: -37.826 (±3.47866); valid loss: 54.6854\n",
      "[Epoch 324/350, Step 94100, ETA 49.65s] step time: 0.00585s (±0.00528s); valid time: 0.05171s; loss: -37.6952 (±5.49935); valid loss: 376.926\n",
      "[Epoch 324/350, Step 94200, ETA 49s] step time: 0.005832s (±0.005515s); valid time: 0.05463s; loss: -38.1383 (±4.27903); valid loss: 43.7736\n",
      "[Epoch 325/350, Step 94300, ETA 48.37s] step time: 0.00579s (±0.005139s); valid time: 0.05078s; loss: -38.0696 (±3.16662); valid loss: -0.537133\n",
      "[Epoch 325/350, Step 94400, ETA 47.72s] step time: 0.005697s (±0.005243s); valid time: 0.05076s; loss: -37.6346 (±4.7246); valid loss: 24.4009\n",
      "[Epoch 325/350, Step 94500, ETA 47.08s] step time: 0.005983s (±0.005195s); valid time: 0.05093s; loss: -37.5261 (±5.37738); valid loss: 218.415\n",
      "[Epoch 326/350, Step 94600, ETA 46.44s] step time: 0.005806s (±0.005068s); valid time: 0.05035s; loss: -36.7165 (±16.8468); valid loss: 38.5945\n",
      "[Epoch 326/350, Step 94700, ETA 45.8s] step time: 0.005673s (±0.005111s); valid time: 0.05067s; loss: -38.2935 (±2.74818); valid loss: 173.323\n",
      "[Epoch 326/350, Step 94800, ETA 45.16s] step time: 0.005979s (±0.005443s); valid time: 0.05358s; loss: -37.3981 (±8.31658); valid loss: 8.59931\n",
      "[Epoch 327/350, Step 94900, ETA 44.52s] step time: 0.005912s (±0.005138s); valid time: 0.05006s; loss: -36.604 (±16.2129); valid loss: 249.143\n",
      "[Epoch 327/350, Step 95000, ETA 43.88s] step time: 0.005965s (±0.005142s); valid time: 0.05084s; loss: -33.8903 (±40.4116); valid loss: 2.50815\n",
      "[Epoch 327/350, Step 95100, ETA 43.23s] step time: 0.005889s (±0.005264s); valid time: 0.05165s; loss: -37.9833 (±3.2797); valid loss: 26.8322\n",
      "[Epoch 328/350, Step 95200, ETA 42.6s] step time: 0.005832s (±0.005139s); valid time: 0.05078s; loss: -38.054 (±3.85715); valid loss: 1047.18\n",
      "[Epoch 328/350, Step 95300, ETA 41.95s] step time: 0.006058s (±0.005306s); valid time: 0.05238s; loss: -37.6591 (±8.2066); valid loss: 1625.37\n",
      "[Epoch 328/350, Step 95400, ETA 41.31s] step time: 0.006069s (±0.007648s); valid time: 0.07327s; loss: -37.795 (±3.99425); valid loss: 3.11482\n",
      "[Epoch 329/350, Step 95500, ETA 40.68s] step time: 0.005904s (±0.005242s); valid time: 0.05183s; loss: -37.6584 (±3.95964); valid loss: 533.272\n",
      "[Epoch 329/350, Step 95600, ETA 40.03s] step time: 0.005929s (±0.005064s); valid time: 0.05005s; loss: -38.0234 (±3.48275); valid loss: 243.042\n",
      "[Epoch 329/350, Step 95700, ETA 39.39s] step time: 0.005929s (±0.005342s); valid time: 0.05262s; loss: -37.9272 (±3.70881); valid loss: 448.644\n",
      "[Epoch 330/350, Step 95800, ETA 38.75s] step time: 0.00577s (±0.004995s); valid time: 0.04957s; loss: -37.9763 (±3.56559); valid loss: 169.035\n",
      "[Epoch 330/350, Step 95900, ETA 38.11s] step time: 0.005828s (±0.005223s); valid time: 0.05078s; loss: -37.7856 (±3.63362); valid loss: 163.325\n",
      "[Epoch 330/350, Step 96000, ETA 37.47s] step time: 0.005748s (±0.005197s); valid time: 0.05152s; loss: -38.3225 (±2.71991); valid loss: 346.028\n",
      "[Epoch 330/350, Step 96030, ETA 37.28s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 96100, ETA 36.83s] step time: 0.005874s (±0.005132s); valid time: 0.05014s; loss: -37.3896 (±6.14025); valid loss: 201.411\n",
      "[Epoch 331/350, Step 96200, ETA 36.19s] step time: 0.005859s (±0.005136s); valid time: 0.05121s; loss: -37.9865 (±2.70223); valid loss: 196.805\n",
      "[Epoch 331/350, Step 96300, ETA 35.55s] step time: 0.00572s (±0.00521s); valid time: 0.05131s; loss: -35.9862 (±22.8376); valid loss: 53.1296\n",
      "[Epoch 332/350, Step 96400, ETA 34.91s] step time: 0.005926s (±0.005326s); valid time: 0.05259s; loss: -37.7286 (±5.10499); valid loss: 34.1142\n",
      "[Epoch 332/350, Step 96500, ETA 34.27s] step time: 0.0058s (±0.004963s); valid time: 0.04877s; loss: -38.3656 (±3.47119); valid loss: 11.0152\n",
      "[Epoch 332/350, Step 96600, ETA 33.62s] step time: 0.005758s (±0.004976s); valid time: 0.04933s; loss: -37.4027 (±4.33157); valid loss: 193.196\n",
      "[Epoch 333/350, Step 96700, ETA 32.99s] step time: 0.005997s (±0.005322s); valid time: 0.05232s; loss: -37.5933 (±6.91622); valid loss: -8.7204\n",
      "[Epoch 333/350, Step 96800, ETA 32.34s] step time: 0.005803s (±0.005139s); valid time: 0.05077s; loss: -24.513 (±133.418); valid loss: 247.896\n",
      "[Epoch 333/350, Step 96900, ETA 31.7s] step time: 0.005907s (±0.005288s); valid time: 0.05215s; loss: -38.1549 (±3.46106); valid loss: 396.371\n",
      "[Epoch 334/350, Step 97000, ETA 31.06s] step time: 0.006114s (±0.005388s); valid time: 0.05194s; loss: -37.4695 (±6.26496); valid loss: 194.8\n",
      "[Epoch 334/350, Step 97100, ETA 30.42s] step time: 0.005958s (±0.005456s); valid time: 0.0539s; loss: -36.1615 (±15.1698); valid loss: 74.1386\n",
      "[Epoch 335/350, Step 97200, ETA 29.79s] step time: 0.006017s (±0.005539s); valid time: 0.05171s; loss: -38.4019 (±2.45871); valid loss: 68.3027\n",
      "[Epoch 335/350, Step 97300, ETA 29.14s] step time: 0.006012s (±0.005853s); valid time: 0.05796s; loss: -37.7817 (±3.95179); valid loss: 4.98721\n",
      "[Epoch 335/350, Step 97400, ETA 28.5s] step time: 0.005748s (±0.005082s); valid time: 0.05061s; loss: -37.6097 (±7.66512); valid loss: 49.1363\n",
      "[Epoch 336/350, Step 97500, ETA 27.86s] step time: 0.005983s (±0.005141s); valid time: 0.0506s; loss: -35.8644 (±17.686); valid loss: 758.018\n",
      "[Epoch 336/350, Step 97600, ETA 27.22s] step time: 0.006021s (±0.005452s); valid time: 0.05347s; loss: -37.699 (±4.97276); valid loss: 11024.1\n",
      "[Epoch 336/350, Step 97700, ETA 26.58s] step time: 0.006021s (±0.006693s); valid time: 0.06633s; loss: -38.1887 (±4.93857); valid loss: 39.9071\n",
      "[Epoch 337/350, Step 97800, ETA 25.94s] step time: 0.006049s (±0.005389s); valid time: 0.05154s; loss: -37.4784 (±4.08684); valid loss: 790.296\n",
      "[Epoch 337/350, Step 97900, ETA 25.3s] step time: 0.00583s (±0.00535s); valid time: 0.05262s; loss: -34.907 (±23.616); valid loss: 87.9833\n",
      "[Epoch 337/350, Step 98000, ETA 24.66s] step time: 0.006006s (±0.005325s); valid time: 0.05218s; loss: 832.715 (±8665.3); valid loss: 63.3709\n",
      "[Epoch 338/350, Step 98100, ETA 24.02s] step time: 0.005938s (±0.005665s); valid time: 0.05533s; loss: -38.0984 (±5.09287); valid loss: 15.1823\n",
      "[Epoch 338/350, Step 98200, ETA 23.38s] step time: 0.005899s (±0.005489s); valid time: 0.05445s; loss: -37.1921 (±8.88047); valid loss: 79.0624\n",
      "[Epoch 338/350, Step 98300, ETA 22.74s] step time: 0.005922s (±0.005191s); valid time: 0.05122s; loss: -38.2952 (±2.76176); valid loss: 3.51453\n",
      "[Epoch 339/350, Step 98400, ETA 22.1s] step time: 0.005968s (±0.005279s); valid time: 0.05191s; loss: -38.0869 (±2.76714); valid loss: 123.385\n",
      "[Epoch 339/350, Step 98500, ETA 21.46s] step time: 0.005836s (±0.005167s); valid time: 0.05153s; loss: -38.2014 (±2.58135); valid loss: 111.794\n",
      "[Epoch 339/350, Step 98600, ETA 20.82s] step time: 0.006002s (±0.005304s); valid time: 0.05226s; loss: -26.5789 (±119.965); valid loss: 7.03185\n",
      "[Epoch 340/350, Step 98700, ETA 20.18s] step time: 0.006219s (±0.00536s); valid time: 0.05224s; loss: -37.8821 (±3.78083); valid loss: 1153.89\n",
      "[Epoch 340/350, Step 98800, ETA 19.54s] step time: 0.005865s (±0.005288s); valid time: 0.05241s; loss: -37.4519 (±7.811); valid loss: 66.6315\n",
      "[Epoch 340/350, Step 98900, ETA 18.9s] step time: 0.006071s (±0.005325s); valid time: 0.05072s; loss: -38.3982 (±2.91457); valid loss: 162.274\n",
      "[Epoch 340/350, Step 98940, ETA 18.64s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 99000, ETA 18.26s] step time: 0.00597s (±0.005155s); valid time: 0.05122s; loss: -37.8933 (±4.73704); valid loss: 50.4998\n",
      "[Epoch 341/350, Step 99100, ETA 17.62s] step time: 0.00594s (±0.005212s); valid time: 0.05169s; loss: -37.9601 (±2.9671); valid loss: -11.6618\n",
      "[Epoch 341/350, Step 99200, ETA 16.98s] step time: 0.005841s (±0.005494s); valid time: 0.05423s; loss: -38.2944 (±2.5664); valid loss: 54.9714\n",
      "[Epoch 342/350, Step 99300, ETA 16.34s] step time: 0.005661s (±0.005137s); valid time: 0.05151s; loss: -35.4116 (±31.6562); valid loss: 150.24\n",
      "[Epoch 342/350, Step 99400, ETA 15.69s] step time: 0.005875s (±0.005247s); valid time: 0.05164s; loss: -38.2616 (±3.03085); valid loss: -1.62847\n",
      "[Epoch 342/350, Step 99500, ETA 15.05s] step time: 0.005834s (±0.005969s); valid time: 0.05935s; loss: -37.5998 (±4.33709); valid loss: 158.214\n",
      "[Epoch 343/350, Step 99600, ETA 14.41s] step time: 0.00583s (±0.005313s); valid time: 0.05266s; loss: -38.5006 (±2.4412); valid loss: -0.271063\n",
      "[Epoch 343/350, Step 99700, ETA 13.77s] step time: 0.006151s (±0.005319s); valid time: 0.0519s; loss: 32.4227 (±695.1); valid loss: 89.4534\n",
      "[Epoch 343/350, Step 99800, ETA 13.13s] step time: 0.005898s (±0.005299s); valid time: 0.05258s; loss: -37.5222 (±7.39481); valid loss: 4.72322\n",
      "[Epoch 344/350, Step 99900, ETA 12.49s] step time: 0.00593s (±0.005129s); valid time: 0.05052s; loss: -37.8172 (±3.90974); valid loss: 53.0008\n",
      "[Epoch 344/350, Step 100000, ETA 11.85s] step time: 0.00587s (±0.005176s); valid time: 0.05082s; loss: -38.1768 (±3.25851); valid loss: 17.0024\n",
      "[Epoch 344/350, Step 100100, ETA 11.21s] step time: 0.005686s (±0.005025s); valid time: 0.04997s; loss: -38.361 (±2.76294); valid loss: 23.9697\n",
      "[Epoch 345/350, Step 100200, ETA 10.57s] step time: 0.005762s (±0.005483s); valid time: 0.05303s; loss: -37.8274 (±3.72719); valid loss: 90.7244\n",
      "[Epoch 345/350, Step 100300, ETA 9.928s] step time: 0.005961s (±0.005984s); valid time: 0.05323s; loss: -36.2984 (±17.6401); valid loss: 26.382\n",
      "[Epoch 346/350, Step 100400, ETA 9.288s] step time: 0.005813s (±0.005284s); valid time: 0.05229s; loss: -38.3268 (±3.64473); valid loss: 17.5848\n",
      "[Epoch 346/350, Step 100500, ETA 8.647s] step time: 0.005919s (±0.005139s); valid time: 0.05043s; loss: -33.2058 (±49.4281); valid loss: 6.18372\n",
      "[Epoch 346/350, Step 100600, ETA 8.007s] step time: 0.005927s (±0.005169s); valid time: 0.05077s; loss: -29.7341 (±81.1964); valid loss: 235.162\n",
      "[Epoch 347/350, Step 100700, ETA 7.367s] step time: 0.005918s (±0.005227s); valid time: 0.05204s; loss: -38.6188 (±2.47487); valid loss: 99.7921\n",
      "[Epoch 347/350, Step 100800, ETA 6.726s] step time: 0.005867s (±0.005113s); valid time: 0.05042s; loss: -38.3301 (±2.98053); valid loss: 341.72\n",
      "[Epoch 347/350, Step 100900, ETA 6.085s] step time: 0.00585s (±0.005186s); valid time: 0.05116s; loss: -37.4339 (±4.83938); valid loss: 52.5616\n",
      "[Epoch 348/350, Step 101000, ETA 5.445s] step time: 0.005816s (±0.004964s); valid time: 0.04914s; loss: -37.6539 (±5.58925); valid loss: 99.2322\n",
      "[Epoch 348/350, Step 101100, ETA 4.804s] step time: 0.005855s (±0.005017s); valid time: 0.05004s; loss: -37.9476 (±3.43012); valid loss: 4.55585\n",
      "[Epoch 348/350, Step 101200, ETA 4.163s] step time: 0.005705s (±0.005006s); valid time: 0.05004s; loss: -38.3249 (±3.18938); valid loss: -4.51014\n",
      "[Epoch 349/350, Step 101300, ETA 3.523s] step time: 0.005736s (±0.00514s); valid time: 0.05071s; loss: -38.0741 (±3.07975); valid loss: 25.4897\n",
      "[Epoch 349/350, Step 101400, ETA 2.882s] step time: 0.005871s (±0.005251s); valid time: 0.05155s; loss: -37.8373 (±3.33407); valid loss: 56.5373\n",
      "[Epoch 349/350, Step 101500, ETA 2.242s] step time: 0.005973s (±0.005386s); valid time: 0.05277s; loss: -35.9585 (±17.7329); valid loss: 204.989\n",
      "[Epoch 350/350, Step 101600, ETA 1.601s] step time: 0.005973s (±0.005639s); valid time: 0.05474s; loss: -38.1589 (±4.57419); valid loss: 121.901\n",
      "[Epoch 350/350, Step 101700, ETA 0.9607s] step time: 0.005891s (±0.005177s); valid time: 0.05067s; loss: -38.2622 (±2.71165); valid loss: 39.606\n",
      "[Epoch 350/350, Step 101800, ETA 0.3202s] step time: 0.005916s (±0.005148s); valid time: 0.05093s; loss: -38.2284 (±3.0716); valid loss: 0.447122\n",
      "[Epoch 350/350, Step 101850, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5qo8mpgw/variables.dat-11100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5qo8mpgw/variables.dat-11100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.49,\n",
      "\t(tp, fp, tn, fn)=(209, 1241, 1010, 40),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.84,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.58,\n",
      "\ty_label%=0.0996,\n",
      ")\n",
      "Testing on realTraffic/TravelTime_451.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 1/350, Step 100] step time: 0.01011s (±0.03358s); valid time: 0.1893s; loss: -15.0926 (±5.0557); valid loss: -16.0753 (*)\n",
      "[Epoch 1/350, Step 200] step time: 0.00571s (±0.00392s); valid time: 0.03664s; loss: -18.0459 (±2.11323); valid loss: -15.8264\n",
      "[Epoch 2/350, Step 300, ETA 9m 42.06s] step time: 0.005694s (±0.003722s); valid time: 0.03672s; loss: -18.4992 (±2.15687); valid loss: -15.8378\n",
      "[Epoch 2/350, Step 400, ETA 9m 8.58s] step time: 0.005778s (±0.004005s); valid time: 0.03959s; loss: -18.7495 (±1.67566); valid loss: -15.6273\n",
      "[Epoch 3/350, Step 500, ETA 8m 56.57s] step time: 0.005752s (±0.003876s); valid time: 0.0382s; loss: -18.8889 (±1.71148); valid loss: -15.955\n",
      "[Epoch 3/350, Step 600, ETA 8m 51.9s] step time: 0.006608s (±0.01084s); valid time: 0.1087s; loss: -19.2905 (±2.19105); valid loss: -16.1058 (*)\n",
      "[Epoch 4/350, Step 700, ETA 8m 45.3s] step time: 0.0058s (±0.004098s); valid time: 0.03836s; loss: -19.965 (±1.7606); valid loss: -15.8321\n",
      "[Epoch 4/350, Step 800, ETA 8m 42.31s] step time: 0.006533s (±0.01119s); valid time: 0.1122s; loss: -20.2543 (±1.7286); valid loss: -16.3734 (*)\n",
      "[Epoch 5/350, Step 900, ETA 8m 43.02s] step time: 0.006392s (±0.01148s); valid time: 0.1153s; loss: -20.3279 (±1.69409); valid loss: -16.4411 (*)\n",
      "[Epoch 5/350, Step 1000, ETA 8m 40.67s] step time: 0.006569s (±0.01126s); valid time: 0.1126s; loss: -21.2858 (±1.57388); valid loss: -16.7451 (*)\n",
      "[Epoch 6/350, Step 1100, ETA 8m 36.1s] step time: 0.005665s (±0.003958s); valid time: 0.03852s; loss: -21.6973 (±1.50806); valid loss: -15.967\n",
      "[Epoch 6/350, Step 1200, ETA 8m 28.74s] step time: 0.005647s (±0.003817s); valid time: 0.03704s; loss: -22.8008 (±0.947942); valid loss: -16.6892\n",
      "[Epoch 7/350, Step 1300, ETA 8m 29.49s] step time: 0.006356s (±0.01072s); valid time: 0.1075s; loss: -23.2216 (±0.762542); valid loss: -17.4601 (*)\n",
      "[Epoch 7/350, Step 1400, ETA 8m 27.57s] step time: 0.006407s (±0.01203s); valid time: 0.12s; loss: -23.4941 (±0.808655); valid loss: -17.8767 (*)\n",
      "[Epoch 7/350, Step 1500, ETA 8m 21.39s] step time: 0.005513s (±0.003878s); valid time: 0.03776s; loss: -23.7075 (±0.846715); valid loss: -17.7459\n",
      "[Epoch 8/350, Step 1600, ETA 8m 19.57s] step time: 0.005801s (±0.004217s); valid time: 0.03832s; loss: -24.1093 (±0.932252); valid loss: -12.6623\n",
      "[Epoch 8/350, Step 1700, ETA 8m 18.22s] step time: 0.006388s (±0.011s); valid time: 0.1103s; loss: -22.6091 (±13.9299); valid loss: -18.7523 (*)\n",
      "[Epoch 9/350, Step 1800, ETA 8m 19.8s] step time: 0.006525s (±0.0111s); valid time: 0.1116s; loss: -24.2956 (±1.79473); valid loss: -19.1175 (*)\n",
      "[Epoch 9/350, Step 1900, ETA 8m 17.82s] step time: 0.006223s (±0.01062s); valid time: 0.1063s; loss: -24.2856 (±1.42132); valid loss: -19.495 (*)\n",
      "[Epoch 10/350, Step 2000, ETA 8m 15.52s] step time: 0.005537s (±0.003764s); valid time: 0.03658s; loss: -24.8453 (±1.03519); valid loss: -18.5642\n",
      "[Epoch 10/350, Step 2100, ETA 8m 12.66s] step time: 0.005901s (±0.004129s); valid time: 0.04001s; loss: -15.3232 (±68.1853); valid loss: -19.122\n",
      "[Epoch 10/350, Step 2150, ETA 8m 10.24s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 2200, ETA 8m 14.03s] step time: 0.006603s (±0.01168s); valid time: 0.116s; loss: -24.777 (±1.5369); valid loss: -19.8998 (*)\n",
      "[Epoch 11/350, Step 2300, ETA 8m 10.15s] step time: 0.005512s (±0.003796s); valid time: 0.03659s; loss: -25.083 (±1.15106); valid loss: -19.7905\n",
      "[Epoch 12/350, Step 2400, ETA 8m 8.972s] step time: 0.005797s (±0.003946s); valid time: 0.03822s; loss: -24.9553 (±1.72093); valid loss: -18.6822\n",
      "[Epoch 12/350, Step 2500, ETA 8m 6.248s] step time: 0.005767s (±0.004502s); valid time: 0.04457s; loss: -25.5435 (±0.994384); valid loss: -19.7385\n",
      "[Epoch 13/350, Step 2600, ETA 8m 4.367s] step time: 0.005467s (±0.00388s); valid time: 0.03779s; loss: -25.3983 (±0.881508); valid loss: -18.8956\n",
      "[Epoch 13/350, Step 2700, ETA 8m 1.965s] step time: 0.005775s (±0.003674s); valid time: 0.03521s; loss: -25.4398 (±0.956737); valid loss: -18.1708\n",
      "[Epoch 14/350, Step 2800, ETA 8m 2.657s] step time: 0.006425s (±0.01109s); valid time: 0.1109s; loss: -25.5476 (±1.01468); valid loss: -19.9659 (*)\n",
      "[Epoch 14/350, Step 2900, ETA 8m 0.1056s] step time: 0.005668s (±0.003813s); valid time: 0.03716s; loss: -25.5198 (±1.47154); valid loss: -19.0182\n",
      "[Epoch 14/350, Step 3000, ETA 7m 57.78s] step time: 0.005702s (±0.004216s); valid time: 0.04014s; loss: -25.0689 (±2.92342); valid loss: -19.6874\n",
      "[Epoch 15/350, Step 3100, ETA 7m 56.25s] step time: 0.005509s (±0.003909s); valid time: 0.03799s; loss: -11.9556 (±134.979); valid loss: -19.7781\n",
      "[Epoch 15/350, Step 3200, ETA 7m 53.89s] step time: 0.005601s (±0.003754s); valid time: 0.03724s; loss: -25.2024 (±2.26929); valid loss: -18.3603\n",
      "[Epoch 16/350, Step 3300, ETA 7m 52.98s] step time: 0.005694s (±0.003816s); valid time: 0.03788s; loss: -25.818 (±1.00021); valid loss: -17.6104\n",
      "[Epoch 16/350, Step 3400, ETA 7m 51.1s] step time: 0.005753s (±0.004003s); valid time: 0.03913s; loss: -25.3875 (±1.41387); valid loss: -19.6279\n",
      "[Epoch 17/350, Step 3500, ETA 7m 50.34s] step time: 0.005745s (±0.003836s); valid time: 0.03778s; loss: -26.0137 (±1.09276); valid loss: -19.2274\n",
      "[Epoch 17/350, Step 3600, ETA 7m 48.6s] step time: 0.00577s (±0.003844s); valid time: 0.03756s; loss: -25.7486 (±2.3932); valid loss: -12.916\n",
      "[Epoch 18/350, Step 3700, ETA 7m 47.7s] step time: 0.005669s (±0.003994s); valid time: 0.03891s; loss: -25.6197 (±2.33715); valid loss: -19.1956\n",
      "[Epoch 18/350, Step 3800, ETA 7m 45.91s] step time: 0.005687s (±0.003861s); valid time: 0.03741s; loss: -25.6565 (±1.15688); valid loss: -19.2262\n",
      "[Epoch 19/350, Step 3900, ETA 7m 45.51s] step time: 0.005921s (±0.004587s); valid time: 0.04476s; loss: -14.8028 (±110.128); valid loss: -17.7892\n",
      "[Epoch 19/350, Step 4000, ETA 7m 44.14s] step time: 0.005865s (±0.00387s); valid time: 0.03644s; loss: -25.7791 (±1.7102); valid loss: -19.3044\n",
      "[Epoch 20/350, Step 4100, ETA 7m 42.98s] step time: 0.005475s (±0.003735s); valid time: 0.03646s; loss: -26.0969 (±1.13996); valid loss: -18.59\n",
      "[Epoch 20/350, Step 4200, ETA 7m 41.55s] step time: 0.005801s (±0.004039s); valid time: 0.03872s; loss: 99.8569 (±1252.91); valid loss: -19.8919\n",
      "[Epoch 20/350, Step 4300, ETA 7m 41.05s] step time: 0.006352s (±0.0116s); valid time: 0.1164s; loss: -26.2428 (±1.37699); valid loss: -20.3001 (*)\n",
      "[Epoch 20/350, Step 4300, ETA 7m 41.05s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 4400, ETA 7m 40.41s] step time: 0.005732s (±0.003972s); valid time: 0.03826s; loss: -26.3315 (±1.40497); valid loss: -19.984\n",
      "[Epoch 21/350, Step 4500, ETA 7m 39.03s] step time: 0.00579s (±0.003943s); valid time: 0.0389s; loss: -26.4489 (±1.2179); valid loss: -17.4921\n",
      "[Epoch 22/350, Step 4600, ETA 7m 39.44s] step time: 0.006424s (±0.01241s); valid time: 0.1241s; loss: -26.6674 (±1.30893); valid loss: -20.4771 (*)\n",
      "[Epoch 22/350, Step 4700, ETA 7m 37.97s] step time: 0.005704s (±0.004054s); valid time: 0.03995s; loss: -26.5771 (±2.64357); valid loss: -19.8589\n",
      "[Epoch 23/350, Step 4800, ETA 7m 37.55s] step time: 0.005819s (±0.003998s); valid time: 0.03931s; loss: -21.3267 (±49.1249); valid loss: -17.189\n",
      "[Epoch 23/350, Step 4900, ETA 7m 35.94s] step time: 0.005586s (±0.003704s); valid time: 0.03648s; loss: -25.9882 (±5.08494); valid loss: -20.2486\n",
      "[Epoch 24/350, Step 5000, ETA 7m 35.25s] step time: 0.005718s (±0.003901s); valid time: 0.0374s; loss: -26.1429 (±10.5268); valid loss: -20.0509\n",
      "[Epoch 24/350, Step 5100, ETA 7m 34.88s] step time: 0.00644s (±0.01171s); valid time: 0.1172s; loss: -25.8815 (±8.65507); valid loss: -20.558 (*)\n",
      "[Epoch 25/350, Step 5200, ETA 7m 34.04s] step time: 0.005594s (±0.003937s); valid time: 0.03715s; loss: -26.9587 (±4.36037); valid loss: -20.4794\n",
      "[Epoch 25/350, Step 5300, ETA 7m 32.74s] step time: 0.005751s (±0.003888s); valid time: 0.03741s; loss: -27.0524 (±1.20668); valid loss: -19.6957\n",
      "[Epoch 26/350, Step 5400, ETA 7m 31.99s] step time: 0.005662s (±0.003725s); valid time: 0.03643s; loss: -26.6864 (±5.11751); valid loss: -17.2206\n",
      "[Epoch 26/350, Step 5500, ETA 7m 30.71s] step time: 0.005736s (±0.003898s); valid time: 0.03695s; loss: -25.4558 (±12.9667); valid loss: -19.7148\n",
      "[Epoch 27/350, Step 5600, ETA 7m 30.13s] step time: 0.005727s (±0.003976s); valid time: 0.03821s; loss: -25.5142 (±11.1368); valid loss: 4531.42\n",
      "[Epoch 27/350, Step 5700, ETA 7m 29.7s] step time: 0.006392s (±0.01144s); valid time: 0.1145s; loss: -24.2292 (±23.6105); valid loss: -20.664 (*)\n",
      "[Epoch 27/350, Step 5800, ETA 7m 28.66s] step time: 0.005894s (±0.003984s); valid time: 0.03845s; loss: -27.4102 (±1.65968); valid loss: -17.8677\n",
      "[Epoch 28/350, Step 5900, ETA 7m 28.12s] step time: 0.005799s (±0.004827s); valid time: 0.04058s; loss: -26.0901 (±11.9209); valid loss: -20.1032\n",
      "[Epoch 28/350, Step 6000, ETA 7m 27.6s] step time: 0.006334s (±0.01037s); valid time: 0.1039s; loss: 43.8553 (±581.459); valid loss: -21.0049 (*)\n",
      "[Epoch 29/350, Step 6100, ETA 7m 27.04s] step time: 0.005805s (±0.00385s); valid time: 0.03708s; loss: -26.4231 (±7.24745); valid loss: -15.1091\n",
      "[Epoch 29/350, Step 6200, ETA 7m 25.67s] step time: 0.005578s (±0.004084s); valid time: 0.03955s; loss: -27.1759 (±1.48177); valid loss: -20.4447\n",
      "[Epoch 30/350, Step 6300, ETA 7m 25.04s] step time: 0.005718s (±0.003895s); valid time: 0.03743s; loss: -26.1998 (±15.0733); valid loss: -20.7898\n",
      "[Epoch 30/350, Step 6400, ETA 7m 23.8s] step time: 0.005664s (±0.003731s); valid time: 0.03644s; loss: -26.3347 (±6.73082); valid loss: -20.576\n",
      "[Epoch 30/350, Step 6450, ETA 7m 23s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 6500, ETA 7m 23.32s] step time: 0.005828s (±0.004033s); valid time: 0.03809s; loss: -27.4585 (±2.94561); valid loss: 0.665245\n",
      "[Epoch 31/350, Step 6600, ETA 7m 22.13s] step time: 0.00568s (±0.003922s); valid time: 0.03724s; loss: 337.88 (±3353.53); valid loss: -20.2177\n",
      "[Epoch 32/350, Step 6700, ETA 7m 21.42s] step time: 0.005636s (±0.003887s); valid time: 0.03822s; loss: -21.752 (±56.3574); valid loss: -20.1772\n",
      "[Epoch 32/350, Step 6800, ETA 7m 20.29s] step time: 0.005713s (±0.003733s); valid time: 0.03644s; loss: -27.8086 (±3.07405); valid loss: -20.8115\n",
      "[Epoch 33/350, Step 6900, ETA 7m 19.86s] step time: 0.005872s (±0.003953s); valid time: 0.03738s; loss: -27.7712 (±1.65096); valid loss: -20.8068\n",
      "[Epoch 33/350, Step 7000, ETA 7m 18.65s] step time: 0.005618s (±0.003739s); valid time: 0.03715s; loss: 24.6895 (±481.674); valid loss: -20.6934\n",
      "[Epoch 34/350, Step 7100, ETA 7m 18.78s] step time: 0.006478s (±0.01049s); valid time: 0.1052s; loss: -27.9951 (±1.50244); valid loss: -21.0271 (*)\n",
      "[Epoch 34/350, Step 7200, ETA 7m 17.83s] step time: 0.005859s (±0.003873s); valid time: 0.03756s; loss: -24.6075 (±26.4695); valid loss: -11.3927\n",
      "[Epoch 34/350, Step 7300, ETA 7m 16.71s] step time: 0.005677s (±0.003811s); valid time: 0.03681s; loss: -25.1385 (±29.0132); valid loss: -18.0788\n",
      "[Epoch 35/350, Step 7400, ETA 7m 16.28s] step time: 0.005919s (±0.003969s); valid time: 0.03821s; loss: -22.6189 (±45.6064); valid loss: -20.5938\n",
      "[Epoch 35/350, Step 7500, ETA 7m 15.87s] step time: 0.00645s (±0.01132s); valid time: 0.1136s; loss: -26.6571 (±11.0085); valid loss: -21.1442 (*)\n",
      "[Epoch 36/350, Step 7600, ETA 7m 15.32s] step time: 0.005773s (±0.004097s); valid time: 0.03984s; loss: -27.191 (±10.5591); valid loss: -19.35\n",
      "[Epoch 36/350, Step 7700, ETA 7m 14.34s] step time: 0.005804s (±0.003906s); valid time: 0.03777s; loss: -26.187 (±12.0854); valid loss: 4.31396\n",
      "[Epoch 37/350, Step 7800, ETA 7m 13.84s] step time: 0.005835s (±0.003955s); valid time: 0.03757s; loss: -25.8635 (±17.0427); valid loss: -21.0684\n",
      "[Epoch 37/350, Step 7900, ETA 7m 12.8s] step time: 0.005726s (±0.0037s); valid time: 0.03637s; loss: -27.542 (±6.18937); valid loss: -20.5985\n",
      "[Epoch 38/350, Step 8000, ETA 7m 12.15s] step time: 0.005662s (±0.003793s); valid time: 0.03705s; loss: -13.4826 (±145.11); valid loss: -20.894\n",
      "[Epoch 38/350, Step 8100, ETA 7m 11.77s] step time: 0.00649s (±0.01141s); valid time: 0.1144s; loss: -24.7456 (±25.3174); valid loss: -21.1467 (*)\n",
      "[Epoch 39/350, Step 8200, ETA 7m 11.85s] step time: 0.006565s (±0.01099s); valid time: 0.1101s; loss: -28.6266 (±1.16223); valid loss: -21.3468 (*)\n",
      "[Epoch 39/350, Step 8300, ETA 7m 10.92s] step time: 0.005839s (±0.003967s); valid time: 0.03745s; loss: -27.7009 (±5.95027); valid loss: -21.3331\n",
      "[Epoch 40/350, Step 8400, ETA 7m 10.2s] step time: 0.005589s (±0.003864s); valid time: 0.038s; loss: -27.885 (±4.70293); valid loss: -18.6272\n",
      "[Epoch 40/350, Step 8500, ETA 7m 9.21s] step time: 0.005746s (±0.003954s); valid time: 0.03813s; loss: -23.9652 (±29.594); valid loss: -4.996\n",
      "[Epoch 40/350, Step 8600, ETA 7m 8.187s] step time: 0.005698s (±0.003872s); valid time: 0.0381s; loss: -27.2833 (±8.83169); valid loss: -20.9378\n",
      "[Epoch 40/350, Step 8600, ETA 7m 8.188s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 8700, ETA 7m 7.72s] step time: 0.005903s (±0.004333s); valid time: 0.04143s; loss: -6.14452 (±161.535); valid loss: -16.8244\n",
      "[Epoch 41/350, Step 8800, ETA 7m 7.45s] step time: 0.006669s (±0.01157s); valid time: 0.116s; loss: 120.953 (±1491.76); valid loss: -21.5285 (*)\n",
      "[Epoch 42/350, Step 8900, ETA 7m 7.33s] step time: 0.006376s (±0.0116s); valid time: 0.1164s; loss: -22.6515 (±47.012); valid loss: -21.8959 (*)\n",
      "[Epoch 42/350, Step 9000, ETA 7m 6.428s] step time: 0.005847s (±0.004189s); valid time: 0.03951s; loss: -27.3067 (±8.84529); valid loss: -19.1049\n",
      "[Epoch 43/350, Step 9100, ETA 7m 5.773s] step time: 0.005596s (±0.003815s); valid time: 0.03705s; loss: 319.843 (±3471.94); valid loss: -19.088\n",
      "[Epoch 43/350, Step 9200, ETA 7m 4.704s] step time: 0.005605s (±0.004196s); valid time: 0.04157s; loss: -28.7509 (±1.78142); valid loss: -7.97618\n",
      "[Epoch 44/350, Step 9300, ETA 7m 4.024s] step time: 0.005556s (±0.00383s); valid time: 0.03716s; loss: 1010.9 (±10343.7); valid loss: -16.8214\n",
      "[Epoch 44/350, Step 9400, ETA 7m 3.036s] step time: 0.005697s (±0.004116s); valid time: 0.03988s; loss: 93.4313 (±1193.97); valid loss: -21.1503\n",
      "[Epoch 45/350, Step 9500, ETA 7m 2.486s] step time: 0.005783s (±0.004348s); valid time: 0.04206s; loss: 4445.23 (±44335.4); valid loss: -21.465\n",
      "[Epoch 45/350, Step 9600, ETA 7m 1.506s] step time: 0.005685s (±0.003875s); valid time: 0.03821s; loss: -29.1378 (±2.87017); valid loss: -19.2707\n",
      "[Epoch 46/350, Step 9700, ETA 7m 0.8427s] step time: 0.005648s (±0.004079s); valid time: 0.04029s; loss: -27.9784 (±8.47705); valid loss: -20.8905\n",
      "[Epoch 46/350, Step 9800, ETA 6m 59.87s] step time: 0.005672s (±0.003796s); valid time: 0.03641s; loss: -25.0134 (±38.2139); valid loss: -21.7603\n",
      "[Epoch 47/350, Step 9900, ETA 6m 59.29s] step time: 0.005762s (±0.004313s); valid time: 0.0411s; loss: -29.0539 (±3.78712); valid loss: -21.5327\n",
      "[Epoch 47/350, Step 10000, ETA 6m 58.35s] step time: 0.005712s (±0.003823s); valid time: 0.03624s; loss: 3402.25 (±34130.2); valid loss: -19.3516\n",
      "[Epoch 47/350, Step 10100, ETA 6m 57.45s] step time: 0.005776s (±0.003962s); valid time: 0.03639s; loss: -27.4082 (±11.0609); valid loss: -18.7292\n",
      "[Epoch 48/350, Step 10200, ETA 6m 57.21s] step time: 0.006293s (±0.01108s); valid time: 0.1106s; loss: 56.2709 (±594.677); valid loss: -22.0465 (*)\n",
      "[Epoch 48/350, Step 10300, ETA 6m 56.23s] step time: 0.005632s (±0.003787s); valid time: 0.03683s; loss: 1.40848 (±180.572); valid loss: -20.9051\n",
      "[Epoch 49/350, Step 10400, ETA 6m 55.66s] step time: 0.005762s (±0.003725s); valid time: 0.03691s; loss: -22.5467 (±47.9139); valid loss: -18.9635\n",
      "[Epoch 49/350, Step 10500, ETA 6m 54.67s] step time: 0.005605s (±0.004329s); valid time: 0.04285s; loss: 101.846 (±1185.66); valid loss: -21.4354\n",
      "[Epoch 50/350, Step 10600, ETA 6m 54.01s] step time: 0.00556s (±0.003938s); valid time: 0.03885s; loss: -29.1596 (±5.15278); valid loss: -20.5426\n",
      "[Epoch 50/350, Step 10700, ETA 6m 53.06s] step time: 0.005657s (±0.003849s); valid time: 0.03771s; loss: -14.8141 (±102.724); valid loss: 3148.52\n",
      "[Epoch 50/350, Step 10750, ETA 6m 52.55s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 51/350, Step 10800, ETA 6m 52.57s] step time: 0.005884s (±0.003934s); valid time: 0.03794s; loss: 195.172 (±2227.56); valid loss: 19.1661\n",
      "[Epoch 51/350, Step 10900, ETA 6m 51.71s] step time: 0.005789s (±0.00388s); valid time: 0.03753s; loss: 3255.25 (±20370.1); valid loss: 1408.42\n",
      "[Epoch 52/350, Step 11000, ETA 6m 51s] step time: 0.005513s (±0.003925s); valid time: 0.0385s; loss: 304.373 (±2689.18); valid loss: -12.8515\n",
      "[Epoch 52/350, Step 11100, ETA 6m 50.1s] step time: 0.005714s (±0.003767s); valid time: 0.03617s; loss: 9551.73 (±82072); valid loss: -21.7544\n",
      "[Epoch 53/350, Step 11200, ETA 6m 49.43s] step time: 0.005562s (±0.003822s); valid time: 0.03756s; loss: -28.7755 (±7.15751); valid loss: 30.3835\n",
      "[Epoch 53/350, Step 11300, ETA 6m 48.51s] step time: 0.005654s (±0.004003s); valid time: 0.03916s; loss: -28.6704 (±5.51535); valid loss: -15.5533\n",
      "[Epoch 54/350, Step 11400, ETA 6m 47.96s] step time: 0.005808s (±0.00409s); valid time: 0.03997s; loss: 12.8249 (±422.858); valid loss: -21.7869\n",
      "[Epoch 54/350, Step 11500, ETA 6m 47.46s] step time: 0.006405s (±0.01126s); valid time: 0.1128s; loss: 900.585 (±9250.52); valid loss: -22.4177 (*)\n",
      "[Epoch 54/350, Step 11600, ETA 6m 46.57s] step time: 0.005691s (±0.004172s); valid time: 0.03947s; loss: 104.7 (±1336.59); valid loss: -22.1406\n",
      "[Epoch 55/350, Step 11700, ETA 6m 45.95s] step time: 0.005667s (±0.00471s); valid time: 0.04647s; loss: 140.807 (±1472.06); valid loss: -22.0724\n",
      "[Epoch 55/350, Step 11800, ETA 6m 45.06s] step time: 0.005677s (±0.003742s); valid time: 0.03667s; loss: -16.0381 (±133.17); valid loss: -20.7908\n",
      "[Epoch 56/350, Step 11900, ETA 6m 44.89s] step time: 0.006515s (±0.01111s); valid time: 0.1111s; loss: -3.38402 (±237.909); valid loss: -22.4755 (*)\n",
      "[Epoch 56/350, Step 12000, ETA 6m 44.01s] step time: 0.005683s (±0.003733s); valid time: 0.03652s; loss: 243.419 (±2181.33); valid loss: 143.968\n",
      "[Epoch 57/350, Step 12100, ETA 6m 43.47s] step time: 0.005822s (±0.003952s); valid time: 0.03833s; loss: -29.3775 (±3.8854); valid loss: -22.3689\n",
      "[Epoch 57/350, Step 12200, ETA 6m 42.63s] step time: 0.005757s (±0.004004s); valid time: 0.03864s; loss: 80.9301 (±961.991); valid loss: -21.8385\n",
      "[Epoch 58/350, Step 12300, ETA 6m 42.44s] step time: 0.00651s (±0.01131s); valid time: 0.1134s; loss: -27.0437 (±23.1805); valid loss: -22.6037 (*)\n",
      "[Epoch 58/350, Step 12400, ETA 6m 41.59s] step time: 0.005742s (±0.003862s); valid time: 0.03676s; loss: 4775.57 (±36280.4); valid loss: -19.4023\n",
      "[Epoch 59/350, Step 12500, ETA 6m 40.96s] step time: 0.005652s (±0.003829s); valid time: 0.03718s; loss: 2263.37 (±22818.3); valid loss: 6183.58\n",
      "[Epoch 59/350, Step 12600, ETA 6m 40.03s] step time: 0.005563s (±0.003725s); valid time: 0.03698s; loss: 20761 (±201293); valid loss: -22.5399\n",
      "[Epoch 60/350, Step 12700, ETA 6m 39.48s] step time: 0.005795s (±0.004093s); valid time: 0.03915s; loss: 37.1995 (±654.814); valid loss: 3044.02\n",
      "[Epoch 60/350, Step 12800, ETA 6m 38.56s] step time: 0.005572s (±0.003749s); valid time: 0.03745s; loss: 12.7506 (±369.246); valid loss: -22.5214\n",
      "[Epoch 60/350, Step 12900, ETA 6m 37.72s] step time: 0.005705s (±0.003783s); valid time: 0.03621s; loss: 6914.72 (±69089.9); valid loss: -17.063\n",
      "[Epoch 60/350, Step 12900, ETA 6m 37.72s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 13000, ETA 6m 37.12s] step time: 0.005723s (±0.003887s); valid time: 0.03777s; loss: -29.4275 (±6.13974); valid loss: -20.1692\n",
      "[Epoch 61/350, Step 13100, ETA 6m 36.23s] step time: 0.005592s (±0.003869s); valid time: 0.03746s; loss: -3.05252 (±257.628); valid loss: -22.3474\n",
      "[Epoch 62/350, Step 13200, ETA 6m 35.65s] step time: 0.005757s (±0.003789s); valid time: 0.03622s; loss: 13911.8 (±97751.8); valid loss: -22.4544\n",
      "[Epoch 62/350, Step 13300, ETA 6m 34.83s] step time: 0.005743s (±0.003976s); valid time: 0.03889s; loss: -30.4227 (±1.3618); valid loss: -14.4909\n",
      "[Epoch 63/350, Step 13400, ETA 6m 34.28s] step time: 0.005733s (±0.003697s); valid time: 0.03682s; loss: 27.3696 (±483.717); valid loss: -19.8423\n",
      "[Epoch 63/350, Step 13500, ETA 6m 33.59s] step time: 0.006008s (±0.004048s); valid time: 0.03827s; loss: -16.216 (±108.898); valid loss: -22.5885\n",
      "[Epoch 64/350, Step 13600, ETA 6m 32.9s] step time: 0.005523s (±0.003763s); valid time: 0.03699s; loss: -30.3939 (±2.33858); valid loss: -19.4855\n",
      "[Epoch 64/350, Step 13700, ETA 6m 32.05s] step time: 0.005671s (±0.003852s); valid time: 0.03623s; loss: -27.8755 (±15.916); valid loss: 207.602\n",
      "[Epoch 65/350, Step 13800, ETA 6m 31.35s] step time: 0.005477s (±0.003909s); valid time: 0.03829s; loss: 9469.27 (±94488.9); valid loss: 6.29115\n",
      "[Epoch 65/350, Step 13900, ETA 6m 30.58s] step time: 0.005819s (±0.003952s); valid time: 0.038s; loss: 11022.7 (±109792); valid loss: -20.0663\n",
      "[Epoch 66/350, Step 14000, ETA 6m 29.99s] step time: 0.005659s (±0.003838s); valid time: 0.03725s; loss: 751.861 (±7781.19); valid loss: 102.445\n",
      "[Epoch 66/350, Step 14100, ETA 6m 29.14s] step time: 0.005629s (±0.003692s); valid time: 0.03603s; loss: 379.34 (±3454.64); valid loss: -9.02607\n",
      "[Epoch 67/350, Step 14200, ETA 6m 28.56s] step time: 0.005741s (±0.003952s); valid time: 0.03761s; loss: 3376.1 (±33897.1); valid loss: 6538.31\n",
      "[Epoch 67/350, Step 14300, ETA 6m 27.83s] step time: 0.005874s (±0.003909s); valid time: 0.03777s; loss: 4638.79 (±46451); valid loss: -21.9219\n",
      "[Epoch 67/350, Step 14400, ETA 6m 27.37s] step time: 0.006538s (±0.01094s); valid time: 0.1095s; loss: 3017.26 (±29943.3); valid loss: -23.2023 (*)\n",
      "[Epoch 68/350, Step 14500, ETA 6m 26.76s] step time: 0.005658s (±0.00376s); valid time: 0.03634s; loss: 5806.55 (±55318); valid loss: -22.0841\n",
      "[Epoch 68/350, Step 14600, ETA 6m 25.89s] step time: 0.005566s (±0.003748s); valid time: 0.03631s; loss: -12.3686 (±184.507); valid loss: -22.4281\n",
      "[Epoch 69/350, Step 14700, ETA 6m 25.31s] step time: 0.005654s (±0.00378s); valid time: 0.03748s; loss: -29.0637 (±16.8982); valid loss: -22.6035\n",
      "[Epoch 69/350, Step 14800, ETA 6m 24.49s] step time: 0.005675s (±0.003895s); valid time: 0.03793s; loss: -9.55827 (±146.383); valid loss: 26.6508\n",
      "[Epoch 70/350, Step 14900, ETA 6m 23.84s] step time: 0.005519s (±0.003904s); valid time: 0.03933s; loss: 406.575 (±4143.76); valid loss: -22.682\n",
      "[Epoch 70/350, Step 15000, ETA 6m 23.07s] step time: 0.00578s (±0.003981s); valid time: 0.03851s; loss: 321.56 (±2552.72); valid loss: 18380\n",
      "[Epoch 70/350, Step 15050, ETA 6m 22.68s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 71/350, Step 15100, ETA 6m 22.55s] step time: 0.005911s (±0.003985s); valid time: 0.03793s; loss: 1557.59 (±13305.3); valid loss: -22.7986\n",
      "[Epoch 71/350, Step 15200, ETA 6m 21.85s] step time: 0.005948s (±0.003912s); valid time: 0.03743s; loss: 1325.6 (±12371.7); valid loss: -23.026\n",
      "[Epoch 72/350, Step 15300, ETA 6m 21.25s] step time: 0.005698s (±0.003901s); valid time: 0.03697s; loss: 9688.3 (±96446.5); valid loss: -22.9652\n",
      "[Epoch 72/350, Step 15400, ETA 6m 20.42s] step time: 0.005641s (±0.00382s); valid time: 0.03701s; loss: -26.3721 (±45.5634); valid loss: -22.2657\n",
      "[Epoch 73/350, Step 15500, ETA 6m 19.85s] step time: 0.005768s (±0.003904s); valid time: 0.03773s; loss: 9398.84 (±87638.6); valid loss: -22.5351\n",
      "[Epoch 73/350, Step 15600, ETA 6m 19.03s] step time: 0.005625s (±0.003783s); valid time: 0.03733s; loss: 485.065 (±3905.98); valid loss: -21.6159\n",
      "[Epoch 74/350, Step 15700, ETA 6m 18.72s] step time: 0.006477s (±0.01121s); valid time: 0.11s; loss: 7117.93 (±71129.8); valid loss: -23.2482 (*)\n",
      "[Epoch 74/350, Step 15800, ETA 6m 18.23s] step time: 0.006499s (±0.01133s); valid time: 0.1135s; loss: 9240.98 (±91741.6); valid loss: -23.2698 (*)\n",
      "[Epoch 74/350, Step 15900, ETA 6m 17.32s] step time: 0.005384s (±0.003752s); valid time: 0.03666s; loss: 7545.33 (±75384.8); valid loss: 337.05\n",
      "[Epoch 75/350, Step 16000, ETA 6m 16.71s] step time: 0.005573s (±0.00357s); valid time: 0.03505s; loss: 635.945 (±6607.75); valid loss: 1978.86\n",
      "[Epoch 75/350, Step 16100, ETA 6m 15.93s] step time: 0.005738s (±0.003755s); valid time: 0.03635s; loss: 2716.59 (±26270.4); valid loss: -20.3945\n",
      "[Epoch 76/350, Step 16200, ETA 6m 15.28s] step time: 0.00558s (±0.003817s); valid time: 0.03729s; loss: 19419.9 (±193288); valid loss: -19.7657\n",
      "[Epoch 76/350, Step 16300, ETA 6m 14.82s] step time: 0.006599s (±0.0111s); valid time: 0.1108s; loss: 55.1801 (±858.792); valid loss: -23.4791 (*)\n",
      "[Epoch 77/350, Step 16400, ETA 6m 14.49s] step time: 0.006461s (±0.0117s); valid time: 0.1112s; loss: -29.3255 (±12.0967); valid loss: -23.5267 (*)\n",
      "[Epoch 77/350, Step 16500, ETA 6m 13.65s] step time: 0.005547s (±0.003897s); valid time: 0.03729s; loss: -29.1708 (±17.4124); valid loss: 1035.28\n",
      "[Epoch 78/350, Step 16600, ETA 6m 13s] step time: 0.005502s (±0.003821s); valid time: 0.03719s; loss: 301.058 (±1939.35); valid loss: -23.3565\n",
      "[Epoch 78/350, Step 16700, ETA 6m 12.14s] step time: 0.005487s (±0.003898s); valid time: 0.03716s; loss: -27.3133 (±27.682); valid loss: -21.7311\n",
      "[Epoch 79/350, Step 16800, ETA 6m 11.54s] step time: 0.005727s (±0.003909s); valid time: 0.03812s; loss: 270.221 (±2995.05); valid loss: 91.4178\n",
      "[Epoch 79/350, Step 16900, ETA 6m 10.8s] step time: 0.0058s (±0.004473s); valid time: 0.04341s; loss: 47.393 (±766.278); valid loss: 1673.74\n",
      "[Epoch 80/350, Step 17000, ETA 6m 10.16s] step time: 0.005554s (±0.004032s); valid time: 0.03909s; loss: 62.7589 (±932.258); valid loss: 70.7056\n",
      "[Epoch 80/350, Step 17100, ETA 6m 9.415s] step time: 0.005762s (±0.00379s); valid time: 0.03745s; loss: -16.4469 (±93.997); valid loss: -8.00308\n",
      "[Epoch 80/350, Step 17200, ETA 6m 8.629s] step time: 0.005669s (±0.003761s); valid time: 0.03666s; loss: -29.4632 (±15.1571); valid loss: 1197.1\n",
      "[Epoch 80/350, Step 17200, ETA 6m 8.63s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 17300, ETA 6m 8.071s] step time: 0.005829s (±0.003953s); valid time: 0.03744s; loss: 7.11706 (±305.359); valid loss: 719.882\n",
      "[Epoch 81/350, Step 17400, ETA 6m 7.555s] step time: 0.006473s (±0.01101s); valid time: 0.1099s; loss: 569.818 (±5978.77); valid loss: -23.6581 (*)\n",
      "[Epoch 82/350, Step 17500, ETA 6m 7.14s] step time: 0.006262s (±0.01108s); valid time: 0.111s; loss: 268.801 (±2149.58); valid loss: -23.678 (*)\n",
      "[Epoch 82/350, Step 17600, ETA 6m 6.359s] step time: 0.005674s (±0.003913s); valid time: 0.03702s; loss: 80.967 (±598.975); valid loss: -23.5991\n",
      "[Epoch 83/350, Step 17700, ETA 6m 5.974s] step time: 0.006318s (±0.01123s); valid time: 0.1124s; loss: 2324.23 (±23440.8); valid loss: -23.7608 (*)\n",
      "[Epoch 83/350, Step 17800, ETA 6m 5.229s] step time: 0.005784s (±0.004816s); valid time: 0.0473s; loss: 5133.83 (±51379.3); valid loss: 120.39\n",
      "[Epoch 84/350, Step 17900, ETA 6m 4.632s] step time: 0.005722s (±0.003995s); valid time: 0.03801s; loss: 1069.49 (±7715.43); valid loss: -23.351\n",
      "[Epoch 84/350, Step 18000, ETA 6m 3.849s] step time: 0.005657s (±0.003945s); valid time: 0.03818s; loss: 5621.38 (±56227.1); valid loss: -23.5911\n",
      "[Epoch 85/350, Step 18100, ETA 6m 3.265s] step time: 0.005773s (±0.003972s); valid time: 0.03854s; loss: -30.0222 (±18.2634); valid loss: -22.1616\n",
      "[Epoch 85/350, Step 18200, ETA 6m 2.504s] step time: 0.005713s (±0.004448s); valid time: 0.04318s; loss: 3306.57 (±33117.2); valid loss: 301.724\n",
      "[Epoch 86/350, Step 18300, ETA 6m 1.929s] step time: 0.005763s (±0.003809s); valid time: 0.03634s; loss: 18168.3 (±127289); valid loss: 5977.87\n",
      "[Epoch 86/350, Step 18400, ETA 6m 1.164s] step time: 0.005683s (±0.003717s); valid time: 0.03581s; loss: 41.4416 (±713.426); valid loss: 16220.5\n",
      "[Epoch 87/350, Step 18500, ETA 6m 0.6319s] step time: 0.005897s (±0.004358s); valid time: 0.04218s; loss: -11.6624 (±136.615); valid loss: -23.5697\n",
      "[Epoch 87/350, Step 18600, ETA 5m 59.91s] step time: 0.005823s (±0.00417s); valid time: 0.03854s; loss: 16.7434 (±407.013); valid loss: -7.50551\n",
      "[Epoch 87/350, Step 18700, ETA 5m 59.14s] step time: 0.005657s (±0.003804s); valid time: 0.03723s; loss: -25.5844 (±40.3259); valid loss: 8337.67\n",
      "[Epoch 88/350, Step 18800, ETA 5m 58.53s] step time: 0.005699s (±0.003854s); valid time: 0.03736s; loss: 133.306 (±1469.2); valid loss: 16.454\n",
      "[Epoch 88/350, Step 18900, ETA 5m 57.79s] step time: 0.005747s (±0.003998s); valid time: 0.03845s; loss: -30.3883 (±10.605); valid loss: -23.6582\n",
      "[Epoch 89/350, Step 19000, ETA 5m 57.26s] step time: 0.005959s (±0.00386s); valid time: 0.03781s; loss: 816.624 (±8390.59); valid loss: -23.1386\n",
      "[Epoch 89/350, Step 19100, ETA 5m 56.49s] step time: 0.005648s (±0.003689s); valid time: 0.03622s; loss: 5436.58 (±54409.5); valid loss: 3.10951\n",
      "[Epoch 90/350, Step 19200, ETA 5m 56.11s] step time: 0.006392s (±0.01119s); valid time: 0.1124s; loss: -11.6187 (±123.337); valid loss: -23.773 (*)\n",
      "[Epoch 90/350, Step 19300, ETA 5m 55.34s] step time: 0.005657s (±0.004741s); valid time: 0.04661s; loss: 24924.2 (±248144); valid loss: 587.596\n",
      "[Epoch 90/350, Step 19350, ETA 5m 54.92s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 91/350, Step 19400, ETA 5m 54.78s] step time: 0.005819s (±0.003875s); valid time: 0.03747s; loss: -26.0384 (±57.5671); valid loss: 169.774\n",
      "[Epoch 91/350, Step 19500, ETA 5m 54.04s] step time: 0.005759s (±0.003853s); valid time: 0.03743s; loss: 11540.8 (±82180); valid loss: 29.3091\n",
      "[Epoch 92/350, Step 19600, ETA 5m 53.44s] step time: 0.005721s (±0.00499s); valid time: 0.0499s; loss: 397.678 (±3863.12); valid loss: 336.168\n",
      "[Epoch 92/350, Step 19700, ETA 5m 52.93s] step time: 0.006557s (±0.0111s); valid time: 0.111s; loss: 3062.89 (±21695.5); valid loss: -23.9142 (*)\n",
      "[Epoch 93/350, Step 19800, ETA 5m 52.3s] step time: 0.005534s (±0.003667s); valid time: 0.03602s; loss: -29.0471 (±20.9543); valid loss: -23.1106\n",
      "[Epoch 93/350, Step 19900, ETA 5m 51.51s] step time: 0.00553s (±0.003842s); valid time: 0.03674s; loss: -5.98779 (±187.324); valid loss: -17.7046\n",
      "[Epoch 94/350, Step 20000, ETA 5m 50.9s] step time: 0.005654s (±0.003997s); valid time: 0.03871s; loss: 826.477 (±8479.43); valid loss: -17.5834\n",
      "[Epoch 94/350, Step 20100, ETA 5m 50.15s] step time: 0.005684s (±0.003771s); valid time: 0.0366s; loss: -31.0158 (±6.24363); valid loss: 174.597\n",
      "[Epoch 94/350, Step 20200, ETA 5m 49.63s] step time: 0.00654s (±0.0119s); valid time: 0.1185s; loss: 6883.44 (±68548.6); valid loss: -23.9219 (*)\n",
      "[Epoch 95/350, Step 20300, ETA 5m 49.05s] step time: 0.005803s (±0.004037s); valid time: 0.03878s; loss: 1325.82 (±9467.72); valid loss: 70.0183\n",
      "[Epoch 95/350, Step 20400, ETA 5m 48.32s] step time: 0.005751s (±0.003907s); valid time: 0.03785s; loss: 7173.16 (±52582.4); valid loss: -23.8192\n",
      "[Epoch 96/350, Step 20500, ETA 5m 47.73s] step time: 0.00577s (±0.004082s); valid time: 0.03994s; loss: 43852.3 (±416676); valid loss: -23.3375\n",
      "[Epoch 96/350, Step 20600, ETA 5m 47.2s] step time: 0.006509s (±0.0119s); valid time: 0.1191s; loss: 64.7074 (±930.594); valid loss: -24.0594 (*)\n",
      "[Epoch 97/350, Step 20700, ETA 5m 46.58s] step time: 0.00565s (±0.003928s); valid time: 0.03798s; loss: -20.0094 (±90.9944); valid loss: -15.3667\n",
      "[Epoch 97/350, Step 20800, ETA 5m 45.9s] step time: 0.005903s (±0.004397s); valid time: 0.04308s; loss: -5.60686 (±246.161); valid loss: 19.0366\n",
      "[Epoch 98/350, Step 20900, ETA 5m 45.29s] step time: 0.005728s (±0.003987s); valid time: 0.03748s; loss: -13.6592 (±167.089); valid loss: 183.918\n",
      "[Epoch 98/350, Step 21000, ETA 5m 44.54s] step time: 0.005653s (±0.003811s); valid time: 0.03756s; loss: -30.187 (±18.5529); valid loss: -23.2661\n",
      "[Epoch 99/350, Step 21100, ETA 5m 43.97s] step time: 0.005859s (±0.004085s); valid time: 0.03968s; loss: 11650.9 (±116240); valid loss: -10.1015\n",
      "[Epoch 99/350, Step 21200, ETA 5m 43.24s] step time: 0.005738s (±0.003751s); valid time: 0.03698s; loss: 2538.03 (±25361.9); valid loss: 144.114\n",
      "[Epoch 100/350, Step 21300, ETA 5m 42.76s] step time: 0.00608s (±0.004163s); valid time: 0.03875s; loss: -3.14806 (±279.334); valid loss: -5.35547\n",
      "[Epoch 100/350, Step 21400, ETA 5m 42.02s] step time: 0.005711s (±0.003855s); valid time: 0.03744s; loss: -31.7081 (±3.41001); valid loss: 111.865\n",
      "[Epoch 100/350, Step 21500, ETA 5m 41.49s] step time: 0.006532s (±0.0119s); valid time: 0.1194s; loss: -30.5187 (±16.5613); valid loss: -24.1653 (*)\n",
      "[Epoch 100/350, Step 21500, ETA 5m 41.49s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 101/350, Step 21600, ETA 5m 40.89s] step time: 0.00568s (±0.003984s); valid time: 0.03881s; loss: 7309.39 (±72005.8); valid loss: -23.9323\n",
      "[Epoch 101/350, Step 21700, ETA 5m 40.14s] step time: 0.005671s (±0.004598s); valid time: 0.04591s; loss: -32.1702 (±2.27977); valid loss: 69.3751\n",
      "[Epoch 102/350, Step 21800, ETA 5m 39.6s] step time: 0.005911s (±0.003891s); valid time: 0.03716s; loss: -19.1697 (±90.6574); valid loss: -21.034\n",
      "[Epoch 102/350, Step 21900, ETA 5m 38.86s] step time: 0.005693s (±0.003825s); valid time: 0.0368s; loss: 3472.55 (±25850.2); valid loss: 14173.9\n",
      "[Epoch 103/350, Step 22000, ETA 5m 38.23s] step time: 0.005545s (±0.003677s); valid time: 0.03695s; loss: -13.4606 (±136.595); valid loss: 897.168\n",
      "[Epoch 103/350, Step 22100, ETA 5m 37.69s] step time: 0.006501s (±0.01104s); valid time: 0.1106s; loss: -32.3472 (±1.84949); valid loss: -24.214 (*)\n",
      "[Epoch 104/350, Step 22200, ETA 5m 37.12s] step time: 0.005864s (±0.003879s); valid time: 0.03713s; loss: 5375.36 (±53801.8); valid loss: -0.681247\n",
      "[Epoch 104/350, Step 22300, ETA 5m 36.35s] step time: 0.005576s (±0.003704s); valid time: 0.03593s; loss: 25071.1 (±249715); valid loss: -22.96\n",
      "[Epoch 105/350, Step 22400, ETA 5m 35.75s] step time: 0.005721s (±0.003796s); valid time: 0.03692s; loss: 9224.68 (±91631.5); valid loss: 48064.7\n",
      "[Epoch 105/350, Step 22500, ETA 5m 34.97s] step time: 0.005525s (±0.003881s); valid time: 0.03794s; loss: -26.2557 (±53.8103); valid loss: -18.2567\n",
      "[Epoch 106/350, Step 22600, ETA 5m 34.4s] step time: 0.005803s (±0.003853s); valid time: 0.03769s; loss: 679.417 (±5922.57); valid loss: 7.82366\n",
      "[Epoch 106/350, Step 22700, ETA 5m 33.67s] step time: 0.005721s (±0.003839s); valid time: 0.0363s; loss: 1280.56 (±10780.4); valid loss: -24.0628\n",
      "[Epoch 107/350, Step 22800, ETA 5m 33.06s] step time: 0.005681s (±0.003866s); valid time: 0.03775s; loss: 17357.1 (±173019); valid loss: 236.624\n",
      "[Epoch 107/350, Step 22900, ETA 5m 32.37s] step time: 0.005885s (±0.003824s); valid time: 0.03588s; loss: 973.666 (±9349.81); valid loss: 1206.75\n",
      "[Epoch 107/350, Step 23000, ETA 5m 31.69s] step time: 0.005882s (±0.00455s); valid time: 0.0394s; loss: 1814.98 (±18380.6); valid loss: 1055.06\n",
      "[Epoch 108/350, Step 23100, ETA 5m 31.08s] step time: 0.005739s (±0.004059s); valid time: 0.03701s; loss: 8270.4 (±82591.4); valid loss: 3154.12\n",
      "[Epoch 108/350, Step 23200, ETA 5m 30.38s] step time: 0.00579s (±0.004101s); valid time: 0.039s; loss: 2021.79 (±20436.9); valid loss: -21.4243\n",
      "[Epoch 109/350, Step 23300, ETA 5m 29.93s] step time: 0.006406s (±0.01122s); valid time: 0.1124s; loss: 144.267 (±1551.77); valid loss: -24.3248 (*)\n",
      "[Epoch 109/350, Step 23400, ETA 5m 29.23s] step time: 0.005819s (±0.003853s); valid time: 0.03682s; loss: -18.7853 (±131.991); valid loss: 3427.86\n",
      "[Epoch 110/350, Step 23500, ETA 5m 28.61s] step time: 0.005589s (±0.003845s); valid time: 0.03756s; loss: 23734.7 (±236477); valid loss: -17.1238\n",
      "[Epoch 110/350, Step 23600, ETA 5m 27.84s] step time: 0.005523s (±0.004086s); valid time: 0.04004s; loss: -28.2456 (±39.2118); valid loss: -23.7073\n",
      "[Epoch 110/350, Step 23650, ETA 5m 27.46s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 23700, ETA 5m 27.26s] step time: 0.005817s (±0.003978s); valid time: 0.03754s; loss: 10126.2 (±101078); valid loss: -22.9178\n",
      "[Epoch 111/350, Step 23800, ETA 5m 26.58s] step time: 0.005903s (±0.005147s); valid time: 0.05138s; loss: 1271.06 (±12918.1); valid loss: -23.6385\n",
      "[Epoch 112/350, Step 23900, ETA 5m 25.96s] step time: 0.0057s (±0.003783s); valid time: 0.03755s; loss: 16.4818 (±263.161); valid loss: 7030.45\n",
      "[Epoch 112/350, Step 24000, ETA 5m 25.24s] step time: 0.005708s (±0.003902s); valid time: 0.0379s; loss: 8563.09 (±85036.4); valid loss: 5370.29\n",
      "[Epoch 113/350, Step 24100, ETA 5m 24.77s] step time: 0.006339s (±0.01116s); valid time: 0.1122s; loss: 12416 (±116518); valid loss: -24.3449 (*)\n",
      "[Epoch 113/350, Step 24200, ETA 5m 24.05s] step time: 0.005719s (±0.00371s); valid time: 0.03595s; loss: 5308.59 (±40366.5); valid loss: 217.127\n",
      "[Epoch 114/350, Step 24300, ETA 5m 23.43s] step time: 0.005646s (±0.003753s); valid time: 0.03634s; loss: -24.1201 (±77.8332); valid loss: -23.5483\n",
      "[Epoch 114/350, Step 24400, ETA 5m 22.66s] step time: 0.005452s (±0.003694s); valid time: 0.03598s; loss: 3810.9 (±27478.8); valid loss: -22.1652\n",
      "[Epoch 114/350, Step 24500, ETA 5m 21.94s] step time: 0.005706s (±0.003977s); valid time: 0.03821s; loss: 2542.09 (±25609.6); valid loss: -20.8817\n",
      "[Epoch 115/350, Step 24600, ETA 5m 21.34s] step time: 0.005718s (±0.004539s); valid time: 0.04399s; loss: 71.908 (±1034.83); valid loss: -23.7008\n",
      "[Epoch 115/350, Step 24700, ETA 5m 20.66s] step time: 0.005886s (±0.003989s); valid time: 0.03859s; loss: -32.5294 (±1.68161); valid loss: -20.1286\n",
      "[Epoch 116/350, Step 24800, ETA 5m 20.04s] step time: 0.00564s (±0.00381s); valid time: 0.03678s; loss: 1396.62 (±14095.2); valid loss: 15.8965\n",
      "[Epoch 116/350, Step 24900, ETA 5m 19.29s] step time: 0.005554s (±0.003594s); valid time: 0.03513s; loss: 7733.32 (±77255.8); valid loss: -17.3649\n",
      "[Epoch 117/350, Step 25000, ETA 5m 18.66s] step time: 0.005563s (±0.003819s); valid time: 0.03759s; loss: 32314.1 (±321840); valid loss: -21.9187\n",
      "[Epoch 117/350, Step 25100, ETA 5m 17.94s] step time: 0.005646s (±0.003789s); valid time: 0.0376s; loss: 994.526 (±10216); valid loss: 7735.12\n",
      "[Epoch 118/350, Step 25200, ETA 5m 17.33s] step time: 0.005685s (±0.00392s); valid time: 0.03755s; loss: 1092.81 (±10015.7); valid loss: 26361.6\n",
      "[Epoch 118/350, Step 25300, ETA 5m 16.61s] step time: 0.005677s (±0.003848s); valid time: 0.03697s; loss: 4.22725 (±252.916); valid loss: 4.4655\n",
      "[Epoch 119/350, Step 25400, ETA 5m 15.99s] step time: 0.005659s (±0.003856s); valid time: 0.0373s; loss: 7364.25 (±71147.1); valid loss: -23.9834\n",
      "[Epoch 119/350, Step 25500, ETA 5m 15.29s] step time: 0.005753s (±0.004044s); valid time: 0.03773s; loss: -5.62824 (±244.388); valid loss: -23.5408\n",
      "[Epoch 120/350, Step 25600, ETA 5m 14.72s] step time: 0.005895s (±0.005067s); valid time: 0.0497s; loss: 8560.99 (±76971.6); valid loss: -24.0886\n",
      "[Epoch 120/350, Step 25700, ETA 5m 13.98s] step time: 0.005556s (±0.003863s); valid time: 0.03775s; loss: 5736.79 (±56424.7); valid loss: 24240.9\n",
      "[Epoch 120/350, Step 25800, ETA 5m 13.25s] step time: 0.005602s (±0.004021s); valid time: 0.039s; loss: 5572.54 (±55723.5); valid loss: 4265.6\n",
      "[Epoch 120/350, Step 25800, ETA 5m 13.25s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 25900, ETA 5m 12.62s] step time: 0.005597s (±0.004061s); valid time: 0.03861s; loss: 28.5561 (±596.904); valid loss: -24.3367\n",
      "[Epoch 121/350, Step 26000, ETA 5m 11.92s] step time: 0.005731s (±0.003891s); valid time: 0.03699s; loss: 3466.98 (±34798.2); valid loss: -16.5882\n",
      "[Epoch 122/350, Step 26100, ETA 5m 11.31s] step time: 0.00574s (±0.004572s); valid time: 0.04514s; loss: 45808.6 (±414780); valid loss: 8058.27\n",
      "[Epoch 122/350, Step 26200, ETA 5m 10.63s] step time: 0.005843s (±0.003997s); valid time: 0.03799s; loss: -30.7454 (±13.5574); valid loss: -24.2632\n",
      "[Epoch 123/350, Step 26300, ETA 5m 10.02s] step time: 0.005618s (±0.003902s); valid time: 0.03803s; loss: 11491.9 (±88094.6); valid loss: -22.1678\n",
      "[Epoch 123/350, Step 26400, ETA 5m 9.281s] step time: 0.005547s (±0.003837s); valid time: 0.03787s; loss: 26.9339 (±590.701); valid loss: -24.2195\n",
      "[Epoch 124/350, Step 26500, ETA 5m 8.653s] step time: 0.005615s (±0.003675s); valid time: 0.03657s; loss: 16768.8 (±141310); valid loss: -18.4625\n",
      "[Epoch 124/350, Step 26600, ETA 5m 7.974s] step time: 0.005834s (±0.004802s); valid time: 0.04772s; loss: 1884.21 (±15804.1); valid loss: 33.3385\n",
      "[Epoch 125/350, Step 26700, ETA 5m 7.339s] step time: 0.005532s (±0.00379s); valid time: 0.03753s; loss: -32.7314 (±1.96474); valid loss: 3202.59\n",
      "[Epoch 125/350, Step 26800, ETA 5m 6.636s] step time: 0.005706s (±0.003996s); valid time: 0.03696s; loss: 15851.4 (±157855); valid loss: 1895.84\n",
      "[Epoch 126/350, Step 26900, ETA 5m 6.168s] step time: 0.006492s (±0.01103s); valid time: 0.1105s; loss: 8543.61 (±84079.3); valid loss: -24.4153 (*)\n",
      "[Epoch 126/350, Step 27000, ETA 5m 5.479s] step time: 0.005776s (±0.003922s); valid time: 0.03787s; loss: 2029.52 (±19916.1); valid loss: 8558\n",
      "[Epoch 127/350, Step 27100, ETA 5m 4.872s] step time: 0.005743s (±0.004017s); valid time: 0.0392s; loss: 1098.07 (±11252.1); valid loss: 17563.8\n",
      "[Epoch 127/350, Step 27200, ETA 5m 4.147s] step time: 0.005578s (±0.003778s); valid time: 0.03679s; loss: 3085.29 (±31013.3); valid loss: -24.3063\n",
      "[Epoch 127/350, Step 27300, ETA 5m 3.586s] step time: 0.0065s (±0.0114s); valid time: 0.1132s; loss: 18820 (±187564); valid loss: -24.479 (*)\n",
      "[Epoch 128/350, Step 27400, ETA 5m 2.981s] step time: 0.005756s (±0.003721s); valid time: 0.03609s; loss: 139.361 (±1703.54); valid loss: 14377\n",
      "[Epoch 128/350, Step 27500, ETA 5m 2.289s] step time: 0.005749s (±0.003962s); valid time: 0.03917s; loss: 18726.1 (±158567); valid loss: -24.4019\n",
      "[Epoch 129/350, Step 27600, ETA 5m 1.667s] step time: 0.005658s (±0.003569s); valid time: 0.03522s; loss: -26.7801 (±58.6298); valid loss: 34844\n",
      "[Epoch 129/350, Step 27700, ETA 5m 0.9382s] step time: 0.00553s (±0.003788s); valid time: 0.03745s; loss: 5376.67 (±42081.2); valid loss: -20.353\n",
      "[Epoch 130/350, Step 27800, ETA 5m 0.3275s] step time: 0.005606s (±0.004032s); valid time: 0.03942s; loss: -16.9502 (±156.245); valid loss: -22.6693\n",
      "[Epoch 130/350, Step 27900, ETA 4m 59.67s] step time: 0.005964s (±0.00401s); valid time: 0.03746s; loss: -32.3948 (±1.84955); valid loss: 1600.81\n",
      "[Epoch 130/350, Step 27950, ETA 4m 59.29s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 28000, ETA 4m 59.05s] step time: 0.005645s (±0.003859s); valid time: 0.0369s; loss: 42499.1 (±297889); valid loss: 8962.11\n",
      "[Epoch 131/350, Step 28100, ETA 4m 58.38s] step time: 0.005834s (±0.004094s); valid time: 0.03933s; loss: -24.7381 (±74.6054); valid loss: -24.2223\n",
      "[Epoch 132/350, Step 28200, ETA 4m 57.76s] step time: 0.005668s (±0.003846s); valid time: 0.0368s; loss: 13010.2 (±75605.9); valid loss: -24.4404\n",
      "[Epoch 132/350, Step 28300, ETA 4m 57.09s] step time: 0.005834s (±0.003825s); valid time: 0.03669s; loss: 1202.96 (±11551.7); valid loss: -24.4496\n",
      "[Epoch 133/350, Step 28400, ETA 4m 56.47s] step time: 0.005601s (±0.003648s); valid time: 0.03621s; loss: 854.218 (±8819.75); valid loss: 526.733\n",
      "[Epoch 133/350, Step 28500, ETA 4m 55.79s] step time: 0.005775s (±0.003804s); valid time: 0.03723s; loss: 22346 (±222356); valid loss: 21400.2\n",
      "[Epoch 134/350, Step 28600, ETA 4m 55.2s] step time: 0.005865s (±0.00384s); valid time: 0.03704s; loss: 42418.8 (±273243); valid loss: -24.4263\n",
      "[Epoch 134/350, Step 28700, ETA 4m 54.65s] step time: 0.00657s (±0.01284s); valid time: 0.1291s; loss: -23.4517 (±92.6133); valid loss: -24.494 (*)\n",
      "[Epoch 134/350, Step 28800, ETA 4m 54.07s] step time: 0.006428s (±0.01115s); valid time: 0.111s; loss: 12720.5 (±126802); valid loss: -24.546 (*)\n",
      "[Epoch 135/350, Step 28900, ETA 4m 53.48s] step time: 0.005868s (±0.004068s); valid time: 0.03855s; loss: -32.8927 (±1.97754); valid loss: 136.111\n",
      "[Epoch 135/350, Step 29000, ETA 4m 52.8s] step time: 0.0058s (±0.004137s); valid time: 0.0408s; loss: 11160.4 (±111362); valid loss: 360.259\n",
      "[Epoch 136/350, Step 29100, ETA 4m 52.21s] step time: 0.005843s (±0.00383s); valid time: 0.037s; loss: -32.8263 (±1.98018); valid loss: 30.9225\n",
      "[Epoch 136/350, Step 29200, ETA 4m 51.52s] step time: 0.005724s (±0.003815s); valid time: 0.0367s; loss: 4948.98 (±41722.9); valid loss: 3935.75\n",
      "[Epoch 137/350, Step 29300, ETA 4m 50.9s] step time: 0.005652s (±0.003676s); valid time: 0.03579s; loss: 4664.28 (±46198.4); valid loss: 7353.14\n",
      "[Epoch 137/350, Step 29400, ETA 4m 50.24s] step time: 0.005886s (±0.004072s); valid time: 0.03944s; loss: 2606.69 (±26015.5); valid loss: -23.8062\n",
      "[Epoch 138/350, Step 29500, ETA 4m 49.67s] step time: 0.005946s (±0.004325s); valid time: 0.04201s; loss: 211.974 (±1948.27); valid loss: 5167.9\n",
      "[Epoch 138/350, Step 29600, ETA 4m 48.99s] step time: 0.005788s (±0.004126s); valid time: 0.04013s; loss: 573.828 (±6025.26); valid loss: -24.1123\n",
      "[Epoch 139/350, Step 29700, ETA 4m 48.43s] step time: 0.006058s (±0.0039s); valid time: 0.037s; loss: 10618 (±105848); valid loss: -10.1974\n",
      "[Epoch 139/350, Step 29800, ETA 4m 47.74s] step time: 0.00575s (±0.004233s); valid time: 0.03784s; loss: 26954.7 (±164483); valid loss: -24.3367\n",
      "[Epoch 140/350, Step 29900, ETA 4m 47.13s] step time: 0.005723s (±0.003861s); valid time: 0.03793s; loss: 24265.3 (±240104); valid loss: 7143.22\n",
      "[Epoch 140/350, Step 30000, ETA 4m 46.56s] step time: 0.00646s (±0.01099s); valid time: 0.1103s; loss: -11.7591 (±132.757); valid loss: -24.6104 (*)\n",
      "[Epoch 140/350, Step 30100, ETA 4m 45.88s] step time: 0.005805s (±0.003812s); valid time: 0.03725s; loss: 1759.07 (±17638); valid loss: -23.6328\n",
      "[Epoch 140/350, Step 30100, ETA 4m 45.88s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 30200, ETA 4m 45.26s] step time: 0.005679s (±0.003736s); valid time: 0.03683s; loss: 17082.1 (±170218); valid loss: -24.4646\n",
      "[Epoch 141/350, Step 30300, ETA 4m 44.58s] step time: 0.005768s (±0.003732s); valid time: 0.03543s; loss: 341.322 (±3630.01); valid loss: -24.3337\n",
      "[Epoch 142/350, Step 30400, ETA 4m 43.98s] step time: 0.005846s (±0.003963s); valid time: 0.03851s; loss: 23000.8 (±229116); valid loss: -24.0891\n",
      "[Epoch 142/350, Step 30500, ETA 4m 43.32s] step time: 0.005861s (±0.003852s); valid time: 0.03762s; loss: -19.2373 (±94.3267); valid loss: -20.7544\n",
      "[Epoch 143/350, Step 30600, ETA 4m 42.7s] step time: 0.005613s (±0.003977s); valid time: 0.03969s; loss: 8774.54 (±86044.4); valid loss: 373.172\n",
      "[Epoch 143/350, Step 30700, ETA 4m 42.01s] step time: 0.005712s (±0.003795s); valid time: 0.03668s; loss: -32.9605 (±1.70879); valid loss: 14758.3\n",
      "[Epoch 144/350, Step 30800, ETA 4m 41.41s] step time: 0.005791s (±0.003755s); valid time: 0.03657s; loss: 2091.34 (±20758.1); valid loss: 30387\n",
      "[Epoch 144/350, Step 30900, ETA 4m 40.7s] step time: 0.005545s (±0.003792s); valid time: 0.03778s; loss: 1483.65 (±11248.4); valid loss: 316.749\n",
      "[Epoch 145/350, Step 31000, ETA 4m 40.08s] step time: 0.005713s (±0.003821s); valid time: 0.03655s; loss: 245.481 (±2634.82); valid loss: 20.8323\n",
      "[Epoch 145/350, Step 31100, ETA 4m 39.4s] step time: 0.005724s (±0.003938s); valid time: 0.03866s; loss: -27.3334 (±54.0919); valid loss: 2942.85\n",
      "[Epoch 146/350, Step 31200, ETA 4m 38.78s] step time: 0.005643s (±0.003745s); valid time: 0.03652s; loss: 6150.28 (±60701.3); valid loss: 629.048\n",
      "[Epoch 146/350, Step 31300, ETA 4m 38.13s] step time: 0.00603s (±0.004117s); valid time: 0.03862s; loss: 5628.65 (±38845.4); valid loss: -23.849\n",
      "[Epoch 147/350, Step 31400, ETA 4m 37.54s] step time: 0.005856s (±0.004267s); valid time: 0.0424s; loss: 389.404 (±4201.46); valid loss: 4357.82\n",
      "[Epoch 147/350, Step 31500, ETA 4m 36.89s] step time: 0.005936s (±0.003897s); valid time: 0.03758s; loss: 13701.7 (±79419.9); valid loss: 18398.5\n",
      "[Epoch 147/350, Step 31600, ETA 4m 36.21s] step time: 0.005711s (±0.003852s); valid time: 0.03612s; loss: 4293.43 (±29788.8); valid loss: 2588.08\n",
      "[Epoch 148/350, Step 31700, ETA 4m 35.58s] step time: 0.005635s (±0.003806s); valid time: 0.03708s; loss: 4271.86 (±25078.1); valid loss: -17.5926\n",
      "[Epoch 148/350, Step 31800, ETA 4m 34.89s] step time: 0.005662s (±0.00394s); valid time: 0.03793s; loss: 41989.1 (±380257); valid loss: 229.539\n",
      "[Epoch 149/350, Step 31900, ETA 4m 34.27s] step time: 0.005658s (±0.004198s); valid time: 0.04089s; loss: 2754.96 (±19523.9); valid loss: -22.1962\n",
      "[Epoch 149/350, Step 32000, ETA 4m 33.58s] step time: 0.005709s (±0.003768s); valid time: 0.03638s; loss: 70.3081 (±982.184); valid loss: -24.5602\n",
      "[Epoch 150/350, Step 32100, ETA 4m 32.97s] step time: 0.005692s (±0.004165s); valid time: 0.04163s; loss: 15.6151 (±460.1); valid loss: -20.0382\n",
      "[Epoch 150/350, Step 32200, ETA 4m 32.28s] step time: 0.005637s (±0.004397s); valid time: 0.04279s; loss: 561.792 (±5892.25); valid loss: 5984.63\n",
      "[Epoch 150/350, Step 32250, ETA 4m 31.93s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 151/350, Step 32300, ETA 4m 31.69s] step time: 0.005759s (±0.003806s); valid time: 0.03659s; loss: 3900.89 (±28670.7); valid loss: 259.9\n",
      "[Epoch 151/350, Step 32400, ETA 4m 30.99s] step time: 0.005579s (±0.003694s); valid time: 0.03655s; loss: -13.2641 (±194.177); valid loss: -24.5603\n",
      "[Epoch 152/350, Step 32500, ETA 4m 30.37s] step time: 0.005695s (±0.00381s); valid time: 0.03623s; loss: 243.787 (±2753.37); valid loss: -24.4853\n",
      "[Epoch 152/350, Step 32600, ETA 4m 29.79s] step time: 0.006469s (±0.01214s); valid time: 0.1216s; loss: 24844 (±190876); valid loss: -24.6134 (*)\n",
      "[Epoch 153/350, Step 32700, ETA 4m 29.16s] step time: 0.005598s (±0.003809s); valid time: 0.03675s; loss: 1849 (±12986.7); valid loss: -20.3229\n",
      "[Epoch 153/350, Step 32800, ETA 4m 28.48s] step time: 0.005711s (±0.003766s); valid time: 0.03729s; loss: 16472.7 (±98187.4); valid loss: 56142.3\n",
      "[Epoch 154/350, Step 32900, ETA 4m 27.86s] step time: 0.005623s (±0.004407s); valid time: 0.03903s; loss: -26.8679 (±40.0925); valid loss: 12.1419\n",
      "[Epoch 154/350, Step 33000, ETA 4m 27.19s] step time: 0.005793s (±0.003851s); valid time: 0.03755s; loss: 3295.15 (±32890); valid loss: -24.5293\n",
      "[Epoch 154/350, Step 33100, ETA 4m 26.52s] step time: 0.005832s (±0.003984s); valid time: 0.03744s; loss: 7362.07 (±72592.7); valid loss: -24.5966\n",
      "[Epoch 155/350, Step 33200, ETA 4m 25.92s] step time: 0.005778s (±0.004384s); valid time: 0.04314s; loss: 7651.45 (±66154); valid loss: -24.5648\n",
      "[Epoch 155/350, Step 33300, ETA 4m 25.23s] step time: 0.005665s (±0.003814s); valid time: 0.03703s; loss: -11.9757 (±203.562); valid loss: -19.1756\n",
      "[Epoch 156/350, Step 33400, ETA 4m 24.66s] step time: 0.006051s (±0.003901s); valid time: 0.03775s; loss: -31.355 (±17.3541); valid loss: 11271\n",
      "[Epoch 156/350, Step 33500, ETA 4m 23.96s] step time: 0.005549s (±0.003786s); valid time: 0.03703s; loss: 52.7673 (±844.259); valid loss: 73.7589\n",
      "[Epoch 157/350, Step 33600, ETA 4m 23.36s] step time: 0.005861s (±0.003906s); valid time: 0.03534s; loss: 9363.72 (±93486.2); valid loss: 20.3776\n",
      "[Epoch 157/350, Step 33700, ETA 4m 22.68s] step time: 0.005678s (±0.003859s); valid time: 0.03756s; loss: 9665.03 (±93347.2); valid loss: -23.7411\n",
      "[Epoch 158/350, Step 33800, ETA 4m 22.07s] step time: 0.005724s (±0.003854s); valid time: 0.03716s; loss: 6445.26 (±55165.2); valid loss: 6829.34\n",
      "[Epoch 158/350, Step 33900, ETA 4m 21.4s] step time: 0.005796s (±0.004211s); valid time: 0.03824s; loss: 2231.6 (±19743.4); valid loss: 20.0693\n",
      "[Epoch 159/350, Step 34000, ETA 4m 20.79s] step time: 0.005695s (±0.003814s); valid time: 0.03714s; loss: -27.2925 (±53.0895); valid loss: -19.361\n",
      "[Epoch 159/350, Step 34100, ETA 4m 20.13s] step time: 0.005868s (±0.003978s); valid time: 0.03844s; loss: 15482 (±112499); valid loss: -5.15873\n",
      "[Epoch 160/350, Step 34200, ETA 4m 19.53s] step time: 0.005814s (±0.004665s); valid time: 0.04526s; loss: 29397.2 (±290287); valid loss: -24.6061\n",
      "[Epoch 160/350, Step 34300, ETA 4m 18.85s] step time: 0.005696s (±0.003813s); valid time: 0.03739s; loss: 10942.8 (±109204); valid loss: -6.89472\n",
      "[Epoch 160/350, Step 34400, ETA 4m 18.2s] step time: 0.005927s (±0.003915s); valid time: 0.03887s; loss: 12787.5 (±127476); valid loss: -24.0972\n",
      "[Epoch 160/350, Step 34400, ETA 4m 18.2s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 34500, ETA 4m 17.57s] step time: 0.005584s (±0.003912s); valid time: 0.03772s; loss: 11163.1 (±79646.2); valid loss: -24.3032\n",
      "[Epoch 161/350, Step 34600, ETA 4m 16.9s] step time: 0.005777s (±0.004s); valid time: 0.03799s; loss: -32.4542 (±4.53967); valid loss: -24.1149\n",
      "[Epoch 162/350, Step 34700, ETA 4m 16.34s] step time: 0.006044s (±0.004232s); valid time: 0.04025s; loss: 5178.53 (±51476.1); valid loss: -24.3552\n",
      "[Epoch 162/350, Step 34800, ETA 4m 15.67s] step time: 0.005757s (±0.003891s); valid time: 0.03814s; loss: 9719.19 (±95738.7); valid loss: 12471.2\n",
      "[Epoch 163/350, Step 34900, ETA 4m 15.05s] step time: 0.005565s (±0.003867s); valid time: 0.03687s; loss: -29.6553 (±29.3856); valid loss: -24.2144\n",
      "[Epoch 163/350, Step 35000, ETA 4m 14.39s] step time: 0.005753s (±0.004116s); valid time: 0.03944s; loss: 125.686 (±1577.95); valid loss: -19.9051\n",
      "[Epoch 164/350, Step 35100, ETA 4m 13.78s] step time: 0.005827s (±0.003924s); valid time: 0.03833s; loss: 15149.8 (±150349); valid loss: -24.4068\n",
      "[Epoch 164/350, Step 35200, ETA 4m 13.12s] step time: 0.005808s (±0.003963s); valid time: 0.03815s; loss: 16014.2 (±92217.7); valid loss: -22.8363\n",
      "[Epoch 165/350, Step 35300, ETA 4m 12.52s] step time: 0.005851s (±0.004537s); valid time: 0.0436s; loss: 470.539 (±3577.13); valid loss: 2887.27\n",
      "[Epoch 165/350, Step 35400, ETA 4m 11.85s] step time: 0.005684s (±0.003915s); valid time: 0.03763s; loss: 16725.8 (±163354); valid loss: -22.6454\n",
      "[Epoch 166/350, Step 35500, ETA 4m 11.32s] step time: 0.006441s (±0.01132s); valid time: 0.1113s; loss: -31.0037 (±12.2196); valid loss: -24.6242 (*)\n",
      "[Epoch 166/350, Step 35600, ETA 4m 10.62s] step time: 0.005441s (±0.003746s); valid time: 0.03696s; loss: -32.7307 (±2.76528); valid loss: -15.7218\n",
      "[Epoch 167/350, Step 35700, ETA 4m 9.994s] step time: 0.005634s (±0.003873s); valid time: 0.03748s; loss: 1469.83 (±10958); valid loss: -24.368\n",
      "[Epoch 167/350, Step 35800, ETA 4m 9.292s] step time: 0.005454s (±0.003759s); valid time: 0.03715s; loss: 7973.94 (±46897.5); valid loss: -24.503\n",
      "[Epoch 167/350, Step 35900, ETA 4m 8.624s] step time: 0.005753s (±0.003871s); valid time: 0.03759s; loss: 6963.52 (±69559.2); valid loss: -24.5519\n",
      "[Epoch 168/350, Step 36000, ETA 4m 8.022s] step time: 0.005831s (±0.003875s); valid time: 0.03755s; loss: 26805.3 (±267011); valid loss: 1225.33\n",
      "[Epoch 168/350, Step 36100, ETA 4m 7.343s] step time: 0.005657s (±0.003858s); valid time: 0.03711s; loss: -30.9216 (±15.9109); valid loss: 1359.73\n",
      "[Epoch 169/350, Step 36200, ETA 4m 6.729s] step time: 0.005741s (±0.003766s); valid time: 0.03686s; loss: 1154.19 (±11814.5); valid loss: -24.3984\n",
      "[Epoch 169/350, Step 36300, ETA 4m 6.069s] step time: 0.005816s (±0.00383s); valid time: 0.03716s; loss: 25805.7 (±182753); valid loss: 13125.4\n",
      "[Epoch 170/350, Step 36400, ETA 4m 5.455s] step time: 0.005647s (±0.003904s); valid time: 0.03883s; loss: 540.507 (±5321); valid loss: -24.4217\n",
      "[Epoch 170/350, Step 36500, ETA 4m 4.778s] step time: 0.005652s (±0.003978s); valid time: 0.03851s; loss: -26.7355 (±61.029); valid loss: 103.006\n",
      "[Epoch 170/350, Step 36550, ETA 4m 4.415s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 171/350, Step 36600, ETA 4m 4.154s] step time: 0.005635s (±0.003858s); valid time: 0.0373s; loss: 4322.07 (±27833.7); valid loss: -22.7132\n",
      "[Epoch 171/350, Step 36700, ETA 4m 3.488s] step time: 0.005736s (±0.003922s); valid time: 0.03772s; loss: 233.518 (±2633.77); valid loss: -20.3156\n",
      "[Epoch 172/350, Step 36800, ETA 4m 2.865s] step time: 0.005649s (±0.003855s); valid time: 0.0376s; loss: -17.0184 (±151.836); valid loss: 2553.07\n",
      "[Epoch 172/350, Step 36900, ETA 4m 2.193s] step time: 0.005694s (±0.003819s); valid time: 0.03656s; loss: 10238.6 (±101878); valid loss: 6814.75\n",
      "[Epoch 173/350, Step 37000, ETA 4m 1.573s] step time: 0.00559s (±0.003726s); valid time: 0.03672s; loss: 1987.46 (±12703.9); valid loss: -15.7806\n",
      "[Epoch 173/350, Step 37100, ETA 4m 0.8962s] step time: 0.005634s (±0.003784s); valid time: 0.03741s; loss: -32.7506 (±2.84097); valid loss: -24.5394\n",
      "[Epoch 174/350, Step 37200, ETA 4m 0.302s] step time: 0.005896s (±0.004017s); valid time: 0.03784s; loss: 7420.49 (±73428.6); valid loss: -18.8826\n",
      "[Epoch 174/350, Step 37300, ETA 3m 59.66s] step time: 0.005941s (±0.004742s); valid time: 0.04253s; loss: 91.6034 (±993.662); valid loss: 11213.3\n",
      "[Epoch 174/350, Step 37400, ETA 3m 59s] step time: 0.005851s (±0.004003s); valid time: 0.03857s; loss: 917.456 (±9403.49); valid loss: -17.3756\n",
      "[Epoch 175/350, Step 37500, ETA 3m 58.44s] step time: 0.006207s (±0.00449s); valid time: 0.04167s; loss: -31.0122 (±10.4091); valid loss: 3077.2\n",
      "[Epoch 175/350, Step 37600, ETA 3m 57.78s] step time: 0.005817s (±0.003818s); valid time: 0.03679s; loss: 4670.6 (±46798.1); valid loss: -23.7653\n",
      "[Epoch 176/350, Step 37700, ETA 3m 57.16s] step time: 0.005628s (±0.00387s); valid time: 0.03788s; loss: 15042.6 (±99085.4); valid loss: 120.63\n",
      "[Epoch 176/350, Step 37800, ETA 3m 56.5s] step time: 0.005808s (±0.003924s); valid time: 0.03827s; loss: 703.832 (±7285.02); valid loss: 4499.79\n",
      "[Epoch 177/350, Step 37900, ETA 3m 55.88s] step time: 0.005673s (±0.003814s); valid time: 0.03702s; loss: 4432.4 (±43161.4); valid loss: 2927.63\n",
      "[Epoch 177/350, Step 38000, ETA 3m 55.34s] step time: 0.006905s (±0.004782s); valid time: 0.03722s; loss: 25748.8 (±178643); valid loss: 3.02795\n",
      "[Epoch 178/350, Step 38100, ETA 3m 54.73s] step time: 0.005791s (±0.003971s); valid time: 0.03808s; loss: 2.39416 (±213.616); valid loss: -24.5922\n",
      "[Epoch 178/350, Step 38200, ETA 3m 54.06s] step time: 0.005717s (±0.003873s); valid time: 0.038s; loss: -32.8706 (±2.48756); valid loss: -24.3087\n",
      "[Epoch 179/350, Step 38300, ETA 3m 53.44s] step time: 0.005644s (±0.003883s); valid time: 0.03685s; loss: 3196.66 (±32133.7); valid loss: -24.6021\n",
      "[Epoch 179/350, Step 38400, ETA 3m 52.75s] step time: 0.005527s (±0.003858s); valid time: 0.0382s; loss: 5431.24 (±53903.4); valid loss: 106.295\n",
      "[Epoch 180/350, Step 38500, ETA 3m 52.14s] step time: 0.005778s (±0.004114s); valid time: 0.04006s; loss: -32.8589 (±2.24208); valid loss: -24.4712\n",
      "[Epoch 180/350, Step 38600, ETA 3m 51.47s] step time: 0.00565s (±0.003879s); valid time: 0.03738s; loss: 302.596 (±3335.95); valid loss: -15.523\n",
      "[Epoch 180/350, Step 38700, ETA 3m 50.81s] step time: 0.005744s (±0.003808s); valid time: 0.03692s; loss: 22259.6 (±166504); valid loss: -21.6759\n",
      "[Epoch 180/350, Step 38700, ETA 3m 50.81s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 38800, ETA 3m 50.22s] step time: 0.005963s (±0.004058s); valid time: 0.03701s; loss: 1648.15 (±10684.4); valid loss: -22.8085\n",
      "[Epoch 181/350, Step 38900, ETA 3m 49.54s] step time: 0.00563s (±0.003818s); valid time: 0.03756s; loss: 5097.41 (±33940.6); valid loss: 76.0346\n",
      "[Epoch 182/350, Step 39000, ETA 3m 48.93s] step time: 0.00578s (±0.004113s); valid time: 0.03918s; loss: 13156.7 (±122556); valid loss: 104.173\n",
      "[Epoch 182/350, Step 39100, ETA 3m 48.27s] step time: 0.005732s (±0.004224s); valid time: 0.04111s; loss: 331.381 (±3459.6); valid loss: -1.04114\n",
      "[Epoch 183/350, Step 39200, ETA 3m 47.66s] step time: 0.005713s (±0.003988s); valid time: 0.03902s; loss: 7142.1 (±71386.5); valid loss: 9840.97\n",
      "[Epoch 183/350, Step 39300, ETA 3m 47.01s] step time: 0.005944s (±0.004102s); valid time: 0.03966s; loss: 36213.4 (±360649); valid loss: 899.441\n",
      "[Epoch 184/350, Step 39400, ETA 3m 46.4s] step time: 0.00576s (±0.004125s); valid time: 0.04016s; loss: 25285.3 (±193518); valid loss: 77.081\n",
      "[Epoch 184/350, Step 39500, ETA 3m 45.73s] step time: 0.005615s (±0.003874s); valid time: 0.03772s; loss: 904.725 (±9135.38); valid loss: -24.5747\n",
      "[Epoch 185/350, Step 39600, ETA 3m 45.19s] step time: 0.006519s (±0.01186s); valid time: 0.1184s; loss: 643.979 (±5247.48); valid loss: -24.6786 (*)\n",
      "[Epoch 185/350, Step 39700, ETA 3m 44.52s] step time: 0.005645s (±0.003944s); valid time: 0.03842s; loss: 1014.36 (±10420.9); valid loss: -24.5666\n",
      "[Epoch 186/350, Step 39800, ETA 3m 43.9s] step time: 0.005641s (±0.003796s); valid time: 0.03696s; loss: 21294.3 (±159697); valid loss: -24.4481\n",
      "[Epoch 186/350, Step 39900, ETA 3m 43.24s] step time: 0.005755s (±0.004492s); valid time: 0.04337s; loss: 17277.6 (±166207); valid loss: 5355.34\n",
      "[Epoch 187/350, Step 40000, ETA 3m 42.62s] step time: 0.005619s (±0.00366s); valid time: 0.03596s; loss: 13020.1 (±129811); valid loss: 23102.7\n",
      "[Epoch 187/350, Step 40100, ETA 3m 41.96s] step time: 0.00578s (±0.004097s); valid time: 0.03701s; loss: 8279.71 (±77603); valid loss: -24.5666\n",
      "[Epoch 187/350, Step 40200, ETA 3m 41.3s] step time: 0.005748s (±0.003875s); valid time: 0.03817s; loss: -20.3286 (±108.073); valid loss: 436.338\n",
      "[Epoch 188/350, Step 40300, ETA 3m 40.68s] step time: 0.005683s (±0.003841s); valid time: 0.0369s; loss: 5485.74 (±49172.8); valid loss: 2036.28\n",
      "[Epoch 188/350, Step 40400, ETA 3m 40.03s] step time: 0.005793s (±0.004494s); valid time: 0.04425s; loss: 12552.5 (±85651.8); valid loss: -22.3\n",
      "[Epoch 189/350, Step 40500, ETA 3m 39.41s] step time: 0.00574s (±0.003769s); valid time: 0.03627s; loss: -29.9318 (±27.9569); valid loss: -15.7443\n",
      "[Epoch 189/350, Step 40600, ETA 3m 38.75s] step time: 0.005679s (±0.003839s); valid time: 0.03676s; loss: 14280.5 (±141309); valid loss: -2.64876\n",
      "[Epoch 190/350, Step 40700, ETA 3m 38.14s] step time: 0.005726s (±0.003903s); valid time: 0.03865s; loss: -21.0661 (±84.9967); valid loss: 14.7081\n",
      "[Epoch 190/350, Step 40800, ETA 3m 37.48s] step time: 0.005735s (±0.003951s); valid time: 0.03722s; loss: 2798.19 (±28003.5); valid loss: -16.3409\n",
      "[Epoch 190/350, Step 40850, ETA 3m 37.13s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 191/350, Step 40900, ETA 3m 36.88s] step time: 0.005948s (±0.005133s); valid time: 0.04926s; loss: 85.891 (±1008.61); valid loss: -24.5159\n",
      "[Epoch 191/350, Step 41000, ETA 3m 36.22s] step time: 0.005767s (±0.003985s); valid time: 0.03815s; loss: 738.878 (±7629.84); valid loss: -19.2351\n",
      "[Epoch 192/350, Step 41100, ETA 3m 35.62s] step time: 0.00584s (±0.004075s); valid time: 0.03904s; loss: 33876 (±255886); valid loss: -24.5701\n",
      "[Epoch 192/350, Step 41200, ETA 3m 34.97s] step time: 0.005863s (±0.003883s); valid time: 0.03754s; loss: -31.8267 (±10.6004); valid loss: 1225.45\n",
      "[Epoch 193/350, Step 41300, ETA 3m 34.35s] step time: 0.005725s (±0.003906s); valid time: 0.03746s; loss: 680.364 (±7029.6); valid loss: 6702.66\n",
      "[Epoch 193/350, Step 41400, ETA 3m 33.71s] step time: 0.005895s (±0.003968s); valid time: 0.0379s; loss: 1593.28 (±16180.3); valid loss: 823.844\n",
      "[Epoch 194/350, Step 41500, ETA 3m 33.09s] step time: 0.005712s (±0.005055s); valid time: 0.0504s; loss: 4292.19 (±39797.8); valid loss: -22.9059\n",
      "[Epoch 194/350, Step 41600, ETA 3m 32.43s] step time: 0.00571s (±0.003819s); valid time: 0.03727s; loss: 51045.6 (±436013); valid loss: -20.7625\n",
      "[Epoch 194/350, Step 41700, ETA 3m 31.78s] step time: 0.005785s (±0.004048s); valid time: 0.03944s; loss: 32104 (±187191); valid loss: 197.741\n",
      "[Epoch 195/350, Step 41800, ETA 3m 31.16s] step time: 0.005754s (±0.00371s); valid time: 0.03609s; loss: -24.2471 (±85.735); valid loss: 3070.22\n",
      "[Epoch 195/350, Step 41900, ETA 3m 30.51s] step time: 0.005753s (±0.003853s); valid time: 0.03712s; loss: 26739.2 (±202457); valid loss: 353.392\n",
      "[Epoch 196/350, Step 42000, ETA 3m 29.9s] step time: 0.00587s (±0.00386s); valid time: 0.03648s; loss: 16128.2 (±131945); valid loss: 540.966\n",
      "[Epoch 196/350, Step 42100, ETA 3m 29.25s] step time: 0.005786s (±0.003968s); valid time: 0.03758s; loss: 4105.05 (±38706.9); valid loss: -23.2558\n",
      "[Epoch 197/350, Step 42200, ETA 3m 28.64s] step time: 0.005811s (±0.003979s); valid time: 0.03799s; loss: -18.1854 (±142.531); valid loss: -19.136\n",
      "[Epoch 197/350, Step 42300, ETA 3m 27.98s] step time: 0.005722s (±0.004011s); valid time: 0.03947s; loss: 10033.4 (±71327.7); valid loss: -21.3857\n",
      "[Epoch 198/350, Step 42400, ETA 3m 27.37s] step time: 0.005862s (±0.003937s); valid time: 0.03843s; loss: 60.1793 (±925.963); valid loss: -18.3917\n",
      "[Epoch 198/350, Step 42500, ETA 3m 26.72s] step time: 0.005778s (±0.004106s); valid time: 0.03986s; loss: 45032 (±392844); valid loss: 4186.81\n",
      "[Epoch 199/350, Step 42600, ETA 3m 26.11s] step time: 0.005787s (±0.003984s); valid time: 0.03896s; loss: -26.9676 (±44.4055); valid loss: 1061.86\n",
      "[Epoch 199/350, Step 42700, ETA 3m 25.45s] step time: 0.005722s (±0.004441s); valid time: 0.04261s; loss: -28.6146 (±28.3443); valid loss: -23.3457\n",
      "[Epoch 200/350, Step 42800, ETA 3m 24.84s] step time: 0.005837s (±0.004892s); valid time: 0.04741s; loss: 14733.7 (±146531); valid loss: 3780.69\n",
      "[Epoch 200/350, Step 42900, ETA 3m 24.18s] step time: 0.005692s (±0.003766s); valid time: 0.03658s; loss: 5154.64 (±38692.5); valid loss: -19.2901\n",
      "[Epoch 200/350, Step 43000, ETA 3m 23.53s] step time: 0.005809s (±0.003734s); valid time: 0.03652s; loss: 127.582 (±1572.06); valid loss: 27.4375\n",
      "[Epoch 200/350, Step 43000, ETA 3m 23.53s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 201/350, Step 43100, ETA 3m 22.91s] step time: 0.005671s (±0.003861s); valid time: 0.0377s; loss: 16022.2 (±159746); valid loss: 17785\n",
      "[Epoch 201/350, Step 43200, ETA 3m 22.27s] step time: 0.005908s (±0.003923s); valid time: 0.03835s; loss: -31.9698 (±6.9317); valid loss: -12.7475\n",
      "[Epoch 202/350, Step 43300, ETA 3m 21.66s] step time: 0.005845s (±0.003913s); valid time: 0.03851s; loss: 26238.5 (±183204); valid loss: 5132.75\n",
      "[Epoch 202/350, Step 43400, ETA 3m 20.99s] step time: 0.005561s (±0.003811s); valid time: 0.03744s; loss: 63.2273 (±882.317); valid loss: 8907.98\n",
      "[Epoch 203/350, Step 43500, ETA 3m 20.39s] step time: 0.00589s (±0.004113s); valid time: 0.03918s; loss: 2459.95 (±24803.3); valid loss: 31.4174\n",
      "[Epoch 203/350, Step 43600, ETA 3m 19.73s] step time: 0.005674s (±0.003733s); valid time: 0.03621s; loss: 12781.2 (±121371); valid loss: -22.5341\n",
      "[Epoch 204/350, Step 43700, ETA 3m 19.13s] step time: 0.005902s (±0.005727s); valid time: 0.05026s; loss: -13.815 (±153.678); valid loss: -24.6721\n",
      "[Epoch 204/350, Step 43800, ETA 3m 18.48s] step time: 0.005823s (±0.00383s); valid time: 0.03754s; loss: 7.62936 (±388.39); valid loss: 968.132\n",
      "[Epoch 205/350, Step 43900, ETA 3m 17.84s] step time: 0.00549s (±0.00376s); valid time: 0.03659s; loss: 2774.73 (±23787); valid loss: -16.7305\n",
      "[Epoch 205/350, Step 44000, ETA 3m 17.18s] step time: 0.005661s (±0.003884s); valid time: 0.0379s; loss: 3755.15 (±37677.5); valid loss: -24.5666\n",
      "[Epoch 206/350, Step 44100, ETA 3m 16.55s] step time: 0.005564s (±0.003808s); valid time: 0.03682s; loss: 348.156 (±3542.25); valid loss: -24.4333\n",
      "[Epoch 206/350, Step 44200, ETA 3m 15.9s] step time: 0.005771s (±0.003845s); valid time: 0.03677s; loss: 18306.2 (±180640); valid loss: -24.4346\n",
      "[Epoch 207/350, Step 44300, ETA 3m 15.31s] step time: 0.006119s (±0.004659s); valid time: 0.04352s; loss: 10791.6 (±80745.2); valid loss: 289.537\n",
      "[Epoch 207/350, Step 44400, ETA 3m 14.66s] step time: 0.005786s (±0.004123s); valid time: 0.03767s; loss: 25212.9 (±248054); valid loss: 104.842\n",
      "[Epoch 207/350, Step 44500, ETA 3m 14.01s] step time: 0.00576s (±0.003936s); valid time: 0.03771s; loss: 13148 (±74731.8); valid loss: -23.5432\n",
      "[Epoch 208/350, Step 44600, ETA 3m 13.39s] step time: 0.005757s (±0.003871s); valid time: 0.03792s; loss: 38143.9 (±283276); valid loss: -13.2573\n",
      "[Epoch 208/350, Step 44700, ETA 3m 12.73s] step time: 0.00565s (±0.003865s); valid time: 0.03843s; loss: 5933.65 (±46714.8); valid loss: -23.9439\n",
      "[Epoch 209/350, Step 44800, ETA 3m 12.16s] step time: 0.006516s (±0.01136s); valid time: 0.1135s; loss: 19379.2 (±142559); valid loss: -24.6968 (*)\n",
      "[Epoch 209/350, Step 44900, ETA 3m 11.51s] step time: 0.005703s (±0.003914s); valid time: 0.03775s; loss: -33.0468 (±2.13075); valid loss: 271.622\n",
      "[Epoch 210/350, Step 45000, ETA 3m 10.89s] step time: 0.005605s (±0.003861s); valid time: 0.03782s; loss: -32.6642 (±3.53162); valid loss: -24.0189\n",
      "[Epoch 210/350, Step 45100, ETA 3m 10.24s] step time: 0.005829s (±0.003905s); valid time: 0.03781s; loss: -32.8225 (±2.25492); valid loss: 31.4682\n",
      "[Epoch 210/350, Step 45150, ETA 3m 9.901s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 45200, ETA 3m 9.627s] step time: 0.005793s (±0.004264s); valid time: 0.0404s; loss: 12510.4 (±121084); valid loss: -24.549\n",
      "[Epoch 211/350, Step 45300, ETA 3m 8.976s] step time: 0.00576s (±0.003877s); valid time: 0.03714s; loss: 28285.7 (±281205); valid loss: 11249.2\n",
      "[Epoch 212/350, Step 45400, ETA 3m 8.364s] step time: 0.005822s (±0.004195s); valid time: 0.03935s; loss: 3049.83 (±30638.8); valid loss: -18.6767\n",
      "[Epoch 212/350, Step 45500, ETA 3m 7.703s] step time: 0.005609s (±0.003832s); valid time: 0.03656s; loss: -30.5036 (±18.8617); valid loss: 262.516\n",
      "[Epoch 213/350, Step 45600, ETA 3m 7.087s] step time: 0.00576s (±0.003954s); valid time: 0.03798s; loss: 4.77878 (±371.839); valid loss: 186.348\n",
      "[Epoch 213/350, Step 45700, ETA 3m 6.434s] step time: 0.005725s (±0.004035s); valid time: 0.03947s; loss: 24333.6 (±242445); valid loss: -24.3803\n",
      "[Epoch 214/350, Step 45800, ETA 3m 5.822s] step time: 0.00583s (±0.004026s); valid time: 0.03845s; loss: 10526.6 (±105068); valid loss: 3299.79\n",
      "[Epoch 214/350, Step 45900, ETA 3m 5.174s] step time: 0.005802s (±0.004883s); valid time: 0.04636s; loss: 11652 (±76502.9); valid loss: 11885.2\n",
      "[Epoch 214/350, Step 46000, ETA 3m 4.533s] step time: 0.005907s (±0.004086s); valid time: 0.04061s; loss: 22595.6 (±168561); valid loss: 97.096\n",
      "[Epoch 215/350, Step 46100, ETA 3m 3.924s] step time: 0.005892s (±0.004112s); valid time: 0.04002s; loss: 10992.7 (±92800.8); valid loss: -24.5869\n",
      "[Epoch 215/350, Step 46200, ETA 3m 3.282s] step time: 0.005875s (±0.004022s); valid time: 0.03814s; loss: 2675.15 (±26857.2); valid loss: 188.392\n",
      "[Epoch 216/350, Step 46300, ETA 3m 2.67s] step time: 0.005862s (±0.003915s); valid time: 0.0378s; loss: 19925.4 (±197839); valid loss: -24.5659\n",
      "[Epoch 216/350, Step 46400, ETA 3m 2.026s] step time: 0.005796s (±0.003865s); valid time: 0.03731s; loss: 118.826 (±1278.17); valid loss: -24.5194\n",
      "[Epoch 217/350, Step 46500, ETA 3m 1.406s] step time: 0.005728s (±0.003872s); valid time: 0.03739s; loss: 53089 (±333004); valid loss: -22.7034\n",
      "[Epoch 217/350, Step 46600, ETA 3m 0.7642s] step time: 0.005874s (±0.004037s); valid time: 0.03887s; loss: 5.9219 (±386.216); valid loss: 34688.2\n",
      "[Epoch 218/350, Step 46700, ETA 3m 0.1402s] step time: 0.00566s (±0.003894s); valid time: 0.03771s; loss: 21.5754 (±537.28); valid loss: -23.8873\n",
      "[Epoch 218/350, Step 46800, ETA 2m 59.48s] step time: 0.005643s (±0.00394s); valid time: 0.03892s; loss: 2285.03 (±18235.2); valid loss: 657.526\n",
      "[Epoch 219/350, Step 46900, ETA 2m 58.88s] step time: 0.006001s (±0.00477s); valid time: 0.04646s; loss: -31.3229 (±15.8737); valid loss: -22.8231\n",
      "[Epoch 219/350, Step 47000, ETA 2m 58.24s] step time: 0.005841s (±0.00404s); valid time: 0.03798s; loss: 46.9737 (±540.915); valid loss: 35.049\n",
      "[Epoch 220/350, Step 47100, ETA 2m 57.63s] step time: 0.006036s (±0.004093s); valid time: 0.03831s; loss: -32.8853 (±2.72645); valid loss: -22.2739\n",
      "[Epoch 220/350, Step 47200, ETA 2m 56.99s] step time: 0.005823s (±0.004097s); valid time: 0.0382s; loss: 593.779 (±6233.67); valid loss: -24.6082\n",
      "[Epoch 220/350, Step 47300, ETA 2m 56.34s] step time: 0.005721s (±0.003906s); valid time: 0.03763s; loss: 47174.9 (±469548); valid loss: 7239.95\n",
      "[Epoch 220/350, Step 47300, ETA 2m 56.34s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 47400, ETA 2m 55.72s] step time: 0.005709s (±0.003911s); valid time: 0.03877s; loss: -6.65841 (±243.001); valid loss: -4.43828\n",
      "[Epoch 221/350, Step 47500, ETA 2m 55.06s] step time: 0.005723s (±0.004001s); valid time: 0.03829s; loss: -32.8665 (±2.05615); valid loss: 703.833\n",
      "[Epoch 222/350, Step 47600, ETA 2m 54.46s] step time: 0.005927s (±0.004465s); valid time: 0.04308s; loss: 51757.3 (±333221); valid loss: 24.1694\n",
      "[Epoch 222/350, Step 47700, ETA 2m 53.81s] step time: 0.005875s (±0.003883s); valid time: 0.03704s; loss: 14.0502 (±411.378); valid loss: -24.623\n",
      "[Epoch 223/350, Step 47800, ETA 2m 53.19s] step time: 0.005624s (±0.003831s); valid time: 0.03851s; loss: 9209.17 (±91900.5); valid loss: -24.1412\n",
      "[Epoch 223/350, Step 47900, ETA 2m 52.55s] step time: 0.005879s (±0.004135s); valid time: 0.04046s; loss: 35416.5 (±281065); valid loss: -24.5519\n",
      "[Epoch 224/350, Step 48000, ETA 2m 51.93s] step time: 0.005742s (±0.004071s); valid time: 0.03975s; loss: 5822.27 (±58178.1); valid loss: -24.3993\n",
      "[Epoch 224/350, Step 48100, ETA 2m 51.28s] step time: 0.005599s (±0.003653s); valid time: 0.03601s; loss: 789.576 (±8181.96); valid loss: 194.373\n",
      "[Epoch 225/350, Step 48200, ETA 2m 50.65s] step time: 0.005721s (±0.003791s); valid time: 0.03535s; loss: 256.05 (±2798.73); valid loss: 1732.46\n",
      "[Epoch 225/350, Step 48300, ETA 2m 50s] step time: 0.005663s (±0.003726s); valid time: 0.03699s; loss: -31.7302 (±11.9292); valid loss: 5075.59\n",
      "[Epoch 226/350, Step 48400, ETA 2m 49.39s] step time: 0.005915s (±0.004238s); valid time: 0.04046s; loss: 142.429 (±1711.05); valid loss: 4897.46\n",
      "[Epoch 226/350, Step 48500, ETA 2m 48.74s] step time: 0.005751s (±0.003869s); valid time: 0.03675s; loss: 277.474 (±2670.13); valid loss: 12724.1\n",
      "[Epoch 227/350, Step 48600, ETA 2m 48.13s] step time: 0.005813s (±0.003937s); valid time: 0.03802s; loss: -14.9766 (±153.166); valid loss: 43.7116\n",
      "[Epoch 227/350, Step 48700, ETA 2m 47.48s] step time: 0.005783s (±0.004327s); valid time: 0.04015s; loss: 5872.19 (±55491.4); valid loss: -24.4478\n",
      "[Epoch 227/350, Step 48800, ETA 2m 46.83s] step time: 0.005716s (±0.004028s); valid time: 0.03843s; loss: 8272.13 (±82635.1); valid loss: -24.5152\n",
      "[Epoch 228/350, Step 48900, ETA 2m 46.22s] step time: 0.005852s (±0.003949s); valid time: 0.03744s; loss: -24.5206 (±71.2821); valid loss: 50944.1\n",
      "[Epoch 228/350, Step 49000, ETA 2m 45.57s] step time: 0.005711s (±0.003824s); valid time: 0.03762s; loss: 2851.12 (±28630.6); valid loss: -24.4748\n",
      "[Epoch 229/350, Step 49100, ETA 2m 44.95s] step time: 0.005702s (±0.003901s); valid time: 0.03835s; loss: 323.508 (±3355.8); valid loss: -24.1494\n",
      "[Epoch 229/350, Step 49200, ETA 2m 44.31s] step time: 0.005908s (±0.004223s); valid time: 0.0402s; loss: 35158.4 (±223034); valid loss: 10065.5\n",
      "[Epoch 230/350, Step 49300, ETA 2m 43.69s] step time: 0.005648s (±0.003998s); valid time: 0.03955s; loss: 518.232 (±5437.19); valid loss: 494.743\n",
      "[Epoch 230/350, Step 49400, ETA 2m 43.03s] step time: 0.005688s (±0.003773s); valid time: 0.03736s; loss: 17.3567 (±457.961); valid loss: -13.3358\n",
      "[Epoch 230/350, Step 49450, ETA 2m 42.7s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 49500, ETA 2m 42.42s] step time: 0.005781s (±0.004267s); valid time: 0.04055s; loss: 24204 (±169772); valid loss: -24.6411\n",
      "[Epoch 231/350, Step 49600, ETA 2m 41.77s] step time: 0.005671s (±0.003952s); valid time: 0.03831s; loss: 16316.9 (±151786); valid loss: 11163.1\n",
      "[Epoch 232/350, Step 49700, ETA 2m 41.15s] step time: 0.00575s (±0.003929s); valid time: 0.03817s; loss: 904.633 (±8309.74); valid loss: 100.496\n",
      "[Epoch 232/350, Step 49800, ETA 2m 40.5s] step time: 0.005777s (±0.004028s); valid time: 0.03847s; loss: 4577.06 (±35805.9); valid loss: 239.094\n",
      "[Epoch 233/350, Step 49900, ETA 2m 39.88s] step time: 0.005606s (±0.003971s); valid time: 0.03907s; loss: 17769.6 (±159847); valid loss: -24.5669\n",
      "[Epoch 233/350, Step 50000, ETA 2m 39.23s] step time: 0.005679s (±0.003943s); valid time: 0.03836s; loss: 3074.42 (±27888.7); valid loss: 464.979\n",
      "[Epoch 234/350, Step 50100, ETA 2m 38.61s] step time: 0.005899s (±0.00454s); valid time: 0.04244s; loss: 125.676 (±1475.58); valid loss: -24.4616\n",
      "[Epoch 234/350, Step 50200, ETA 2m 37.97s] step time: 0.005756s (±0.003994s); valid time: 0.0384s; loss: 2795.09 (±27254.8); valid loss: -1.74296\n",
      "[Epoch 234/350, Step 50300, ETA 2m 37.32s] step time: 0.005808s (±0.003983s); valid time: 0.03795s; loss: 2472.42 (±15908.6); valid loss: -24.1769\n",
      "[Epoch 235/350, Step 50400, ETA 2m 36.7s] step time: 0.005655s (±0.003912s); valid time: 0.03868s; loss: 1459.82 (±14832); valid loss: -23.8457\n",
      "[Epoch 235/350, Step 50500, ETA 2m 36.06s] step time: 0.005991s (±0.004021s); valid time: 0.03793s; loss: 6507.74 (±65018.8); valid loss: -24.6117\n",
      "[Epoch 236/350, Step 50600, ETA 2m 35.44s] step time: 0.005746s (±0.004592s); valid time: 0.04477s; loss: 845.155 (±8671.13); valid loss: 2776.33\n",
      "[Epoch 236/350, Step 50700, ETA 2m 34.79s] step time: 0.005613s (±0.00385s); valid time: 0.03754s; loss: 2548.78 (±25672.7); valid loss: -24.2531\n",
      "[Epoch 237/350, Step 50800, ETA 2m 34.18s] step time: 0.005885s (±0.004903s); valid time: 0.04782s; loss: -30.906 (±20.0067); valid loss: -9.51857\n",
      "[Epoch 237/350, Step 50900, ETA 2m 33.54s] step time: 0.005961s (±0.003917s); valid time: 0.0381s; loss: 2859.72 (±28609.2); valid loss: -23.1829\n",
      "[Epoch 238/350, Step 51000, ETA 2m 32.92s] step time: 0.005717s (±0.003962s); valid time: 0.03794s; loss: 1261.06 (±12842.6); valid loss: -18.2988\n",
      "[Epoch 238/350, Step 51100, ETA 2m 32.28s] step time: 0.005875s (±0.004041s); valid time: 0.03747s; loss: 5633.46 (±56293.4); valid loss: 2987.44\n",
      "[Epoch 239/350, Step 51200, ETA 2m 31.66s] step time: 0.005843s (±0.004424s); valid time: 0.04205s; loss: 17283.7 (±169608); valid loss: 518.243\n",
      "[Epoch 239/350, Step 51300, ETA 2m 31.02s] step time: 0.005784s (±0.003902s); valid time: 0.03755s; loss: 14434.6 (±118535); valid loss: -24.4468\n",
      "[Epoch 240/350, Step 51400, ETA 2m 30.4s] step time: 0.005748s (±0.004595s); valid time: 0.04547s; loss: 40641.8 (±404670); valid loss: -24.6112\n",
      "[Epoch 240/350, Step 51500, ETA 2m 29.75s] step time: 0.005726s (±0.003999s); valid time: 0.03917s; loss: 107.05 (±1358.23); valid loss: 7841.74\n",
      "[Epoch 240/350, Step 51600, ETA 2m 29.11s] step time: 0.005958s (±0.00425s); valid time: 0.04092s; loss: 387.136 (±4172.91); valid loss: -5.84658\n",
      "[Epoch 240/350, Step 51600, ETA 2m 29.11s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 51700, ETA 2m 28.49s] step time: 0.005731s (±0.004432s); valid time: 0.04344s; loss: 14924.4 (±148822); valid loss: -22.5526\n",
      "[Epoch 241/350, Step 51800, ETA 2m 27.85s] step time: 0.00577s (±0.00406s); valid time: 0.03906s; loss: 33770.4 (±272897); valid loss: -19.6312\n",
      "[Epoch 242/350, Step 51900, ETA 2m 27.24s] step time: 0.005904s (±0.004109s); valid time: 0.03793s; loss: 18467.6 (±166890); valid loss: 5269.79\n",
      "[Epoch 242/350, Step 52000, ETA 2m 26.58s] step time: 0.005582s (±0.003782s); valid time: 0.03734s; loss: 8219.69 (±56049.7); valid loss: 3871.31\n",
      "[Epoch 243/350, Step 52100, ETA 2m 25.96s] step time: 0.005637s (±0.003835s); valid time: 0.03777s; loss: 1990.39 (±20093.9); valid loss: -24.5991\n",
      "[Epoch 243/350, Step 52200, ETA 2m 25.31s] step time: 0.005607s (±0.003885s); valid time: 0.03818s; loss: -30.9142 (±21.8013); valid loss: -24.0021\n",
      "[Epoch 244/350, Step 52300, ETA 2m 24.69s] step time: 0.005839s (±0.003896s); valid time: 0.03698s; loss: 3325.47 (±25267.1); valid loss: -23.1973\n",
      "[Epoch 244/350, Step 52400, ETA 2m 24.05s] step time: 0.005809s (±0.003882s); valid time: 0.03695s; loss: 522.146 (±4430.39); valid loss: 7147.43\n",
      "[Epoch 245/350, Step 52500, ETA 2m 23.43s] step time: 0.005727s (±0.003905s); valid time: 0.03772s; loss: 10207.8 (±78429.4); valid loss: -22.1802\n",
      "[Epoch 245/350, Step 52600, ETA 2m 22.79s] step time: 0.005863s (±0.003983s); valid time: 0.03799s; loss: -23.8139 (±59.5337); valid loss: -24.1588\n",
      "[Epoch 246/350, Step 52700, ETA 2m 22.16s] step time: 0.005536s (±0.003831s); valid time: 0.03779s; loss: 2844.99 (±25377.5); valid loss: -7.2765\n",
      "[Epoch 246/350, Step 52800, ETA 2m 21.52s] step time: 0.00576s (±0.003968s); valid time: 0.03782s; loss: 17669 (±101789); valid loss: 186.727\n",
      "[Epoch 247/350, Step 52900, ETA 2m 20.9s] step time: 0.005762s (±0.003937s); valid time: 0.03772s; loss: 2820.7 (±28338.4); valid loss: 8.84605\n",
      "[Epoch 247/350, Step 53000, ETA 2m 20.25s] step time: 0.005663s (±0.003953s); valid time: 0.03862s; loss: 12352.2 (±93392.2); valid loss: 696.87\n",
      "[Epoch 247/350, Step 53100, ETA 2m 19.61s] step time: 0.005782s (±0.00407s); valid time: 0.03922s; loss: 8154.31 (±79890.6); valid loss: 975.677\n",
      "[Epoch 248/350, Step 53200, ETA 2m 18.98s] step time: 0.005649s (±0.004596s); valid time: 0.04455s; loss: 2025.4 (±20240.7); valid loss: -22.4233\n",
      "[Epoch 248/350, Step 53300, ETA 2m 18.34s] step time: 0.005754s (±0.004108s); valid time: 0.04064s; loss: -19.6734 (±100.07); valid loss: 12387.9\n",
      "[Epoch 249/350, Step 53400, ETA 2m 17.72s] step time: 0.005981s (±0.003886s); valid time: 0.03727s; loss: 12993.7 (±129478); valid loss: 41.1947\n",
      "[Epoch 249/350, Step 53500, ETA 2m 17.08s] step time: 0.005719s (±0.004626s); valid time: 0.04559s; loss: 12665.2 (±126232); valid loss: 1307.83\n",
      "[Epoch 250/350, Step 53600, ETA 2m 16.46s] step time: 0.00573s (±0.003956s); valid time: 0.03915s; loss: 17615.9 (±135140); valid loss: 47134.5\n",
      "[Epoch 250/350, Step 53700, ETA 2m 15.82s] step time: 0.005803s (±0.003941s); valid time: 0.03854s; loss: -15.7409 (±156.248); valid loss: -24.5159\n",
      "[Epoch 250/350, Step 53750, ETA 2m 15.49s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 251/350, Step 53800, ETA 2m 15.2s] step time: 0.005782s (±0.004148s); valid time: 0.04023s; loss: 14488.8 (±100258); valid loss: 1099.21\n",
      "[Epoch 251/350, Step 53900, ETA 2m 14.57s] step time: 0.005924s (±0.004071s); valid time: 0.03905s; loss: 87.1413 (±695.812); valid loss: 255.249\n",
      "[Epoch 252/350, Step 54000, ETA 2m 13.94s] step time: 0.00566s (±0.003993s); valid time: 0.03904s; loss: 10146.2 (±101265); valid loss: 14762.3\n",
      "[Epoch 252/350, Step 54100, ETA 2m 13.3s] step time: 0.005771s (±0.00401s); valid time: 0.03917s; loss: -25.4475 (±75.5029); valid loss: -24.5044\n",
      "[Epoch 253/350, Step 54200, ETA 2m 12.68s] step time: 0.005818s (±0.004133s); valid time: 0.04069s; loss: 2666.81 (±23474); valid loss: -24.656\n",
      "[Epoch 253/350, Step 54300, ETA 2m 12.04s] step time: 0.005674s (±0.004238s); valid time: 0.04147s; loss: 14015.1 (±128534); valid loss: 3527.67\n",
      "[Epoch 254/350, Step 54400, ETA 2m 11.41s] step time: 0.005733s (±0.004112s); valid time: 0.03956s; loss: 28941.8 (±211682); valid loss: -24.001\n",
      "[Epoch 254/350, Step 54500, ETA 2m 10.77s] step time: 0.005786s (±0.004653s); valid time: 0.04587s; loss: 4050.11 (±40624.1); valid loss: -24.6378\n",
      "[Epoch 254/350, Step 54600, ETA 2m 10.13s] step time: 0.005652s (±0.004064s); valid time: 0.03752s; loss: 13079.2 (±129744); valid loss: 8719.8\n",
      "[Epoch 255/350, Step 54700, ETA 2m 9.502s] step time: 0.005674s (±0.004119s); valid time: 0.0403s; loss: 2055.36 (±15730.4); valid loss: 25489.1\n",
      "[Epoch 255/350, Step 54800, ETA 2m 8.862s] step time: 0.00578s (±0.003863s); valid time: 0.03746s; loss: 10927.3 (±107383); valid loss: -24.6149\n",
      "[Epoch 256/350, Step 54900, ETA 2m 8.244s] step time: 0.005762s (±0.003879s); valid time: 0.03757s; loss: -32.8726 (±2.24779); valid loss: -23.779\n",
      "[Epoch 256/350, Step 55000, ETA 2m 7.605s] step time: 0.005827s (±0.00407s); valid time: 0.03924s; loss: 13377.7 (±129335); valid loss: 6144.15\n",
      "[Epoch 257/350, Step 55100, ETA 2m 6.98s] step time: 0.005666s (±0.004223s); valid time: 0.04175s; loss: 43178.2 (±352387); valid loss: 2718.24\n",
      "[Epoch 257/350, Step 55200, ETA 2m 6.336s] step time: 0.005677s (±0.003971s); valid time: 0.03763s; loss: 14976.2 (±131086); valid loss: -24.6471\n",
      "[Epoch 258/350, Step 55300, ETA 2m 5.709s] step time: 0.005629s (±0.004065s); valid time: 0.03983s; loss: 330.407 (±3381.55); valid loss: -16.8717\n",
      "[Epoch 258/350, Step 55400, ETA 2m 5.077s] step time: 0.005987s (±0.004677s); valid time: 0.04403s; loss: 43604.4 (±260015); valid loss: -21.4155\n",
      "[Epoch 259/350, Step 55500, ETA 2m 4.454s] step time: 0.005735s (±0.0042s); valid time: 0.04102s; loss: 19085 (±189942); valid loss: 40801.2\n",
      "[Epoch 259/350, Step 55600, ETA 2m 3.814s] step time: 0.005774s (±0.004049s); valid time: 0.03854s; loss: 10324.3 (±102991); valid loss: 30147\n",
      "[Epoch 260/350, Step 55700, ETA 2m 3.2s] step time: 0.005999s (±0.004462s); valid time: 0.04094s; loss: -32.6594 (±2.09921); valid loss: -21.497\n",
      "[Epoch 260/350, Step 55800, ETA 2m 2.554s] step time: 0.005594s (±0.003755s); valid time: 0.03692s; loss: 6031.23 (±57868.4); valid loss: -24.1914\n",
      "[Epoch 260/350, Step 55900, ETA 2m 1.917s] step time: 0.005858s (±0.004355s); valid time: 0.03887s; loss: 7900.17 (±78902.5); valid loss: 110.131\n",
      "[Epoch 260/350, Step 55900, ETA 2m 1.917s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 56000, ETA 2m 1.298s] step time: 0.005866s (±0.004097s); valid time: 0.03973s; loss: -25.2956 (±61.0966); valid loss: 699.027\n",
      "[Epoch 261/350, Step 56100, ETA 2m 0.6556s] step time: 0.005689s (±0.003896s); valid time: 0.03862s; loss: 3465.18 (±34755.4); valid loss: -6.85972\n",
      "[Epoch 262/350, Step 56200, ETA 2m 0.03597s] step time: 0.005858s (±0.004095s); valid time: 0.03825s; loss: 37229.3 (±315972); valid loss: 31.5515\n",
      "[Epoch 262/350, Step 56300, ETA 1m 59.4s] step time: 0.005852s (±0.004045s); valid time: 0.03882s; loss: 9346.9 (±93316.2); valid loss: -24.6477\n",
      "[Epoch 263/350, Step 56400, ETA 1m 58.77s] step time: 0.005586s (±0.003939s); valid time: 0.03961s; loss: 7358.81 (±56515.9); valid loss: -22.272\n",
      "[Epoch 263/350, Step 56500, ETA 1m 58.14s] step time: 0.005799s (±0.003844s); valid time: 0.03766s; loss: 32833.1 (±242181); valid loss: 418.183\n",
      "[Epoch 264/350, Step 56600, ETA 1m 57.52s] step time: 0.005883s (±0.004064s); valid time: 0.03882s; loss: 15301.8 (±119623); valid loss: 165.136\n",
      "[Epoch 264/350, Step 56700, ETA 1m 56.88s] step time: 0.005774s (±0.003912s); valid time: 0.03789s; loss: 1468.17 (±14454); valid loss: -24.5855\n",
      "[Epoch 265/350, Step 56800, ETA 1m 56.26s] step time: 0.005997s (±0.00395s); valid time: 0.03759s; loss: 14445.3 (±130740); valid loss: -24.6174\n",
      "[Epoch 265/350, Step 56900, ETA 1m 55.62s] step time: 0.00566s (±0.003913s); valid time: 0.03855s; loss: -28.521 (±42.2414); valid loss: 3626.1\n",
      "[Epoch 266/350, Step 57000, ETA 1m 55s] step time: 0.005695s (±0.00398s); valid time: 0.03802s; loss: 4547.09 (±38545.3); valid loss: 6.95229\n",
      "[Epoch 266/350, Step 57100, ETA 1m 54.36s] step time: 0.006018s (±0.004672s); valid time: 0.04535s; loss: -9.47634 (±218.071); valid loss: 7184.27\n",
      "[Epoch 267/350, Step 57200, ETA 1m 53.74s] step time: 0.005795s (±0.003903s); valid time: 0.03885s; loss: 433.32 (±3755.67); valid loss: -19.0458\n",
      "[Epoch 267/350, Step 57300, ETA 1m 53.11s] step time: 0.005845s (±0.004027s); valid time: 0.03863s; loss: -33.0512 (±2.05397); valid loss: 109.399\n",
      "[Epoch 267/350, Step 57400, ETA 1m 52.46s] step time: 0.00565s (±0.004323s); valid time: 0.04175s; loss: 384.006 (±3915.7); valid loss: 30547.7\n",
      "[Epoch 268/350, Step 57500, ETA 1m 51.84s] step time: 0.005902s (±0.003875s); valid time: 0.03707s; loss: -32.8317 (±3.08902); valid loss: 6.69465\n",
      "[Epoch 268/350, Step 57600, ETA 1m 51.2s] step time: 0.005732s (±0.00378s); valid time: 0.03676s; loss: 91.3306 (±870.696); valid loss: -24.572\n",
      "[Epoch 269/350, Step 57700, ETA 1m 50.58s] step time: 0.005723s (±0.003846s); valid time: 0.03754s; loss: 2429.54 (±24487.6); valid loss: -24.0689\n",
      "[Epoch 269/350, Step 57800, ETA 1m 49.94s] step time: 0.005731s (±0.003786s); valid time: 0.03713s; loss: -32.0646 (±11.457); valid loss: 102.014\n",
      "[Epoch 270/350, Step 57900, ETA 1m 49.31s] step time: 0.005559s (±0.003887s); valid time: 0.03879s; loss: 16820.4 (±118910); valid loss: 8623.83\n",
      "[Epoch 270/350, Step 58000, ETA 1m 48.68s] step time: 0.005863s (±0.004163s); valid time: 0.03904s; loss: -28.4331 (±37.1105); valid loss: -24.5636\n",
      "[Epoch 270/350, Step 58050, ETA 1m 48.35s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 271/350, Step 58100, ETA 1m 48.06s] step time: 0.00592s (±0.003933s); valid time: 0.03735s; loss: 614.159 (±6361.57); valid loss: -19.1819\n",
      "[Epoch 271/350, Step 58200, ETA 1m 47.43s] step time: 0.005872s (±0.003899s); valid time: 0.0381s; loss: -32.5848 (±4.26928); valid loss: -14.1746\n",
      "[Epoch 272/350, Step 58300, ETA 1m 46.8s] step time: 0.00588s (±0.004331s); valid time: 0.04086s; loss: 6505.66 (±58352.4); valid loss: -24.5234\n",
      "[Epoch 272/350, Step 58400, ETA 1m 46.17s] step time: 0.006022s (±0.004066s); valid time: 0.03897s; loss: 580.205 (±4389.27); valid loss: 326.235\n",
      "[Epoch 273/350, Step 58500, ETA 1m 45.55s] step time: 0.005673s (±0.004206s); valid time: 0.04102s; loss: -29.8379 (±23.0385); valid loss: 49.8169\n",
      "[Epoch 273/350, Step 58600, ETA 1m 44.91s] step time: 0.005844s (±0.003931s); valid time: 0.03837s; loss: 11608.5 (±66423.5); valid loss: -9.46639\n",
      "[Epoch 274/350, Step 58700, ETA 1m 44.29s] step time: 0.005693s (±0.004284s); valid time: 0.04217s; loss: 1998.22 (±13567.4); valid loss: 5.4683\n",
      "[Epoch 274/350, Step 58800, ETA 1m 43.65s] step time: 0.005954s (±0.004021s); valid time: 0.03936s; loss: 165.396 (±1973.25); valid loss: -8.66116\n",
      "[Epoch 274/350, Step 58900, ETA 1m 43.02s] step time: 0.005934s (±0.004425s); valid time: 0.04221s; loss: -31.5023 (±11.1572); valid loss: 808.832\n",
      "[Epoch 275/350, Step 59000, ETA 1m 42.39s] step time: 0.005688s (±0.003807s); valid time: 0.03783s; loss: 25484.8 (±128180); valid loss: 7547.7\n",
      "[Epoch 275/350, Step 59100, ETA 1m 41.76s] step time: 0.006038s (±0.004582s); valid time: 0.04343s; loss: 94.4695 (±1194.39); valid loss: -24.4208\n",
      "[Epoch 276/350, Step 59200, ETA 1m 41.14s] step time: 0.00576s (±0.004828s); valid time: 0.04787s; loss: -31.6457 (±15.4681); valid loss: -24.625\n",
      "[Epoch 276/350, Step 59300, ETA 1m 40.5s] step time: 0.005694s (±0.003818s); valid time: 0.03768s; loss: 13004.9 (±92973.4); valid loss: 5203.02\n",
      "[Epoch 277/350, Step 59400, ETA 1m 39.88s] step time: 0.005817s (±0.003958s); valid time: 0.0381s; loss: 3789.34 (±37940.9); valid loss: 8754.15\n",
      "[Epoch 277/350, Step 59500, ETA 1m 39.24s] step time: 0.005733s (±0.004224s); valid time: 0.04073s; loss: -31.8681 (±12.675); valid loss: 21.6762\n",
      "[Epoch 278/350, Step 59600, ETA 1m 38.61s] step time: 0.005727s (±0.004005s); valid time: 0.03802s; loss: 210.668 (±2161.44); valid loss: -23.5572\n",
      "[Epoch 278/350, Step 59700, ETA 1m 37.98s] step time: 0.005952s (±0.004122s); valid time: 0.03954s; loss: 17782.8 (±162526); valid loss: 12916.9\n",
      "[Epoch 279/350, Step 59800, ETA 1m 37.36s] step time: 0.005738s (±0.004256s); valid time: 0.04143s; loss: 4859.35 (±48673.7); valid loss: 6700.38\n",
      "[Epoch 279/350, Step 59900, ETA 1m 36.72s] step time: 0.005865s (±0.004168s); valid time: 0.03899s; loss: 201.195 (±2324.57); valid loss: -21.4444\n",
      "[Epoch 280/350, Step 60000, ETA 1m 36.1s] step time: 0.005908s (±0.005464s); valid time: 0.05372s; loss: 28.0661 (±599.98); valid loss: -12.0021\n",
      "[Epoch 280/350, Step 60100, ETA 1m 35.46s] step time: 0.005799s (±0.003971s); valid time: 0.03796s; loss: 21789.7 (±204074); valid loss: -24.5121\n",
      "[Epoch 280/350, Step 60200, ETA 1m 34.83s] step time: 0.005746s (±0.004044s); valid time: 0.03941s; loss: -30.3594 (±17.3656); valid loss: 14028.7\n",
      "[Epoch 280/350, Step 60200, ETA 1m 34.83s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 60300, ETA 1m 34.2s] step time: 0.005837s (±0.003917s); valid time: 0.03829s; loss: 8341.97 (±80935.5); valid loss: 155.499\n",
      "[Epoch 281/350, Step 60400, ETA 1m 33.57s] step time: 0.005884s (±0.003999s); valid time: 0.03771s; loss: 11659.5 (±112766); valid loss: -22.7521\n",
      "[Epoch 282/350, Step 60500, ETA 1m 32.95s] step time: 0.005842s (±0.003943s); valid time: 0.03869s; loss: -32.8283 (±1.9893); valid loss: 162.751\n",
      "[Epoch 282/350, Step 60600, ETA 1m 32.31s] step time: 0.005674s (±0.003899s); valid time: 0.03809s; loss: -31.5324 (±10.5355); valid loss: -22.6036\n",
      "[Epoch 283/350, Step 60700, ETA 1m 31.69s] step time: 0.005817s (±0.003811s); valid time: 0.03759s; loss: 1911.66 (±19346.3); valid loss: 385.686\n",
      "[Epoch 283/350, Step 60800, ETA 1m 31.05s] step time: 0.005722s (±0.004021s); valid time: 0.0394s; loss: 28563.3 (±164677); valid loss: -24.4933\n",
      "[Epoch 284/350, Step 60900, ETA 1m 30.43s] step time: 0.005889s (±0.004092s); valid time: 0.03841s; loss: 5158.4 (±50434.8); valid loss: 9543.75\n",
      "[Epoch 284/350, Step 61000, ETA 1m 29.79s] step time: 0.005776s (±0.004175s); valid time: 0.04007s; loss: 5316.98 (±38780.5); valid loss: 18955.8\n",
      "[Epoch 285/350, Step 61100, ETA 1m 29.16s] step time: 0.005639s (±0.003804s); valid time: 0.03697s; loss: 14027.6 (±96552.8); valid loss: -4.11966\n",
      "[Epoch 285/350, Step 61200, ETA 1m 28.52s] step time: 0.005741s (±0.004127s); valid time: 0.04021s; loss: 3167.55 (±31841.8); valid loss: -24.0345\n",
      "[Epoch 286/350, Step 61300, ETA 1m 27.9s] step time: 0.005717s (±0.004564s); valid time: 0.04549s; loss: -8.50581 (±234.517); valid loss: 7626.74\n",
      "[Epoch 286/350, Step 61400, ETA 1m 27.26s] step time: 0.005764s (±0.003861s); valid time: 0.03756s; loss: -32.9824 (±2.38644); valid loss: -24.5191\n",
      "[Epoch 287/350, Step 61500, ETA 1m 26.63s] step time: 0.005672s (±0.003898s); valid time: 0.03747s; loss: 18801.5 (±153158); valid loss: 50.146\n",
      "[Epoch 287/350, Step 61600, ETA 1m 25.99s] step time: 0.005535s (±0.003709s); valid time: 0.03595s; loss: 6540.68 (±65195.8); valid loss: -24.6132\n",
      "[Epoch 287/350, Step 61700, ETA 1m 25.36s] step time: 0.005757s (±0.003999s); valid time: 0.03898s; loss: 203.606 (±2339.59); valid loss: 937.045\n",
      "[Epoch 288/350, Step 61800, ETA 1m 24.73s] step time: 0.005699s (±0.004713s); valid time: 0.04637s; loss: 11715.3 (±85815.6); valid loss: -19.0033\n",
      "[Epoch 288/350, Step 61900, ETA 1m 24.09s] step time: 0.005755s (±0.004373s); valid time: 0.04274s; loss: 1602.6 (±16218.7); valid loss: 49.8895\n",
      "[Epoch 289/350, Step 62000, ETA 1m 23.47s] step time: 0.005822s (±0.003982s); valid time: 0.03674s; loss: 3976.61 (±39760.8); valid loss: -24.5939\n",
      "[Epoch 289/350, Step 62100, ETA 1m 22.84s] step time: 0.005739s (±0.004643s); valid time: 0.04538s; loss: 129.872 (±1622.1); valid loss: -16.2234\n",
      "[Epoch 290/350, Step 62200, ETA 1m 22.21s] step time: 0.005871s (±0.004285s); valid time: 0.04179s; loss: 21173.9 (±163312); valid loss: 231.903\n",
      "[Epoch 290/350, Step 62300, ETA 1m 21.58s] step time: 0.005787s (±0.003947s); valid time: 0.03817s; loss: 6176.81 (±47267.8); valid loss: 299.751\n",
      "[Epoch 290/350, Step 62350, ETA 1m 21.26s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 291/350, Step 62400, ETA 1m 20.95s] step time: 0.005812s (±0.004047s); valid time: 0.03867s; loss: 373.921 (±3254.07); valid loss: 22130\n",
      "[Epoch 291/350, Step 62500, ETA 1m 20.32s] step time: 0.005839s (±0.004155s); valid time: 0.04048s; loss: 15263.4 (±152201); valid loss: 7.06395\n",
      "[Epoch 292/350, Step 62600, ETA 1m 19.69s] step time: 0.005646s (±0.004172s); valid time: 0.04092s; loss: 599.522 (±5549.51); valid loss: -9.35325\n",
      "[Epoch 292/350, Step 62700, ETA 1m 19.06s] step time: 0.005763s (±0.003951s); valid time: 0.03778s; loss: 23062.8 (±227819); valid loss: 67.9818\n",
      "[Epoch 293/350, Step 62800, ETA 1m 18.43s] step time: 0.00575s (±0.003973s); valid time: 0.03728s; loss: 48001.5 (±400621); valid loss: -21.3589\n",
      "[Epoch 293/350, Step 62900, ETA 1m 17.79s] step time: 0.005646s (±0.004007s); valid time: 0.03874s; loss: 10367.4 (±103482); valid loss: -24.6798\n",
      "[Epoch 294/350, Step 63000, ETA 1m 17.17s] step time: 0.005768s (±0.003976s); valid time: 0.03772s; loss: 9419.21 (±93902); valid loss: -23.1371\n",
      "[Epoch 294/350, Step 63100, ETA 1m 16.53s] step time: 0.005728s (±0.004732s); valid time: 0.04646s; loss: 7136.9 (±60751.8); valid loss: -17.9664\n",
      "[Epoch 294/350, Step 63200, ETA 1m 15.89s] step time: 0.005718s (±0.003917s); valid time: 0.03772s; loss: 5972.4 (±58053.9); valid loss: 8043\n",
      "[Epoch 295/350, Step 63300, ETA 1m 15.27s] step time: 0.005692s (±0.003834s); valid time: 0.03771s; loss: 612.025 (±6362.64); valid loss: -18.1683\n",
      "[Epoch 295/350, Step 63400, ETA 1m 14.63s] step time: 0.005615s (±0.003796s); valid time: 0.03724s; loss: 39231.3 (±381834); valid loss: 9547.36\n",
      "[Epoch 296/350, Step 63500, ETA 1m 14s] step time: 0.005688s (±0.003753s); valid time: 0.03608s; loss: 10388.2 (±103578); valid loss: -19.359\n",
      "[Epoch 296/350, Step 63600, ETA 1m 13.37s] step time: 0.005743s (±0.004635s); valid time: 0.04514s; loss: 7035.73 (±70288.2); valid loss: -24.2282\n",
      "[Epoch 297/350, Step 63700, ETA 1m 12.74s] step time: 0.005698s (±0.004003s); valid time: 0.03808s; loss: 20178.4 (±152348); valid loss: -24.642\n",
      "[Epoch 297/350, Step 63800, ETA 1m 12.1s] step time: 0.005773s (±0.004121s); valid time: 0.03849s; loss: -31.0335 (±15.2119); valid loss: 5331.46\n",
      "[Epoch 298/350, Step 63900, ETA 1m 11.48s] step time: 0.00568s (±0.00394s); valid time: 0.03792s; loss: 18235.6 (±178017); valid loss: -20.2964\n",
      "[Epoch 298/350, Step 64000, ETA 1m 10.84s] step time: 0.005774s (±0.004075s); valid time: 0.03969s; loss: 6778.47 (±67356.7); valid loss: -19.8652\n",
      "[Epoch 299/350, Step 64100, ETA 1m 10.22s] step time: 0.00574s (±0.003812s); valid time: 0.03635s; loss: 14226.2 (±110916); valid loss: 103.866\n",
      "[Epoch 299/350, Step 64200, ETA 1m 9.583s] step time: 0.005819s (±0.003882s); valid time: 0.03797s; loss: 8296.55 (±64307.1); valid loss: 20.9503\n",
      "[Epoch 300/350, Step 64300, ETA 1m 8.959s] step time: 0.005862s (±0.004023s); valid time: 0.03943s; loss: 10051.7 (±93405.7); valid loss: 17719.3\n",
      "[Epoch 300/350, Step 64400, ETA 1m 8.322s] step time: 0.005629s (±0.003935s); valid time: 0.03842s; loss: 4764.55 (±44731.9); valid loss: 79.0816\n",
      "[Epoch 300/350, Step 64500, ETA 1m 7.691s] step time: 0.005957s (±0.004007s); valid time: 0.03803s; loss: 18871 (±188092); valid loss: -24.3724\n",
      "[Epoch 300/350, Step 64500, ETA 1m 7.691s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 301/350, Step 64600, ETA 1m 7.065s] step time: 0.005697s (±0.003798s); valid time: 0.03661s; loss: 2010.37 (±20310.1); valid loss: 6248.79\n",
      "[Epoch 301/350, Step 64700, ETA 1m 6.429s] step time: 0.005681s (±0.003857s); valid time: 0.03808s; loss: 19101.3 (±183007); valid loss: -23.8913\n",
      "[Epoch 302/350, Step 64800, ETA 1m 5.803s] step time: 0.005786s (±0.004026s); valid time: 0.03803s; loss: 1487.04 (±10583.3); valid loss: -24.5968\n",
      "[Epoch 302/350, Step 64900, ETA 1m 5.168s] step time: 0.005706s (±0.004397s); valid time: 0.04316s; loss: 7057.64 (±48136.2); valid loss: 9.44463\n",
      "[Epoch 303/350, Step 65000, ETA 1m 4.541s] step time: 0.005635s (±0.00395s); valid time: 0.03816s; loss: 2454.94 (±23766.7); valid loss: -23.4567\n",
      "[Epoch 303/350, Step 65100, ETA 1m 3.907s] step time: 0.005787s (±0.00455s); valid time: 0.04468s; loss: 11237.7 (±88126.6); valid loss: 3325.12\n",
      "[Epoch 304/350, Step 65200, ETA 1m 3.282s] step time: 0.005793s (±0.003885s); valid time: 0.03772s; loss: 21596.2 (±155234); valid loss: -24.2931\n",
      "[Epoch 304/350, Step 65300, ETA 1m 2.649s] step time: 0.005829s (±0.003957s); valid time: 0.03722s; loss: -25.9217 (±67.2848); valid loss: -15.7798\n",
      "[Epoch 305/350, Step 65400, ETA 1m 2.024s] step time: 0.005819s (±0.003897s); valid time: 0.03784s; loss: -32.6388 (±2.5536); valid loss: -24.6936\n",
      "[Epoch 305/350, Step 65500, ETA 1m 1.389s] step time: 0.005668s (±0.003846s); valid time: 0.03683s; loss: 215.022 (±2014.55); valid loss: 4992.01\n",
      "[Epoch 306/350, Step 65600, ETA 1m 0.764s] step time: 0.005796s (±0.004197s); valid time: 0.04138s; loss: 31.4096 (±643.487); valid loss: 116.554\n",
      "[Epoch 306/350, Step 65700, ETA 1m 0.1301s] step time: 0.005755s (±0.003703s); valid time: 0.03663s; loss: 3893.59 (±32867.3); valid loss: 1993\n",
      "[Epoch 307/350, Step 65800, ETA 59.51s] step time: 0.006036s (±0.003915s); valid time: 0.03704s; loss: 22691.2 (±225058); valid loss: -24.5199\n",
      "[Epoch 307/350, Step 65900, ETA 58.88s] step time: 0.005858s (±0.0041s); valid time: 0.03896s; loss: -3.33498 (±293.112); valid loss: -24.5505\n",
      "[Epoch 307/350, Step 66000, ETA 58.24s] step time: 0.005907s (±0.004315s); valid time: 0.0418s; loss: 18875.5 (±133078); valid loss: -18.4235\n",
      "[Epoch 308/350, Step 66100, ETA 57.62s] step time: 0.005913s (±0.003982s); valid time: 0.03807s; loss: 9433.15 (±94186.1); valid loss: 844.219\n",
      "[Epoch 308/350, Step 66200, ETA 56.99s] step time: 0.005945s (±0.003979s); valid time: 0.03825s; loss: 677.113 (±5337.72); valid loss: -24.6378\n",
      "[Epoch 309/350, Step 66300, ETA 56.36s] step time: 0.005789s (±0.003827s); valid time: 0.03728s; loss: 42.3123 (±726.779); valid loss: 865.643\n",
      "[Epoch 309/350, Step 66400, ETA 55.73s] step time: 0.005907s (±0.004113s); valid time: 0.0398s; loss: 151.419 (±1792.58); valid loss: 11.726\n",
      "[Epoch 310/350, Step 66500, ETA 55.1s] step time: 0.005638s (±0.003959s); valid time: 0.03922s; loss: 4091.12 (±41036.6); valid loss: -10.9188\n",
      "[Epoch 310/350, Step 66600, ETA 54.47s] step time: 0.005961s (±0.003917s); valid time: 0.03826s; loss: 4570.15 (±44833); valid loss: 18248.1\n",
      "[Epoch 310/350, Step 66650, ETA 54.16s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 66700, ETA 53.85s] step time: 0.006084s (±0.003994s); valid time: 0.03817s; loss: 9763.52 (±72144.8); valid loss: -24.35\n",
      "[Epoch 311/350, Step 66800, ETA 53.22s] step time: 0.005886s (±0.004013s); valid time: 0.0377s; loss: 62584.4 (±425300); valid loss: 3435.2\n",
      "[Epoch 312/350, Step 66900, ETA 52.59s] step time: 0.005814s (±0.003998s); valid time: 0.03841s; loss: 20371.8 (±192998); valid loss: -24.5699\n",
      "[Epoch 312/350, Step 67000, ETA 51.96s] step time: 0.005853s (±0.004144s); valid time: 0.03949s; loss: 43852.7 (±398608); valid loss: 0.254247\n",
      "[Epoch 313/350, Step 67100, ETA 51.33s] step time: 0.005724s (±0.004113s); valid time: 0.03917s; loss: 6980.35 (±69650.2); valid loss: 29.8082\n",
      "[Epoch 313/350, Step 67200, ETA 50.7s] step time: 0.005909s (±0.003972s); valid time: 0.03864s; loss: 576.961 (±6029.92); valid loss: 5033.77\n",
      "[Epoch 314/350, Step 67300, ETA 50.08s] step time: 0.005879s (±0.004528s); valid time: 0.04528s; loss: 5032.72 (±49527.8); valid loss: 29.0947\n",
      "[Epoch 314/350, Step 67400, ETA 49.44s] step time: 0.005803s (±0.003902s); valid time: 0.03812s; loss: 12218.2 (±121888); valid loss: 38.7492\n",
      "[Epoch 314/350, Step 67500, ETA 48.81s] step time: 0.005738s (±0.00401s); valid time: 0.03848s; loss: 43026.9 (±251852); valid loss: 544.733\n",
      "[Epoch 315/350, Step 67600, ETA 48.18s] step time: 0.005627s (±0.003811s); valid time: 0.03682s; loss: 70421.6 (±696301); valid loss: -24.6553\n",
      "[Epoch 315/350, Step 67700, ETA 47.55s] step time: 0.00617s (±0.004109s); valid time: 0.03888s; loss: -31.3094 (±11.944); valid loss: 48.8152\n",
      "[Epoch 316/350, Step 67800, ETA 46.93s] step time: 0.005897s (±0.003939s); valid time: 0.03768s; loss: 1521.77 (±11042.8); valid loss: 510.009\n",
      "[Epoch 316/350, Step 67900, ETA 46.29s] step time: 0.005722s (±0.003852s); valid time: 0.0381s; loss: -31.9821 (±6.71713); valid loss: 3661.09\n",
      "[Epoch 317/350, Step 68000, ETA 45.67s] step time: 0.00587s (±0.004041s); valid time: 0.0391s; loss: 6338.26 (±63394.7); valid loss: -23.0689\n",
      "[Epoch 317/350, Step 68100, ETA 45.03s] step time: 0.005672s (±0.003942s); valid time: 0.03869s; loss: 314.582 (±3360.38); valid loss: -21.6612\n",
      "[Epoch 318/350, Step 68200, ETA 44.41s] step time: 0.005726s (±0.003942s); valid time: 0.03772s; loss: 4886.26 (±47746.7); valid loss: -24.6524\n",
      "[Epoch 318/350, Step 68300, ETA 43.77s] step time: 0.005777s (±0.003908s); valid time: 0.03768s; loss: -31.9313 (±8.94055); valid loss: 28.8633\n",
      "[Epoch 319/350, Step 68400, ETA 43.14s] step time: 0.005677s (±0.003777s); valid time: 0.03701s; loss: 3898.06 (±25144.8); valid loss: 3377.62\n",
      "[Epoch 319/350, Step 68500, ETA 42.51s] step time: 0.005797s (±0.004045s); valid time: 0.0378s; loss: -19.4731 (±134.505); valid loss: -6.58301\n",
      "[Epoch 320/350, Step 68600, ETA 41.88s] step time: 0.005744s (±0.004941s); valid time: 0.04939s; loss: -13.1582 (±192.36); valid loss: -17.2199\n",
      "[Epoch 320/350, Step 68700, ETA 41.25s] step time: 0.005819s (±0.003844s); valid time: 0.03727s; loss: 15178.5 (±149593); valid loss: -20.9371\n",
      "[Epoch 320/350, Step 68800, ETA 40.62s] step time: 0.005859s (±0.004069s); valid time: 0.03946s; loss: 14404.1 (±143556); valid loss: -22.8154\n",
      "[Epoch 320/350, Step 68800, ETA 40.62s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 68900, ETA 39.99s] step time: 0.005832s (±0.004135s); valid time: 0.0407s; loss: 23496.2 (±174580); valid loss: -24.0606\n",
      "[Epoch 321/350, Step 69000, ETA 39.36s] step time: 0.005857s (±0.004129s); valid time: 0.03932s; loss: -32.386 (±5.52575); valid loss: 1.52262\n",
      "[Epoch 322/350, Step 69100, ETA 38.74s] step time: 0.005886s (±0.004135s); valid time: 0.03995s; loss: 424.058 (±4542.75); valid loss: 10.8873\n",
      "[Epoch 322/350, Step 69200, ETA 38.1s] step time: 0.005734s (±0.003911s); valid time: 0.03844s; loss: 32168.8 (±320310); valid loss: -24.276\n",
      "[Epoch 323/350, Step 69300, ETA 37.47s] step time: 0.005588s (±0.003878s); valid time: 0.03853s; loss: -32.9483 (±3.03741); valid loss: -24.0886\n",
      "[Epoch 323/350, Step 69400, ETA 36.84s] step time: 0.005672s (±0.003943s); valid time: 0.03817s; loss: 8167.33 (±67404.5); valid loss: 3383.52\n",
      "[Epoch 324/350, Step 69500, ETA 36.22s] step time: 0.005866s (±0.003962s); valid time: 0.03871s; loss: 5516.06 (±43929.9); valid loss: 5760.91\n",
      "[Epoch 324/350, Step 69600, ETA 35.58s] step time: 0.005941s (±0.004055s); valid time: 0.0392s; loss: 1513.38 (±15305.7); valid loss: -22.8456\n",
      "[Epoch 325/350, Step 69700, ETA 34.96s] step time: 0.005722s (±0.003814s); valid time: 0.0372s; loss: 43676.2 (±434905); valid loss: -12.5008\n",
      "[Epoch 325/350, Step 69800, ETA 34.33s] step time: 0.005949s (±0.004165s); valid time: 0.03938s; loss: -30.0874 (±28.6906); valid loss: 54.3584\n",
      "[Epoch 326/350, Step 69900, ETA 33.7s] step time: 0.005926s (±0.004469s); valid time: 0.04262s; loss: 21.0773 (±519.056); valid loss: 13113.5\n",
      "[Epoch 326/350, Step 70000, ETA 33.07s] step time: 0.006006s (±0.004337s); valid time: 0.04187s; loss: 62740.1 (±624584); valid loss: -24.3242\n",
      "[Epoch 327/350, Step 70100, ETA 32.44s] step time: 0.005818s (±0.003963s); valid time: 0.03833s; loss: 7424.74 (±70268.8); valid loss: -23.4646\n",
      "[Epoch 327/350, Step 70200, ETA 31.81s] step time: 0.005924s (±0.004366s); valid time: 0.04216s; loss: 15488.5 (±117788); valid loss: -22.9985\n",
      "[Epoch 327/350, Step 70300, ETA 31.18s] step time: 0.005847s (±0.003931s); valid time: 0.03708s; loss: -29.9801 (±27.4788); valid loss: -24.5685\n",
      "[Epoch 328/350, Step 70400, ETA 30.55s] step time: 0.005911s (±0.004151s); valid time: 0.03947s; loss: -26.9347 (±57.9461); valid loss: 3372.89\n",
      "[Epoch 328/350, Step 70500, ETA 29.92s] step time: 0.00577s (±0.004245s); valid time: 0.04151s; loss: 1689.75 (±17140.1); valid loss: 4899.88\n",
      "[Epoch 329/350, Step 70600, ETA 29.29s] step time: 0.005997s (±0.004204s); valid time: 0.03924s; loss: 46361 (±340346); valid loss: -19.7489\n",
      "[Epoch 329/350, Step 70700, ETA 28.66s] step time: 0.005832s (±0.00394s); valid time: 0.03865s; loss: 14570.6 (±85339.1); valid loss: 738.186\n",
      "[Epoch 330/350, Step 70800, ETA 28.03s] step time: 0.00566s (±0.003787s); valid time: 0.03768s; loss: 3580.53 (±35947.3); valid loss: 5197.73\n",
      "[Epoch 330/350, Step 70900, ETA 27.4s] step time: 0.006011s (±0.004071s); valid time: 0.03771s; loss: 56.7815 (±892.544); valid loss: 12581.2\n",
      "[Epoch 330/350, Step 70950, ETA 27.09s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 71000, ETA 26.77s] step time: 0.005847s (±0.004123s); valid time: 0.03812s; loss: -29.9386 (±29.5776); valid loss: -24.089\n",
      "[Epoch 331/350, Step 71100, ETA 26.14s] step time: 0.005841s (±0.004099s); valid time: 0.03963s; loss: 318.493 (±3480.56); valid loss: 5429.58\n",
      "[Epoch 332/350, Step 71200, ETA 25.52s] step time: 0.005853s (±0.004302s); valid time: 0.04141s; loss: 41216 (±368723); valid loss: 271.827\n",
      "[Epoch 332/350, Step 71300, ETA 24.88s] step time: 0.005902s (±0.004013s); valid time: 0.03941s; loss: -30.38 (±26.273); valid loss: -24.6238\n",
      "[Epoch 333/350, Step 71400, ETA 24.26s] step time: 0.005866s (±0.004377s); valid time: 0.04311s; loss: 9.37522 (±351.041); valid loss: -22.7765\n",
      "[Epoch 333/350, Step 71500, ETA 23.63s] step time: 0.005831s (±0.004028s); valid time: 0.03871s; loss: 55027.1 (±488750); valid loss: 14707.2\n",
      "[Epoch 334/350, Step 71600, ETA 23s] step time: 0.005779s (±0.003915s); valid time: 0.03842s; loss: -32.8555 (±2.42373); valid loss: -16.458\n",
      "[Epoch 334/350, Step 71700, ETA 22.37s] step time: 0.005872s (±0.003834s); valid time: 0.03729s; loss: -19.1088 (±117.785); valid loss: -24.5961\n",
      "[Epoch 334/350, Step 71800, ETA 21.73s] step time: 0.005705s (±0.003966s); valid time: 0.0385s; loss: 2828.49 (±28254.1); valid loss: -24.6452\n",
      "[Epoch 335/350, Step 71900, ETA 21.11s] step time: 0.005838s (±0.004048s); valid time: 0.03793s; loss: 22983.7 (±219713); valid loss: -19.6211\n",
      "[Epoch 335/350, Step 72000, ETA 20.48s] step time: 0.005866s (±0.004045s); valid time: 0.03817s; loss: 16185.3 (±161280); valid loss: 3491.74\n",
      "[Epoch 336/350, Step 72100, ETA 19.85s] step time: 0.005735s (±0.003945s); valid time: 0.03818s; loss: 19441.5 (±192856); valid loss: -24.2993\n",
      "[Epoch 336/350, Step 72200, ETA 19.22s] step time: 0.005855s (±0.004154s); valid time: 0.04025s; loss: 311.3 (±3413.55); valid loss: -24.5472\n",
      "[Epoch 337/350, Step 72300, ETA 18.59s] step time: 0.005852s (±0.00402s); valid time: 0.03863s; loss: 35220.9 (±239490); valid loss: 44.3648\n",
      "[Epoch 337/350, Step 72400, ETA 17.96s] step time: 0.005744s (±0.003968s); valid time: 0.03909s; loss: 23462.5 (±168020); valid loss: 2496.29\n",
      "[Epoch 338/350, Step 72500, ETA 17.33s] step time: 0.005868s (±0.003891s); valid time: 0.03797s; loss: -31.3496 (±9.91032); valid loss: 10747.6\n",
      "[Epoch 338/350, Step 72600, ETA 16.7s] step time: 0.005831s (±0.004098s); valid time: 0.04026s; loss: 162.308 (±1592.72); valid loss: -24.6304\n",
      "[Epoch 339/350, Step 72700, ETA 16.07s] step time: 0.005965s (±0.004224s); valid time: 0.04053s; loss: 29637.6 (±294261); valid loss: -4.40736\n",
      "[Epoch 339/350, Step 72800, ETA 15.44s] step time: 0.006001s (±0.004134s); valid time: 0.03944s; loss: 17221 (±120985); valid loss: -15.5316\n",
      "[Epoch 340/350, Step 72900, ETA 14.81s] step time: 0.005805s (±0.004066s); valid time: 0.04037s; loss: 5140.3 (±35716); valid loss: 16938\n",
      "[Epoch 340/350, Step 73000, ETA 14.18s] step time: 0.005939s (±0.003998s); valid time: 0.0393s; loss: 12.7269 (±266.486); valid loss: 3097.79\n",
      "[Epoch 340/350, Step 73100, ETA 13.55s] step time: 0.005976s (±0.003969s); valid time: 0.03788s; loss: -30.0991 (±26.1105); valid loss: 817.534\n",
      "[Epoch 340/350, Step 73100, ETA 13.55s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 73200, ETA 12.92s] step time: 0.005851s (±0.003981s); valid time: 0.03816s; loss: -32.8353 (±2.31598); valid loss: -24.6477\n",
      "[Epoch 341/350, Step 73300, ETA 12.29s] step time: 0.005754s (±0.00423s); valid time: 0.0405s; loss: 50686.3 (±497868); valid loss: 3119.67\n",
      "[Epoch 342/350, Step 73400, ETA 11.66s] step time: 0.00586s (±0.004116s); valid time: 0.0397s; loss: 53273.2 (±372708); valid loss: -24.6127\n",
      "[Epoch 342/350, Step 73500, ETA 11.03s] step time: 0.005761s (±0.004503s); valid time: 0.04206s; loss: 22343.3 (±220003); valid loss: 113.515\n",
      "[Epoch 343/350, Step 73600, ETA 10.4s] step time: 0.005591s (±0.003792s); valid time: 0.03714s; loss: 1725.82 (±17485.3); valid loss: -24.6558\n",
      "[Epoch 343/350, Step 73700, ETA 9.767s] step time: 0.005616s (±0.00387s); valid time: 0.03818s; loss: 2247.3 (±22395.9); valid loss: -18.1554\n",
      "[Epoch 344/350, Step 73800, ETA 9.137s] step time: 0.00589s (±0.003896s); valid time: 0.03804s; loss: 26535.4 (±199602); valid loss: -24.6337\n",
      "[Epoch 344/350, Step 73900, ETA 8.507s] step time: 0.005765s (±0.00491s); valid time: 0.04834s; loss: 11723.4 (±116233); valid loss: 1364.12\n",
      "[Epoch 345/350, Step 74000, ETA 7.877s] step time: 0.005853s (±0.00396s); valid time: 0.03873s; loss: 26357.8 (±205742); valid loss: 628.922\n",
      "[Epoch 345/350, Step 74100, ETA 7.246s] step time: 0.005808s (±0.003831s); valid time: 0.03694s; loss: -10.9102 (±155.953); valid loss: -9.13966\n",
      "[Epoch 346/350, Step 74200, ETA 6.616s] step time: 0.005626s (±0.003806s); valid time: 0.03719s; loss: 2472.22 (±24923); valid loss: 48.3152\n",
      "[Epoch 346/350, Step 74300, ETA 5.986s] step time: 0.005794s (±0.004054s); valid time: 0.03877s; loss: 4782.5 (±40679.9); valid loss: 3157.02\n",
      "[Epoch 347/350, Step 74400, ETA 5.356s] step time: 0.005787s (±0.003998s); valid time: 0.0386s; loss: 605.772 (±6350.54); valid loss: 6772.03\n",
      "[Epoch 347/350, Step 74500, ETA 4.726s] step time: 0.005802s (±0.004984s); valid time: 0.04904s; loss: 13260.6 (±91790.5); valid loss: 4.29861\n",
      "[Epoch 347/350, Step 74600, ETA 4.095s] step time: 0.005778s (±0.003953s); valid time: 0.03813s; loss: 1387.93 (±14105.4); valid loss: 1704.18\n",
      "[Epoch 348/350, Step 74700, ETA 3.466s] step time: 0.005798s (±0.00396s); valid time: 0.03694s; loss: 9776.4 (±97353.3); valid loss: -16.783\n",
      "[Epoch 348/350, Step 74800, ETA 2.835s] step time: 0.005898s (±0.003952s); valid time: 0.03652s; loss: 35.3158 (±437.039); valid loss: 7007\n",
      "[Epoch 349/350, Step 74900, ETA 2.205s] step time: 0.005862s (±0.003871s); valid time: 0.03691s; loss: 2955.63 (±29692); valid loss: -24.5822\n",
      "[Epoch 349/350, Step 75000, ETA 1.575s] step time: 0.00594s (±0.004084s); valid time: 0.03939s; loss: 48652.1 (±382948); valid loss: 71.2513\n",
      "[Epoch 350/350, Step 75100, ETA 0.9452s] step time: 0.005762s (±0.003925s); valid time: 0.03753s; loss: -31.0417 (±19.6182); valid loss: 581.589\n",
      "[Epoch 350/350, Step 75200, ETA 0.3151s] step time: 0.005751s (±0.00427s); valid time: 0.0408s; loss: 1429.33 (±13994.5); valid loss: -24.5383\n",
      "[Epoch 350/350, Step 75250, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpy6ph30ir/variables.dat-44800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpy6ph30ir/variables.dat-44800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.63,\n",
      "\t(tp, fp, tn, fn)=(102, 676, 1269, 115),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.47,\n",
      "\tf1=0.21,\n",
      "\troc_auc=0.56,\n",
      "\ty_pred%=0.3598519888991674,\n",
      "\ty_label%=0.10037002775208141,\n",
      ")\n",
      "Testing on realTraffic/occupancy_6005.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 2/350, Step 100, ETA 4m 5.273s] step time: 0.009926s (±0.03415s); valid time: 0.171s; loss: -3.8901 (±3.13762); valid loss: 6.34657 (*)\n",
      "[Epoch 3/350, Step 200, ETA 3m 17.87s] step time: 0.006102s (±0.008239s); valid time: 0.08262s; loss: -6.13653 (±1.08443); valid loss: 4.65668 (*)\n",
      "[Epoch 5/350, Step 300, ETA 3m 4.851s] step time: 0.006378s (±0.009004s); valid time: 0.08935s; loss: -7.08115 (±1.23949); valid loss: 1.37398 (*)\n",
      "[Epoch 6/350, Step 400, ETA 2m 56.91s] step time: 0.006263s (±0.008907s); valid time: 0.08928s; loss: -8.51973 (±1.08886); valid loss: -0.290319 (*)\n",
      "[Epoch 8/350, Step 500, ETA 2m 52.29s] step time: 0.006264s (±0.008585s); valid time: 0.08597s; loss: -9.22791 (±0.954812); valid loss: -0.743742 (*)\n",
      "[Epoch 9/350, Step 600, ETA 2m 48.48s] step time: 0.006295s (±0.008641s); valid time: 0.086s; loss: -9.45208 (±1.03435); valid loss: -0.806089 (*)\n",
      "[Epoch 10/350, Step 670, ETA 2m 44.34s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 700, ETA 2m 46.7s] step time: 0.006483s (±0.009858s); valid time: 0.09843s; loss: -9.72447 (±0.927281); valid loss: -1.10276 (*)\n",
      "[Epoch 12/350, Step 800, ETA 2m 44.19s] step time: 0.006289s (±0.008601s); valid time: 0.08584s; loss: -9.97107 (±0.881563); valid loss: -1.1739 (*)\n",
      "[Epoch 14/350, Step 900, ETA 2m 42.91s] step time: 0.006458s (±0.009157s); valid time: 0.09184s; loss: -10.0641 (±0.925353); valid loss: -1.45193 (*)\n",
      "[Epoch 15/350, Step 1000, ETA 2m 41s] step time: 0.006283s (±0.008578s); valid time: 0.08502s; loss: -10.3277 (±1.02951); valid loss: -1.51066 (*)\n",
      "[Epoch 17/350, Step 1100, ETA 2m 39.69s] step time: 0.006316s (±0.009307s); valid time: 0.09055s; loss: -10.3299 (±1.01265); valid loss: -1.67299 (*)\n",
      "[Epoch 18/350, Step 1200, ETA 2m 38.52s] step time: 0.006495s (±0.009032s); valid time: 0.08947s; loss: -10.5842 (±1.13589); valid loss: -1.74298 (*)\n",
      "[Epoch 20/350, Step 1300, ETA 2m 37.58s] step time: 0.00643s (±0.009372s); valid time: 0.09328s; loss: -10.6819 (±0.952823); valid loss: -2.00209 (*)\n",
      "[Epoch 20/350, Step 1340, ETA 2m 36.5s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 1400, ETA 2m 36.26s] step time: 0.006304s (±0.008583s); valid time: 0.08547s; loss: -10.8633 (±0.989631); valid loss: -2.06746 (*)\n",
      "[Epoch 23/350, Step 1500, ETA 2m 35.54s] step time: 0.006428s (±0.009199s); valid time: 0.09192s; loss: -10.9217 (±1.00889); valid loss: -2.37896 (*)\n",
      "[Epoch 24/350, Step 1600, ETA 2m 34.52s] step time: 0.00645s (±0.008707s); valid time: 0.08647s; loss: -11.0441 (±1.02778); valid loss: -2.49624 (*)\n",
      "[Epoch 26/350, Step 1700, ETA 2m 33.54s] step time: 0.00624s (±0.008842s); valid time: 0.08815s; loss: -11.1202 (±1.06195); valid loss: -2.64498 (*)\n",
      "[Epoch 27/350, Step 1800, ETA 2m 32.5s] step time: 0.006368s (±0.009362s); valid time: 0.09342s; loss: -11.3286 (±0.923258); valid loss: -2.74921 (*)\n",
      "[Epoch 29/350, Step 1900, ETA 2m 31.85s] step time: 0.006503s (±0.00904s); valid time: 0.08971s; loss: -11.3469 (±1.0525); valid loss: -2.83399 (*)\n",
      "[Epoch 30/350, Step 2000, ETA 2m 30.91s] step time: 0.006417s (±0.008917s); valid time: 0.08888s; loss: -11.5159 (±1.05272); valid loss: -3.02033 (*)\n",
      "[Epoch 30/350, Step 2010, ETA 2m 30.79s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 2100, ETA 2m 30.14s] step time: 0.006403s (±0.008752s); valid time: 0.08706s; loss: -11.8235 (±1.15285); valid loss: -3.14551 (*)\n",
      "[Epoch 33/350, Step 2200, ETA 2m 29.61s] step time: 0.006695s (±0.009115s); valid time: 0.09002s; loss: -11.6545 (±1.02333); valid loss: -3.26327 (*)\n",
      "[Epoch 35/350, Step 2300, ETA 2m 28.85s] step time: 0.006369s (±0.009422s); valid time: 0.09492s; loss: -11.7965 (±1.33151); valid loss: -3.49038 (*)\n",
      "[Epoch 36/350, Step 2400, ETA 2m 27.84s] step time: 0.006285s (±0.008645s); valid time: 0.08723s; loss: -11.9015 (±1.00659); valid loss: -3.49058 (*)\n",
      "[Epoch 38/350, Step 2500, ETA 2m 27.15s] step time: 0.00645s (±0.008804s); valid time: 0.08796s; loss: -12.0898 (±1.15923); valid loss: -3.5373 (*)\n",
      "[Epoch 39/350, Step 2600, ETA 2m 26.26s] step time: 0.006382s (±0.009494s); valid time: 0.09483s; loss: -11.9232 (±1.09105); valid loss: -3.64617 (*)\n",
      "[Epoch 40/350, Step 2680, ETA 2m 24.93s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 2700, ETA 2m 25.43s] step time: 0.006269s (±0.009139s); valid time: 0.09158s; loss: -12.2782 (±1.23996); valid loss: -3.93011 (*)\n",
      "[Epoch 42/350, Step 2800, ETA 2m 24.07s] step time: 0.005613s (±0.001734s); valid time: 0.01363s; loss: -12.2504 (±1.24782); valid loss: -3.91796\n",
      "[Epoch 44/350, Step 2900, ETA 2m 23.4s] step time: 0.006401s (±0.009568s); valid time: 0.09521s; loss: -12.5176 (±1.1523); valid loss: -4.14415 (*)\n",
      "[Epoch 45/350, Step 3000, ETA 2m 22.52s] step time: 0.006288s (±0.008959s); valid time: 0.08813s; loss: -12.5017 (±1.16922); valid loss: -4.16441 (*)\n",
      "[Epoch 47/350, Step 3100, ETA 2m 22.01s] step time: 0.006655s (±0.009123s); valid time: 0.09114s; loss: -12.5844 (±1.19822); valid loss: -4.37534 (*)\n",
      "[Epoch 48/350, Step 3200, ETA 2m 20.65s] step time: 0.005531s (±0.001854s); valid time: 0.01493s; loss: -12.5687 (±1.13366); valid loss: -4.3663\n",
      "[Epoch 50/350, Step 3300, ETA 2m 19.88s] step time: 0.006265s (±0.00823s); valid time: 0.08192s; loss: -12.7067 (±1.22267); valid loss: -4.5649 (*)\n",
      "[Epoch 50/350, Step 3350, ETA 2m 19.24s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 51/350, Step 3400, ETA 2m 19.27s] step time: 0.006632s (±0.009067s); valid time: 0.08956s; loss: -12.662 (±1.06856); valid loss: -4.60922 (*)\n",
      "[Epoch 53/350, Step 3500, ETA 2m 18.59s] step time: 0.006406s (±0.009536s); valid time: 0.09375s; loss: -12.8825 (±1.18552); valid loss: -4.93358 (*)\n",
      "[Epoch 54/350, Step 3600, ETA 2m 17.37s] step time: 0.005561s (±0.001706s); valid time: 0.01286s; loss: -12.8739 (±1.15203); valid loss: -4.84915\n",
      "[Epoch 56/350, Step 3700, ETA 2m 16.7s] step time: 0.006409s (±0.008967s); valid time: 0.08924s; loss: -13.0159 (±1.28304); valid loss: -4.98573 (*)\n",
      "[Epoch 57/350, Step 3800, ETA 2m 16.01s] step time: 0.006497s (±0.009719s); valid time: 0.09733s; loss: -12.8503 (±1.26061); valid loss: -5.04112 (*)\n",
      "[Epoch 59/350, Step 3900, ETA 2m 15.37s] step time: 0.006384s (±0.008712s); valid time: 0.0872s; loss: -13.1846 (±1.15181); valid loss: -5.10799 (*)\n",
      "[Epoch 60/350, Step 4000, ETA 2m 14.58s] step time: 0.006316s (±0.008691s); valid time: 0.08669s; loss: -13.0936 (±1.10884); valid loss: -5.23439 (*)\n",
      "[Epoch 60/350, Step 4020, ETA 2m 14.31s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 62/350, Step 4100, ETA 2m 13.8s] step time: 0.006173s (±0.009083s); valid time: 0.08811s; loss: -13.184 (±1.35429); valid loss: -5.27245 (*)\n",
      "[Epoch 63/350, Step 4200, ETA 2m 12.61s] step time: 0.005438s (±0.001594s); valid time: 0.01259s; loss: -13.2983 (±1.36418); valid loss: -5.0823\n",
      "[Epoch 65/350, Step 4300, ETA 2m 11.91s] step time: 0.00629s (±0.008803s); valid time: 0.08774s; loss: -13.2494 (±1.29097); valid loss: -5.35997 (*)\n",
      "[Epoch 66/350, Step 4400, ETA 2m 11.08s] step time: 0.006157s (±0.008836s); valid time: 0.08809s; loss: -13.2459 (±1.25068); valid loss: -5.50991 (*)\n",
      "[Epoch 68/350, Step 4500, ETA 2m 10.39s] step time: 0.006242s (±0.008724s); valid time: 0.08716s; loss: -13.5474 (±1.28567); valid loss: -5.57799 (*)\n",
      "[Epoch 69/350, Step 4600, ETA 2m 9.419s] step time: 0.005794s (±0.002009s); valid time: 0.01553s; loss: -13.4128 (±1.23077); valid loss: -5.33139\n",
      "[Epoch 70/350, Step 4690, ETA 2m 8.371s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 71/350, Step 4700, ETA 2m 8.371s] step time: 0.005389s (±0.001523s); valid time: 0.01222s; loss: -13.3408 (±1.17705); valid loss: -5.49121\n",
      "[Epoch 72/350, Step 4800, ETA 2m 7.352s] step time: 0.005595s (±0.001716s); valid time: 0.01229s; loss: -13.5923 (±1.1155); valid loss: -5.3976\n",
      "[Epoch 74/350, Step 4900, ETA 2m 6.659s] step time: 0.006234s (±0.009332s); valid time: 0.09349s; loss: -13.5111 (±1.3847); valid loss: -5.71192 (*)\n",
      "[Epoch 75/350, Step 5000, ETA 2m 5.651s] step time: 0.005517s (±0.001807s); valid time: 0.01321s; loss: -13.4107 (±1.4089); valid loss: -5.57533\n",
      "[Epoch 77/350, Step 5100, ETA 2m 5.098s] step time: 0.006598s (±0.008692s); valid time: 0.08697s; loss: -13.5138 (±1.40406); valid loss: -5.78108 (*)\n",
      "[Epoch 78/350, Step 5200, ETA 2m 4.085s] step time: 0.005469s (±0.001735s); valid time: 0.01332s; loss: -13.7196 (±1.2878); valid loss: -5.7242\n",
      "[Epoch 80/350, Step 5300, ETA 2m 3.422s] step time: 0.006292s (±0.008608s); valid time: 0.0862s; loss: -13.4865 (±1.2451); valid loss: -5.82894 (*)\n",
      "[Epoch 80/350, Step 5360, ETA 2m 2.79s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 5400, ETA 2m 2.531s] step time: 0.005761s (±0.002099s); valid time: 0.01323s; loss: -13.5752 (±1.30737); valid loss: -5.82668\n",
      "[Epoch 83/350, Step 5500, ETA 2m 1.844s] step time: 0.006212s (±0.008673s); valid time: 0.08661s; loss: -13.6991 (±1.27407); valid loss: -5.90271 (*)\n",
      "[Epoch 84/350, Step 5600, ETA 2m 0.9245s] step time: 0.005634s (±0.00197s); valid time: 0.01276s; loss: -13.7629 (±1.19778); valid loss: -5.82381\n",
      "[Epoch 86/350, Step 5700, ETA 2m 0.2646s] step time: 0.006271s (±0.009444s); valid time: 0.09486s; loss: -13.6552 (±1.32132); valid loss: -5.97761 (*)\n",
      "[Epoch 87/350, Step 5800, ETA 1m 59.55s] step time: 0.006246s (±0.00906s); valid time: 0.09049s; loss: -13.7927 (±1.22785); valid loss: -6.01753 (*)\n",
      "[Epoch 89/350, Step 5900, ETA 1m 58.71s] step time: 0.005658s (±0.00182s); valid time: 0.01295s; loss: -13.785 (±1.24864); valid loss: -5.97273\n",
      "[Epoch 90/350, Step 6000, ETA 1m 57.78s] step time: 0.005505s (±0.00149s); valid time: 0.01307s; loss: -13.8344 (±1.24934); valid loss: -5.89198\n",
      "[Epoch 90/350, Step 6030, ETA 1m 57.5s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 6100, ETA 1m 56.91s] step time: 0.005495s (±0.001748s); valid time: 0.01319s; loss: -13.8823 (±1.33202); valid loss: -5.84478\n",
      "[Epoch 93/350, Step 6200, ETA 1m 56.03s] step time: 0.005611s (±0.001847s); valid time: 0.01294s; loss: -13.8297 (±1.19692); valid loss: -5.99123\n",
      "[Epoch 95/350, Step 6300, ETA 1m 55.39s] step time: 0.006219s (±0.009285s); valid time: 0.09262s; loss: -13.9207 (±1.30752); valid loss: -6.02078 (*)\n",
      "[Epoch 96/350, Step 6400, ETA 1m 54.69s] step time: 0.006241s (±0.008772s); valid time: 0.08779s; loss: -13.9103 (±1.16017); valid loss: -6.02172 (*)\n",
      "[Epoch 98/350, Step 6500, ETA 1m 54.02s] step time: 0.006172s (±0.009674s); valid time: 0.09562s; loss: -13.9131 (±1.20351); valid loss: -6.09231 (*)\n",
      "[Epoch 99/350, Step 6600, ETA 1m 53.18s] step time: 0.005641s (±0.001786s); valid time: 0.01289s; loss: -13.9525 (±1.21638); valid loss: -6.07579\n",
      "[Epoch 100/350, Step 6700, ETA 1m 52.33s] step time: 0.005606s (±0.001669s); valid time: 0.01264s; loss: -13.9257 (±1.3233); valid loss: -6.06708\n",
      "[Epoch 100/350, Step 6700, ETA 1m 52.33s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 102/350, Step 6800, ETA 1m 51.51s] step time: 0.00553s (±0.001718s); valid time: 0.01271s; loss: -13.9059 (±1.15545); valid loss: -6.04914\n",
      "[Epoch 103/350, Step 6900, ETA 1m 50.84s] step time: 0.006186s (±0.009022s); valid time: 0.09024s; loss: -13.9705 (±1.25165); valid loss: -6.14339 (*)\n",
      "[Epoch 105/350, Step 7000, ETA 1m 50.18s] step time: 0.006191s (±0.008581s); valid time: 0.08528s; loss: -13.9836 (±1.1268); valid loss: -6.14525 (*)\n",
      "[Epoch 106/350, Step 7100, ETA 1m 49.48s] step time: 0.006099s (±0.0093s); valid time: 0.09178s; loss: -13.9584 (±1.26484); valid loss: -6.20122 (*)\n",
      "[Epoch 108/350, Step 7200, ETA 1m 48.68s] step time: 0.005575s (±0.001703s); valid time: 0.01286s; loss: -13.9729 (±1.3519); valid loss: -6.17961\n",
      "[Epoch 109/350, Step 7300, ETA 1m 47.99s] step time: 0.006095s (±0.008779s); valid time: 0.08816s; loss: -14.0386 (±1.32947); valid loss: -6.27978 (*)\n",
      "[Epoch 110/350, Step 7370, ETA 1m 47.44s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 7400, ETA 1m 47.22s] step time: 0.005649s (±0.001811s); valid time: 0.01258s; loss: -14.0698 (±1.16045); valid loss: -6.17095\n",
      "[Epoch 112/350, Step 7500, ETA 1m 46.37s] step time: 0.005431s (±0.001944s); valid time: 0.01306s; loss: -14.0557 (±1.49742); valid loss: -6.18279\n",
      "[Epoch 114/350, Step 7600, ETA 1m 45.55s] step time: 0.005415s (±0.0016s); valid time: 0.01256s; loss: -13.986 (±1.16723); valid loss: -6.20923\n",
      "[Epoch 115/350, Step 7700, ETA 1m 44.72s] step time: 0.005417s (±0.00176s); valid time: 0.01415s; loss: -14.1144 (±1.35142); valid loss: -6.25149\n",
      "[Epoch 117/350, Step 7800, ETA 1m 43.96s] step time: 0.005628s (±0.001776s); valid time: 0.01286s; loss: -14.0682 (±1.19234); valid loss: -6.16693\n",
      "[Epoch 118/350, Step 7900, ETA 1m 43.29s] step time: 0.006194s (±0.008642s); valid time: 0.08638s; loss: -14.2057 (±1.2362); valid loss: -6.3452 (*)\n",
      "[Epoch 120/350, Step 8000, ETA 1m 42.51s] step time: 0.005463s (±0.001666s); valid time: 0.0128s; loss: -13.9675 (±1.09437); valid loss: -6.20664\n",
      "[Epoch 120/350, Step 8040, ETA 1m 42.17s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 8100, ETA 1m 41.71s] step time: 0.005514s (±0.001738s); valid time: 0.01258s; loss: -14.2071 (±1.07079); valid loss: -6.23967\n",
      "[Epoch 123/350, Step 8200, ETA 1m 40.94s] step time: 0.005472s (±0.001734s); valid time: 0.01238s; loss: -14.1244 (±1.19715); valid loss: -6.24053\n",
      "[Epoch 124/350, Step 8300, ETA 1m 40.14s] step time: 0.005466s (±0.002286s); valid time: 0.013s; loss: -14.1448 (±1.2021); valid loss: -6.23669\n",
      "[Epoch 126/350, Step 8400, ETA 1m 39.39s] step time: 0.005553s (±0.001592s); valid time: 0.01238s; loss: -14.088 (±1.43262); valid loss: -6.27373\n",
      "[Epoch 127/350, Step 8500, ETA 1m 38.63s] step time: 0.005634s (±0.001824s); valid time: 0.01296s; loss: -14.2376 (±1.25537); valid loss: -6.25496\n",
      "[Epoch 129/350, Step 8600, ETA 1m 37.89s] step time: 0.00553s (±0.001578s); valid time: 0.01254s; loss: -14.0966 (±1.29536); valid loss: -6.31223\n",
      "[Epoch 130/350, Step 8700, ETA 1m 37.14s] step time: 0.005659s (±0.001657s); valid time: 0.01274s; loss: -14.1842 (±1.26132); valid loss: -6.33135\n",
      "[Epoch 130/350, Step 8710, ETA 1m 37.05s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 8800, ETA 1m 36.38s] step time: 0.005429s (±0.001882s); valid time: 0.01564s; loss: -14.1573 (±1.32005); valid loss: -6.31255\n",
      "[Epoch 133/350, Step 8900, ETA 1m 35.76s] step time: 0.006391s (±0.00938s); valid time: 0.09216s; loss: -14.1909 (±1.18274); valid loss: -6.34544 (*)\n",
      "[Epoch 135/350, Step 9000, ETA 1m 35.17s] step time: 0.006385s (±0.009126s); valid time: 0.09149s; loss: -14.1824 (±1.26492); valid loss: -6.35472 (*)\n",
      "[Epoch 136/350, Step 9100, ETA 1m 34.54s] step time: 0.00641s (±0.009409s); valid time: 0.09448s; loss: -14.279 (±1.3164); valid loss: -6.39101 (*)\n",
      "[Epoch 138/350, Step 9200, ETA 1m 33.84s] step time: 0.005684s (±0.002031s); valid time: 0.01375s; loss: -14.0704 (±1.28795); valid loss: -6.35473\n",
      "[Epoch 139/350, Step 9300, ETA 1m 33.24s] step time: 0.006555s (±0.009415s); valid time: 0.093s; loss: -14.3137 (±1.23092); valid loss: -6.39738 (*)\n",
      "[Epoch 140/350, Step 9380, ETA 1m 32.61s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 9400, ETA 1m 32.61s] step time: 0.006217s (±0.00942s); valid time: 0.0934s; loss: -14.2134 (±1.21615); valid loss: -6.40713 (*)\n",
      "[Epoch 142/350, Step 9500, ETA 1m 32s] step time: 0.006544s (±0.008871s); valid time: 0.08808s; loss: -14.2607 (±1.26948); valid loss: -6.42821 (*)\n",
      "[Epoch 144/350, Step 9600, ETA 1m 31.28s] step time: 0.005591s (±0.001703s); valid time: 0.01236s; loss: -14.2714 (±1.27643); valid loss: -6.41718\n",
      "[Epoch 145/350, Step 9700, ETA 1m 30.53s] step time: 0.005534s (±0.001519s); valid time: 0.01225s; loss: -14.2754 (±1.28386); valid loss: -6.39337\n",
      "[Epoch 147/350, Step 9800, ETA 1m 29.8s] step time: 0.005521s (±0.001701s); valid time: 0.01249s; loss: -14.1453 (±1.2247); valid loss: -6.41548\n",
      "[Epoch 148/350, Step 9900, ETA 1m 29.09s] step time: 0.005729s (±0.001745s); valid time: 0.0127s; loss: -14.2225 (±1.24734); valid loss: -6.40492\n",
      "[Epoch 150/350, Step 10000, ETA 1m 28.33s] step time: 0.005294s (±0.001529s); valid time: 0.01276s; loss: -14.2518 (±1.24642); valid loss: -6.405\n",
      "[Epoch 150/350, Step 10050, ETA 1m 27.94s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 151/350, Step 10100, ETA 1m 27.62s] step time: 0.005711s (±0.002132s); valid time: 0.01287s; loss: -14.2717 (±1.28727); valid loss: -6.39363\n",
      "[Epoch 153/350, Step 10200, ETA 1m 26.88s] step time: 0.00536s (±0.001428s); valid time: 0.01207s; loss: -14.232 (±1.27192); valid loss: -6.41102\n",
      "[Epoch 154/350, Step 10300, ETA 1m 26.14s] step time: 0.005564s (±0.00164s); valid time: 0.01237s; loss: -14.2891 (±1.0723); valid loss: -6.41635\n",
      "[Epoch 156/350, Step 10400, ETA 1m 25.42s] step time: 0.005401s (±0.001582s); valid time: 0.01289s; loss: -14.2525 (±1.27261); valid loss: -6.3825\n",
      "[Epoch 157/350, Step 10500, ETA 1m 24.77s] step time: 0.006231s (±0.008942s); valid time: 0.08906s; loss: -14.3475 (±1.19899); valid loss: -6.49303 (*)\n",
      "[Epoch 159/350, Step 10600, ETA 1m 24.06s] step time: 0.005503s (±0.00165s); valid time: 0.01333s; loss: -14.3415 (±1.33811); valid loss: -6.39974\n",
      "[Epoch 160/350, Step 10700, ETA 1m 23.31s] step time: 0.005387s (±0.001496s); valid time: 0.01244s; loss: -14.1505 (±1.34089); valid loss: -6.41817\n",
      "[Epoch 160/350, Step 10720, ETA 1m 23.16s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 162/350, Step 10800, ETA 1m 22.61s] step time: 0.005513s (±0.001836s); valid time: 0.01301s; loss: -14.2848 (±1.33511); valid loss: -6.37198\n",
      "[Epoch 163/350, Step 10900, ETA 1m 21.88s] step time: 0.005482s (±0.001533s); valid time: 0.01234s; loss: -14.3584 (±1.55209); valid loss: -6.44249\n",
      "[Epoch 165/350, Step 11000, ETA 1m 21.19s] step time: 0.005674s (±0.001867s); valid time: 0.01291s; loss: -14.198 (±1.34367); valid loss: -6.4268\n",
      "[Epoch 166/350, Step 11100, ETA 1m 20.48s] step time: 0.005589s (±0.001785s); valid time: 0.01297s; loss: -14.3648 (±1.36824); valid loss: -6.38971\n",
      "[Epoch 168/350, Step 11200, ETA 1m 19.79s] step time: 0.005581s (±0.001579s); valid time: 0.01272s; loss: -14.2283 (±1.30766); valid loss: -6.42551\n",
      "[Epoch 169/350, Step 11300, ETA 1m 19.15s] step time: 0.006261s (±0.009145s); valid time: 0.09099s; loss: -14.376 (±1.46589); valid loss: -6.51767 (*)\n",
      "[Epoch 170/350, Step 11390, ETA 1m 18.5s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 171/350, Step 11400, ETA 1m 18.47s] step time: 0.005568s (±0.002036s); valid time: 0.0129s; loss: -14.2424 (±1.36777); valid loss: -6.44951\n",
      "[Epoch 172/350, Step 11500, ETA 1m 17.74s] step time: 0.005398s (±0.001556s); valid time: 0.01253s; loss: -14.3094 (±1.43424); valid loss: -6.50798\n",
      "[Epoch 174/350, Step 11600, ETA 1m 17.05s] step time: 0.005543s (±0.001529s); valid time: 0.01252s; loss: -14.2891 (±1.38214); valid loss: -6.41095\n",
      "[Epoch 175/350, Step 11700, ETA 1m 16.32s] step time: 0.005344s (±0.001627s); valid time: 0.01209s; loss: -14.4122 (±1.2903); valid loss: -6.47427\n",
      "[Epoch 177/350, Step 11800, ETA 1m 15.63s] step time: 0.005543s (±0.00176s); valid time: 0.0127s; loss: -14.2544 (±1.25112); valid loss: -6.39856\n",
      "[Epoch 178/350, Step 11900, ETA 1m 14.92s] step time: 0.005423s (±0.001571s); valid time: 0.01247s; loss: -14.3054 (±1.24183); valid loss: -6.46513\n",
      "[Epoch 180/350, Step 12000, ETA 1m 14.25s] step time: 0.005741s (±0.002044s); valid time: 0.0127s; loss: -14.2156 (±1.10124); valid loss: -6.42106\n",
      "[Epoch 180/350, Step 12060, ETA 1m 13.81s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 12100, ETA 1m 13.56s] step time: 0.005571s (±0.002022s); valid time: 0.01232s; loss: -14.3964 (±1.31751); valid loss: -6.46666\n",
      "[Epoch 183/350, Step 12200, ETA 1m 12.87s] step time: 0.005511s (±0.001838s); valid time: 0.01284s; loss: -14.2321 (±1.19208); valid loss: -6.43966\n",
      "[Epoch 184/350, Step 12300, ETA 1m 12.17s] step time: 0.005483s (±0.001735s); valid time: 0.0125s; loss: -14.2947 (±1.33821); valid loss: -6.44421\n",
      "[Epoch 186/350, Step 12400, ETA 1m 11.48s] step time: 0.00542s (±0.001595s); valid time: 0.01223s; loss: -14.3473 (±1.20591); valid loss: -6.44345\n",
      "[Epoch 187/350, Step 12500, ETA 1m 10.78s] step time: 0.005521s (±0.001754s); valid time: 0.01211s; loss: -14.3321 (±1.15842); valid loss: -6.43968\n",
      "[Epoch 189/350, Step 12600, ETA 1m 10.11s] step time: 0.005551s (±0.00193s); valid time: 0.0137s; loss: -14.2926 (±1.27167); valid loss: -6.43665\n",
      "[Epoch 190/350, Step 12700, ETA 1m 9.409s] step time: 0.005454s (±0.001558s); valid time: 0.01254s; loss: -14.2387 (±1.3165); valid loss: -6.45928\n",
      "[Epoch 190/350, Step 12730, ETA 1m 9.188s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 12800, ETA 1m 8.734s] step time: 0.005534s (±0.001751s); valid time: 0.01306s; loss: -14.4059 (±1.16698); valid loss: -6.49209\n",
      "[Epoch 193/350, Step 12900, ETA 1m 8.042s] step time: 0.005486s (±0.001724s); valid time: 0.01287s; loss: -14.4095 (±1.29093); valid loss: -6.43282\n",
      "[Epoch 195/350, Step 13000, ETA 1m 7.371s] step time: 0.005576s (±0.001763s); valid time: 0.01329s; loss: -14.2813 (±1.29468); valid loss: -6.47938\n",
      "[Epoch 196/350, Step 13100, ETA 1m 6.682s] step time: 0.005498s (±0.002031s); valid time: 0.01258s; loss: -14.3326 (±1.38854); valid loss: -6.41799\n",
      "[Epoch 198/350, Step 13200, ETA 1m 6.002s] step time: 0.005428s (±0.001642s); valid time: 0.01317s; loss: -14.2819 (±1.3911); valid loss: -6.44093\n",
      "[Epoch 199/350, Step 13300, ETA 1m 5.316s] step time: 0.00548s (±0.001638s); valid time: 0.01243s; loss: -14.3943 (±1.47427); valid loss: -6.46079\n",
      "[Epoch 200/350, Step 13400, ETA 1m 4.624s] step time: 0.005393s (±0.001613s); valid time: 0.01332s; loss: -14.2717 (±1.35981); valid loss: -6.48588\n",
      "[Epoch 200/350, Step 13400, ETA 1m 4.625s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 202/350, Step 13500, ETA 1m 3.962s] step time: 0.005603s (±0.001752s); valid time: 0.01309s; loss: -14.292 (±1.4269); valid loss: -6.43796\n",
      "[Epoch 203/350, Step 13600, ETA 1m 3.282s] step time: 0.005432s (±0.001335s); valid time: 0.01274s; loss: -14.3323 (±1.30652); valid loss: -6.46072\n",
      "[Epoch 205/350, Step 13700, ETA 1m 2.625s] step time: 0.005661s (±0.001817s); valid time: 0.0132s; loss: -14.4026 (±1.26292); valid loss: -6.49716\n",
      "[Epoch 206/350, Step 13800, ETA 1m 1.996s] step time: 0.006195s (±0.008753s); valid time: 0.08739s; loss: -14.2679 (±1.14516); valid loss: -6.5276 (*)\n",
      "[Epoch 208/350, Step 13900, ETA 1m 1.326s] step time: 0.005447s (±0.001722s); valid time: 0.01332s; loss: -14.3434 (±1.23704); valid loss: -6.43302\n",
      "[Epoch 209/350, Step 14000, ETA 1m 0.6463s] step time: 0.005318s (±0.001427s); valid time: 0.01323s; loss: -14.3629 (±1.17419); valid loss: -6.46317\n",
      "[Epoch 210/350, Step 14070, ETA 1m 0.1655s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 14100, ETA 59.97s] step time: 0.005339s (±0.001515s); valid time: 0.01294s; loss: -14.2938 (±1.09655); valid loss: -6.49889\n",
      "[Epoch 212/350, Step 14200, ETA 59.29s] step time: 0.005426s (±0.001736s); valid time: 0.01233s; loss: -14.3718 (±1.23573); valid loss: -6.50301\n",
      "[Epoch 214/350, Step 14300, ETA 58.62s] step time: 0.005395s (±0.00182s); valid time: 0.0133s; loss: -14.3939 (±1.39018); valid loss: -6.49745\n",
      "[Epoch 215/350, Step 14400, ETA 57.94s] step time: 0.005407s (±0.001496s); valid time: 0.01217s; loss: -14.2841 (±1.41216); valid loss: -6.45733\n",
      "[Epoch 217/350, Step 14500, ETA 57.29s] step time: 0.00568s (±0.001836s); valid time: 0.01309s; loss: -14.4064 (±1.45828); valid loss: -6.47544\n",
      "[Epoch 218/350, Step 14600, ETA 56.62s] step time: 0.005507s (±0.001681s); valid time: 0.01253s; loss: -14.2806 (±1.30266); valid loss: -6.44295\n",
      "[Epoch 220/350, Step 14700, ETA 55.96s] step time: 0.005495s (±0.001556s); valid time: 0.01233s; loss: -14.5283 (±1.16046); valid loss: -6.49399\n",
      "[Epoch 220/350, Step 14740, ETA 55.69s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 14800, ETA 55.31s] step time: 0.005708s (±0.002142s); valid time: 0.0132s; loss: -14.1843 (±1.55736); valid loss: -6.481\n",
      "[Epoch 223/350, Step 14900, ETA 54.64s] step time: 0.005377s (±0.001768s); valid time: 0.01386s; loss: -14.3022 (±1.23267); valid loss: -6.46739\n",
      "[Epoch 224/350, Step 15000, ETA 53.98s] step time: 0.005495s (±0.001806s); valid time: 0.01298s; loss: -14.394 (±1.56577); valid loss: -6.49025\n",
      "[Epoch 226/350, Step 15100, ETA 53.32s] step time: 0.005454s (±0.001709s); valid time: 0.01285s; loss: -14.3408 (±1.35621); valid loss: -6.50308\n",
      "[Epoch 227/350, Step 15200, ETA 52.65s] step time: 0.005572s (±0.001841s); valid time: 0.0122s; loss: -14.3726 (±1.27046); valid loss: -6.4842\n",
      "[Epoch 229/350, Step 15300, ETA 52s] step time: 0.00547s (±0.002357s); valid time: 0.02196s; loss: -14.3698 (±1.31354); valid loss: -6.46093\n",
      "[Epoch 230/350, Step 15400, ETA 51.33s] step time: 0.005419s (±0.001549s); valid time: 0.01281s; loss: -14.4087 (±1.32299); valid loss: -6.49205\n",
      "[Epoch 230/350, Step 15410, ETA 51.26s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 15500, ETA 50.69s] step time: 0.005696s (±0.001774s); valid time: 0.01288s; loss: -14.3497 (±1.32475); valid loss: -6.45222\n",
      "[Epoch 233/350, Step 15600, ETA 50.02s] step time: 0.005374s (±0.00144s); valid time: 0.01323s; loss: -14.3676 (±1.34114); valid loss: -6.50653\n",
      "[Epoch 235/350, Step 15700, ETA 49.38s] step time: 0.005775s (±0.001929s); valid time: 0.01234s; loss: -14.3548 (±1.32359); valid loss: -6.50554\n",
      "[Epoch 236/350, Step 15800, ETA 48.72s] step time: 0.005483s (±0.001648s); valid time: 0.0127s; loss: -14.3918 (±1.33906); valid loss: -6.49585\n",
      "[Epoch 238/350, Step 15900, ETA 48.07s] step time: 0.005479s (±0.001903s); valid time: 0.01446s; loss: -14.4301 (±1.42296); valid loss: -6.45956\n",
      "[Epoch 239/350, Step 16000, ETA 47.45s] step time: 0.006302s (±0.00902s); valid time: 0.08968s; loss: -14.188 (±1.2026); valid loss: -6.54882 (*)\n",
      "[Epoch 240/350, Step 16080, ETA 46.92s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 16100, ETA 46.79s] step time: 0.005421s (±0.001628s); valid time: 0.01262s; loss: -14.3755 (±1.17717); valid loss: -6.5087\n",
      "[Epoch 242/350, Step 16200, ETA 46.14s] step time: 0.005528s (±0.00173s); valid time: 0.01327s; loss: -14.3279 (±1.44678); valid loss: -6.51416\n",
      "[Epoch 244/350, Step 16300, ETA 45.48s] step time: 0.005283s (±0.001494s); valid time: 0.01232s; loss: -14.3137 (±1.32427); valid loss: -6.48452\n",
      "[Epoch 245/350, Step 16400, ETA 44.82s] step time: 0.005486s (±0.001679s); valid time: 0.01282s; loss: -14.2896 (±1.24948); valid loss: -6.48417\n",
      "[Epoch 247/350, Step 16500, ETA 44.18s] step time: 0.005573s (±0.001668s); valid time: 0.01224s; loss: -14.3292 (±1.13768); valid loss: -6.49873\n",
      "[Epoch 248/350, Step 16600, ETA 43.52s] step time: 0.005384s (±0.001643s); valid time: 0.01245s; loss: -14.3901 (±1.30883); valid loss: -6.50863\n",
      "[Epoch 250/350, Step 16700, ETA 42.88s] step time: 0.005713s (±0.001979s); valid time: 0.01327s; loss: -14.3391 (±1.16738); valid loss: -6.44879\n",
      "[Epoch 250/350, Step 16750, ETA 42.54s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 251/350, Step 16800, ETA 42.23s] step time: 0.005606s (±0.001957s); valid time: 0.01235s; loss: -14.3201 (±1.35001); valid loss: -6.47745\n",
      "[Epoch 253/350, Step 16900, ETA 41.59s] step time: 0.005666s (±0.001596s); valid time: 0.01241s; loss: -14.3113 (±1.36873); valid loss: -6.51133\n",
      "[Epoch 254/350, Step 17000, ETA 40.94s] step time: 0.00554s (±0.001807s); valid time: 0.01247s; loss: -14.4101 (±1.3541); valid loss: -6.53663\n",
      "[Epoch 256/350, Step 17100, ETA 40.29s] step time: 0.005406s (±0.001598s); valid time: 0.01268s; loss: -14.4161 (±1.33996); valid loss: -6.51892\n",
      "[Epoch 257/350, Step 17200, ETA 39.64s] step time: 0.005518s (±0.001659s); valid time: 0.01274s; loss: -14.3859 (±1.37203); valid loss: -6.54823\n",
      "[Epoch 259/350, Step 17300, ETA 39s] step time: 0.005486s (±0.001613s); valid time: 0.01324s; loss: -14.2011 (±1.214); valid loss: -6.51616\n",
      "[Epoch 260/350, Step 17400, ETA 38.36s] step time: 0.005708s (±0.001759s); valid time: 0.01235s; loss: -14.3142 (±1.2325); valid loss: -6.53158\n",
      "[Epoch 260/350, Step 17420, ETA 38.23s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 262/350, Step 17500, ETA 37.71s] step time: 0.005468s (±0.001523s); valid time: 0.013s; loss: -14.4545 (±1.26963); valid loss: -6.49\n",
      "[Epoch 263/350, Step 17600, ETA 37.07s] step time: 0.005483s (±0.001533s); valid time: 0.01285s; loss: -14.3676 (±1.18704); valid loss: -6.54394\n",
      "[Epoch 265/350, Step 17700, ETA 36.43s] step time: 0.005563s (±0.001823s); valid time: 0.01229s; loss: -14.2331 (±1.20306); valid loss: -6.46866\n",
      "[Epoch 266/350, Step 17800, ETA 35.77s] step time: 0.005387s (±0.00147s); valid time: 0.01287s; loss: -14.4162 (±1.30458); valid loss: -6.45967\n",
      "[Epoch 268/350, Step 17900, ETA 35.13s] step time: 0.005544s (±0.001688s); valid time: 0.01247s; loss: -14.3587 (±1.3201); valid loss: -6.51351\n",
      "[Epoch 269/350, Step 18000, ETA 34.49s] step time: 0.005484s (±0.002321s); valid time: 0.02073s; loss: -14.2605 (±1.23762); valid loss: -6.43835\n",
      "[Epoch 270/350, Step 18090, ETA 33.9s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 271/350, Step 18100, ETA 33.85s] step time: 0.005466s (±0.00164s); valid time: 0.0125s; loss: -14.5192 (±1.2835); valid loss: -6.5003\n",
      "[Epoch 272/350, Step 18200, ETA 33.2s] step time: 0.005525s (±0.001683s); valid time: 0.01306s; loss: -14.203 (±1.27949); valid loss: -6.49143\n",
      "[Epoch 274/350, Step 18300, ETA 32.57s] step time: 0.005586s (±0.001704s); valid time: 0.01279s; loss: -14.2993 (±1.33476); valid loss: -6.52819\n",
      "[Epoch 275/350, Step 18400, ETA 31.93s] step time: 0.005678s (±0.00184s); valid time: 0.01325s; loss: -14.5314 (±1.32151); valid loss: -6.52851\n",
      "[Epoch 277/350, Step 18500, ETA 31.29s] step time: 0.005459s (±0.001545s); valid time: 0.01217s; loss: -14.3128 (±1.16265); valid loss: -6.49927\n",
      "[Epoch 278/350, Step 18600, ETA 30.65s] step time: 0.005602s (±0.001971s); valid time: 0.01376s; loss: -14.3347 (±1.35187); valid loss: -6.44971\n",
      "[Epoch 280/350, Step 18700, ETA 30.01s] step time: 0.005554s (±0.001842s); valid time: 0.01321s; loss: -14.3862 (±1.3643); valid loss: -6.49454\n",
      "[Epoch 280/350, Step 18760, ETA 29.62s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 18800, ETA 29.37s] step time: 0.005581s (±0.00175s); valid time: 0.01323s; loss: -14.3537 (±1.27409); valid loss: -6.53684\n",
      "[Epoch 283/350, Step 18900, ETA 28.74s] step time: 0.005653s (±0.002114s); valid time: 0.0128s; loss: -14.3388 (±1.18538); valid loss: -6.52024\n",
      "[Epoch 284/350, Step 19000, ETA 28.09s] step time: 0.005471s (±0.00153s); valid time: 0.01237s; loss: -14.33 (±1.25437); valid loss: -6.50984\n",
      "[Epoch 286/350, Step 19100, ETA 27.46s] step time: 0.005677s (±0.001928s); valid time: 0.0121s; loss: -14.373 (±1.29218); valid loss: -6.47771\n",
      "[Epoch 287/350, Step 19200, ETA 26.82s] step time: 0.005578s (±0.001718s); valid time: 0.01266s; loss: -14.4105 (±1.3586); valid loss: -6.48086\n",
      "[Epoch 289/350, Step 19300, ETA 26.19s] step time: 0.005549s (±0.001869s); valid time: 0.01278s; loss: -14.2634 (±1.28028); valid loss: -6.50178\n",
      "[Epoch 290/350, Step 19400, ETA 25.55s] step time: 0.005443s (±0.00158s); valid time: 0.01331s; loss: -14.3831 (±1.41563); valid loss: -6.46303\n",
      "[Epoch 290/350, Step 19430, ETA 25.35s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 19500, ETA 24.91s] step time: 0.005326s (±0.001672s); valid time: 0.0126s; loss: -14.3671 (±1.30468); valid loss: -6.51142\n",
      "[Epoch 293/350, Step 19600, ETA 24.27s] step time: 0.005493s (±0.001642s); valid time: 0.01302s; loss: -14.3287 (±1.3355); valid loss: -6.50439\n",
      "[Epoch 295/350, Step 19700, ETA 23.63s] step time: 0.005528s (±0.001763s); valid time: 0.01267s; loss: -14.3764 (±1.21267); valid loss: -6.49387\n",
      "[Epoch 296/350, Step 19800, ETA 22.99s] step time: 0.005394s (±0.001534s); valid time: 0.01249s; loss: -14.2893 (±1.37369); valid loss: -6.46715\n",
      "[Epoch 298/350, Step 19900, ETA 22.37s] step time: 0.005827s (±0.001778s); valid time: 0.01399s; loss: -14.4572 (±1.34951); valid loss: -6.47423\n",
      "[Epoch 299/350, Step 20000, ETA 21.73s] step time: 0.005583s (±0.001603s); valid time: 0.01324s; loss: -14.3635 (±1.37852); valid loss: -6.49252\n",
      "[Epoch 300/350, Step 20100, ETA 21.1s] step time: 0.005468s (±0.00178s); valid time: 0.01315s; loss: -14.3789 (±1.25761); valid loss: -6.48372\n",
      "[Epoch 300/350, Step 20100, ETA 21.1s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 302/350, Step 20200, ETA 20.46s] step time: 0.005585s (±0.001827s); valid time: 0.01199s; loss: -14.1482 (±1.30106); valid loss: -6.4561\n",
      "[Epoch 303/350, Step 20300, ETA 19.83s] step time: 0.005388s (±0.001553s); valid time: 0.01264s; loss: -14.525 (±1.29427); valid loss: -6.45991\n",
      "[Epoch 305/350, Step 20400, ETA 19.19s] step time: 0.005522s (±0.001756s); valid time: 0.01354s; loss: -14.2845 (±1.35508); valid loss: -6.49543\n",
      "[Epoch 306/350, Step 20500, ETA 18.56s] step time: 0.005427s (±0.001698s); valid time: 0.01256s; loss: -14.4609 (±1.29619); valid loss: -6.5248\n",
      "[Epoch 308/350, Step 20600, ETA 17.93s] step time: 0.00557s (±0.001872s); valid time: 0.01311s; loss: -14.4022 (±1.33888); valid loss: -6.4799\n",
      "[Epoch 309/350, Step 20700, ETA 17.29s] step time: 0.005347s (±0.001315s); valid time: 0.01246s; loss: -14.3435 (±1.13849); valid loss: -6.48231\n",
      "[Epoch 310/350, Step 20770, ETA 16.85s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 20800, ETA 16.66s] step time: 0.005434s (±0.001651s); valid time: 0.01358s; loss: -14.3499 (±1.3691); valid loss: -6.51437\n",
      "[Epoch 312/350, Step 20900, ETA 16.03s] step time: 0.005438s (±0.001753s); valid time: 0.0125s; loss: -14.4278 (±1.43013); valid loss: -6.49293\n",
      "[Epoch 314/350, Step 21000, ETA 15.4s] step time: 0.005627s (±0.002022s); valid time: 0.01257s; loss: -14.2933 (±1.44479); valid loss: -6.48777\n",
      "[Epoch 315/350, Step 21100, ETA 14.76s] step time: 0.00539s (±0.001608s); valid time: 0.01284s; loss: -14.3074 (±1.25516); valid loss: -6.48758\n",
      "[Epoch 317/350, Step 21200, ETA 14.13s] step time: 0.005431s (±0.001546s); valid time: 0.01249s; loss: -14.3501 (±1.4129); valid loss: -6.44547\n",
      "[Epoch 318/350, Step 21300, ETA 13.5s] step time: 0.005453s (±0.001659s); valid time: 0.01207s; loss: -14.3016 (±1.31597); valid loss: -6.47986\n",
      "[Epoch 320/350, Step 21400, ETA 12.87s] step time: 0.005417s (±0.001631s); valid time: 0.01246s; loss: -14.3133 (±1.23742); valid loss: -6.49807\n",
      "[Epoch 320/350, Step 21440, ETA 12.61s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 21500, ETA 12.24s] step time: 0.005518s (±0.00192s); valid time: 0.01261s; loss: -14.3124 (±1.27787); valid loss: -6.46045\n",
      "[Epoch 323/350, Step 21600, ETA 11.61s] step time: 0.005757s (±0.001747s); valid time: 0.01261s; loss: -14.4807 (±1.37641); valid loss: -6.48634\n",
      "[Epoch 324/350, Step 21700, ETA 10.98s] step time: 0.005683s (±0.00183s); valid time: 0.01289s; loss: -14.2782 (±1.49648); valid loss: -6.47923\n",
      "[Epoch 326/350, Step 21800, ETA 10.35s] step time: 0.005588s (±0.001817s); valid time: 0.0126s; loss: -14.3093 (±1.50016); valid loss: -6.51794\n",
      "[Epoch 327/350, Step 21900, ETA 9.723s] step time: 0.005513s (±0.001652s); valid time: 0.01284s; loss: -14.36 (±1.30566); valid loss: -6.4831\n",
      "[Epoch 329/350, Step 22000, ETA 9.094s] step time: 0.005515s (±0.00157s); valid time: 0.01226s; loss: -14.2788 (±1.34752); valid loss: -6.4814\n",
      "[Epoch 330/350, Step 22100, ETA 8.465s] step time: 0.005572s (±0.001953s); valid time: 0.01296s; loss: -14.4422 (±1.21174); valid loss: -6.44475\n",
      "[Epoch 330/350, Step 22110, ETA 8.402s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 22200, ETA 7.837s] step time: 0.005454s (±0.001613s); valid time: 0.01237s; loss: -14.4039 (±1.38064); valid loss: -6.50596\n",
      "[Epoch 333/350, Step 22300, ETA 7.208s] step time: 0.005632s (±0.001885s); valid time: 0.01636s; loss: -14.2036 (±1.31041); valid loss: -6.52557\n",
      "[Epoch 335/350, Step 22400, ETA 6.582s] step time: 0.005702s (±0.001725s); valid time: 0.01382s; loss: -14.4767 (±1.32487); valid loss: -6.50397\n",
      "[Epoch 336/350, Step 22500, ETA 5.953s] step time: 0.005449s (±0.001537s); valid time: 0.01235s; loss: -14.3114 (±1.35535); valid loss: -6.52726\n",
      "[Epoch 338/350, Step 22600, ETA 5.326s] step time: 0.005571s (±0.001753s); valid time: 0.01284s; loss: -14.3101 (±1.38009); valid loss: -6.54358\n",
      "[Epoch 339/350, Step 22700, ETA 4.698s] step time: 0.005515s (±0.00165s); valid time: 0.01215s; loss: -14.4193 (±1.23339); valid loss: -6.47801\n",
      "[Epoch 340/350, Step 22780, ETA 4.197s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 22800, ETA 4.072s] step time: 0.005684s (±0.002314s); valid time: 0.01265s; loss: -14.2723 (±1.23876); valid loss: -6.46678\n",
      "[Epoch 342/350, Step 22900, ETA 3.444s] step time: 0.005497s (±0.001569s); valid time: 0.01284s; loss: -14.4251 (±1.29327); valid loss: -6.46921\n",
      "[Epoch 344/350, Step 23000, ETA 2.818s] step time: 0.005507s (±0.001695s); valid time: 0.01245s; loss: -14.3589 (±1.2765); valid loss: -6.54517\n",
      "[Epoch 345/350, Step 23100, ETA 2.191s] step time: 0.005567s (±0.001934s); valid time: 0.01291s; loss: -14.4183 (±1.33325); valid loss: -6.49619\n",
      "[Epoch 347/350, Step 23200, ETA 1.565s] step time: 0.00542s (±0.001712s); valid time: 0.01313s; loss: -14.3947 (±1.20108); valid loss: -6.4615\n",
      "[Epoch 348/350, Step 23300, ETA 0.9388s] step time: 0.005604s (±0.001891s); valid time: 0.01317s; loss: -14.2282 (±1.21077); valid loss: -6.51138\n",
      "[Epoch 350/350, Step 23400, ETA 0.3129s] step time: 0.005533s (±0.001632s); valid time: 0.01308s; loss: -14.3409 (±1.41948); valid loss: -6.44374\n",
      "[Epoch 350/350, Step 23450, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9cy7zyre/variables.dat-16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9cy7zyre/variables.dat-16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.49,\n",
      "\t(tp, fp, tn, fn)=(166, 1137, 1004, 73),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.69,\n",
      "\tf1=0.22,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.5474789915966387,\n",
      "\ty_label%=0.1004201680672269,\n",
      ")\n",
      "Testing on realTraffic/occupancy_t4013.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 60, ETA 21.71s] Learning rate decreased to 0.00075\n",
      "[Epoch 17/350, Step 100, ETA 20.82s] step time: 0.00984s (±0.03274s); valid time: 0.16s; loss: 149.945 (±7.30455); valid loss: 69.7679 (*)\n",
      "[Epoch 20/350, Step 120, ETA 19.15s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 30/350, Step 180, ETA 16.15s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 34/350, Step 200, ETA 16.25s] step time: 0.006143s (±0.007575s); valid time: 0.07515s; loss: 133.761 (±3.62796); valid loss: 66.1328 (*)\n",
      "[Epoch 40/350, Step 240, ETA 15.21s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 300, ETA 14.4s] step time: 0.006305s (±0.007847s); valid time: 0.07689s; loss: 123.886 (±2.37556); valid loss: 63.7868 (*)\n",
      "[Epoch 50/350, Step 300, ETA 14.4s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 360, ETA 13.3s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 67/350, Step 400, ETA 12.74s] step time: 0.005374s (±0.001531s); valid time: 0.001803s; loss: 119.244 (±1.55626); valid loss: 64.0925\n",
      "[Epoch 70/350, Step 420, ETA 12.44s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 80/350, Step 480, ETA 11.68s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 84/350, Step 500, ETA 11.45s] step time: 0.005259s (±0.0006445s); valid time: 0.001838s; loss: 117.788 (±1.3525); valid loss: 64.2946\n",
      "[Epoch 90/350, Step 540, ETA 11.01s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 600, ETA 10.44s] step time: 0.005446s (±0.001499s); valid time: 0.001808s; loss: 117.212 (±1.32888); valid loss: 63.8578\n",
      "[Epoch 100/350, Step 600, ETA 10.44s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 660, ETA 9.891s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 117/350, Step 700, ETA 9.554s] step time: 0.005444s (±0.001362s); valid time: 0.001785s; loss: 116.993 (±1.48464); valid loss: 64.3543\n",
      "[Epoch 120/350, Step 720, ETA 9.417s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 130/350, Step 780, ETA 8.921s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 134/350, Step 800, ETA 8.755s] step time: 0.005532s (±0.001387s); valid time: 0.001741s; loss: 116.726 (±1.41168); valid loss: 63.9484\n",
      "[Epoch 140/350, Step 840, ETA 8.433s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 900, ETA 7.978s] step time: 0.005403s (±0.001471s); valid time: 0.001761s; loss: 116.621 (±1.24881); valid loss: 63.9738\n",
      "[Epoch 150/350, Step 900, ETA 7.979s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 960, ETA 7.516s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 167/350, Step 1000, ETA 7.224s] step time: 0.005259s (±0.001156s); valid time: 0.001821s; loss: 116.726 (±1.48877); valid loss: 64.0915\n",
      "[Epoch 170/350, Step 1020, ETA 7.078s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 180/350, Step 1080, ETA 6.659s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 184/350, Step 1100, ETA 6.518s] step time: 0.005464s (±0.001435s); valid time: 0.001822s; loss: 116.573 (±1.24369); valid loss: 64.0829\n",
      "[Epoch 190/350, Step 1140, ETA 6.235s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 1200, ETA 5.821s] step time: 0.005372s (±0.001063s); valid time: 0.001985s; loss: 116.578 (±1.41655); valid loss: 63.838\n",
      "[Epoch 200/350, Step 1200, ETA 5.821s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 1260, ETA 5.413s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 217/350, Step 1300, ETA 5.144s] step time: 0.005433s (±0.00144s); valid time: 0.001738s; loss: 116.503 (±1.38581); valid loss: 63.8677\n",
      "[Epoch 220/350, Step 1320, ETA 5.006s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 230/350, Step 1380, ETA 4.604s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 234/350, Step 1400, ETA 4.472s] step time: 0.005284s (±0.0009007s); valid time: 0.001839s; loss: 116.605 (±1.44485); valid loss: 63.9534\n",
      "[Epoch 240/350, Step 1440, ETA 4.212s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1500, ETA 3.82s] step time: 0.005522s (±0.001509s); valid time: 0.001979s; loss: 116.499 (±1.32959); valid loss: 63.8078\n",
      "[Epoch 250/350, Step 1500, ETA 3.82s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1560, ETA 3.429s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 267/350, Step 1600, ETA 3.177s] step time: 0.005586s (±0.001385s); valid time: 0.001971s; loss: 116.487 (±1.25951); valid loss: 64.0001\n",
      "[Epoch 270/350, Step 1620, ETA 3.047s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 280/350, Step 1680, ETA 2.66s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 284/350, Step 1700, ETA 2.532s] step time: 0.005392s (±0.0009948s); valid time: 0.001817s; loss: 116.521 (±1.442); valid loss: 63.9951\n",
      "[Epoch 290/350, Step 1740, ETA 2.278s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1800, ETA 1.896s] step time: 0.005577s (±0.001521s); valid time: 0.001882s; loss: 116.459 (±1.54072); valid loss: 63.9676\n",
      "[Epoch 300/350, Step 1800, ETA 1.896s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1860, ETA 1.514s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 317/350, Step 1900, ETA 1.261s] step time: 0.005478s (±0.001323s); valid time: 0.001865s; loss: 116.526 (±1.29894); valid loss: 63.9579\n",
      "[Epoch 320/350, Step 1920, ETA 1.135s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 330/350, Step 1980, ETA 0.7552s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 334/350, Step 2000, ETA 0.6291s] step time: 0.00546s (±0.001232s); valid time: 0.001961s; loss: 116.512 (±1.39246); valid loss: 64.0774\n",
      "[Epoch 340/350, Step 2040, ETA 0.3772s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 2100, ETA 0s] step time: 0.005582s (±0.001338s); valid time: 0.001935s; loss: 116.543 (±1.56402); valid loss: 64.0562\n",
      "[Epoch 350/350, Step 2100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1qcs8c1w/variables.dat-300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1qcs8c1w/variables.dat-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.42,\n",
      "\t(tp, fp, tn, fn)=(235, 1433, 817, 15),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.94,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.65,\n",
      "\ty_pred%=0.6672,\n",
      "\ty_label%=0.1,\n",
      ")\n",
      "Testing on realTraffic/speed_6005.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 2/350, Step 100, ETA 3m 59.17s] step time: 0.009669s (±0.03276s); valid time: 0.1763s; loss: -3.28667 (±3.9355); valid loss: 7.46144 (*)\n",
      "[Epoch 3/350, Step 200, ETA 3m 15.24s] step time: 0.006082s (±0.008449s); valid time: 0.08486s; loss: -6.52163 (±0.922861); valid loss: 7.0239 (*)\n",
      "[Epoch 5/350, Step 300, ETA 3m 2.677s] step time: 0.006279s (±0.008714s); valid time: 0.08711s; loss: -7.38162 (±1.05332); valid loss: 6.90318 (*)\n",
      "[Epoch 6/350, Step 400, ETA 2m 54.25s] step time: 0.006051s (±0.008516s); valid time: 0.08507s; loss: -7.98002 (±1.04286); valid loss: 6.83752 (*)\n",
      "[Epoch 8/350, Step 500, ETA 2m 50.6s] step time: 0.006278s (±0.009108s); valid time: 0.09126s; loss: -8.49363 (±1.22436); valid loss: 6.82925 (*)\n",
      "[Epoch 9/350, Step 600, ETA 2m 43.88s] step time: 0.005368s (±0.001483s); valid time: 0.01228s; loss: -8.84621 (±1.1114); valid loss: 6.86214\n",
      "[Epoch 10/350, Step 670, ETA 2m 39.95s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 700, ETA 2m 41.98s] step time: 0.006252s (±0.009966s); valid time: 0.09974s; loss: -9.28789 (±1.00376); valid loss: 6.7978 (*)\n",
      "[Epoch 12/350, Step 800, ETA 2m 39.91s] step time: 0.006244s (±0.008954s); valid time: 0.08931s; loss: -9.59142 (±1.26741); valid loss: 6.77169 (*)\n",
      "[Epoch 14/350, Step 900, ETA 2m 38.57s] step time: 0.00625s (±0.008629s); valid time: 0.08545s; loss: -9.75174 (±1.06864); valid loss: 6.74806 (*)\n",
      "[Epoch 15/350, Step 1000, ETA 2m 36.71s] step time: 0.006116s (±0.00871s); valid time: 0.08639s; loss: -10.0474 (±1.17644); valid loss: 6.73381 (*)\n",
      "[Epoch 17/350, Step 1100, ETA 2m 36.04s] step time: 0.006426s (±0.009002s); valid time: 0.08968s; loss: -10.2356 (±1.1645); valid loss: 6.68719 (*)\n",
      "[Epoch 18/350, Step 1200, ETA 2m 33.34s] step time: 0.005424s (±0.001602s); valid time: 0.01307s; loss: -10.3191 (±1.24113); valid loss: 6.73365\n",
      "[Epoch 20/350, Step 1300, ETA 2m 32.44s] step time: 0.006212s (±0.008744s); valid time: 0.08709s; loss: -10.4159 (±1.30101); valid loss: 6.66486 (*)\n",
      "[Epoch 20/350, Step 1340, ETA 2m 31.71s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/350, Step 1400, ETA 2m 31.78s] step time: 0.006487s (±0.009379s); valid time: 0.0932s; loss: -10.7301 (±1.24123); valid loss: 6.61965 (*)\n",
      "[Epoch 23/350, Step 1500, ETA 2m 30.01s] step time: 0.00559s (±0.001745s); valid time: 0.01246s; loss: -10.8493 (±1.28353); valid loss: 6.62362\n",
      "[Epoch 24/350, Step 1600, ETA 2m 27.97s] step time: 0.005445s (±0.001546s); valid time: 0.01286s; loss: -10.9238 (±1.25021); valid loss: 6.64833\n",
      "[Epoch 26/350, Step 1700, ETA 2m 26.58s] step time: 0.005649s (±0.001728s); valid time: 0.01251s; loss: -11.0093 (±1.37785); valid loss: 6.64645\n",
      "[Epoch 27/350, Step 1800, ETA 2m 25.79s] step time: 0.006242s (±0.008013s); valid time: 0.0798s; loss: -11.125 (±1.28195); valid loss: 6.5798 (*)\n",
      "[Epoch 29/350, Step 1900, ETA 2m 24.6s] step time: 0.005713s (±0.002061s); valid time: 0.01391s; loss: -11.3078 (±1.13538); valid loss: 6.60198\n",
      "[Epoch 30/350, Step 2000, ETA 2m 23.01s] step time: 0.005459s (±0.001743s); valid time: 0.01229s; loss: -11.2004 (±1.3295); valid loss: 6.61468\n",
      "[Epoch 30/350, Step 2010, ETA 2m 22.84s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 2100, ETA 2m 21.63s] step time: 0.005405s (±0.00163s); valid time: 0.01229s; loss: -11.4897 (±1.20415); valid loss: 6.62185\n",
      "[Epoch 33/350, Step 2200, ETA 2m 20.24s] step time: 0.005486s (±0.001596s); valid time: 0.01244s; loss: -11.4278 (±1.26982); valid loss: 6.6647\n",
      "[Epoch 35/350, Step 2300, ETA 2m 19.66s] step time: 0.006125s (±0.008522s); valid time: 0.08431s; loss: -11.5868 (±1.19787); valid loss: 6.47972 (*)\n",
      "[Epoch 36/350, Step 2400, ETA 2m 18.49s] step time: 0.005551s (±0.001554s); valid time: 0.01204s; loss: -11.6469 (±1.34977); valid loss: 6.57154\n",
      "[Epoch 38/350, Step 2500, ETA 2m 17.44s] step time: 0.005566s (±0.001605s); valid time: 0.01171s; loss: -11.8463 (±1.47216); valid loss: 6.6079\n",
      "[Epoch 39/350, Step 2600, ETA 2m 16.21s] step time: 0.005448s (±0.001641s); valid time: 0.0131s; loss: -11.8095 (±1.41691); valid loss: 6.63101\n",
      "[Epoch 40/350, Step 2680, ETA 2m 15.29s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 41/350, Step 2700, ETA 2m 15.2s] step time: 0.005508s (±0.001778s); valid time: 0.01245s; loss: -11.8184 (±1.27336); valid loss: 6.51009\n",
      "[Epoch 42/350, Step 2800, ETA 2m 14.02s] step time: 0.005395s (±0.001655s); valid time: 0.01234s; loss: -12.0123 (±1.41369); valid loss: 6.57389\n",
      "[Epoch 44/350, Step 2900, ETA 2m 13.07s] step time: 0.005476s (±0.001618s); valid time: 0.01203s; loss: -12.0528 (±1.53871); valid loss: 6.5471\n",
      "[Epoch 45/350, Step 3000, ETA 2m 11.97s] step time: 0.005419s (±0.001978s); valid time: 0.01241s; loss: -12.126 (±1.30629); valid loss: 6.54108\n",
      "[Epoch 47/350, Step 3100, ETA 2m 11s] step time: 0.005399s (±0.001464s); valid time: 0.01212s; loss: -12.1229 (±1.17996); valid loss: 6.50997\n",
      "[Epoch 48/350, Step 3200, ETA 2m 10.49s] step time: 0.00624s (±0.00861s); valid time: 0.08565s; loss: -12.3682 (±1.36893); valid loss: 6.44293 (*)\n",
      "[Epoch 50/350, Step 3300, ETA 2m 9.476s] step time: 0.00529s (±0.001537s); valid time: 0.01204s; loss: -12.2259 (±1.35406); valid loss: 6.46269\n",
      "[Epoch 50/350, Step 3350, ETA 2m 8.951s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 51/350, Step 3400, ETA 2m 8.565s] step time: 0.005561s (±0.001838s); valid time: 0.01237s; loss: -12.4268 (±1.32459); valid loss: 6.57532\n",
      "[Epoch 53/350, Step 3500, ETA 2m 7.726s] step time: 0.005509s (±0.002414s); valid time: 0.01506s; loss: -12.2982 (±1.18133); valid loss: 6.44986\n",
      "[Epoch 54/350, Step 3600, ETA 2m 6.749s] step time: 0.005403s (±0.001471s); valid time: 0.01233s; loss: -12.6053 (±1.58325); valid loss: 6.53812\n",
      "[Epoch 56/350, Step 3700, ETA 2m 5.959s] step time: 0.005536s (±0.002043s); valid time: 0.01352s; loss: -12.461 (±1.40263); valid loss: 6.47917\n",
      "[Epoch 57/350, Step 3800, ETA 2m 5.022s] step time: 0.005405s (±0.001321s); valid time: 0.01133s; loss: -12.7241 (±1.43355); valid loss: 6.48399\n",
      "[Epoch 59/350, Step 3900, ETA 2m 4.239s] step time: 0.005503s (±0.00176s); valid time: 0.01228s; loss: -12.5183 (±1.44017); valid loss: 6.45566\n",
      "[Epoch 60/350, Step 4000, ETA 2m 3.655s] step time: 0.00607s (±0.008692s); valid time: 0.08693s; loss: -12.523 (±1.28762); valid loss: 6.42104 (*)\n",
      "[Epoch 60/350, Step 4020, ETA 2m 3.488s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 62/350, Step 4100, ETA 2m 3.198s] step time: 0.006176s (±0.009047s); valid time: 0.08997s; loss: -12.814 (±1.44745); valid loss: 6.40597 (*)\n",
      "[Epoch 63/350, Step 4200, ETA 2m 2.355s] step time: 0.005511s (±0.001516s); valid time: 0.01222s; loss: -12.704 (±1.40535); valid loss: 6.43154\n",
      "[Epoch 65/350, Step 4300, ETA 2m 1.576s] step time: 0.005478s (±0.001652s); valid time: 0.0146s; loss: -12.8085 (±1.53188); valid loss: 6.51965\n",
      "[Epoch 66/350, Step 4400, ETA 2m 0.7417s] step time: 0.005484s (±0.00164s); valid time: 0.01219s; loss: -12.8114 (±1.38832); valid loss: 6.43213\n",
      "[Epoch 68/350, Step 4500, ETA 1m 59.92s] step time: 0.005311s (±0.001505s); valid time: 0.01245s; loss: -12.8057 (±1.44238); valid loss: 6.50586\n",
      "[Epoch 69/350, Step 4600, ETA 1m 59.12s] step time: 0.00554s (±0.001788s); valid time: 0.0124s; loss: -12.8923 (±1.6839); valid loss: 6.48593\n",
      "[Epoch 70/350, Step 4690, ETA 1m 58.36s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 71/350, Step 4700, ETA 1m 58.37s] step time: 0.005451s (±0.001694s); valid time: 0.01273s; loss: -12.9098 (±1.40801); valid loss: 6.42032\n",
      "[Epoch 72/350, Step 4800, ETA 1m 57.57s] step time: 0.005457s (±0.001571s); valid time: 0.01218s; loss: -12.9898 (±1.4514); valid loss: 6.42727\n",
      "[Epoch 74/350, Step 4900, ETA 1m 56.85s] step time: 0.005508s (±0.001814s); valid time: 0.01213s; loss: -13.0573 (±1.48074); valid loss: 6.47381\n",
      "[Epoch 75/350, Step 5000, ETA 1m 56.01s] step time: 0.005344s (±0.0016s); valid time: 0.01264s; loss: -12.9348 (±1.39718); valid loss: 6.4314\n",
      "[Epoch 77/350, Step 5100, ETA 1m 55.28s] step time: 0.005458s (±0.001764s); valid time: 0.01382s; loss: -13.0927 (±1.55911); valid loss: 6.49773\n",
      "[Epoch 78/350, Step 5200, ETA 1m 54.54s] step time: 0.005572s (±0.001747s); valid time: 0.01206s; loss: -12.982 (±1.44711); valid loss: 6.46806\n",
      "[Epoch 80/350, Step 5300, ETA 1m 53.83s] step time: 0.005461s (±0.001549s); valid time: 0.01316s; loss: -13.2466 (±1.45198); valid loss: 6.45171\n",
      "[Epoch 80/350, Step 5360, ETA 1m 53.34s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 5400, ETA 1m 53.12s] step time: 0.005613s (±0.001805s); valid time: 0.01246s; loss: -13.0831 (±1.54387); valid loss: 6.4187\n",
      "[Epoch 83/350, Step 5500, ETA 1m 52.42s] step time: 0.00548s (±0.001493s); valid time: 0.01264s; loss: -13.0079 (±1.44979); valid loss: 6.44004\n",
      "[Epoch 84/350, Step 5600, ETA 1m 51.67s] step time: 0.005476s (±0.001659s); valid time: 0.01427s; loss: -13.3778 (±1.49488); valid loss: 6.43424\n",
      "[Epoch 86/350, Step 5700, ETA 1m 51s] step time: 0.005562s (±0.001708s); valid time: 0.01252s; loss: -13.0901 (±1.44383); valid loss: 6.40681\n",
      "[Epoch 87/350, Step 5800, ETA 1m 50.21s] step time: 0.00531s (±0.001535s); valid time: 0.01246s; loss: -13.3532 (±1.64524); valid loss: 6.4194\n",
      "[Epoch 89/350, Step 5900, ETA 1m 49.52s] step time: 0.005482s (±0.001778s); valid time: 0.01258s; loss: -13.0746 (±1.51462); valid loss: 6.43667\n",
      "[Epoch 90/350, Step 6000, ETA 1m 48.99s] step time: 0.006158s (±0.008745s); valid time: 0.08726s; loss: -13.1338 (±1.42047); valid loss: 6.35619 (*)\n",
      "[Epoch 90/350, Step 6030, ETA 1m 48.74s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 6100, ETA 1m 48.31s] step time: 0.00542s (±0.001494s); valid time: 0.01219s; loss: -13.35 (±1.56539); valid loss: 6.43011\n",
      "[Epoch 93/350, Step 6200, ETA 1m 47.64s] step time: 0.005679s (±0.002562s); valid time: 0.01615s; loss: -13.3079 (±1.40284); valid loss: 6.44978\n",
      "[Epoch 95/350, Step 6300, ETA 1m 46.94s] step time: 0.005388s (±0.001607s); valid time: 0.0123s; loss: -13.3723 (±1.44255); valid loss: 6.42738\n",
      "[Epoch 96/350, Step 6400, ETA 1m 46.27s] step time: 0.005657s (±0.001704s); valid time: 0.01173s; loss: -13.3251 (±1.63588); valid loss: 6.44766\n",
      "[Epoch 98/350, Step 6500, ETA 1m 45.62s] step time: 0.005568s (±0.001853s); valid time: 0.01309s; loss: -13.3777 (±1.41272); valid loss: 6.41623\n",
      "[Epoch 99/350, Step 6600, ETA 1m 44.94s] step time: 0.005602s (±0.001758s); valid time: 0.01324s; loss: -13.3203 (±1.42348); valid loss: 6.40837\n",
      "[Epoch 100/350, Step 6700, ETA 1m 44.22s] step time: 0.005431s (±0.001421s); valid time: 0.0118s; loss: -13.4042 (±1.69144); valid loss: 6.43285\n",
      "[Epoch 100/350, Step 6700, ETA 1m 44.22s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 102/350, Step 6800, ETA 1m 43.64s] step time: 0.005788s (±0.001989s); valid time: 0.01252s; loss: -13.4097 (±1.45981); valid loss: 6.44178\n",
      "[Epoch 103/350, Step 6900, ETA 1m 42.95s] step time: 0.005521s (±0.001556s); valid time: 0.01218s; loss: -13.415 (±1.29575); valid loss: 6.41352\n",
      "[Epoch 105/350, Step 7000, ETA 1m 42.28s] step time: 0.005441s (±0.001513s); valid time: 0.01245s; loss: -13.4921 (±1.27207); valid loss: 6.45793\n",
      "[Epoch 106/350, Step 7100, ETA 1m 41.58s] step time: 0.00546s (±0.001553s); valid time: 0.01308s; loss: -13.472 (±1.50751); valid loss: 6.48992\n",
      "[Epoch 108/350, Step 7200, ETA 1m 40.91s] step time: 0.005399s (±0.001647s); valid time: 0.01223s; loss: -13.4849 (±1.49382); valid loss: 6.42454\n",
      "[Epoch 109/350, Step 7300, ETA 1m 40.23s] step time: 0.005461s (±0.001604s); valid time: 0.01218s; loss: -13.3611 (±1.2981); valid loss: 6.45168\n",
      "[Epoch 110/350, Step 7370, ETA 1m 39.73s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 7400, ETA 1m 39.58s] step time: 0.005471s (±0.001636s); valid time: 0.01234s; loss: -13.5256 (±1.70042); valid loss: 6.42281\n",
      "[Epoch 112/350, Step 7500, ETA 1m 38.87s] step time: 0.005393s (±0.001557s); valid time: 0.01226s; loss: -13.4695 (±1.38221); valid loss: 6.43317\n",
      "[Epoch 114/350, Step 7600, ETA 1m 38.22s] step time: 0.005478s (±0.0017s); valid time: 0.01288s; loss: -13.4679 (±1.56621); valid loss: 6.43528\n",
      "[Epoch 115/350, Step 7700, ETA 1m 37.53s] step time: 0.005466s (±0.001798s); valid time: 0.01306s; loss: -13.5995 (±1.30458); valid loss: 6.47622\n",
      "[Epoch 117/350, Step 7800, ETA 1m 36.88s] step time: 0.005478s (±0.001692s); valid time: 0.01269s; loss: -13.4967 (±1.49362); valid loss: 6.38908\n",
      "[Epoch 118/350, Step 7900, ETA 1m 36.24s] step time: 0.005692s (±0.002586s); valid time: 0.0138s; loss: -13.524 (±1.4644); valid loss: 6.41147\n",
      "[Epoch 120/350, Step 8000, ETA 1m 35.57s] step time: 0.005353s (±0.001575s); valid time: 0.01263s; loss: -13.5475 (±1.58996); valid loss: 6.37627\n",
      "[Epoch 120/350, Step 8040, ETA 1m 35.27s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 121/350, Step 8100, ETA 1m 34.91s] step time: 0.005565s (±0.00223s); valid time: 0.01591s; loss: -13.4554 (±1.4552); valid loss: 6.42218\n",
      "[Epoch 123/350, Step 8200, ETA 1m 34.27s] step time: 0.005498s (±0.001646s); valid time: 0.0131s; loss: -13.5013 (±1.52043); valid loss: 6.41533\n",
      "[Epoch 124/350, Step 8300, ETA 1m 33.6s] step time: 0.005505s (±0.00162s); valid time: 0.01244s; loss: -13.6873 (±1.29949); valid loss: 6.43385\n",
      "[Epoch 126/350, Step 8400, ETA 1m 32.97s] step time: 0.005537s (±0.001846s); valid time: 0.01515s; loss: -13.4876 (±1.47694); valid loss: 6.43281\n",
      "[Epoch 127/350, Step 8500, ETA 1m 32.33s] step time: 0.005615s (±0.001659s); valid time: 0.0127s; loss: -13.7098 (±1.64271); valid loss: 6.3808\n",
      "[Epoch 129/350, Step 8600, ETA 1m 31.72s] step time: 0.005619s (±0.001719s); valid time: 0.01283s; loss: -13.4505 (±1.38165); valid loss: 6.38908\n",
      "[Epoch 130/350, Step 8700, ETA 1m 31.05s] step time: 0.00542s (±0.001518s); valid time: 0.01251s; loss: -13.5887 (±1.5583); valid loss: 6.39629\n",
      "[Epoch 130/350, Step 8710, ETA 1m 30.98s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 8800, ETA 1m 30.41s] step time: 0.005455s (±0.001958s); valid time: 0.01382s; loss: -13.4646 (±1.63945); valid loss: 6.40786\n",
      "[Epoch 133/350, Step 8900, ETA 1m 29.74s] step time: 0.005435s (±0.001538s); valid time: 0.01394s; loss: -13.7274 (±1.4481); valid loss: 6.42141\n",
      "[Epoch 135/350, Step 9000, ETA 1m 29.13s] step time: 0.005604s (±0.001706s); valid time: 0.01318s; loss: -13.5905 (±1.52474); valid loss: 6.43658\n",
      "[Epoch 136/350, Step 9100, ETA 1m 28.47s] step time: 0.005503s (±0.001795s); valid time: 0.01537s; loss: -13.6818 (±1.70852); valid loss: 6.41914\n",
      "[Epoch 138/350, Step 9200, ETA 1m 27.83s] step time: 0.005449s (±0.001625s); valid time: 0.01271s; loss: -13.5463 (±1.57362); valid loss: 6.38584\n",
      "[Epoch 139/350, Step 9300, ETA 1m 27.19s] step time: 0.005634s (±0.001654s); valid time: 0.012s; loss: -13.6126 (±1.70203); valid loss: 6.4083\n",
      "[Epoch 140/350, Step 9380, ETA 1m 26.64s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 141/350, Step 9400, ETA 1m 26.55s] step time: 0.005409s (±0.001755s); valid time: 0.01561s; loss: -13.6283 (±1.59458); valid loss: 6.41691\n",
      "[Epoch 142/350, Step 9500, ETA 1m 25.91s] step time: 0.00557s (±0.001772s); valid time: 0.01242s; loss: -13.7371 (±1.58025); valid loss: 6.37607\n",
      "[Epoch 144/350, Step 9600, ETA 1m 25.29s] step time: 0.005611s (±0.001807s); valid time: 0.01229s; loss: -13.5408 (±1.61719); valid loss: 6.38395\n",
      "[Epoch 145/350, Step 9700, ETA 1m 24.66s] step time: 0.0056s (±0.001962s); valid time: 0.0124s; loss: -13.6927 (±1.41333); valid loss: 6.36068\n",
      "[Epoch 147/350, Step 9800, ETA 1m 24.06s] step time: 0.005709s (±0.002307s); valid time: 0.0188s; loss: -13.4592 (±1.53683); valid loss: 6.38003\n",
      "[Epoch 148/350, Step 9900, ETA 1m 23.4s] step time: 0.005429s (±0.001429s); valid time: 0.01241s; loss: -13.6709 (±1.65869); valid loss: 6.37448\n",
      "[Epoch 150/350, Step 10000, ETA 1m 22.76s] step time: 0.005445s (±0.001574s); valid time: 0.01144s; loss: -13.6644 (±1.43215); valid loss: 6.39561\n",
      "[Epoch 150/350, Step 10050, ETA 1m 22.42s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 151/350, Step 10100, ETA 1m 22.12s] step time: 0.005544s (±0.002331s); valid time: 0.01505s; loss: -13.6409 (±1.51546); valid loss: 6.38732\n",
      "[Epoch 153/350, Step 10200, ETA 1m 21.47s] step time: 0.005328s (±0.001518s); valid time: 0.01249s; loss: -13.6554 (±1.6297); valid loss: 6.41897\n",
      "[Epoch 154/350, Step 10300, ETA 1m 20.82s] step time: 0.005433s (±0.00156s); valid time: 0.01231s; loss: -13.698 (±1.56109); valid loss: 6.39938\n",
      "[Epoch 156/350, Step 10400, ETA 1m 20.17s] step time: 0.005345s (±0.001381s); valid time: 0.01201s; loss: -13.5706 (±1.60874); valid loss: 6.39008\n",
      "[Epoch 157/350, Step 10500, ETA 1m 19.52s] step time: 0.005429s (±0.00169s); valid time: 0.01203s; loss: -13.7776 (±1.73546); valid loss: 6.37685\n",
      "[Epoch 159/350, Step 10600, ETA 1m 18.88s] step time: 0.005363s (±0.001599s); valid time: 0.01188s; loss: -13.6162 (±1.56277); valid loss: 6.39222\n",
      "[Epoch 160/350, Step 10700, ETA 1m 18.23s] step time: 0.005418s (±0.001563s); valid time: 0.01206s; loss: -13.6547 (±1.61847); valid loss: 6.4524\n",
      "[Epoch 160/350, Step 10720, ETA 1m 18.09s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 162/350, Step 10800, ETA 1m 17.61s] step time: 0.005586s (±0.001815s); valid time: 0.0124s; loss: -13.588 (±1.43012); valid loss: 6.39095\n",
      "[Epoch 163/350, Step 10900, ETA 1m 16.95s] step time: 0.005332s (±0.001551s); valid time: 0.01205s; loss: -13.6546 (±1.66171); valid loss: 6.42311\n",
      "[Epoch 165/350, Step 11000, ETA 1m 16.33s] step time: 0.005499s (±0.001563s); valid time: 0.01211s; loss: -13.7441 (±1.45768); valid loss: 6.41005\n",
      "[Epoch 166/350, Step 11100, ETA 1m 15.67s] step time: 0.005305s (±0.001354s); valid time: 0.01235s; loss: -13.6376 (±1.4691); valid loss: 6.4121\n",
      "[Epoch 168/350, Step 11200, ETA 1m 15.07s] step time: 0.005629s (±0.001976s); valid time: 0.01263s; loss: -13.6489 (±1.63506); valid loss: 6.40039\n",
      "[Epoch 169/350, Step 11300, ETA 1m 14.41s] step time: 0.005295s (±0.001565s); valid time: 0.01205s; loss: -13.6806 (±1.49958); valid loss: 6.40334\n",
      "[Epoch 170/350, Step 11390, ETA 1m 13.82s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 171/350, Step 11400, ETA 1m 13.78s] step time: 0.005372s (±0.001635s); valid time: 0.0125s; loss: -13.7195 (±1.38368); valid loss: 6.40624\n",
      "[Epoch 172/350, Step 11500, ETA 1m 13.14s] step time: 0.005524s (±0.002015s); valid time: 0.01273s; loss: -13.7174 (±1.56599); valid loss: 6.39467\n",
      "[Epoch 174/350, Step 11600, ETA 1m 12.51s] step time: 0.005354s (±0.001627s); valid time: 0.0124s; loss: -13.6905 (±1.62792); valid loss: 6.40053\n",
      "[Epoch 175/350, Step 11700, ETA 1m 11.89s] step time: 0.005631s (±0.001522s); valid time: 0.01252s; loss: -13.596 (±1.69601); valid loss: 6.37647\n",
      "[Epoch 177/350, Step 11800, ETA 1m 11.26s] step time: 0.005367s (±0.001576s); valid time: 0.01238s; loss: -13.725 (±1.44248); valid loss: 6.37238\n",
      "[Epoch 178/350, Step 11900, ETA 1m 10.63s] step time: 0.005518s (±0.00169s); valid time: 0.01261s; loss: -13.8108 (±1.43291); valid loss: 6.36447\n",
      "[Epoch 180/350, Step 12000, ETA 1m 10.01s] step time: 0.005529s (±0.001771s); valid time: 0.0124s; loss: -13.6084 (±1.56514); valid loss: 6.40329\n",
      "[Epoch 180/350, Step 12060, ETA 1m 9.609s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 12100, ETA 1m 9.372s] step time: 0.005377s (±0.001746s); valid time: 0.01254s; loss: -13.8013 (±1.591); valid loss: 6.39523\n",
      "[Epoch 183/350, Step 12200, ETA 1m 8.751s] step time: 0.005452s (±0.001669s); valid time: 0.01308s; loss: -13.5422 (±1.53647); valid loss: 6.40979\n",
      "[Epoch 184/350, Step 12300, ETA 1m 8.136s] step time: 0.005672s (±0.002004s); valid time: 0.01493s; loss: -13.7046 (±1.41519); valid loss: 6.37387\n",
      "[Epoch 186/350, Step 12400, ETA 1m 7.533s] step time: 0.005634s (±0.001924s); valid time: 0.01238s; loss: -13.7138 (±1.55833); valid loss: 6.36876\n",
      "[Epoch 187/350, Step 12500, ETA 1m 6.89s] step time: 0.005348s (±0.001525s); valid time: 0.01182s; loss: -13.7942 (±1.53566); valid loss: 6.39652\n",
      "[Epoch 189/350, Step 12600, ETA 1m 6.285s] step time: 0.005622s (±0.001959s); valid time: 0.01249s; loss: -13.51 (±1.44345); valid loss: 6.39954\n",
      "[Epoch 190/350, Step 12700, ETA 1m 5.664s] step time: 0.005595s (±0.001688s); valid time: 0.01227s; loss: -13.6198 (±1.54746); valid loss: 6.39518\n",
      "[Epoch 190/350, Step 12730, ETA 1m 5.461s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 12800, ETA 1m 5.044s] step time: 0.005441s (±0.001675s); valid time: 0.01205s; loss: -13.8555 (±1.3513); valid loss: 6.41735\n",
      "[Epoch 193/350, Step 12900, ETA 1m 4.412s] step time: 0.005449s (±0.001813s); valid time: 0.01346s; loss: -13.7552 (±1.52551); valid loss: 6.3955\n",
      "[Epoch 195/350, Step 13000, ETA 1m 3.862s] step time: 0.006264s (±0.01014s); valid time: 0.09974s; loss: -13.7013 (±1.38593); valid loss: 6.34706 (*)\n",
      "[Epoch 196/350, Step 13100, ETA 1m 3.228s] step time: 0.005428s (±0.001575s); valid time: 0.01219s; loss: -13.707 (±1.44818); valid loss: 6.41196\n",
      "[Epoch 198/350, Step 13200, ETA 1m 2.619s] step time: 0.005563s (±0.001815s); valid time: 0.01249s; loss: -13.6818 (±1.3719); valid loss: 6.35227\n",
      "[Epoch 199/350, Step 13300, ETA 1m 1.99s] step time: 0.005467s (±0.001657s); valid time: 0.01339s; loss: -13.652 (±1.70412); valid loss: 6.40387\n",
      "[Epoch 200/350, Step 13400, ETA 1m 1.367s] step time: 0.005549s (±0.001697s); valid time: 0.01234s; loss: -13.7136 (±1.48673); valid loss: 6.3647\n",
      "[Epoch 200/350, Step 13400, ETA 1m 1.367s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 202/350, Step 13500, ETA 1m 0.7462s] step time: 0.005412s (±0.001588s); valid time: 0.01242s; loss: -13.5905 (±1.62888); valid loss: 6.36017\n",
      "[Epoch 203/350, Step 13600, ETA 1m 0.1114s] step time: 0.005309s (±0.001484s); valid time: 0.01222s; loss: -13.8378 (±1.48644); valid loss: 6.38056\n",
      "[Epoch 205/350, Step 13700, ETA 59.51s] step time: 0.005694s (±0.002207s); valid time: 0.01236s; loss: -13.6919 (±1.39359); valid loss: 6.36507\n",
      "[Epoch 206/350, Step 13800, ETA 58.88s] step time: 0.005306s (±0.001535s); valid time: 0.01435s; loss: -13.686 (±1.47964); valid loss: 6.38817\n",
      "[Epoch 208/350, Step 13900, ETA 58.26s] step time: 0.005509s (±0.00177s); valid time: 0.01292s; loss: -13.7886 (±1.4225); valid loss: 6.3987\n",
      "[Epoch 209/350, Step 14000, ETA 57.63s] step time: 0.005228s (±0.001389s); valid time: 0.01362s; loss: -13.6263 (±1.65553); valid loss: 6.41797\n",
      "[Epoch 210/350, Step 14070, ETA 57.19s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 14100, ETA 57.02s] step time: 0.005494s (±0.001735s); valid time: 0.01225s; loss: -13.8613 (±1.51078); valid loss: 6.39421\n",
      "[Epoch 212/350, Step 14200, ETA 56.39s] step time: 0.00548s (±0.002021s); valid time: 0.01211s; loss: -13.6012 (±1.36727); valid loss: 6.42468\n",
      "[Epoch 214/350, Step 14300, ETA 55.76s] step time: 0.00526s (±0.001622s); valid time: 0.0126s; loss: -13.6426 (±1.59154); valid loss: 6.40429\n",
      "[Epoch 215/350, Step 14400, ETA 55.14s] step time: 0.005434s (±0.001724s); valid time: 0.01209s; loss: -13.7304 (±1.52587); valid loss: 6.38569\n",
      "[Epoch 217/350, Step 14500, ETA 54.51s] step time: 0.005223s (±0.00147s); valid time: 0.01267s; loss: -13.6618 (±1.50196); valid loss: 6.36658\n",
      "[Epoch 218/350, Step 14600, ETA 53.89s] step time: 0.005474s (±0.001823s); valid time: 0.01274s; loss: -13.7754 (±1.31812); valid loss: 6.36358\n",
      "[Epoch 220/350, Step 14700, ETA 53.28s] step time: 0.00553s (±0.001745s); valid time: 0.0127s; loss: -13.6921 (±1.72257); valid loss: 6.41544\n",
      "[Epoch 220/350, Step 14740, ETA 53.03s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 221/350, Step 14800, ETA 52.66s] step time: 0.005577s (±0.002443s); valid time: 0.01849s; loss: -13.751 (±1.6336); valid loss: 6.40113\n",
      "[Epoch 223/350, Step 14900, ETA 52.05s] step time: 0.005442s (±0.001685s); valid time: 0.01217s; loss: -13.6891 (±1.4031); valid loss: 6.40241\n",
      "[Epoch 224/350, Step 15000, ETA 51.43s] step time: 0.005436s (±0.001534s); valid time: 0.01183s; loss: -13.7608 (±1.49822); valid loss: 6.40757\n",
      "[Epoch 226/350, Step 15100, ETA 50.81s] step time: 0.005474s (±0.001675s); valid time: 0.01301s; loss: -13.6384 (±1.62059); valid loss: 6.38623\n",
      "[Epoch 227/350, Step 15200, ETA 50.19s] step time: 0.005323s (±0.001397s); valid time: 0.01181s; loss: -13.7259 (±1.60972); valid loss: 6.38245\n",
      "[Epoch 229/350, Step 15300, ETA 49.57s] step time: 0.005423s (±0.001825s); valid time: 0.01247s; loss: -13.8108 (±1.60526); valid loss: 6.42235\n",
      "[Epoch 230/350, Step 15400, ETA 48.95s] step time: 0.005455s (±0.001698s); valid time: 0.01282s; loss: -13.6166 (±1.39595); valid loss: 6.37193\n",
      "[Epoch 230/350, Step 15410, ETA 48.9s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 15500, ETA 48.35s] step time: 0.005575s (±0.001771s); valid time: 0.01246s; loss: -13.7394 (±1.50218); valid loss: 6.36085\n",
      "[Epoch 233/350, Step 15600, ETA 47.73s] step time: 0.005468s (±0.002004s); valid time: 0.01754s; loss: -13.6677 (±1.56447); valid loss: 6.39744\n",
      "[Epoch 235/350, Step 15700, ETA 47.12s] step time: 0.005481s (±0.001427s); valid time: 0.0125s; loss: -13.7109 (±1.45654); valid loss: 6.36199\n",
      "[Epoch 236/350, Step 15800, ETA 46.49s] step time: 0.005358s (±0.001712s); valid time: 0.0124s; loss: -13.6187 (±1.515); valid loss: 6.41457\n",
      "[Epoch 238/350, Step 15900, ETA 45.88s] step time: 0.005472s (±0.001736s); valid time: 0.01238s; loss: -13.8045 (±1.67748); valid loss: 6.38686\n",
      "[Epoch 239/350, Step 16000, ETA 45.27s] step time: 0.00554s (±0.00173s); valid time: 0.01249s; loss: -13.7467 (±1.59507); valid loss: 6.37001\n",
      "[Epoch 240/350, Step 16080, ETA 44.77s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/350, Step 16100, ETA 44.66s] step time: 0.005486s (±0.001861s); valid time: 0.01253s; loss: -13.655 (±1.53072); valid loss: 6.39666\n",
      "[Epoch 242/350, Step 16200, ETA 44.04s] step time: 0.005302s (±0.001504s); valid time: 0.01273s; loss: -13.66 (±1.49662); valid loss: 6.40726\n",
      "[Epoch 244/350, Step 16300, ETA 43.43s] step time: 0.005519s (±0.001918s); valid time: 0.01262s; loss: -13.8594 (±1.36752); valid loss: 6.34851\n",
      "[Epoch 245/350, Step 16400, ETA 42.81s] step time: 0.005338s (±0.001408s); valid time: 0.01257s; loss: -13.8078 (±1.4427); valid loss: 6.37303\n",
      "[Epoch 247/350, Step 16500, ETA 42.2s] step time: 0.005424s (±0.001658s); valid time: 0.01241s; loss: -13.7917 (±1.51612); valid loss: 6.39363\n",
      "[Epoch 248/350, Step 16600, ETA 41.58s] step time: 0.005405s (±0.001809s); valid time: 0.01219s; loss: -13.653 (±1.80387); valid loss: 6.40172\n",
      "[Epoch 250/350, Step 16700, ETA 40.97s] step time: 0.005463s (±0.001733s); valid time: 0.01285s; loss: -13.6329 (±1.4854); valid loss: 6.42229\n",
      "[Epoch 250/350, Step 16750, ETA 40.66s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 251/350, Step 16800, ETA 40.36s] step time: 0.005677s (±0.001813s); valid time: 0.01282s; loss: -13.6651 (±1.43631); valid loss: 6.4023\n",
      "[Epoch 253/350, Step 16900, ETA 39.76s] step time: 0.0055s (±0.001518s); valid time: 0.01224s; loss: -13.7018 (±1.57168); valid loss: 6.38006\n",
      "[Epoch 254/350, Step 17000, ETA 39.14s] step time: 0.005559s (±0.001648s); valid time: 0.0123s; loss: -13.802 (±1.57932); valid loss: 6.3849\n",
      "[Epoch 256/350, Step 17100, ETA 38.53s] step time: 0.00533s (±0.001589s); valid time: 0.01235s; loss: -13.6973 (±1.55782); valid loss: 6.39666\n",
      "[Epoch 257/350, Step 17200, ETA 37.92s] step time: 0.005555s (±0.001748s); valid time: 0.01308s; loss: -13.6487 (±1.78687); valid loss: 6.40178\n",
      "[Epoch 259/350, Step 17300, ETA 37.31s] step time: 0.005374s (±0.00147s); valid time: 0.01216s; loss: -13.6718 (±1.43743); valid loss: 6.40617\n",
      "[Epoch 260/350, Step 17400, ETA 36.7s] step time: 0.005516s (±0.001573s); valid time: 0.01221s; loss: -13.8391 (±1.60279); valid loss: 6.39829\n",
      "[Epoch 260/350, Step 17420, ETA 36.57s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 262/350, Step 17500, ETA 36.1s] step time: 0.005582s (±0.001672s); valid time: 0.01263s; loss: -13.7272 (±1.53143); valid loss: 6.38052\n",
      "[Epoch 263/350, Step 17600, ETA 35.48s] step time: 0.005351s (±0.001571s); valid time: 0.01295s; loss: -13.6228 (±1.27154); valid loss: 6.3946\n",
      "[Epoch 265/350, Step 17700, ETA 34.88s] step time: 0.005769s (±0.001936s); valid time: 0.01407s; loss: -13.7394 (±1.52107); valid loss: 6.36653\n",
      "[Epoch 266/350, Step 17800, ETA 34.27s] step time: 0.005573s (±0.001754s); valid time: 0.01312s; loss: -13.7929 (±1.39397); valid loss: 6.37229\n",
      "[Epoch 268/350, Step 17900, ETA 33.67s] step time: 0.005497s (±0.001685s); valid time: 0.01292s; loss: -13.7284 (±1.5674); valid loss: 6.37137\n",
      "[Epoch 269/350, Step 18000, ETA 33.06s] step time: 0.005566s (±0.001787s); valid time: 0.01235s; loss: -13.5081 (±1.60409); valid loss: 6.39738\n",
      "[Epoch 270/350, Step 18090, ETA 32.51s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 271/350, Step 18100, ETA 32.45s] step time: 0.005483s (±0.002114s); valid time: 0.01244s; loss: -13.9056 (±1.67956); valid loss: 6.38682\n",
      "[Epoch 272/350, Step 18200, ETA 31.84s] step time: 0.005591s (±0.001742s); valid time: 0.01235s; loss: -13.657 (±1.37764); valid loss: 6.37276\n",
      "[Epoch 274/350, Step 18300, ETA 31.24s] step time: 0.005569s (±0.001823s); valid time: 0.01262s; loss: -13.7952 (±1.56921); valid loss: 6.42497\n",
      "[Epoch 275/350, Step 18400, ETA 30.62s] step time: 0.005275s (±0.001588s); valid time: 0.01285s; loss: -13.738 (±1.61449); valid loss: 6.38675\n",
      "[Epoch 277/350, Step 18500, ETA 30.02s] step time: 0.005636s (±0.001881s); valid time: 0.01364s; loss: -13.7275 (±1.4598); valid loss: 6.38429\n",
      "[Epoch 278/350, Step 18600, ETA 29.4s] step time: 0.005395s (±0.001839s); valid time: 0.0144s; loss: -13.7455 (±1.47808); valid loss: 6.3791\n",
      "[Epoch 280/350, Step 18700, ETA 28.8s] step time: 0.005721s (±0.001668s); valid time: 0.01214s; loss: -13.6871 (±1.56352); valid loss: 6.37721\n",
      "[Epoch 280/350, Step 18760, ETA 28.43s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 18800, ETA 28.19s] step time: 0.005528s (±0.001692s); valid time: 0.01228s; loss: -13.6932 (±1.32928); valid loss: 6.38117\n",
      "[Epoch 283/350, Step 18900, ETA 27.59s] step time: 0.005614s (±0.001879s); valid time: 0.01337s; loss: -13.6981 (±1.60308); valid loss: 6.3715\n",
      "[Epoch 284/350, Step 19000, ETA 26.98s] step time: 0.005658s (±0.0019s); valid time: 0.01251s; loss: -13.9269 (±1.5979); valid loss: 6.4076\n",
      "[Epoch 286/350, Step 19100, ETA 26.38s] step time: 0.005506s (±0.001506s); valid time: 0.01229s; loss: -13.4967 (±1.51091); valid loss: 6.38431\n",
      "[Epoch 287/350, Step 19200, ETA 25.77s] step time: 0.00564s (±0.001817s); valid time: 0.01277s; loss: -13.8106 (±1.74607); valid loss: 6.36585\n",
      "[Epoch 289/350, Step 19300, ETA 25.16s] step time: 0.005315s (±0.001549s); valid time: 0.01166s; loss: -13.6886 (±1.31483); valid loss: 6.39381\n",
      "[Epoch 290/350, Step 19400, ETA 24.55s] step time: 0.005465s (±0.001743s); valid time: 0.01209s; loss: -13.6586 (±1.6357); valid loss: 6.38289\n",
      "[Epoch 290/350, Step 19430, ETA 24.37s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 19500, ETA 23.95s] step time: 0.005596s (±0.00184s); valid time: 0.01261s; loss: -13.7496 (±1.50626); valid loss: 6.37416\n",
      "[Epoch 293/350, Step 19600, ETA 23.34s] step time: 0.005413s (±0.001638s); valid time: 0.01269s; loss: -13.749 (±1.45341); valid loss: 6.38938\n",
      "[Epoch 295/350, Step 19700, ETA 22.73s] step time: 0.005658s (±0.001874s); valid time: 0.01287s; loss: -13.7352 (±1.29133); valid loss: 6.40828\n",
      "[Epoch 296/350, Step 19800, ETA 22.12s] step time: 0.005336s (±0.001444s); valid time: 0.01255s; loss: -13.7498 (±1.3914); valid loss: 6.38422\n",
      "[Epoch 298/350, Step 19900, ETA 21.52s] step time: 0.005781s (±0.001985s); valid time: 0.01398s; loss: -13.7415 (±1.48211); valid loss: 6.38703\n",
      "[Epoch 299/350, Step 20000, ETA 20.91s] step time: 0.005609s (±0.001818s); valid time: 0.01229s; loss: -13.6997 (±1.56067); valid loss: 6.40309\n",
      "[Epoch 300/350, Step 20100, ETA 20.3s] step time: 0.005302s (±0.001452s); valid time: 0.01264s; loss: -13.7768 (±1.59911); valid loss: 6.37736\n",
      "[Epoch 300/350, Step 20100, ETA 20.3s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 302/350, Step 20200, ETA 19.7s] step time: 0.005621s (±0.001666s); valid time: 0.01209s; loss: -13.6818 (±1.45687); valid loss: 6.40787\n",
      "[Epoch 303/350, Step 20300, ETA 19.09s] step time: 0.005313s (±0.00163s); valid time: 0.01186s; loss: -13.8155 (±1.52807); valid loss: 6.41299\n",
      "[Epoch 305/350, Step 20400, ETA 18.48s] step time: 0.00548s (±0.001747s); valid time: 0.01323s; loss: -13.7643 (±1.62357); valid loss: 6.38301\n",
      "[Epoch 306/350, Step 20500, ETA 17.87s] step time: 0.005303s (±0.001292s); valid time: 0.01231s; loss: -13.644 (±1.52005); valid loss: 6.36181\n",
      "[Epoch 308/350, Step 20600, ETA 17.26s] step time: 0.005424s (±0.001542s); valid time: 0.01204s; loss: -13.7266 (±1.53729); valid loss: 6.37712\n",
      "[Epoch 309/350, Step 20700, ETA 16.66s] step time: 0.005381s (±0.001551s); valid time: 0.01367s; loss: -13.6893 (±1.42934); valid loss: 6.37368\n",
      "[Epoch 310/350, Step 20770, ETA 16.23s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 20800, ETA 16.05s] step time: 0.005423s (±0.001486s); valid time: 0.01224s; loss: -13.6951 (±1.53301); valid loss: 6.38574\n",
      "[Epoch 312/350, Step 20900, ETA 15.44s] step time: 0.005477s (±0.001817s); valid time: 0.01204s; loss: -13.7585 (±1.66325); valid loss: 6.39223\n",
      "[Epoch 314/350, Step 21000, ETA 14.83s] step time: 0.005218s (±0.001348s); valid time: 0.01212s; loss: -13.757 (±1.37336); valid loss: 6.3759\n",
      "[Epoch 315/350, Step 21100, ETA 14.23s] step time: 0.005472s (±0.001814s); valid time: 0.01253s; loss: -13.7197 (±1.62345); valid loss: 6.37358\n",
      "[Epoch 317/350, Step 21200, ETA 13.62s] step time: 0.005501s (±0.001963s); valid time: 0.01248s; loss: -13.6533 (±1.49796); valid loss: 6.39671\n",
      "[Epoch 318/350, Step 21300, ETA 13.01s] step time: 0.005257s (±0.001508s); valid time: 0.01197s; loss: -13.7955 (±1.59988); valid loss: 6.38172\n",
      "[Epoch 320/350, Step 21400, ETA 12.41s] step time: 0.005629s (±0.001865s); valid time: 0.0126s; loss: -13.7387 (±1.47515); valid loss: 6.3638\n",
      "[Epoch 320/350, Step 21440, ETA 12.16s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 321/350, Step 21500, ETA 11.8s] step time: 0.005409s (±0.001702s); valid time: 0.01291s; loss: -13.6806 (±1.26674); valid loss: 6.37982\n",
      "[Epoch 323/350, Step 21600, ETA 11.2s] step time: 0.005561s (±0.001813s); valid time: 0.01242s; loss: -13.6994 (±1.72454); valid loss: 6.36611\n",
      "[Epoch 324/350, Step 21700, ETA 10.59s] step time: 0.005499s (±0.001527s); valid time: 0.01255s; loss: -13.7181 (±1.69341); valid loss: 6.41013\n",
      "[Epoch 326/350, Step 21800, ETA 9.986s] step time: 0.005762s (±0.002034s); valid time: 0.0125s; loss: -13.6387 (±1.47078); valid loss: 6.3716\n",
      "[Epoch 327/350, Step 21900, ETA 9.38s] step time: 0.005499s (±0.001911s); valid time: 0.01237s; loss: -13.8229 (±1.50473); valid loss: 6.40036\n",
      "[Epoch 329/350, Step 22000, ETA 8.776s] step time: 0.005594s (±0.001809s); valid time: 0.01305s; loss: -13.7857 (±1.54778); valid loss: 6.35173\n",
      "[Epoch 330/350, Step 22100, ETA 8.169s] step time: 0.005397s (±0.001557s); valid time: 0.01232s; loss: -13.7735 (±1.59774); valid loss: 6.38934\n",
      "[Epoch 330/350, Step 22110, ETA 8.108s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 22200, ETA 7.564s] step time: 0.00544s (±0.001462s); valid time: 0.01241s; loss: -13.6482 (±1.57928); valid loss: 6.38563\n",
      "[Epoch 333/350, Step 22300, ETA 6.958s] step time: 0.005488s (±0.001548s); valid time: 0.01242s; loss: -13.7592 (±1.58421); valid loss: 6.44336\n",
      "[Epoch 335/350, Step 22400, ETA 6.354s] step time: 0.005758s (±0.001974s); valid time: 0.01311s; loss: -13.5733 (±1.62314); valid loss: 6.37359\n",
      "[Epoch 336/350, Step 22500, ETA 5.748s] step time: 0.005457s (±0.001679s); valid time: 0.01271s; loss: -13.7394 (±1.34233); valid loss: 6.41003\n",
      "[Epoch 338/350, Step 22600, ETA 5.144s] step time: 0.005538s (±0.001796s); valid time: 0.01274s; loss: -13.7972 (±1.74619); valid loss: 6.38779\n",
      "[Epoch 339/350, Step 22700, ETA 4.538s] step time: 0.005369s (±0.00163s); valid time: 0.01279s; loss: -13.6259 (±1.59039); valid loss: 6.37212\n",
      "[Epoch 340/350, Step 22780, ETA 4.053s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 341/350, Step 22800, ETA 3.933s] step time: 0.005559s (±0.001858s); valid time: 0.01261s; loss: -13.7482 (±1.63814); valid loss: 6.36274\n",
      "[Epoch 342/350, Step 22900, ETA 3.327s] step time: 0.005544s (±0.001699s); valid time: 0.01181s; loss: -13.8256 (±1.69141); valid loss: 6.38633\n",
      "[Epoch 344/350, Step 23000, ETA 2.723s] step time: 0.00553s (±0.001637s); valid time: 0.0125s; loss: -13.8502 (±1.50705); valid loss: 6.38016\n",
      "[Epoch 345/350, Step 23100, ETA 2.118s] step time: 0.006117s (±0.002564s); valid time: 0.01255s; loss: -13.6087 (±1.47344); valid loss: 6.37638\n",
      "[Epoch 347/350, Step 23200, ETA 1.513s] step time: 0.005548s (±0.00196s); valid time: 0.01268s; loss: -13.8487 (±1.5756); valid loss: 6.35548\n",
      "[Epoch 348/350, Step 23300, ETA 0.9078s] step time: 0.005541s (±0.00172s); valid time: 0.01294s; loss: -13.6309 (±1.61549); valid loss: 6.37231\n",
      "[Epoch 350/350, Step 23400, ETA 0.3027s] step time: 0.005767s (±0.00303s); valid time: 0.02738s; loss: -13.7074 (±1.39665); valid loss: 6.34761\n",
      "[Epoch 350/350, Step 23450, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpc48c59i1/variables.dat-13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpc48c59i1/variables.dat-13000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.68,\n",
      "\t(tp, fp, tn, fn)=(98, 667, 1594, 141),\n",
      "\tprecision=0.13,\n",
      "\trecall=0.41,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.56,\n",
      "\ty_pred%=0.306,\n",
      "\ty_label%=0.0956,\n",
      ")\n",
      "Testing on realTraffic/speed_7578.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 18.57s] step time: 0.009965s (±0.03374s); valid time: 0.168s; loss: -5.27589 (±8.82287); valid loss: -1.10431 (*)\n",
      "[Epoch 6/350, Step 200, ETA 1m 53.29s] step time: 0.006264s (±0.008847s); valid time: 0.08839s; loss: -9.62327 (±0.923219); valid loss: -1.27753 (*)\n",
      "[Epoch 8/350, Step 300, ETA 1m 43.65s] step time: 0.006204s (±0.008908s); valid time: 0.08868s; loss: -10.238 (±0.950539); valid loss: -1.61331 (*)\n",
      "[Epoch 10/350, Step 380, ETA 1m 37.2s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 400, ETA 1m 38.57s] step time: 0.006125s (±0.008049s); valid time: 0.08011s; loss: -10.7749 (±0.910393); valid loss: -1.6353 (*)\n",
      "[Epoch 14/350, Step 500, ETA 1m 35.47s] step time: 0.00622s (±0.009169s); valid time: 0.0919s; loss: -11.0666 (±0.926138); valid loss: -1.70803 (*)\n",
      "[Epoch 16/350, Step 600, ETA 1m 32.95s] step time: 0.0062s (±0.008353s); valid time: 0.08327s; loss: -11.3112 (±0.955764); valid loss: -1.8583 (*)\n",
      "[Epoch 19/350, Step 700, ETA 1m 31.11s] step time: 0.006192s (±0.00848s); valid time: 0.08443s; loss: -11.4663 (±1.04238); valid loss: -2.02224 (*)\n",
      "[Epoch 20/350, Step 760, ETA 1m 29.21s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 800, ETA 1m 29.76s] step time: 0.006305s (±0.009345s); valid time: 0.09243s; loss: -11.7182 (±0.999863); valid loss: -2.1633 (*)\n",
      "[Epoch 24/350, Step 900, ETA 1m 28.12s] step time: 0.006072s (±0.008085s); valid time: 0.08091s; loss: -11.8145 (±0.941246); valid loss: -2.20499 (*)\n",
      "[Epoch 27/350, Step 1000, ETA 1m 27.05s] step time: 0.006296s (±0.008502s); valid time: 0.08355s; loss: -11.9457 (±0.946291); valid loss: -2.21464 (*)\n",
      "[Epoch 29/350, Step 1100, ETA 1m 25.94s] step time: 0.006296s (±0.008765s); valid time: 0.08754s; loss: -12.0618 (±0.998397); valid loss: -2.34534 (*)\n",
      "[Epoch 30/350, Step 1140, ETA 1m 25.24s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 1200, ETA 1m 24.27s] step time: 0.005434s (±0.001183s); valid time: 0.007976s; loss: -12.1111 (±1.04913); valid loss: -2.34456\n",
      "[Epoch 35/350, Step 1300, ETA 1m 23.46s] step time: 0.006322s (±0.008433s); valid time: 0.08358s; loss: -12.2219 (±0.960549); valid loss: -2.37024 (*)\n",
      "[Epoch 37/350, Step 1400, ETA 1m 22.38s] step time: 0.006081s (±0.008892s); valid time: 0.08887s; loss: -12.2513 (±1.03703); valid loss: -2.44382 (*)\n",
      "[Epoch 40/350, Step 1500, ETA 1m 20.95s] step time: 0.005478s (±0.001545s); valid time: 0.007863s; loss: -12.3747 (±1.07382); valid loss: -2.41801\n",
      "[Epoch 40/350, Step 1520, ETA 1m 20.58s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 43/350, Step 1600, ETA 1m 20.29s] step time: 0.006354s (±0.008259s); valid time: 0.08248s; loss: -12.4709 (±1.14339); valid loss: -2.45685 (*)\n",
      "[Epoch 45/350, Step 1700, ETA 1m 18.98s] step time: 0.00553s (±0.001193s); valid time: 0.008213s; loss: -12.4796 (±0.954782); valid loss: -2.44385\n",
      "[Epoch 48/350, Step 1800, ETA 1m 18.3s] step time: 0.006296s (±0.008571s); valid time: 0.08591s; loss: -12.5762 (±1.08584); valid loss: -2.46628 (*)\n",
      "[Epoch 50/350, Step 1900, ETA 1m 17.59s] step time: 0.006266s (±0.009249s); valid time: 0.09187s; loss: -12.5907 (±1.06046); valid loss: -2.5337 (*)\n",
      "[Epoch 50/350, Step 1900, ETA 1m 17.59s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2000, ETA 1m 16.47s] step time: 0.0055s (±0.001069s); valid time: 0.00764s; loss: -12.6565 (±1.04993); valid loss: -2.51478\n",
      "[Epoch 56/350, Step 2100, ETA 1m 15.41s] step time: 0.005494s (±0.001424s); valid time: 0.008171s; loss: -12.6809 (±1.00018); valid loss: -2.48996\n",
      "[Epoch 58/350, Step 2200, ETA 1m 14.69s] step time: 0.006223s (±0.008435s); valid time: 0.08459s; loss: -12.7452 (±1.0506); valid loss: -2.54466 (*)\n",
      "[Epoch 60/350, Step 2280, ETA 1m 13.89s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2300, ETA 1m 13.76s] step time: 0.005674s (±0.001647s); valid time: 0.008276s; loss: -12.8439 (±1.14383); valid loss: -2.52991\n",
      "[Epoch 64/350, Step 2400, ETA 1m 13.15s] step time: 0.006323s (±0.008928s); valid time: 0.08871s; loss: -12.8623 (±1.12647); valid loss: -2.56902 (*)\n",
      "[Epoch 66/350, Step 2500, ETA 1m 12.16s] step time: 0.005546s (±0.001284s); valid time: 0.008083s; loss: -12.8727 (±1.01427); valid loss: -2.56115\n",
      "[Epoch 69/350, Step 2600, ETA 1m 11.52s] step time: 0.006227s (±0.008217s); valid time: 0.08179s; loss: -12.9883 (±1.01651); valid loss: -2.58034 (*)\n",
      "[Epoch 70/350, Step 2660, ETA 1m 10.99s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 2700, ETA 1m 11.04s] step time: 0.006625s (±0.008337s); valid time: 0.08155s; loss: -12.8772 (±1.0312); valid loss: -2.59745 (*)\n",
      "[Epoch 74/350, Step 2800, ETA 1m 10.32s] step time: 0.006078s (±0.008328s); valid time: 0.08336s; loss: -13.0356 (±1.11938); valid loss: -2.61464 (*)\n",
      "[Epoch 77/350, Step 2900, ETA 1m 9.38s] step time: 0.005419s (±0.001455s); valid time: 0.008194s; loss: -12.9786 (±1.13021); valid loss: -2.61401\n",
      "[Epoch 79/350, Step 3000, ETA 1m 8.705s] step time: 0.006231s (±0.009466s); valid time: 0.09445s; loss: -13.0599 (±1.13123); valid loss: -2.6454 (*)\n",
      "[Epoch 80/350, Step 3040, ETA 1m 8.339s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 82/350, Step 3100, ETA 1m 8.14s] step time: 0.006447s (±0.00949s); valid time: 0.09385s; loss: -13.0628 (±1.04362); valid loss: -2.66177 (*)\n",
      "[Epoch 85/350, Step 3200, ETA 1m 7.304s] step time: 0.005622s (±0.001566s); valid time: 0.007948s; loss: -13.0805 (±1.06423); valid loss: -2.66165\n",
      "[Epoch 87/350, Step 3300, ETA 1m 6.434s] step time: 0.005547s (±0.001251s); valid time: 0.007869s; loss: -13.0742 (±1.06666); valid loss: -2.62477\n",
      "[Epoch 90/350, Step 3400, ETA 1m 5.898s] step time: 0.006515s (±0.008954s); valid time: 0.08719s; loss: -13.1221 (±1.27409); valid loss: -2.70467 (*)\n",
      "[Epoch 90/350, Step 3420, ETA 1m 5.702s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 93/350, Step 3500, ETA 1m 5.28s] step time: 0.006307s (±0.008613s); valid time: 0.08572s; loss: -13.1586 (±1.24121); valid loss: -2.74293 (*)\n",
      "[Epoch 95/350, Step 3600, ETA 1m 4.367s] step time: 0.005308s (±0.001299s); valid time: 0.008041s; loss: -13.1649 (±1.12936); valid loss: -2.64169\n",
      "[Epoch 98/350, Step 3700, ETA 1m 3.596s] step time: 0.005691s (±0.001843s); valid time: 0.008032s; loss: -13.1248 (±1.09713); valid loss: -2.66286\n",
      "[Epoch 100/350, Step 3800, ETA 1m 2.766s] step time: 0.005505s (±0.002505s); valid time: 0.01398s; loss: -13.171 (±0.982568); valid loss: -2.66503\n",
      "[Epoch 100/350, Step 3800, ETA 1m 2.766s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 3900, ETA 1m 1.989s] step time: 0.005601s (±0.001193s); valid time: 0.008527s; loss: -13.1855 (±1.00277); valid loss: -2.68876\n",
      "[Epoch 106/350, Step 4000, ETA 1m 1.199s] step time: 0.005505s (±0.001733s); valid time: 0.00791s; loss: -13.2678 (±1.0789); valid loss: -2.66161\n",
      "[Epoch 108/350, Step 4100, ETA 1m 0.3388s] step time: 0.005265s (±0.001199s); valid time: 0.007768s; loss: -13.205 (±1.02971); valid loss: -2.70341\n",
      "[Epoch 110/350, Step 4180, ETA 59.67s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 4200, ETA 59.53s] step time: 0.005344s (±0.001371s); valid time: 0.008193s; loss: -13.2298 (±0.979761); valid loss: -2.7225\n",
      "[Epoch 114/350, Step 4300, ETA 58.81s] step time: 0.005661s (±0.001922s); valid time: 0.009288s; loss: -13.2452 (±1.14424); valid loss: -2.69693\n",
      "[Epoch 116/350, Step 4400, ETA 58.03s] step time: 0.005499s (±0.001617s); valid time: 0.007825s; loss: -13.2842 (±0.856456); valid loss: -2.70074\n",
      "[Epoch 119/350, Step 4500, ETA 57.26s] step time: 0.005389s (±0.001369s); valid time: 0.008029s; loss: -13.2575 (±1.06449); valid loss: -2.71738\n",
      "[Epoch 120/350, Step 4560, ETA 56.78s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 4600, ETA 56.51s] step time: 0.005459s (±0.001466s); valid time: 0.008298s; loss: -13.2894 (±1.07261); valid loss: -2.68558\n",
      "[Epoch 124/350, Step 4700, ETA 55.79s] step time: 0.005676s (±0.00152s); valid time: 0.008348s; loss: -13.294 (±1.15205); valid loss: -2.68427\n",
      "[Epoch 127/350, Step 4800, ETA 55.04s] step time: 0.00539s (±0.001324s); valid time: 0.007778s; loss: -13.2559 (±1.11793); valid loss: -2.71936\n",
      "[Epoch 129/350, Step 4900, ETA 54.27s] step time: 0.005367s (±0.001753s); valid time: 0.008203s; loss: -13.3475 (±1.02296); valid loss: -2.73943\n",
      "[Epoch 130/350, Step 4940, ETA 53.97s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 5000, ETA 53.56s] step time: 0.005554s (±0.001708s); valid time: 0.008696s; loss: -13.2902 (±1.03738); valid loss: -2.74107\n",
      "[Epoch 135/350, Step 5100, ETA 52.82s] step time: 0.005378s (±0.001242s); valid time: 0.00786s; loss: -13.3261 (±1.11196); valid loss: -2.72615\n",
      "[Epoch 137/350, Step 5200, ETA 52.1s] step time: 0.005517s (±0.00169s); valid time: 0.007977s; loss: -13.3434 (±1.04349); valid loss: -2.70082\n",
      "[Epoch 140/350, Step 5300, ETA 51.36s] step time: 0.005284s (±0.001296s); valid time: 0.008317s; loss: -13.373 (±1.05161); valid loss: -2.70276\n",
      "[Epoch 140/350, Step 5320, ETA 51.21s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 143/350, Step 5400, ETA 50.67s] step time: 0.005522s (±0.00174s); valid time: 0.008278s; loss: -13.2818 (±1.03539); valid loss: -2.68372\n",
      "[Epoch 145/350, Step 5500, ETA 49.93s] step time: 0.005341s (±0.00146s); valid time: 0.008019s; loss: -13.3287 (±1.11811); valid loss: -2.71799\n",
      "[Epoch 148/350, Step 5600, ETA 49.35s] step time: 0.006264s (±0.008634s); valid time: 0.0849s; loss: -13.3144 (±1.11526); valid loss: -2.77889 (*)\n",
      "[Epoch 150/350, Step 5700, ETA 48.65s] step time: 0.005544s (±0.001309s); valid time: 0.008109s; loss: -13.3796 (±1.12922); valid loss: -2.74143\n",
      "[Epoch 150/350, Step 5700, ETA 48.65s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 5800, ETA 47.94s] step time: 0.005348s (±0.001435s); valid time: 0.008605s; loss: -13.3382 (±1.09688); valid loss: -2.73531\n",
      "[Epoch 156/350, Step 5900, ETA 47.24s] step time: 0.005421s (±0.001448s); valid time: 0.008229s; loss: -13.3483 (±1.11654); valid loss: -2.75186\n",
      "[Epoch 158/350, Step 6000, ETA 46.53s] step time: 0.005379s (±0.001505s); valid time: 0.007788s; loss: -13.3733 (±1.11759); valid loss: -2.70792\n",
      "[Epoch 160/350, Step 6080, ETA 45.98s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 6100, ETA 45.86s] step time: 0.005538s (±0.001678s); valid time: 0.008334s; loss: -13.2832 (±1.17499); valid loss: -2.71391\n",
      "[Epoch 164/350, Step 6200, ETA 45.18s] step time: 0.005489s (±0.001419s); valid time: 0.007968s; loss: -13.4613 (±1.14945); valid loss: -2.75555\n",
      "[Epoch 166/350, Step 6300, ETA 44.47s] step time: 0.005304s (±0.001235s); valid time: 0.007949s; loss: -13.327 (±1.15156); valid loss: -2.746\n",
      "[Epoch 169/350, Step 6400, ETA 43.81s] step time: 0.005582s (±0.001405s); valid time: 0.008206s; loss: -13.4554 (±1.15292); valid loss: -2.77287\n",
      "[Epoch 170/350, Step 6460, ETA 43.39s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 6500, ETA 43.13s] step time: 0.005416s (±0.001413s); valid time: 0.008299s; loss: -13.3051 (±1.12959); valid loss: -2.74636\n",
      "[Epoch 174/350, Step 6600, ETA 42.45s] step time: 0.00549s (±0.0015s); valid time: 0.007631s; loss: -13.3994 (±1.22043); valid loss: -2.73959\n",
      "[Epoch 177/350, Step 6700, ETA 41.78s] step time: 0.005461s (±0.001813s); valid time: 0.01006s; loss: -13.4063 (±1.10964); valid loss: -2.76802\n",
      "[Epoch 179/350, Step 6800, ETA 41.09s] step time: 0.005282s (±0.001239s); valid time: 0.008159s; loss: -13.3933 (±1.12462); valid loss: -2.73728\n",
      "[Epoch 180/350, Step 6840, ETA 40.82s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 182/350, Step 6900, ETA 40.43s] step time: 0.005486s (±0.001564s); valid time: 0.008411s; loss: -13.3327 (±1.04588); valid loss: -2.75851\n",
      "[Epoch 185/350, Step 7000, ETA 39.75s] step time: 0.005347s (±0.00124s); valid time: 0.008161s; loss: -13.3702 (±1.09614); valid loss: -2.75513\n",
      "[Epoch 187/350, Step 7100, ETA 39.09s] step time: 0.005503s (±0.001269s); valid time: 0.007782s; loss: -13.4207 (±1.0532); valid loss: -2.76527\n",
      "[Epoch 190/350, Step 7200, ETA 38.42s] step time: 0.005353s (±0.001235s); valid time: 0.008773s; loss: -13.3638 (±1.09958); valid loss: -2.76214\n",
      "[Epoch 190/350, Step 7220, ETA 38.3s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 193/350, Step 7300, ETA 37.77s] step time: 0.005478s (±0.001594s); valid time: 0.01078s; loss: -13.3808 (±1.24795); valid loss: -2.76166\n",
      "[Epoch 195/350, Step 7400, ETA 37.11s] step time: 0.005487s (±0.001362s); valid time: 0.008013s; loss: -13.3641 (±1.09616); valid loss: -2.74563\n",
      "[Epoch 198/350, Step 7500, ETA 36.45s] step time: 0.005404s (±0.001281s); valid time: 0.007879s; loss: -13.3689 (±0.974423); valid loss: -2.7393\n",
      "[Epoch 200/350, Step 7600, ETA 35.8s] step time: 0.005568s (±0.001531s); valid time: 0.00788s; loss: -13.4059 (±1.09488); valid loss: -2.69825\n",
      "[Epoch 200/350, Step 7600, ETA 35.8s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 7700, ETA 35.2s] step time: 0.006142s (±0.00853s); valid time: 0.08472s; loss: -13.3908 (±1.02967); valid loss: -2.77986 (*)\n",
      "[Epoch 206/350, Step 7800, ETA 34.56s] step time: 0.00552s (±0.001658s); valid time: 0.007456s; loss: -13.4174 (±1.14709); valid loss: -2.75167\n",
      "[Epoch 208/350, Step 7900, ETA 33.91s] step time: 0.00556s (±0.001614s); valid time: 0.007687s; loss: -13.3964 (±1.09057); valid loss: -2.74901\n",
      "[Epoch 210/350, Step 7980, ETA 33.38s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 8000, ETA 33.26s] step time: 0.005411s (±0.001255s); valid time: 0.008132s; loss: -13.4108 (±1.04471); valid loss: -2.71978\n",
      "[Epoch 214/350, Step 8100, ETA 32.63s] step time: 0.005691s (±0.00165s); valid time: 0.008324s; loss: -13.3776 (±1.26059); valid loss: -2.76675\n",
      "[Epoch 216/350, Step 8200, ETA 31.96s] step time: 0.005214s (±0.001134s); valid time: 0.008077s; loss: -13.3701 (±1.21812); valid loss: -2.74084\n",
      "[Epoch 219/350, Step 8300, ETA 31.32s] step time: 0.00549s (±0.00159s); valid time: 0.01098s; loss: -13.4041 (±1.10266); valid loss: -2.71705\n",
      "[Epoch 220/350, Step 8360, ETA 30.93s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 8400, ETA 30.7s] step time: 0.005853s (±0.001848s); valid time: 0.007596s; loss: -13.3724 (±1.22443); valid loss: -2.76603\n",
      "[Epoch 224/350, Step 8500, ETA 30.05s] step time: 0.005331s (±0.0009915s); valid time: 0.007654s; loss: -13.4098 (±0.958792); valid loss: -2.73493\n",
      "[Epoch 227/350, Step 8600, ETA 29.41s] step time: 0.005499s (±0.001609s); valid time: 0.007975s; loss: -13.3883 (±1.13134); valid loss: -2.7427\n",
      "[Epoch 229/350, Step 8700, ETA 28.76s] step time: 0.005446s (±0.001117s); valid time: 0.008075s; loss: -13.3712 (±0.935418); valid loss: -2.74309\n",
      "[Epoch 230/350, Step 8740, ETA 28.51s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 8800, ETA 28.13s] step time: 0.005523s (±0.001645s); valid time: 0.007723s; loss: -13.4178 (±1.16892); valid loss: -2.7429\n",
      "[Epoch 235/350, Step 8900, ETA 27.49s] step time: 0.005465s (±0.001204s); valid time: 0.008204s; loss: -13.3125 (±1.16585); valid loss: -2.75015\n",
      "[Epoch 237/350, Step 9000, ETA 26.84s] step time: 0.005384s (±0.001294s); valid time: 0.007772s; loss: -13.5022 (±1.16051); valid loss: -2.7586\n",
      "[Epoch 240/350, Step 9100, ETA 26.22s] step time: 0.005739s (±0.001435s); valid time: 0.008074s; loss: -13.3943 (±1.15559); valid loss: -2.72928\n",
      "[Epoch 240/350, Step 9120, ETA 26.09s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 243/350, Step 9200, ETA 25.58s] step time: 0.005268s (±0.001236s); valid time: 0.008085s; loss: -13.3436 (±1.13802); valid loss: -2.74347\n",
      "[Epoch 245/350, Step 9300, ETA 24.94s] step time: 0.005409s (±0.001596s); valid time: 0.00807s; loss: -13.3908 (±1.26438); valid loss: -2.72492\n",
      "[Epoch 248/350, Step 9400, ETA 24.31s] step time: 0.005536s (±0.001451s); valid time: 0.007845s; loss: -13.4099 (±1.11518); valid loss: -2.72314\n",
      "[Epoch 250/350, Step 9500, ETA 23.67s] step time: 0.005606s (±0.002005s); valid time: 0.007726s; loss: -13.4062 (±1.28917); valid loss: -2.75394\n",
      "[Epoch 250/350, Step 9500, ETA 23.67s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 9600, ETA 23.04s] step time: 0.005407s (±0.001709s); valid time: 0.01073s; loss: -13.3736 (±1.12423); valid loss: -2.76633\n",
      "[Epoch 256/350, Step 9700, ETA 22.4s] step time: 0.00535s (±0.001591s); valid time: 0.007782s; loss: -13.3811 (±1.0952); valid loss: -2.76917\n",
      "[Epoch 258/350, Step 9800, ETA 21.76s] step time: 0.005393s (±0.001292s); valid time: 0.007454s; loss: -13.4127 (±1.11158); valid loss: -2.73309\n",
      "[Epoch 260/350, Step 9880, ETA 21.25s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 9900, ETA 21.16s] step time: 0.006065s (±0.008396s); valid time: 0.08434s; loss: -13.4681 (±1.18486); valid loss: -2.78109 (*)\n",
      "[Epoch 264/350, Step 10000, ETA 20.53s] step time: 0.005731s (±0.002002s); valid time: 0.008547s; loss: -13.3382 (±1.13849); valid loss: -2.75706\n",
      "[Epoch 266/350, Step 10100, ETA 19.9s] step time: 0.00544s (±0.001486s); valid time: 0.008115s; loss: -13.3647 (±1.13169); valid loss: -2.74628\n",
      "[Epoch 269/350, Step 10200, ETA 19.27s] step time: 0.005384s (±0.001537s); valid time: 0.007902s; loss: -13.3745 (±1.05951); valid loss: -2.77329\n",
      "[Epoch 270/350, Step 10260, ETA 18.89s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 10300, ETA 18.65s] step time: 0.005703s (±0.001714s); valid time: 0.008308s; loss: -13.4498 (±1.21758); valid loss: -2.76055\n",
      "[Epoch 274/350, Step 10400, ETA 18.04s] step time: 0.006215s (±0.008294s); valid time: 0.08335s; loss: -13.3812 (±1.20324); valid loss: -2.80677 (*)\n",
      "[Epoch 277/350, Step 10500, ETA 17.41s] step time: 0.005513s (±0.001629s); valid time: 0.007947s; loss: -13.4615 (±1.19602); valid loss: -2.76596\n",
      "[Epoch 279/350, Step 10600, ETA 16.78s] step time: 0.005575s (±0.001819s); valid time: 0.007795s; loss: -13.3496 (±1.20235); valid loss: -2.7321\n",
      "[Epoch 280/350, Step 10640, ETA 16.53s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 282/350, Step 10700, ETA 16.16s] step time: 0.005395s (±0.001528s); valid time: 0.007843s; loss: -13.3954 (±1.13016); valid loss: -2.74104\n",
      "[Epoch 285/350, Step 10800, ETA 15.53s] step time: 0.005538s (±0.001299s); valid time: 0.008138s; loss: -13.4466 (±1.09165); valid loss: -2.71578\n",
      "[Epoch 287/350, Step 10900, ETA 14.9s] step time: 0.005427s (±0.001384s); valid time: 0.007847s; loss: -13.3605 (±1.01611); valid loss: -2.72909\n",
      "[Epoch 290/350, Step 11000, ETA 14.28s] step time: 0.005476s (±0.001235s); valid time: 0.008192s; loss: -13.3438 (±1.1979); valid loss: -2.78095\n",
      "[Epoch 290/350, Step 11020, ETA 14.15s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 293/350, Step 11100, ETA 13.65s] step time: 0.005506s (±0.001976s); valid time: 0.01116s; loss: -13.4605 (±1.18817); valid loss: -2.7344\n",
      "[Epoch 295/350, Step 11200, ETA 13.03s] step time: 0.005576s (±0.001559s); valid time: 0.007702s; loss: -13.463 (±1.10192); valid loss: -2.71942\n",
      "[Epoch 298/350, Step 11300, ETA 12.4s] step time: 0.005499s (±0.001292s); valid time: 0.008044s; loss: -13.3236 (±1.05708); valid loss: -2.74681\n",
      "[Epoch 300/350, Step 11400, ETA 11.78s] step time: 0.005292s (±0.001246s); valid time: 0.007795s; loss: -13.4101 (±1.13456); valid loss: -2.72345\n",
      "[Epoch 300/350, Step 11400, ETA 11.78s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 11500, ETA 11.15s] step time: 0.005513s (±0.001604s); valid time: 0.008234s; loss: -13.448 (±0.935236); valid loss: -2.73219\n",
      "[Epoch 306/350, Step 11600, ETA 10.53s] step time: 0.005472s (±0.001247s); valid time: 0.007984s; loss: -13.3537 (±1.18401); valid loss: -2.73367\n",
      "[Epoch 308/350, Step 11700, ETA 9.909s] step time: 0.005646s (±0.001479s); valid time: 0.008046s; loss: -13.3777 (±1.22718); valid loss: -2.74951\n",
      "[Epoch 310/350, Step 11780, ETA 9.409s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 11800, ETA 9.287s] step time: 0.005464s (±0.001463s); valid time: 0.008075s; loss: -13.3517 (±1.22799); valid loss: -2.78005\n",
      "[Epoch 314/350, Step 11900, ETA 8.664s] step time: 0.005343s (±0.001261s); valid time: 0.007691s; loss: -13.4224 (±1.10324); valid loss: -2.75135\n",
      "[Epoch 316/350, Step 12000, ETA 8.043s] step time: 0.005557s (±0.001585s); valid time: 0.008377s; loss: -13.4507 (±1.08411); valid loss: -2.78929\n",
      "[Epoch 319/350, Step 12100, ETA 7.424s] step time: 0.005623s (±0.001529s); valid time: 0.008085s; loss: -13.4458 (±1.22305); valid loss: -2.7365\n",
      "[Epoch 320/350, Step 12160, ETA 7.051s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 12200, ETA 6.805s] step time: 0.005579s (±0.001715s); valid time: 0.007983s; loss: -13.3894 (±1.16057); valid loss: -2.76762\n",
      "[Epoch 324/350, Step 12300, ETA 6.185s] step time: 0.005581s (±0.001447s); valid time: 0.008486s; loss: -13.3957 (±1.21569); valid loss: -2.75356\n",
      "[Epoch 327/350, Step 12400, ETA 5.565s] step time: 0.005504s (±0.001329s); valid time: 0.007811s; loss: -13.444 (±1.08751); valid loss: -2.73592\n",
      "[Epoch 329/350, Step 12500, ETA 4.946s] step time: 0.005627s (±0.001528s); valid time: 0.008186s; loss: -13.3563 (±1.11577); valid loss: -2.79246\n",
      "[Epoch 330/350, Step 12540, ETA 4.698s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 12600, ETA 4.327s] step time: 0.005541s (±0.001739s); valid time: 0.007841s; loss: -13.3297 (±0.991693); valid loss: -2.73225\n",
      "[Epoch 335/350, Step 12700, ETA 3.708s] step time: 0.005551s (±0.0016s); valid time: 0.007751s; loss: -13.4665 (±1.29778); valid loss: -2.75858\n",
      "[Epoch 337/350, Step 12800, ETA 3.089s] step time: 0.005393s (±0.00166s); valid time: 0.007852s; loss: -13.4019 (±1.17814); valid loss: -2.74285\n",
      "[Epoch 340/350, Step 12900, ETA 2.47s] step time: 0.005476s (±0.001459s); valid time: 0.007844s; loss: -13.402 (±1.19701); valid loss: -2.74292\n",
      "[Epoch 340/350, Step 12920, ETA 2.346s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 343/350, Step 13000, ETA 1.852s] step time: 0.005389s (±0.001415s); valid time: 0.008068s; loss: -13.3882 (±1.19552); valid loss: -2.70276\n",
      "[Epoch 345/350, Step 13100, ETA 1.234s] step time: 0.005319s (±0.001082s); valid time: 0.008007s; loss: -13.406 (±1.0406); valid loss: -2.7768\n",
      "[Epoch 348/350, Step 13200, ETA 0.6169s] step time: 0.00549s (±0.001365s); valid time: 0.007747s; loss: -13.3822 (±1.14053); valid loss: -2.75288\n",
      "[Epoch 350/350, Step 13300, ETA 0s] step time: 0.005261s (±0.001191s); valid time: 0.008057s; loss: -13.4208 (±1.05741); valid loss: -2.74858\n",
      "[Epoch 350/350, Step 13300, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqxxra04_/variables.dat-10400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqxxra04_/variables.dat-10400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.9,\n",
      "\t(tp, fp, tn, fn)=(37, 39, 972, 79),\n",
      "\tprecision=0.49,\n",
      "\trecall=0.32,\n",
      "\tf1=0.39,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.06743566992014197,\n",
      "\ty_label%=0.10292812777284827,\n",
      ")\n",
      "Testing on realTraffic/speed_t4013.csv ...\n",
      "reindexing\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 10/350, Step 60, ETA 21.2s] Learning rate decreased to 0.00075\n",
      "[Epoch 17/350, Step 100, ETA 25.08s] step time: 0.01195s (±0.04689s); valid time: 0.3798s; loss: 150.154 (±11.5471); valid loss: 256.092 (*)\n",
      "[Epoch 20/350, Step 120, ETA 22.64s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 30/350, Step 180, ETA 18.31s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 34/350, Step 200, ETA 18.29s] step time: 0.006165s (±0.008292s); valid time: 0.0827s; loss: 134.95 (±1.94597); valid loss: 192.739 (*)\n",
      "[Epoch 40/350, Step 240, ETA 16.85s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 50/350, Step 300, ETA 15.65s] step time: 0.006259s (±0.007892s); valid time: 0.07838s; loss: 132.609 (±1.60385); valid loss: 183.442 (*)\n",
      "[Epoch 50/350, Step 300, ETA 15.65s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 60/350, Step 360, ETA 14.38s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 67/350, Step 400, ETA 14.03s] step time: 0.00633s (±0.007781s); valid time: 0.07709s; loss: 131.958 (±1.61875); valid loss: 176.085 (*)\n",
      "[Epoch 70/350, Step 420, ETA 13.66s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 80/350, Step 480, ETA 12.71s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 84/350, Step 500, ETA 12.73s] step time: 0.006217s (±0.008234s); valid time: 0.08147s; loss: 131.598 (±1.37963); valid loss: 171.723 (*)\n",
      "[Epoch 90/350, Step 540, ETA 12.2s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 100/350, Step 600, ETA 11.63s] step time: 0.006194s (±0.007934s); valid time: 0.07849s; loss: 131.505 (±1.57629); valid loss: 169.484 (*)\n",
      "[Epoch 100/350, Step 600, ETA 11.63s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 110/350, Step 660, ETA 10.96s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 117/350, Step 700, ETA 10.69s] step time: 0.006332s (±0.00782s); valid time: 0.07763s; loss: 131.48 (±1.65181); valid loss: 168.098 (*)\n",
      "[Epoch 120/350, Step 720, ETA 10.46s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 130/350, Step 780, ETA 9.885s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 134/350, Step 800, ETA 9.809s] step time: 0.006374s (±0.008077s); valid time: 0.07908s; loss: 131.213 (±1.85516); valid loss: 167.375 (*)\n",
      "[Epoch 140/350, Step 840, ETA 9.404s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 150/350, Step 900, ETA 8.842s] step time: 0.005434s (±0.001194s); valid time: 0.001797s; loss: 131.386 (±1.52755); valid loss: 167.436\n",
      "[Epoch 150/350, Step 900, ETA 8.843s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 160/350, Step 960, ETA 8.307s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 167/350, Step 1000, ETA 8.039s] step time: 0.006203s (±0.007809s); valid time: 0.0773s; loss: 131.189 (±1.61721); valid loss: 167.276 (*)\n",
      "[Epoch 170/350, Step 1020, ETA 7.856s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 180/350, Step 1080, ETA 7.338s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 184/350, Step 1100, ETA 7.249s] step time: 0.006115s (±0.007817s); valid time: 0.07793s; loss: 131.233 (±1.64103); valid loss: 167.175 (*)\n",
      "[Epoch 190/350, Step 1140, ETA 6.918s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 200/350, Step 1200, ETA 6.43s] step time: 0.005445s (±0.001692s); valid time: 0.001711s; loss: 131.22 (±1.60041); valid loss: 167.2\n",
      "[Epoch 200/350, Step 1200, ETA 6.43s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 210/350, Step 1260, ETA 5.963s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 217/350, Step 1300, ETA 5.701s] step time: 0.006353s (±0.007892s); valid time: 0.07704s; loss: 131.208 (±1.64828); valid loss: 166.577 (*)\n",
      "[Epoch 220/350, Step 1320, ETA 5.54s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 230/350, Step 1380, ETA 5.079s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 234/350, Step 1400, ETA 4.928s] step time: 0.005352s (±0.0008504s); valid time: 0.001984s; loss: 131.254 (±1.62873); valid loss: 166.836\n",
      "[Epoch 240/350, Step 1440, ETA 4.634s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 250/350, Step 1500, ETA 4.185s] step time: 0.005518s (±0.001383s); valid time: 0.001814s; loss: 131.296 (±1.47568); valid loss: 166.76\n",
      "[Epoch 250/350, Step 1500, ETA 4.185s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 260/350, Step 1560, ETA 3.745s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 267/350, Step 1600, ETA 3.464s] step time: 0.005678s (±0.001651s); valid time: 0.002076s; loss: 131.261 (±1.45237); valid loss: 166.779\n",
      "[Epoch 270/350, Step 1620, ETA 3.318s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 280/350, Step 1680, ETA 2.89s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 284/350, Step 1700, ETA 2.749s] step time: 0.005454s (±0.001349s); valid time: 0.001847s; loss: 131.342 (±1.52301); valid loss: 166.866\n",
      "[Epoch 290/350, Step 1740, ETA 2.47s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 300/350, Step 1800, ETA 2.048s] step time: 0.005517s (±0.001454s); valid time: 0.001912s; loss: 131.378 (±1.43189); valid loss: 167.052\n",
      "[Epoch 300/350, Step 1800, ETA 2.049s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 310/350, Step 1860, ETA 1.632s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 317/350, Step 1900, ETA 1.356s] step time: 0.005328s (±0.0015s); valid time: 0.001806s; loss: 131.182 (±1.46961); valid loss: 167.615\n",
      "[Epoch 320/350, Step 1920, ETA 1.219s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 330/350, Step 1980, ETA 0.8092s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 334/350, Step 2000, ETA 0.6771s] step time: 0.006095s (±0.007655s); valid time: 0.07619s; loss: 131.396 (±1.63164); valid loss: 166.397 (*)\n",
      "[Epoch 340/350, Step 2040, ETA 0.4055s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 350/350, Step 2100, ETA 0s] step time: 0.005607s (±0.001621s); valid time: 0.001889s; loss: 131.317 (±1.5273); valid loss: 167.518\n",
      "[Epoch 350/350, Step 2100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpo662vf0w/variables.dat-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpo662vf0w/variables.dat-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(161, 495, 1750, 89),\n",
      "\tprecision=0.25,\n",
      "\trecall=0.64,\n",
      "\tf1=0.36,\n",
      "\troc_auc=0.71,\n",
      "\ty_pred%=0.2629258517034068,\n",
      "\ty_label%=0.10020040080160321,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AAPL.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 44.68s] step time: 0.009717s (±0.03308s); valid time: 0.1666s; loss: 45.2256 (±23.0605); valid loss: 358.552 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 12.84s] step time: 0.005869s (±0.007867s); valid time: 0.07791s; loss: -18.6868 (±8.25666); valid loss: 334.915 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 4.16s] step time: 0.006397s (±0.008507s); valid time: 0.08466s; loss: -26.9674 (±5.48553); valid loss: 315.391 (*)\n",
      "[Epoch 9/350, Step 400, ETA 1m 58.71s] step time: 0.006215s (±0.00901s); valid time: 0.09021s; loss: -29.5235 (±6.30417); valid loss: 301.238 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 54.49s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 55.14s] step time: 0.006195s (±0.008141s); valid time: 0.08138s; loss: -31.5388 (±6.00115); valid loss: 290.179 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 53.19s] step time: 0.006247s (±0.008502s); valid time: 0.08414s; loss: -32.2133 (±6.37665); valid loss: 281.629 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 50.9s] step time: 0.006114s (±0.008882s); valid time: 0.08872s; loss: -32.7938 (±6.80454); valid loss: 273.713 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 48.93s] step time: 0.006007s (±0.008481s); valid time: 0.08452s; loss: -33.7205 (±6.02464); valid loss: 264.83 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 47.84s] step time: 0.006405s (±0.008719s); valid time: 0.0865s; loss: -33.5959 (±6.06766); valid loss: 258.321 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 47.16s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 45.27s] step time: 0.005397s (±0.001315s); valid time: 0.008275s; loss: -34.5882 (±5.89523); valid loss: 262.632\n",
      "[Epoch 24/350, Step 1100, ETA 1m 43.1s] step time: 0.005433s (±0.001735s); valid time: 0.008475s; loss: -35.0657 (±6.43225); valid loss: 317.596\n",
      "[Epoch 27/350, Step 1200, ETA 1m 41.44s] step time: 0.005512s (±0.001516s); valid time: 0.008512s; loss: -35.9408 (±5.52101); valid loss: 517.976\n",
      "[Epoch 29/350, Step 1300, ETA 1m 40.57s] step time: 0.006156s (±0.008581s); valid time: 0.08546s; loss: -37.3019 (±5.70065); valid loss: 249.096 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 39.32s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 39.88s] step time: 0.006138s (±0.008164s); valid time: 0.08143s; loss: -37.758 (±5.42266); valid loss: 4.75997 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 39.24s] step time: 0.006372s (±0.009252s); valid time: 0.09209s; loss: -37.8872 (±5.23173); valid loss: -3.2045 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 38.37s] step time: 0.006114s (±0.008483s); valid time: 0.08465s; loss: -38.5159 (±5.40095); valid loss: -4.97565 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 37.66s] step time: 0.006277s (±0.008144s); valid time: 0.08131s; loss: -38.8422 (±5.41008); valid loss: -5.88892 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 37.18s] step time: 0.006449s (±0.009273s); valid time: 0.09009s; loss: -38.8927 (±5.32142); valid loss: -7.02674 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 36.51s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 36.44s] step time: 0.006242s (±0.008996s); valid time: 0.09003s; loss: -38.7841 (±5.53024); valid loss: -7.14821 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 35.71s] step time: 0.006259s (±0.008271s); valid time: 0.08207s; loss: -39.1148 (±5.30203); valid loss: -7.60176 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 34.9s] step time: 0.006128s (±0.009351s); valid time: 0.09308s; loss: -39.1247 (±5.36324); valid loss: -8.11609 (*)\n",
      "[Epoch 48/350, Step 2200, ETA 1m 34.12s] step time: 0.006159s (±0.00861s); valid time: 0.0862s; loss: -38.9327 (±5.70385); valid loss: -8.38076 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.38s] step time: 0.006197s (±0.008004s); valid time: 0.07916s; loss: -39.1709 (±6.58255); valid loss: -8.61378 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.43s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 32.37s] step time: 0.005508s (±0.001198s); valid time: 0.008307s; loss: -39.2109 (±5.42131); valid loss: -8.33436\n",
      "[Epoch 55/350, Step 2500, ETA 1m 31.72s] step time: 0.006334s (±0.008935s); valid time: 0.08459s; loss: -39.1671 (±5.08859); valid loss: -8.75631 (*)\n",
      "[Epoch 57/350, Step 2600, ETA 1m 30.67s] step time: 0.00556s (±0.00156s); valid time: 0.008144s; loss: -39.663 (±5.04945); valid loss: -8.66907\n",
      "[Epoch 59/350, Step 2700, ETA 1m 29.94s] step time: 0.006147s (±0.008631s); valid time: 0.0858s; loss: -38.8535 (±5.4479); valid loss: -8.87097 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 29.32s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 29.35s] step time: 0.006402s (±0.008442s); valid time: 0.08302s; loss: -39.6289 (±5.20882); valid loss: -9.5551 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 28.8s] step time: 0.0064s (±0.01014s); valid time: 0.1013s; loss: -39.4851 (±5.09074); valid loss: -9.6986 (*)\n",
      "[Epoch 66/350, Step 3000, ETA 1m 27.83s] step time: 0.005497s (±0.00144s); valid time: 0.008414s; loss: -39.3538 (±5.48763); valid loss: -9.38702\n",
      "[Epoch 68/350, Step 3100, ETA 1m 26.8s] step time: 0.005336s (±0.001164s); valid time: 0.008157s; loss: -39.5482 (±5.38027); valid loss: -9.60429\n",
      "[Epoch 70/350, Step 3200, ETA 1m 26.13s] step time: 0.006204s (±0.009267s); valid time: 0.09277s; loss: -39.5437 (±5.33923); valid loss: -9.70132 (*)\n",
      "[Epoch 70/350, Step 3220, ETA 1m 25.89s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 25.43s] step time: 0.006115s (±0.00832s); valid time: 0.08313s; loss: -39.9459 (±5.98337); valid loss: -9.99352 (*)\n",
      "[Epoch 74/350, Step 3400, ETA 1m 24.74s] step time: 0.006169s (±0.008456s); valid time: 0.08431s; loss: -39.4561 (±5.28533); valid loss: -10.1299 (*)\n",
      "[Epoch 77/350, Step 3500, ETA 1m 23.81s] step time: 0.005351s (±0.001274s); valid time: 0.00845s; loss: -39.7222 (±5.0725); valid loss: -9.77135\n",
      "[Epoch 79/350, Step 3600, ETA 1m 22.9s] step time: 0.005493s (±0.001435s); valid time: 0.008717s; loss: -39.2979 (±5.51273); valid loss: -9.94228\n",
      "[Epoch 80/350, Step 3680, ETA 1m 22.15s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 22.27s] step time: 0.006204s (±0.008232s); valid time: 0.08268s; loss: -39.9807 (±5.34791); valid loss: -10.2331 (*)\n",
      "[Epoch 83/350, Step 3800, ETA 1m 21.64s] step time: 0.006278s (±0.008462s); valid time: 0.08367s; loss: -39.6576 (±6.10712); valid loss: -10.4971 (*)\n",
      "[Epoch 85/350, Step 3900, ETA 1m 20.76s] step time: 0.005505s (±0.0015s); valid time: 0.00838s; loss: -39.9104 (±5.04001); valid loss: -10.4363\n",
      "[Epoch 87/350, Step 4000, ETA 1m 19.88s] step time: 0.005433s (±0.001839s); valid time: 0.01389s; loss: -39.6165 (±4.80329); valid loss: -10.17\n",
      "[Epoch 90/350, Step 4100, ETA 1m 19.25s] step time: 0.006102s (±0.008257s); valid time: 0.0821s; loss: -39.7789 (±5.78526); valid loss: -10.6079 (*)\n",
      "[Epoch 90/350, Step 4140, ETA 1m 18.88s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 18.42s] step time: 0.005527s (±0.001705s); valid time: 0.008665s; loss: -39.7128 (±4.74578); valid loss: -10.3641\n",
      "[Epoch 94/350, Step 4300, ETA 1m 17.57s] step time: 0.005425s (±0.001329s); valid time: 0.008126s; loss: -39.9247 (±5.03129); valid loss: -10.577\n",
      "[Epoch 96/350, Step 4400, ETA 1m 16.95s] step time: 0.006266s (±0.008896s); valid time: 0.08855s; loss: -39.6375 (±5.35454); valid loss: -10.6831 (*)\n",
      "[Epoch 98/350, Step 4500, ETA 1m 16.29s] step time: 0.006092s (±0.008569s); valid time: 0.08504s; loss: -39.9816 (±5.26066); valid loss: -10.7155 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.51s] step time: 0.005606s (±0.001544s); valid time: 0.008732s; loss: -39.7944 (±4.91719); valid loss: -10.2294\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.51s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 14.9s] step time: 0.006098s (±0.00884s); valid time: 0.08693s; loss: -39.8613 (±4.97015); valid loss: -11.0302 (*)\n",
      "[Epoch 105/350, Step 4800, ETA 1m 14.06s] step time: 0.005357s (±0.00131s); valid time: 0.008765s; loss: -39.8971 (±5.1808); valid loss: -10.8018\n",
      "[Epoch 107/350, Step 4900, ETA 1m 13.26s] step time: 0.005416s (±0.001515s); valid time: 0.008671s; loss: -40.3252 (±5.26996); valid loss: -10.87\n",
      "[Epoch 109/350, Step 5000, ETA 1m 12.47s] step time: 0.005478s (±0.001507s); valid time: 0.008833s; loss: -39.5204 (±5.58244); valid loss: -10.8344\n",
      "[Epoch 110/350, Step 5060, ETA 1m 11.99s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 11.71s] step time: 0.005522s (±0.001325s); valid time: 0.008674s; loss: -39.98 (±5.57689); valid loss: -10.9195\n",
      "[Epoch 114/350, Step 5200, ETA 1m 10.95s] step time: 0.005466s (±0.00162s); valid time: 0.008175s; loss: -39.8952 (±5.10351); valid loss: -10.5555\n",
      "[Epoch 116/350, Step 5300, ETA 1m 10.32s] step time: 0.006171s (±0.008795s); valid time: 0.08803s; loss: -40.2553 (±5.93779); valid loss: -11.1688 (*)\n",
      "[Epoch 118/350, Step 5400, ETA 1m 9.551s] step time: 0.005441s (±0.001541s); valid time: 0.008932s; loss: -39.7155 (±5.20643); valid loss: -11.1178\n",
      "[Epoch 120/350, Step 5500, ETA 1m 8.861s] step time: 0.005827s (±0.001906s); valid time: 0.009653s; loss: -40.202 (±4.86062); valid loss: -10.933\n",
      "[Epoch 120/350, Step 5520, ETA 1m 8.7s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 8.131s] step time: 0.005574s (±0.001453s); valid time: 0.008715s; loss: -39.7343 (±5.43718); valid loss: -10.8074\n",
      "[Epoch 124/350, Step 5700, ETA 1m 7.398s] step time: 0.005576s (±0.001823s); valid time: 0.00951s; loss: -39.837 (±5.76295); valid loss: -10.7445\n",
      "[Epoch 127/350, Step 5800, ETA 1m 6.653s] step time: 0.005373s (±0.001487s); valid time: 0.01045s; loss: -39.964 (±5.80803); valid loss: -10.9549\n",
      "[Epoch 129/350, Step 5900, ETA 1m 5.914s] step time: 0.005481s (±0.001281s); valid time: 0.008052s; loss: -39.9967 (±5.2963); valid loss: -10.5686\n",
      "[Epoch 130/350, Step 5980, ETA 1m 5.282s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 5.173s] step time: 0.005426s (±0.001473s); valid time: 0.008894s; loss: -40.1857 (±4.79522); valid loss: -10.7707\n",
      "[Epoch 133/350, Step 6100, ETA 1m 4.418s] step time: 0.005334s (±0.001316s); valid time: 0.008443s; loss: -39.5834 (±4.67854); valid loss: -10.9282\n",
      "[Epoch 135/350, Step 6200, ETA 1m 3.687s] step time: 0.005432s (±0.001662s); valid time: 0.008729s; loss: -40.0158 (±5.29427); valid loss: -11.0286\n",
      "[Epoch 137/350, Step 6300, ETA 1m 2.96s] step time: 0.005436s (±0.001484s); valid time: 0.008645s; loss: -40.2138 (±5.3566); valid loss: -11.0889\n",
      "[Epoch 140/350, Step 6400, ETA 1m 2.267s] step time: 0.005523s (±0.001635s); valid time: 0.008531s; loss: -40.3058 (±5.6683); valid loss: -11.0152\n",
      "[Epoch 140/350, Step 6440, ETA 1m 1.962s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 1.68s] step time: 0.006322s (±0.009411s); valid time: 0.08981s; loss: -39.7381 (±5.19837); valid loss: -11.3142 (*)\n",
      "[Epoch 144/350, Step 6600, ETA 1m 0.9948s] step time: 0.005586s (±0.00139s); valid time: 0.009562s; loss: -40.0782 (±4.79497); valid loss: -10.9333\n",
      "[Epoch 146/350, Step 6700, ETA 1m 0.3124s] step time: 0.005646s (±0.001414s); valid time: 0.008105s; loss: -40.0882 (±5.15545); valid loss: -11.1231\n",
      "[Epoch 148/350, Step 6800, ETA 59.58s] step time: 0.005303s (±0.00126s); valid time: 0.008391s; loss: -40.071 (±5.32664); valid loss: -11.0156\n",
      "[Epoch 150/350, Step 6900, ETA 58.89s] step time: 0.005557s (±0.001607s); valid time: 0.008426s; loss: -39.9156 (±4.62232); valid loss: -10.9084\n",
      "[Epoch 150/350, Step 6900, ETA 58.89s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 58.21s] step time: 0.005475s (±0.001563s); valid time: 0.008852s; loss: -40.2213 (±4.57133); valid loss: -11.0157\n",
      "[Epoch 155/350, Step 7100, ETA 57.5s] step time: 0.00542s (±0.001431s); valid time: 0.008698s; loss: -40.2419 (±5.36404); valid loss: -10.9418\n",
      "[Epoch 157/350, Step 7200, ETA 56.81s] step time: 0.005478s (±0.001509s); valid time: 0.008361s; loss: -39.6361 (±4.62454); valid loss: -11.1364\n",
      "[Epoch 159/350, Step 7300, ETA 56.11s] step time: 0.005407s (±0.001302s); valid time: 0.008541s; loss: -40.036 (±5.14651); valid loss: -10.7874\n",
      "[Epoch 160/350, Step 7360, ETA 55.7s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 55.46s] step time: 0.005762s (±0.001854s); valid time: 0.00891s; loss: -40.3318 (±5.35231); valid loss: -11.292\n",
      "[Epoch 164/350, Step 7500, ETA 54.79s] step time: 0.005507s (±0.001652s); valid time: 0.01257s; loss: -40.1195 (±5.64872); valid loss: -11.1597\n",
      "[Epoch 166/350, Step 7600, ETA 54.11s] step time: 0.005544s (±0.001628s); valid time: 0.008852s; loss: -39.8972 (±4.53281); valid loss: -11.2251\n",
      "[Epoch 168/350, Step 7700, ETA 53.43s] step time: 0.00547s (±0.0012s); valid time: 0.008722s; loss: -40.2215 (±6.24461); valid loss: -11.0626\n",
      "[Epoch 170/350, Step 7800, ETA 52.76s] step time: 0.005499s (±0.00119s); valid time: 0.009159s; loss: -39.815 (±5.50042); valid loss: -10.5489\n",
      "[Epoch 170/350, Step 7820, ETA 52.62s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 52.1s] step time: 0.005667s (±0.001505s); valid time: 0.008811s; loss: -39.9286 (±5.69962); valid loss: -11.2772\n",
      "[Epoch 174/350, Step 8000, ETA 51.45s] step time: 0.005667s (±0.001804s); valid time: 0.008298s; loss: -40.2363 (±5.44817); valid loss: -11.0903\n",
      "[Epoch 177/350, Step 8100, ETA 50.79s] step time: 0.005461s (±0.001277s); valid time: 0.008455s; loss: -40.2094 (±5.08568); valid loss: -11.033\n",
      "[Epoch 179/350, Step 8200, ETA 50.13s] step time: 0.005582s (±0.001859s); valid time: 0.009044s; loss: -39.7052 (±5.95964); valid loss: -11.2553\n",
      "[Epoch 180/350, Step 8280, ETA 49.57s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.53s] step time: 0.006267s (±0.009213s); valid time: 0.0915s; loss: -40.5272 (±5.43385); valid loss: -11.3148 (*)\n",
      "[Epoch 183/350, Step 8400, ETA 48.87s] step time: 0.005653s (±0.00164s); valid time: 0.008674s; loss: -39.98 (±5.19428); valid loss: -10.9702\n",
      "[Epoch 185/350, Step 8500, ETA 48.27s] step time: 0.006187s (±0.00927s); valid time: 0.09243s; loss: -40.1426 (±5.86095); valid loss: -11.3288 (*)\n",
      "[Epoch 187/350, Step 8600, ETA 47.66s] step time: 0.006209s (±0.008687s); valid time: 0.08666s; loss: -40.0577 (±5.48344); valid loss: -11.3834 (*)\n",
      "[Epoch 190/350, Step 8700, ETA 47.02s] step time: 0.005705s (±0.001595s); valid time: 0.008775s; loss: -40.1127 (±5.37208); valid loss: -11.0467\n",
      "[Epoch 190/350, Step 8740, ETA 46.74s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 46.35s] step time: 0.005459s (±0.001491s); valid time: 0.008268s; loss: -40.2063 (±5.27849); valid loss: -11.1161\n",
      "[Epoch 194/350, Step 8900, ETA 45.69s] step time: 0.005554s (±0.001656s); valid time: 0.008702s; loss: -39.7746 (±5.66562); valid loss: -11.124\n",
      "[Epoch 196/350, Step 9000, ETA 45.02s] step time: 0.005446s (±0.001389s); valid time: 0.00815s; loss: -39.9077 (±5.05168); valid loss: -10.9844\n",
      "[Epoch 198/350, Step 9100, ETA 44.34s] step time: 0.00527s (±0.001373s); valid time: 0.008857s; loss: -40.5158 (±5.61013); valid loss: -11.1913\n",
      "[Epoch 200/350, Step 9200, ETA 43.68s] step time: 0.005459s (±0.001302s); valid time: 0.008567s; loss: -40.0005 (±5.61515); valid loss: -11.1737\n",
      "[Epoch 200/350, Step 9200, ETA 43.68s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 43.08s] step time: 0.006113s (±0.008728s); valid time: 0.08723s; loss: -40.1124 (±6.42548); valid loss: -11.4949 (*)\n",
      "[Epoch 205/350, Step 9400, ETA 42.41s] step time: 0.005423s (±0.001299s); valid time: 0.008371s; loss: -40.059 (±5.20898); valid loss: -11.0812\n",
      "[Epoch 207/350, Step 9500, ETA 41.76s] step time: 0.005562s (±0.001772s); valid time: 0.008931s; loss: -40.2619 (±5.0348); valid loss: -11.1681\n",
      "[Epoch 209/350, Step 9600, ETA 41.09s] step time: 0.005326s (±0.001805s); valid time: 0.008669s; loss: -39.8506 (±5.04951); valid loss: -11.1295\n",
      "[Epoch 210/350, Step 9660, ETA 40.7s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.45s] step time: 0.005718s (±0.001815s); valid time: 0.008915s; loss: -40.4419 (±5.77718); valid loss: -11.137\n",
      "[Epoch 214/350, Step 9800, ETA 39.79s] step time: 0.005293s (±0.001172s); valid time: 0.008632s; loss: -39.8429 (±5.2867); valid loss: -10.9783\n",
      "[Epoch 216/350, Step 9900, ETA 39.14s] step time: 0.005487s (±0.001369s); valid time: 0.008615s; loss: -40.1035 (±5.15003); valid loss: -11.1827\n",
      "[Epoch 218/350, Step 10000, ETA 38.53s] step time: 0.006254s (±0.008437s); valid time: 0.08445s; loss: -40.102 (±5.09585); valid loss: -11.6563 (*)\n",
      "[Epoch 220/350, Step 10100, ETA 37.87s] step time: 0.00527s (±0.001374s); valid time: 0.009862s; loss: -40.1951 (±5.73556); valid loss: -11.0631\n",
      "[Epoch 220/350, Step 10120, ETA 37.73s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 37.21s] step time: 0.005439s (±0.001512s); valid time: 0.008364s; loss: -40.1503 (±4.78008); valid loss: -11.0395\n",
      "[Epoch 224/350, Step 10300, ETA 36.56s] step time: 0.005416s (±0.001495s); valid time: 0.008749s; loss: -39.8449 (±4.87944); valid loss: -11.2133\n",
      "[Epoch 227/350, Step 10400, ETA 35.92s] step time: 0.005596s (±0.001939s); valid time: 0.008829s; loss: -40.3537 (±5.36688); valid loss: -11.4286\n",
      "[Epoch 229/350, Step 10500, ETA 35.28s] step time: 0.00561s (±0.001954s); valid time: 0.008547s; loss: -39.8136 (±5.8269); valid loss: -10.7902\n",
      "[Epoch 230/350, Step 10580, ETA 34.76s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.63s] step time: 0.00545s (±0.001247s); valid time: 0.008891s; loss: -39.9652 (±5.67071); valid loss: -11.1393\n",
      "[Epoch 233/350, Step 10700, ETA 33.98s] step time: 0.00553s (±0.001526s); valid time: 0.008417s; loss: -40.3911 (±5.43384); valid loss: -11.1216\n",
      "[Epoch 235/350, Step 10800, ETA 33.34s] step time: 0.00545s (±0.001656s); valid time: 0.008801s; loss: -39.9949 (±5.95365); valid loss: -10.9259\n",
      "[Epoch 237/350, Step 10900, ETA 32.69s] step time: 0.005404s (±0.001478s); valid time: 0.008134s; loss: -40.2378 (±5.95965); valid loss: -11.0425\n",
      "[Epoch 240/350, Step 11000, ETA 32.04s] step time: 0.005349s (±0.001461s); valid time: 0.008218s; loss: -40.1403 (±4.91019); valid loss: -11.2565\n",
      "[Epoch 240/350, Step 11040, ETA 31.79s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.41s] step time: 0.005645s (±0.001945s); valid time: 0.009224s; loss: -40.1477 (±5.66792); valid loss: -11.1118\n",
      "[Epoch 244/350, Step 11200, ETA 30.77s] step time: 0.00552s (±0.001551s); valid time: 0.008017s; loss: -39.9775 (±5.21832); valid loss: -11.1954\n",
      "[Epoch 246/350, Step 11300, ETA 30.12s] step time: 0.005328s (±0.001302s); valid time: 0.008582s; loss: -40.2203 (±4.77328); valid loss: -11.2116\n",
      "[Epoch 248/350, Step 11400, ETA 29.48s] step time: 0.005517s (±0.001544s); valid time: 0.008215s; loss: -40.0554 (±5.27242); valid loss: -11.2025\n",
      "[Epoch 250/350, Step 11500, ETA 28.83s] step time: 0.005258s (±0.001312s); valid time: 0.008548s; loss: -40.2127 (±4.70405); valid loss: -10.9885\n",
      "[Epoch 250/350, Step 11500, ETA 28.83s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.2s] step time: 0.005515s (±0.001389s); valid time: 0.00833s; loss: -39.9528 (±5.68638); valid loss: -11.262\n",
      "[Epoch 255/350, Step 11700, ETA 27.56s] step time: 0.005423s (±0.001667s); valid time: 0.0107s; loss: -40.3 (±5.32806); valid loss: -10.9197\n",
      "[Epoch 257/350, Step 11800, ETA 26.92s] step time: 0.005409s (±0.001431s); valid time: 0.008405s; loss: -40.2475 (±5.23427); valid loss: -11.0756\n",
      "[Epoch 259/350, Step 11900, ETA 26.28s] step time: 0.005466s (±0.001465s); valid time: 0.008344s; loss: -39.8396 (±5.12716); valid loss: -10.8945\n",
      "[Epoch 260/350, Step 11960, ETA 25.89s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.64s] step time: 0.005338s (±0.001422s); valid time: 0.007957s; loss: -40.0982 (±5.9473); valid loss: -11.2188\n",
      "[Epoch 264/350, Step 12100, ETA 25.01s] step time: 0.005605s (±0.001728s); valid time: 0.008779s; loss: -40.2994 (±5.19892); valid loss: -11.2177\n",
      "[Epoch 266/350, Step 12200, ETA 24.37s] step time: 0.00537s (±0.001295s); valid time: 0.008134s; loss: -40.0155 (±5.73995); valid loss: -11.5412\n",
      "[Epoch 268/350, Step 12300, ETA 23.75s] step time: 0.005653s (±0.001705s); valid time: 0.008576s; loss: -39.9991 (±5.24239); valid loss: -11.3088\n",
      "[Epoch 270/350, Step 12400, ETA 23.12s] step time: 0.005618s (±0.001866s); valid time: 0.008627s; loss: -40.5346 (±5.14392); valid loss: -11.2029\n",
      "[Epoch 270/350, Step 12420, ETA 22.99s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.48s] step time: 0.005393s (±0.001154s); valid time: 0.008646s; loss: -39.6817 (±5.58841); valid loss: -10.9229\n",
      "[Epoch 274/350, Step 12600, ETA 21.87s] step time: 0.00625s (±0.008308s); valid time: 0.08297s; loss: -40.3596 (±5.7707); valid loss: -11.7773 (*)\n",
      "[Epoch 277/350, Step 12700, ETA 21.24s] step time: 0.005364s (±0.001367s); valid time: 0.008513s; loss: -39.9731 (±5.33796); valid loss: -11.1013\n",
      "[Epoch 279/350, Step 12800, ETA 20.6s] step time: 0.005441s (±0.001723s); valid time: 0.009853s; loss: -39.9444 (±5.58892); valid loss: -11.5055\n",
      "[Epoch 280/350, Step 12880, ETA 20.09s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.97s] step time: 0.005321s (±0.001227s); valid time: 0.008386s; loss: -39.862 (±5.60534); valid loss: -11.346\n",
      "[Epoch 283/350, Step 13000, ETA 19.34s] step time: 0.005418s (±0.001443s); valid time: 0.009351s; loss: -40.2883 (±4.82322); valid loss: -11.5747\n",
      "[Epoch 285/350, Step 13100, ETA 18.71s] step time: 0.005529s (±0.001702s); valid time: 0.008223s; loss: -40.4369 (±4.8887); valid loss: -11.0062\n",
      "[Epoch 287/350, Step 13200, ETA 18.08s] step time: 0.005442s (±0.00131s); valid time: 0.008596s; loss: -40.1334 (±4.59578); valid loss: -11.117\n",
      "[Epoch 290/350, Step 13300, ETA 17.45s] step time: 0.005521s (±0.00144s); valid time: 0.00964s; loss: -39.8388 (±5.71255); valid loss: -11.0849\n",
      "[Epoch 290/350, Step 13340, ETA 17.2s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.82s] step time: 0.005587s (±0.001694s); valid time: 0.008578s; loss: -40.311 (±5.89788); valid loss: -11.1859\n",
      "[Epoch 294/350, Step 13500, ETA 16.19s] step time: 0.00546s (±0.001335s); valid time: 0.008881s; loss: -39.7117 (±5.24636); valid loss: -11.3338\n",
      "[Epoch 296/350, Step 13600, ETA 15.57s] step time: 0.005804s (±0.001938s); valid time: 0.008444s; loss: -40.2597 (±5.43355); valid loss: -11.0537\n",
      "[Epoch 298/350, Step 13700, ETA 14.94s] step time: 0.005419s (±0.001423s); valid time: 0.0088s; loss: -40.3503 (±5.35915); valid loss: -10.8484\n",
      "[Epoch 300/350, Step 13800, ETA 14.32s] step time: 0.005481s (±0.001391s); valid time: 0.008383s; loss: -40.0454 (±5.41026); valid loss: -10.7272\n",
      "[Epoch 300/350, Step 13800, ETA 14.32s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.69s] step time: 0.005555s (±0.001762s); valid time: 0.008799s; loss: -40.1247 (±4.36889); valid loss: -11.1097\n",
      "[Epoch 305/350, Step 14000, ETA 13.06s] step time: 0.005334s (±0.001444s); valid time: 0.008707s; loss: -39.8169 (±5.48897); valid loss: -11.5196\n",
      "[Epoch 307/350, Step 14100, ETA 12.44s] step time: 0.00548s (±0.001246s); valid time: 0.008489s; loss: -40.0331 (±5.84259); valid loss: -11.0875\n",
      "[Epoch 309/350, Step 14200, ETA 11.81s] step time: 0.005319s (±0.001368s); valid time: 0.008294s; loss: -40.2144 (±5.58519); valid loss: -11.2288\n",
      "[Epoch 310/350, Step 14260, ETA 11.43s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.19s] step time: 0.005597s (±0.001811s); valid time: 0.008478s; loss: -40.3445 (±5.05236); valid loss: -10.9687\n",
      "[Epoch 314/350, Step 14400, ETA 10.56s] step time: 0.00541s (±0.001113s); valid time: 0.008993s; loss: -40.2624 (±5.62629); valid loss: -11.2188\n",
      "[Epoch 316/350, Step 14500, ETA 9.937s] step time: 0.005492s (±0.001623s); valid time: 0.008887s; loss: -39.8956 (±5.14191); valid loss: -11.4468\n",
      "[Epoch 318/350, Step 14600, ETA 9.313s] step time: 0.005412s (±0.001329s); valid time: 0.008522s; loss: -40.3518 (±4.61872); valid loss: -11.1341\n",
      "[Epoch 320/350, Step 14700, ETA 8.689s] step time: 0.005404s (±0.001159s); valid time: 0.008911s; loss: -39.8702 (±5.23201); valid loss: -11.2339\n",
      "[Epoch 320/350, Step 14720, ETA 8.563s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.067s] step time: 0.005558s (±0.00163s); valid time: 0.008365s; loss: -40.5709 (±5.84797); valid loss: -11.2234\n",
      "[Epoch 324/350, Step 14900, ETA 7.444s] step time: 0.005436s (±0.001247s); valid time: 0.008376s; loss: -39.7146 (±5.59528); valid loss: -10.8722\n",
      "[Epoch 327/350, Step 15000, ETA 6.822s] step time: 0.005454s (±0.001401s); valid time: 0.008142s; loss: -40.1676 (±5.09769); valid loss: -11.0515\n",
      "[Epoch 329/350, Step 15100, ETA 6.2s] step time: 0.005444s (±0.001929s); valid time: 0.009505s; loss: -40.2202 (±5.83221); valid loss: -11.076\n",
      "[Epoch 330/350, Step 15180, ETA 5.702s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.578s] step time: 0.005411s (±0.001407s); valid time: 0.008629s; loss: -39.9492 (±5.298); valid loss: -11.3459\n",
      "[Epoch 333/350, Step 15300, ETA 4.958s] step time: 0.005629s (±0.001665s); valid time: 0.008263s; loss: -40.5618 (±5.51494); valid loss: -11.2546\n",
      "[Epoch 335/350, Step 15400, ETA 4.336s] step time: 0.005287s (±0.00116s); valid time: 0.00817s; loss: -39.7342 (±5.76022); valid loss: -11.2773\n",
      "[Epoch 337/350, Step 15500, ETA 3.716s] step time: 0.005509s (±0.001553s); valid time: 0.008574s; loss: -40.2095 (±5.68499); valid loss: -11.1619\n",
      "[Epoch 340/350, Step 15600, ETA 3.096s] step time: 0.00543s (±0.001327s); valid time: 0.008422s; loss: -40.2953 (±5.97408); valid loss: -11.0145\n",
      "[Epoch 340/350, Step 15640, ETA 2.848s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.476s] step time: 0.005515s (±0.001744s); valid time: 0.008565s; loss: -40.0618 (±5.28273); valid loss: -11.1508\n",
      "[Epoch 344/350, Step 15800, ETA 1.856s] step time: 0.005306s (±0.001242s); valid time: 0.008388s; loss: -40.2147 (±5.47548); valid loss: -11.2828\n",
      "[Epoch 346/350, Step 15900, ETA 1.237s] step time: 0.005358s (±0.001345s); valid time: 0.008456s; loss: -39.837 (±5.59045); valid loss: -11.0141\n",
      "[Epoch 348/350, Step 16000, ETA 0.6184s] step time: 0.005447s (±0.001413s); valid time: 0.008391s; loss: -40.17 (±6.42814); valid loss: -11.1584\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005371s (±0.001146s); valid time: 0.008638s; loss: -40.2125 (±5.30966); valid loss: -11.1245\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzbm_whr3/variables.dat-12600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzbm_whr3/variables.dat-12600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.85,\n",
      "\t(tp, fp, tn, fn)=(813, 1588, 12726, 775),\n",
      "\tprecision=0.34,\n",
      "\trecall=0.51,\n",
      "\tf1=0.41,\n",
      "\troc_auc=0.7,\n",
      "\ty_pred%=0.15098729719532133,\n",
      "\ty_label%=0.09986165262231166,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_AMZN.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 39.37s] step time: 0.0096s (±0.03248s); valid time: 0.1687s; loss: 129.142 (±11.9446); valid loss: 123.219 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 9.431s] step time: 0.005992s (±0.007635s); valid time: 0.07587s; loss: 106.459 (±4.24539); valid loss: 108.996 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 0.2779s] step time: 0.00624s (±0.008293s); valid time: 0.08218s; loss: 100.113 (±4.50626); valid loss: 101.87 (*)\n",
      "[Epoch 9/350, Step 400, ETA 1m 55.3s] step time: 0.006237s (±0.009267s); valid time: 0.09213s; loss: 91.2967 (±4.24942); valid loss: 94.9634 (*)\n",
      "[Epoch 10/350, Step 450, ETA 1m 51.89s] Learning rate decreased to 0.00075\n",
      "[Epoch 12/350, Step 500, ETA 1m 52.13s] step time: 0.006147s (±0.008124s); valid time: 0.08079s; loss: 87.0161 (±2.98305); valid loss: 93.0958 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 49.6s] step time: 0.006194s (±0.008331s); valid time: 0.08291s; loss: 85.3371 (±3.65852); valid loss: 92.3348 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 47.55s] step time: 0.006157s (±0.009432s); valid time: 0.09428s; loss: 84.1127 (±3.52682); valid loss: 92.0722 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 45.91s] step time: 0.006183s (±0.008498s); valid time: 0.08512s; loss: 84.4648 (±10.5474); valid loss: 91.4013 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 44.7s] step time: 0.006281s (±0.009011s); valid time: 0.09016s; loss: 88.4578 (±45.0968); valid loss: 90.6748 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 44.71s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 23/350, Step 1000, ETA 1m 43.83s] step time: 0.006363s (±0.009364s); valid time: 0.09421s; loss: 81.9119 (±3.57967); valid loss: 90.4027 (*)\n",
      "[Epoch 25/350, Step 1100, ETA 1m 42.51s] step time: 0.006092s (±0.008265s); valid time: 0.08215s; loss: 81.8916 (±3.79081); valid loss: 90.2559 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 41.89s] step time: 0.006594s (±0.008439s); valid time: 0.08333s; loss: 81.4571 (±3.59336); valid loss: 89.8655 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 40.99s] step time: 0.006369s (±0.008911s); valid time: 0.08908s; loss: 82.1342 (±12.6383); valid loss: 89.8631 (*)\n",
      "[Epoch 30/350, Step 1350, ETA 1m 40.11s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 32/350, Step 1400, ETA 1m 40.12s] step time: 0.006255s (±0.008706s); valid time: 0.08644s; loss: 83.0221 (±21.5624); valid loss: 89.4771 (*)\n",
      "[Epoch 34/350, Step 1500, ETA 1m 38.47s] step time: 0.005516s (±0.001452s); valid time: 0.008859s; loss: 80.3931 (±3.5405); valid loss: 89.4973\n",
      "[Epoch 36/350, Step 1600, ETA 1m 37.48s] step time: 0.006108s (±0.008611s); valid time: 0.08614s; loss: 80.1514 (±3.54928); valid loss: 89.2863 (*)\n",
      "[Epoch 38/350, Step 1700, ETA 1m 35.99s] step time: 0.005448s (±0.00135s); valid time: 0.008059s; loss: 80.5901 (±4.73344); valid loss: 89.4595\n",
      "[Epoch 40/350, Step 1800, ETA 1m 35.17s] step time: 0.006211s (±0.008259s); valid time: 0.08219s; loss: 85.1862 (±53.9164); valid loss: 89.2641 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 35.18s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 43/350, Step 1900, ETA 1m 34.48s] step time: 0.00624s (±0.008544s); valid time: 0.0852s; loss: 79.4107 (±3.52301); valid loss: 89.1521 (*)\n",
      "[Epoch 45/350, Step 2000, ETA 1m 33.73s] step time: 0.006191s (±0.008579s); valid time: 0.0857s; loss: 91.1991 (±98.0115); valid loss: 88.8893 (*)\n",
      "[Epoch 47/350, Step 2100, ETA 1m 32.46s] step time: 0.005446s (±0.00124s); valid time: 0.008512s; loss: 78.8678 (±3.92628); valid loss: 88.8947\n",
      "[Epoch 49/350, Step 2200, ETA 1m 31.2s] step time: 0.005392s (±0.00136s); valid time: 0.008865s; loss: 80.191 (±14.5954); valid loss: 89.4028\n",
      "[Epoch 50/350, Step 2250, ETA 1m 30.61s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 52/350, Step 2300, ETA 1m 30.17s] step time: 0.005523s (±0.001517s); valid time: 0.009143s; loss: 78.4561 (±3.33075); valid loss: 95.408\n",
      "[Epoch 54/350, Step 2400, ETA 1m 29.02s] step time: 0.005401s (±0.001333s); valid time: 0.009094s; loss: 78.2245 (±3.12953); valid loss: 89.1779\n",
      "[Epoch 56/350, Step 2500, ETA 1m 28.39s] step time: 0.006281s (±0.008708s); valid time: 0.08661s; loss: 109.633 (±314.796); valid loss: 88.8482 (*)\n",
      "[Epoch 58/350, Step 2600, ETA 1m 27.76s] step time: 0.006279s (±0.008496s); valid time: 0.08428s; loss: 108.617 (±300.319); valid loss: 88.7935 (*)\n",
      "[Epoch 60/350, Step 2700, ETA 1m 27.08s] step time: 0.0062s (±0.008704s); valid time: 0.08676s; loss: 73440 (±729898); valid loss: 88.7862 (*)\n",
      "[Epoch 60/350, Step 2700, ETA 1m 27.08s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 63/350, Step 2800, ETA 1m 26.22s] step time: 0.005693s (±0.001781s); valid time: 0.009s; loss: 78.9759 (±10.3736); valid loss: 88.8036\n",
      "[Epoch 65/350, Step 2900, ETA 1m 25.25s] step time: 0.005484s (±0.001308s); valid time: 0.009079s; loss: 624.203 (±5297.04); valid loss: 88.875\n",
      "[Epoch 67/350, Step 3000, ETA 1m 24.32s] step time: 0.005546s (±0.001415s); valid time: 0.008707s; loss: 89.3478 (±118.942); valid loss: 89.4517\n",
      "[Epoch 69/350, Step 3100, ETA 1m 23.4s] step time: 0.005538s (±0.001549s); valid time: 0.009193s; loss: 87.4837 (±67.3221); valid loss: 89.0346\n",
      "[Epoch 70/350, Step 3150, ETA 1m 22.94s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3200, ETA 1m 22.59s] step time: 0.005631s (±0.001488s); valid time: 0.008417s; loss: 77.5366 (±3.69567); valid loss: 89.2108\n",
      "[Epoch 74/350, Step 3300, ETA 1m 21.74s] step time: 0.005626s (±0.00163s); valid time: 0.008379s; loss: 2780.99 (±26900.3); valid loss: 89.3374\n",
      "[Epoch 76/350, Step 3400, ETA 1m 20.91s] step time: 0.005635s (±0.002158s); valid time: 0.01181s; loss: 77.6208 (±4.92111); valid loss: 88.8707\n",
      "[Epoch 78/350, Step 3500, ETA 1m 20.3s] step time: 0.006203s (±0.008939s); valid time: 0.08954s; loss: 226.086 (±1481.47); valid loss: 88.6503 (*)\n",
      "[Epoch 80/350, Step 3600, ETA 1m 19.48s] step time: 0.005524s (±0.001358s); valid time: 0.008603s; loss: 77.7502 (±6.3332); valid loss: 88.8653\n",
      "[Epoch 80/350, Step 3600, ETA 1m 19.48s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 83/350, Step 3700, ETA 1m 18.63s] step time: 0.005356s (±0.001417s); valid time: 0.008788s; loss: 76.9072 (±2.63851); valid loss: 89.3278\n",
      "[Epoch 85/350, Step 3800, ETA 1m 17.86s] step time: 0.005666s (±0.00171s); valid time: 0.009209s; loss: 78.4915 (±14.1951); valid loss: 89.0561\n",
      "[Epoch 87/350, Step 3900, ETA 1m 17.07s] step time: 0.00558s (±0.001696s); valid time: 0.009691s; loss: 232.546 (±1415.8); valid loss: 89.7888\n",
      "[Epoch 89/350, Step 4000, ETA 1m 16.28s] step time: 0.005582s (±0.001862s); valid time: 0.01014s; loss: 77.0489 (±3.50534); valid loss: 88.7976\n",
      "[Epoch 90/350, Step 4050, ETA 1m 15.89s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4100, ETA 1m 15.55s] step time: 0.00561s (±0.001614s); valid time: 0.008206s; loss: 76.9066 (±2.91485); valid loss: 89.3782\n",
      "[Epoch 94/350, Step 4200, ETA 1m 14.96s] step time: 0.006254s (±0.008868s); valid time: 0.08809s; loss: 76.9519 (±3.52779); valid loss: 88.6287 (*)\n",
      "[Epoch 96/350, Step 4300, ETA 1m 14.19s] step time: 0.005552s (±0.001585s); valid time: 0.008955s; loss: 151551 (±1.50711e+06); valid loss: 88.95\n",
      "[Epoch 98/350, Step 4400, ETA 1m 13.4s] step time: 0.005451s (±0.00134s); valid time: 0.008586s; loss: 77.0498 (±3.11724); valid loss: 88.8606\n",
      "[Epoch 100/350, Step 4500, ETA 1m 12.67s] step time: 0.005681s (±0.001721s); valid time: 0.008722s; loss: 76.9334 (±2.71965); valid loss: 90.8124\n",
      "[Epoch 100/350, Step 4500, ETA 1m 12.68s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4600, ETA 1m 11.93s] step time: 0.005491s (±0.001568s); valid time: 0.008701s; loss: 559.972 (±4795.84); valid loss: 88.8754\n",
      "[Epoch 105/350, Step 4700, ETA 1m 11.17s] step time: 0.005473s (±0.00139s); valid time: 0.008499s; loss: 78.4502 (±17.0294); valid loss: 89.3476\n",
      "[Epoch 107/350, Step 4800, ETA 1m 10.42s] step time: 0.005501s (±0.00146s); valid time: 0.00825s; loss: 79.5983 (±28.7336); valid loss: 89.741\n",
      "[Epoch 109/350, Step 4900, ETA 1m 9.674s] step time: 0.005482s (±0.001325s); valid time: 0.008307s; loss: 76.8664 (±3.26644); valid loss: 89.0698\n",
      "[Epoch 110/350, Step 4950, ETA 1m 9.282s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 112/350, Step 5000, ETA 1m 8.954s] step time: 0.005461s (±0.001855s); valid time: 0.008786s; loss: 76.5751 (±3.10818); valid loss: 88.6399\n",
      "[Epoch 114/350, Step 5100, ETA 1m 8.209s] step time: 0.005467s (±0.001473s); valid time: 0.008368s; loss: 77.6651 (±9.79295); valid loss: 385.631\n",
      "[Epoch 116/350, Step 5200, ETA 1m 7.462s] step time: 0.005419s (±0.001527s); valid time: 0.008337s; loss: 159.993 (±783.16); valid loss: 88.8123\n",
      "[Epoch 118/350, Step 5300, ETA 1m 6.728s] step time: 0.005444s (±0.001514s); valid time: 0.008687s; loss: 76.8521 (±3.71588); valid loss: 88.8132\n",
      "[Epoch 120/350, Step 5400, ETA 1m 6.146s] step time: 0.006234s (±0.009117s); valid time: 0.0903s; loss: 314.617 (±2333.91); valid loss: 88.5313 (*)\n",
      "[Epoch 120/350, Step 5400, ETA 1m 6.147s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 123/350, Step 5500, ETA 1m 5.448s] step time: 0.005505s (±0.001338s); valid time: 0.008945s; loss: 77.1017 (±4.66945); valid loss: 89.1145\n",
      "[Epoch 125/350, Step 5600, ETA 1m 4.722s] step time: 0.005413s (±0.001197s); valid time: 0.008408s; loss: 175.818 (±984.773); valid loss: 96.8011\n",
      "[Epoch 127/350, Step 5700, ETA 1m 3.985s] step time: 0.005348s (±0.001377s); valid time: 0.008369s; loss: 77.0144 (±7.48047); valid loss: 93.2967\n",
      "[Epoch 129/350, Step 5800, ETA 1m 3.292s] step time: 0.00554s (±0.001691s); valid time: 0.008904s; loss: 76.9897 (±4.33821); valid loss: 91.1927\n",
      "[Epoch 130/350, Step 5850, ETA 1m 2.929s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 132/350, Step 5900, ETA 1m 2.584s] step time: 0.005361s (±0.001676s); valid time: 0.008538s; loss: 76.5393 (±3.14284); valid loss: 93.2659\n",
      "[Epoch 134/350, Step 6000, ETA 1m 1.922s] step time: 0.005733s (±0.001731s); valid time: 0.008446s; loss: 139.23 (±623.98); valid loss: 88.9301\n",
      "[Epoch 136/350, Step 6100, ETA 1m 1.203s] step time: 0.005367s (±0.001482s); valid time: 0.00825s; loss: 10336.5 (±102078); valid loss: 88.7681\n",
      "[Epoch 138/350, Step 6200, ETA 1m 0.5121s] step time: 0.005526s (±0.001559s); valid time: 0.008336s; loss: 114.979 (±380.944); valid loss: 112.643\n",
      "[Epoch 140/350, Step 6300, ETA 59.82s] step time: 0.005513s (±0.001448s); valid time: 0.008408s; loss: 76.4739 (±3.09306); valid loss: 88.8104\n",
      "[Epoch 140/350, Step 6300, ETA 59.82s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 143/350, Step 6400, ETA 59.13s] step time: 0.005345s (±0.001311s); valid time: 0.008346s; loss: 109.723 (±325.101); valid loss: 89.0194\n",
      "[Epoch 145/350, Step 6500, ETA 58.54s] step time: 0.006167s (±0.008525s); valid time: 0.08522s; loss: 728.715 (±5412.48); valid loss: 88.3508 (*)\n",
      "[Epoch 147/350, Step 6600, ETA 57.86s] step time: 0.005487s (±0.001412s); valid time: 0.008544s; loss: 76.6016 (±3.38737); valid loss: 89.6961\n",
      "[Epoch 149/350, Step 6700, ETA 57.15s] step time: 0.005263s (±0.001292s); valid time: 0.008646s; loss: 151.313 (±667.654); valid loss: 88.9949\n",
      "[Epoch 150/350, Step 6750, ETA 56.8s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 152/350, Step 6800, ETA 56.49s] step time: 0.005561s (±0.001581s); valid time: 0.008365s; loss: 76.5241 (±3.34311); valid loss: 88.7401\n",
      "[Epoch 154/350, Step 6900, ETA 55.8s] step time: 0.005418s (±0.001339s); valid time: 0.008566s; loss: 76.5312 (±3.34494); valid loss: 93.3966\n",
      "[Epoch 156/350, Step 7000, ETA 55.14s] step time: 0.005546s (±0.001457s); valid time: 0.008524s; loss: 78.7664 (±20.4156); valid loss: 88.6345\n",
      "[Epoch 158/350, Step 7100, ETA 54.48s] step time: 0.005596s (±0.001791s); valid time: 0.01063s; loss: 77.2265 (±5.04826); valid loss: 89.2149\n",
      "[Epoch 160/350, Step 7200, ETA 53.8s] step time: 0.005456s (±0.001619s); valid time: 0.008087s; loss: 76.9083 (±5.9419); valid loss: 88.9263\n",
      "[Epoch 160/350, Step 7200, ETA 53.8s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 163/350, Step 7300, ETA 53.14s] step time: 0.005455s (±0.001563s); valid time: 0.008451s; loss: 366140 (±3.64229e+06); valid loss: 88.9866\n",
      "[Epoch 165/350, Step 7400, ETA 52.47s] step time: 0.005456s (±0.001185s); valid time: 0.008406s; loss: 871.651 (±7913.6); valid loss: 89.1744\n",
      "[Epoch 167/350, Step 7500, ETA 51.8s] step time: 0.005418s (±0.001334s); valid time: 0.007782s; loss: 25556.5 (±253509); valid loss: 88.9489\n",
      "[Epoch 169/350, Step 7600, ETA 51.13s] step time: 0.00546s (±0.001415s); valid time: 0.008322s; loss: 76.616 (±3.25626); valid loss: 89.2937\n",
      "[Epoch 170/350, Step 7650, ETA 50.8s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7700, ETA 50.49s] step time: 0.005571s (±0.00162s); valid time: 0.008697s; loss: 83.5583 (±64.5049); valid loss: 90.0487\n",
      "[Epoch 174/350, Step 7800, ETA 49.83s] step time: 0.005454s (±0.001453s); valid time: 0.008591s; loss: 367.944 (±2899.42); valid loss: 90.3519\n",
      "[Epoch 176/350, Step 7900, ETA 49.16s] step time: 0.005423s (±0.001449s); valid time: 0.008369s; loss: 78.0659 (±15.8926); valid loss: 207.102\n",
      "[Epoch 178/350, Step 8000, ETA 48.51s] step time: 0.005525s (±0.001682s); valid time: 0.008804s; loss: 76.5746 (±3.15265); valid loss: 89.5412\n",
      "[Epoch 180/350, Step 8100, ETA 47.85s] step time: 0.005369s (±0.001214s); valid time: 0.008893s; loss: 76.949 (±4.6229); valid loss: 88.6931\n",
      "[Epoch 180/350, Step 8100, ETA 47.85s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 183/350, Step 8200, ETA 47.2s] step time: 0.005423s (±0.001215s); valid time: 0.008222s; loss: 76.2618 (±3.09776); valid loss: 89.6325\n",
      "[Epoch 185/350, Step 8300, ETA 46.54s] step time: 0.005432s (±0.001522s); valid time: 0.009281s; loss: 121.883 (±419.035); valid loss: 92.8484\n",
      "[Epoch 187/350, Step 8400, ETA 45.87s] step time: 0.005345s (±0.001263s); valid time: 0.008385s; loss: 81.8901 (±53.4626); valid loss: 88.9541\n",
      "[Epoch 189/350, Step 8500, ETA 45.23s] step time: 0.005582s (±0.001462s); valid time: 0.008071s; loss: 76.5021 (±3.00553); valid loss: 89.2131\n",
      "[Epoch 190/350, Step 8550, ETA 44.91s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8600, ETA 44.6s] step time: 0.00552s (±0.001775s); valid time: 0.008938s; loss: 76.8104 (±3.93596); valid loss: 88.8058\n",
      "[Epoch 194/350, Step 8700, ETA 43.95s] step time: 0.005505s (±0.001485s); valid time: 0.00847s; loss: 79.7329 (±31.6761); valid loss: 89.2012\n",
      "[Epoch 196/350, Step 8800, ETA 43.3s] step time: 0.005393s (±0.001312s); valid time: 0.009008s; loss: 84.9277 (±81.9771); valid loss: 89.3555\n",
      "[Epoch 198/350, Step 8900, ETA 42.67s] step time: 0.005671s (±0.00176s); valid time: 0.008624s; loss: 389.396 (±3114.53); valid loss: 89.054\n",
      "[Epoch 200/350, Step 9000, ETA 42.04s] step time: 0.005661s (±0.001503s); valid time: 0.00834s; loss: 152.726 (±755.758); valid loss: 89.0133\n",
      "[Epoch 200/350, Step 9000, ETA 42.04s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9100, ETA 41.4s] step time: 0.005408s (±0.001235s); valid time: 0.008974s; loss: 133.466 (±567.051); valid loss: 88.927\n",
      "[Epoch 205/350, Step 9200, ETA 40.75s] step time: 0.005464s (±0.001457s); valid time: 0.008676s; loss: 76.2739 (±2.79103); valid loss: 176.498\n",
      "[Epoch 207/350, Step 9300, ETA 40.11s] step time: 0.005432s (±0.001531s); valid time: 0.008901s; loss: 1152.02 (±10698.1); valid loss: 89.5661\n",
      "[Epoch 209/350, Step 9400, ETA 39.48s] step time: 0.005633s (±0.001392s); valid time: 0.008989s; loss: 133.586 (±565.019); valid loss: 89.1862\n",
      "[Epoch 210/350, Step 9450, ETA 39.16s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 212/350, Step 9500, ETA 38.86s] step time: 0.005741s (±0.001926s); valid time: 0.009332s; loss: 201544 (±1.94314e+06); valid loss: 95.338\n",
      "[Epoch 214/350, Step 9600, ETA 38.23s] step time: 0.005527s (±0.001165s); valid time: 0.008584s; loss: 76.5749 (±3.24745); valid loss: 92.2706\n",
      "[Epoch 216/350, Step 9700, ETA 37.6s] step time: 0.005676s (±0.001969s); valid time: 0.008428s; loss: 805.774 (±7256.81); valid loss: 88.8871\n",
      "[Epoch 218/350, Step 9800, ETA 36.95s] step time: 0.005226s (±0.001066s); valid time: 0.008123s; loss: 76.6397 (±3.25032); valid loss: 88.9694\n",
      "[Epoch 220/350, Step 9900, ETA 36.31s] step time: 0.005515s (±0.001828s); valid time: 0.008491s; loss: 83.998 (±67.9892); valid loss: 89.4182\n",
      "[Epoch 220/350, Step 9900, ETA 36.31s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 223/350, Step 10000, ETA 35.68s] step time: 0.005401s (±0.001556s); valid time: 0.008606s; loss: 575341 (±5.70574e+06); valid loss: 89.6228\n",
      "[Epoch 225/350, Step 10100, ETA 35.04s] step time: 0.005392s (±0.001287s); valid time: 0.008491s; loss: 76.5384 (±3.10224); valid loss: 89.8776\n",
      "[Epoch 227/350, Step 10200, ETA 34.41s] step time: 0.005475s (±0.001393s); valid time: 0.008803s; loss: 76.3257 (±3.31466); valid loss: 89.2675\n",
      "[Epoch 229/350, Step 10300, ETA 33.77s] step time: 0.005414s (±0.001482s); valid time: 0.008869s; loss: 76.5792 (±3.07784); valid loss: 90.0076\n",
      "[Epoch 230/350, Step 10350, ETA 33.46s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 232/350, Step 10400, ETA 33.15s] step time: 0.005576s (±0.001636s); valid time: 0.009129s; loss: 26729.4 (±265186); valid loss: 90.0432\n",
      "[Epoch 234/350, Step 10500, ETA 32.51s] step time: 0.005407s (±0.001217s); valid time: 0.008744s; loss: 77.7795 (±12.8537); valid loss: 88.7966\n",
      "[Epoch 236/350, Step 10600, ETA 31.89s] step time: 0.00564s (±0.001404s); valid time: 0.008479s; loss: 76.7891 (±3.04968); valid loss: 1212.09\n",
      "[Epoch 238/350, Step 10700, ETA 31.26s] step time: 0.005519s (±0.001598s); valid time: 0.008594s; loss: 200.975 (±1202.74); valid loss: 88.978\n",
      "[Epoch 240/350, Step 10800, ETA 30.62s] step time: 0.005344s (±0.001445s); valid time: 0.008664s; loss: 101957 (±1.01369e+06); valid loss: 88.7748\n",
      "[Epoch 240/350, Step 10800, ETA 30.62s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 243/350, Step 10900, ETA 29.99s] step time: 0.00538s (±0.001245s); valid time: 0.008323s; loss: 1.16036e+06 (±1.15446e+07); valid loss: 88.7317\n",
      "[Epoch 245/350, Step 11000, ETA 29.36s] step time: 0.005412s (±0.00142s); valid time: 0.008392s; loss: 76.6577 (±3.1455); valid loss: 89.3926\n",
      "[Epoch 247/350, Step 11100, ETA 28.74s] step time: 0.00569s (±0.002072s); valid time: 0.008423s; loss: 708.57 (±6264.51); valid loss: 89.5957\n",
      "[Epoch 249/350, Step 11200, ETA 28.11s] step time: 0.00554s (±0.001606s); valid time: 0.008507s; loss: 76.3941 (±3.01024); valid loss: 88.8015\n",
      "[Epoch 250/350, Step 11250, ETA 27.8s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 252/350, Step 11300, ETA 27.5s] step time: 0.005656s (±0.001855s); valid time: 0.008579s; loss: 136.462 (±595.46); valid loss: 225.147\n",
      "[Epoch 254/350, Step 11400, ETA 26.88s] step time: 0.005685s (±0.001822s); valid time: 0.008452s; loss: 96.653 (±199.694); valid loss: 88.8777\n",
      "[Epoch 256/350, Step 11500, ETA 26.25s] step time: 0.00536s (±0.001143s); valid time: 0.008684s; loss: 279684 (±2.78206e+06); valid loss: 88.552\n",
      "[Epoch 258/350, Step 11600, ETA 25.62s] step time: 0.005408s (±0.001299s); valid time: 0.008447s; loss: 2003.12 (±18120); valid loss: 255.135\n",
      "[Epoch 260/350, Step 11700, ETA 25s] step time: 0.005573s (±0.002378s); valid time: 0.01352s; loss: 222.388 (±1435.34); valid loss: 88.4547\n",
      "[Epoch 260/350, Step 11700, ETA 25s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 263/350, Step 11800, ETA 24.37s] step time: 0.005345s (±0.001415s); valid time: 0.008559s; loss: 76.4712 (±3.169); valid loss: 89.4063\n",
      "[Epoch 265/350, Step 11900, ETA 23.75s] step time: 0.005364s (±0.001105s); valid time: 0.008442s; loss: 76.9437 (±5.10905); valid loss: 89.1396\n",
      "[Epoch 267/350, Step 12000, ETA 23.12s] step time: 0.005443s (±0.001574s); valid time: 0.008383s; loss: 76.5999 (±3.75081); valid loss: 89.795\n",
      "[Epoch 269/350, Step 12100, ETA 22.5s] step time: 0.005476s (±0.001493s); valid time: 0.008306s; loss: 76.781 (±3.37761); valid loss: 88.9481\n",
      "[Epoch 270/350, Step 12150, ETA 22.18s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12200, ETA 21.88s] step time: 0.005456s (±0.001705s); valid time: 0.008707s; loss: 94.533 (±181.699); valid loss: 89.5566\n",
      "[Epoch 274/350, Step 12300, ETA 21.25s] step time: 0.005481s (±0.00164s); valid time: 0.008386s; loss: 285.14 (±2068.87); valid loss: 89.0368\n",
      "[Epoch 276/350, Step 12400, ETA 20.64s] step time: 0.005626s (±0.001437s); valid time: 0.008832s; loss: 84.4659 (±54.0669); valid loss: 89.3925\n",
      "[Epoch 278/350, Step 12500, ETA 20.01s] step time: 0.005426s (±0.001401s); valid time: 0.008899s; loss: 84.3428 (±76.6589); valid loss: 129.82\n",
      "[Epoch 280/350, Step 12600, ETA 19.39s] step time: 0.005489s (±0.001668s); valid time: 0.008678s; loss: 76.2673 (±2.90727); valid loss: 90.4052\n",
      "[Epoch 280/350, Step 12600, ETA 19.39s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 283/350, Step 12700, ETA 18.77s] step time: 0.005362s (±0.001293s); valid time: 0.01059s; loss: 76.5407 (±3.40665); valid loss: 90.1577\n",
      "[Epoch 285/350, Step 12800, ETA 18.15s] step time: 0.005393s (±0.001759s); valid time: 0.008632s; loss: 561896 (±5.59003e+06); valid loss: 88.9788\n",
      "[Epoch 287/350, Step 12900, ETA 17.53s] step time: 0.005443s (±0.002082s); valid time: 0.01436s; loss: 76.2246 (±3.37222); valid loss: 89.3935\n",
      "[Epoch 289/350, Step 13000, ETA 16.9s] step time: 0.005206s (±0.001039s); valid time: 0.008263s; loss: 76.6528 (±3.20274); valid loss: 89.2424\n",
      "[Epoch 290/350, Step 13050, ETA 16.59s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13100, ETA 16.29s] step time: 0.005579s (±0.001802s); valid time: 0.008478s; loss: 1087.02 (±10052.6); valid loss: 92.7583\n",
      "[Epoch 294/350, Step 13200, ETA 15.67s] step time: 0.005427s (±0.001306s); valid time: 0.008756s; loss: 84.9243 (±80.9453); valid loss: 90.3621\n",
      "[Epoch 296/350, Step 13300, ETA 15.05s] step time: 0.005538s (±0.001588s); valid time: 0.01144s; loss: 76.4907 (±3.44444); valid loss: 88.5165\n",
      "[Epoch 298/350, Step 13400, ETA 14.43s] step time: 0.005434s (±0.001272s); valid time: 0.008908s; loss: 95.375 (±165.876); valid loss: 88.4694\n",
      "[Epoch 300/350, Step 13500, ETA 13.82s] step time: 0.005516s (±0.001352s); valid time: 0.008268s; loss: 77.6873 (±9.82262); valid loss: 88.9688\n",
      "[Epoch 300/350, Step 13500, ETA 13.82s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13600, ETA 13.2s] step time: 0.00564s (±0.002018s); valid time: 0.008729s; loss: 77.5658 (±6.54101); valid loss: 88.7378\n",
      "[Epoch 305/350, Step 13700, ETA 12.59s] step time: 0.005403s (±0.00142s); valid time: 0.008598s; loss: 77.6783 (±12.1543); valid loss: 165.758\n",
      "[Epoch 307/350, Step 13800, ETA 11.97s] step time: 0.005475s (±0.001521s); valid time: 0.008756s; loss: 1382.88 (±9614.57); valid loss: 95.1086\n",
      "[Epoch 309/350, Step 13900, ETA 11.35s] step time: 0.005449s (±0.001278s); valid time: 0.008629s; loss: 76.499 (±3.15321); valid loss: 89.0549\n",
      "[Epoch 310/350, Step 13950, ETA 11.04s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 312/350, Step 14000, ETA 10.74s] step time: 0.005747s (±0.001849s); valid time: 0.009107s; loss: 350.46 (±2286.38); valid loss: 89.1806\n",
      "[Epoch 314/350, Step 14100, ETA 10.13s] step time: 0.005601s (±0.001345s); valid time: 0.008484s; loss: 266.239 (±1788.7); valid loss: 88.9393\n",
      "[Epoch 316/350, Step 14200, ETA 9.509s] step time: 0.005312s (±0.001268s); valid time: 0.008242s; loss: 1144 (±10606.1); valid loss: 1128.75\n",
      "[Epoch 318/350, Step 14300, ETA 8.894s] step time: 0.005507s (±0.001552s); valid time: 0.008508s; loss: 76.5479 (±2.65401); valid loss: 88.9675\n",
      "[Epoch 320/350, Step 14400, ETA 8.278s] step time: 0.005339s (±0.001353s); valid time: 0.008474s; loss: 83.7648 (±66.5754); valid loss: 89.7123\n",
      "[Epoch 320/350, Step 14400, ETA 8.278s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 323/350, Step 14500, ETA 7.664s] step time: 0.005489s (±0.00147s); valid time: 0.00999s; loss: 91.9653 (±149.676); valid loss: 89.395\n",
      "[Epoch 325/350, Step 14600, ETA 7.052s] step time: 0.005802s (±0.001647s); valid time: 0.008439s; loss: 750.204 (±6704.69); valid loss: 89.2048\n",
      "[Epoch 327/350, Step 14700, ETA 6.437s] step time: 0.005435s (±0.001613s); valid time: 0.008508s; loss: 76.7791 (±3.30605); valid loss: 88.7308\n",
      "[Epoch 329/350, Step 14800, ETA 5.823s] step time: 0.005436s (±0.001692s); valid time: 0.008433s; loss: 76.7633 (±5.68772); valid loss: 89.6235\n",
      "[Epoch 330/350, Step 14850, ETA 5.517s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 332/350, Step 14900, ETA 5.21s] step time: 0.005634s (±0.001904s); valid time: 0.008414s; loss: 76.6691 (±3.4389); valid loss: 95.5829\n",
      "[Epoch 334/350, Step 15000, ETA 4.596s] step time: 0.005387s (±0.001455s); valid time: 0.008554s; loss: 76.6142 (±3.02837); valid loss: 89.096\n",
      "[Epoch 336/350, Step 15100, ETA 3.982s] step time: 0.005437s (±0.001503s); valid time: 0.008793s; loss: 907.46 (±6751.33); valid loss: 92.3571\n",
      "[Epoch 338/350, Step 15200, ETA 3.369s] step time: 0.005519s (±0.001443s); valid time: 0.008747s; loss: 90.0455 (±133.16); valid loss: 89.8931\n",
      "[Epoch 340/350, Step 15300, ETA 2.756s] step time: 0.005438s (±0.001382s); valid time: 0.008509s; loss: 427203 (±4.2092e+06); valid loss: 88.8769\n",
      "[Epoch 340/350, Step 15300, ETA 2.756s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 343/350, Step 15400, ETA 2.143s] step time: 0.005505s (±0.001713s); valid time: 0.009019s; loss: 91.9375 (±155.203); valid loss: 88.8995\n",
      "[Epoch 345/350, Step 15500, ETA 1.531s] step time: 0.005575s (±0.001446s); valid time: 0.009801s; loss: 194.556 (±1032.43); valid loss: 90.2743\n",
      "[Epoch 347/350, Step 15600, ETA 0.9183s] step time: 0.005474s (±0.001371s); valid time: 0.009073s; loss: 76.2044 (±3.58031); valid loss: 89.818\n",
      "[Epoch 349/350, Step 15700, ETA 0.3061s] step time: 0.005715s (±0.00148s); valid time: 0.008663s; loss: 158.819 (±818.538); valid loss: 92.3783\n",
      "[Epoch 350/350, Step 15750, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsmhktnx5/variables.dat-6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsmhktnx5/variables.dat-6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.72,\n",
      "\t(tp, fp, tn, fn)=(732, 3647, 10604, 848),\n",
      "\tprecision=0.17,\n",
      "\trecall=0.46,\n",
      "\tf1=0.25,\n",
      "\troc_auc=0.6,\n",
      "\ty_pred%=0.2766091845114017,\n",
      "\ty_label%=0.09980418166887751,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CRM.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 50.52s] step time: 0.01007s (±0.03355s); valid time: 0.1706s; loss: 122.119 (±12.9536); valid loss: 112.5 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 16.93s] step time: 0.006048s (±0.007581s); valid time: 0.07569s; loss: 100.748 (±3.90206); valid loss: 103.483 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 7.628s] step time: 0.006429s (±0.008533s); valid time: 0.08506s; loss: 97.6625 (±3.33374); valid loss: 101.96 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 1.666s] step time: 0.006289s (±0.009089s); valid time: 0.08965s; loss: 95.1451 (±3.70766); valid loss: 98.1706 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 57.5s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 57.96s] step time: 0.006337s (±0.008379s); valid time: 0.08279s; loss: 91.1476 (±3.27087); valid loss: 95.5446 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 55.93s] step time: 0.006429s (±0.009869s); valid time: 0.0963s; loss: 89.7883 (±3.65104); valid loss: 94.9507 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 53.55s] step time: 0.00626s (±0.008268s); valid time: 0.08247s; loss: 88.992 (±3.41422); valid loss: 94.7136 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 51.38s] step time: 0.006072s (±0.008584s); valid time: 0.08614s; loss: 88.7094 (±3.13279); valid loss: 94.4238 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 49.9s] step time: 0.006376s (±0.008623s); valid time: 0.08587s; loss: 88.4365 (±3.51353); valid loss: 94.3317 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 49.21s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 47.47s] step time: 0.005636s (±0.001601s); valid time: 0.00835s; loss: 88.1413 (±3.39777); valid loss: 94.4314\n",
      "[Epoch 24/350, Step 1100, ETA 1m 46.13s] step time: 0.006207s (±0.00813s); valid time: 0.0812s; loss: 87.9286 (±3.06171); valid loss: 94.2356 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.06s] step time: 0.0062s (±0.008223s); valid time: 0.08206s; loss: 87.8004 (±3.55075); valid loss: 94.151 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.12s] step time: 0.00639s (±0.008738s); valid time: 0.08637s; loss: 87.4561 (±3.1201); valid loss: 93.8318 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 42.48s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 43.18s] step time: 0.006334s (±0.009348s); valid time: 0.09277s; loss: 87.3224 (±3.26622); valid loss: 93.7419 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 42.56s] step time: 0.006603s (±0.008516s); valid time: 0.08498s; loss: 87.3256 (±3.12633); valid loss: 93.7384 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 41.1s] step time: 0.005667s (±0.001451s); valid time: 0.008761s; loss: 86.9303 (±3.7104); valid loss: 94.0846\n",
      "[Epoch 37/350, Step 1700, ETA 1m 39.75s] step time: 0.005703s (±0.001494s); valid time: 0.01178s; loss: 86.7823 (±3.22154); valid loss: 93.8499\n",
      "[Epoch 40/350, Step 1800, ETA 1m 38.92s] step time: 0.006195s (±0.008428s); valid time: 0.08379s; loss: 86.7753 (±3.61891); valid loss: 93.5412 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 38.3s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 37.65s] step time: 0.005654s (±0.001964s); valid time: 0.008517s; loss: 86.6896 (±3.54798); valid loss: 93.5487\n",
      "[Epoch 44/350, Step 2000, ETA 1m 36.33s] step time: 0.005499s (±0.001498s); valid time: 0.008654s; loss: 86.4782 (±3.25116); valid loss: 93.567\n",
      "[Epoch 46/350, Step 2100, ETA 1m 35.08s] step time: 0.00551s (±0.001565s); valid time: 0.008202s; loss: 86.6461 (±3.22482); valid loss: 93.6789\n",
      "[Epoch 48/350, Step 2200, ETA 1m 33.79s] step time: 0.005359s (±0.001141s); valid time: 0.008341s; loss: 86.2927 (±3.55054); valid loss: 93.6672\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.16s] step time: 0.006355s (±0.008771s); valid time: 0.08748s; loss: 86.4138 (±3.69193); valid loss: 93.5078 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.17s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 32.09s] step time: 0.005479s (±0.001465s); valid time: 0.008696s; loss: 86.1396 (±3.67412); valid loss: 94.0408\n",
      "[Epoch 55/350, Step 2500, ETA 1m 31.16s] step time: 0.005756s (±0.001491s); valid time: 0.008435s; loss: 86.0136 (±3.59363); valid loss: 93.6512\n",
      "[Epoch 57/350, Step 2600, ETA 1m 30.15s] step time: 0.00557s (±0.0015s); valid time: 0.009737s; loss: 86.2574 (±3.29296); valid loss: 93.608\n",
      "[Epoch 59/350, Step 2700, ETA 1m 29.45s] step time: 0.006134s (±0.008462s); valid time: 0.08455s; loss: 86.1018 (±3.37037); valid loss: 93.4022 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 28.88s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 28.55s] step time: 0.005712s (±0.001704s); valid time: 0.008626s; loss: 85.8562 (±3.93021); valid loss: 93.616\n",
      "[Epoch 64/350, Step 2900, ETA 1m 27.74s] step time: 0.005741s (±0.001495s); valid time: 0.008636s; loss: 86.0705 (±3.35593); valid loss: 93.6673\n",
      "[Epoch 66/350, Step 3000, ETA 1m 26.77s] step time: 0.005501s (±0.001363s); valid time: 0.008544s; loss: 85.7646 (±3.61182); valid loss: 93.5738\n",
      "[Epoch 68/350, Step 3100, ETA 1m 25.88s] step time: 0.005466s (±0.001168s); valid time: 0.007835s; loss: 85.9461 (±3.34578); valid loss: 93.8175\n",
      "[Epoch 70/350, Step 3200, ETA 1m 24.91s] step time: 0.005389s (±0.001474s); valid time: 0.008615s; loss: 133.236 (±474.939); valid loss: 93.9119\n",
      "[Epoch 70/350, Step 3220, ETA 1m 24.72s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 24.03s] step time: 0.005519s (±0.001664s); valid time: 0.008254s; loss: 85.7757 (±3.11222); valid loss: 93.6193\n",
      "[Epoch 74/350, Step 3400, ETA 1m 23.21s] step time: 0.005668s (±0.001711s); valid time: 0.008411s; loss: 85.8828 (±3.36942); valid loss: 93.7709\n",
      "[Epoch 77/350, Step 3500, ETA 1m 22.33s] step time: 0.005357s (±0.001305s); valid time: 0.008619s; loss: 85.8613 (±3.28021); valid loss: 93.9423\n",
      "[Epoch 79/350, Step 3600, ETA 1m 21.45s] step time: 0.00543s (±0.001444s); valid time: 0.007931s; loss: 85.8685 (±3.6898); valid loss: 94.6437\n",
      "[Epoch 80/350, Step 3680, ETA 1m 20.77s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 20.65s] step time: 0.005569s (±0.001595s); valid time: 0.008377s; loss: 86.2367 (±6.58853); valid loss: 93.5871\n",
      "[Epoch 83/350, Step 3800, ETA 1m 19.85s] step time: 0.005586s (±0.001586s); valid time: 0.008559s; loss: 85.5437 (±3.54592); valid loss: 93.9523\n",
      "[Epoch 85/350, Step 3900, ETA 1m 19.03s] step time: 0.00548s (±0.0014s); valid time: 0.00846s; loss: 85.6667 (±3.22786); valid loss: 93.7038\n",
      "[Epoch 87/350, Step 4000, ETA 1m 18.21s] step time: 0.005456s (±0.001302s); valid time: 0.008363s; loss: 85.5399 (±3.76386); valid loss: 93.8138\n",
      "[Epoch 90/350, Step 4100, ETA 1m 17.47s] step time: 0.005581s (±0.001577s); valid time: 0.008226s; loss: 85.6347 (±3.69238); valid loss: 93.971\n",
      "[Epoch 90/350, Step 4140, ETA 1m 17.13s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 16.74s] step time: 0.005684s (±0.001884s); valid time: 0.007836s; loss: 85.5781 (±2.97539); valid loss: 93.9683\n",
      "[Epoch 94/350, Step 4300, ETA 1m 15.98s] step time: 0.00556s (±0.001702s); valid time: 0.01171s; loss: 85.6751 (±3.46694); valid loss: 93.7393\n",
      "[Epoch 96/350, Step 4400, ETA 1m 15.25s] step time: 0.005647s (±0.00176s); valid time: 0.00851s; loss: 85.2349 (±3.28813); valid loss: 93.9425\n",
      "[Epoch 98/350, Step 4500, ETA 1m 14.45s] step time: 0.005381s (±0.001521s); valid time: 0.01042s; loss: 86.3114 (±8.67782); valid loss: 93.8048\n",
      "[Epoch 100/350, Step 4600, ETA 1m 13.67s] step time: 0.005384s (±0.001404s); valid time: 0.008307s; loss: 87.6564 (±22.1256); valid loss: 93.8868\n",
      "[Epoch 100/350, Step 4600, ETA 1m 13.67s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 12.93s] step time: 0.005421s (±0.001329s); valid time: 0.008557s; loss: 86.0501 (±7.07058); valid loss: 94.8718\n",
      "[Epoch 105/350, Step 4800, ETA 1m 12.19s] step time: 0.005524s (±0.001453s); valid time: 0.008347s; loss: 85.4574 (±3.86316); valid loss: 93.7883\n",
      "[Epoch 107/350, Step 4900, ETA 1m 11.41s] step time: 0.005302s (±0.00131s); valid time: 0.008019s; loss: 85.6092 (±4.05965); valid loss: 93.8098\n",
      "[Epoch 109/350, Step 5000, ETA 1m 10.67s] step time: 0.005472s (±0.001474s); valid time: 0.008102s; loss: 85.568 (±3.73811); valid loss: 94.0121\n",
      "[Epoch 110/350, Step 5060, ETA 1m 10.23s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 9.983s] step time: 0.005619s (±0.00147s); valid time: 0.00852s; loss: 85.3043 (±3.43013); valid loss: 93.8742\n",
      "[Epoch 114/350, Step 5200, ETA 1m 9.281s] step time: 0.005449s (±0.001209s); valid time: 0.008649s; loss: 124.704 (±390.324); valid loss: 94.2623\n",
      "[Epoch 116/350, Step 5300, ETA 1m 8.571s] step time: 0.005538s (±0.001499s); valid time: 0.008361s; loss: 85.5003 (±3.8211); valid loss: 93.8925\n",
      "[Epoch 118/350, Step 5400, ETA 1m 7.828s] step time: 0.005298s (±0.001071s); valid time: 0.008262s; loss: 85.3868 (±3.59403); valid loss: 93.9182\n",
      "[Epoch 120/350, Step 5500, ETA 1m 7.117s] step time: 0.005484s (±0.001351s); valid time: 0.008793s; loss: 85.6724 (±4.64609); valid loss: 94.1942\n",
      "[Epoch 120/350, Step 5520, ETA 1m 6.957s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 6.434s] step time: 0.00557s (±0.001377s); valid time: 0.008261s; loss: 85.6073 (±3.73056); valid loss: 94.0521\n",
      "[Epoch 124/350, Step 5700, ETA 1m 5.731s] step time: 0.005449s (±0.001417s); valid time: 0.008072s; loss: 85.2026 (±3.44857); valid loss: 95.1051\n",
      "[Epoch 127/350, Step 5800, ETA 1m 5.036s] step time: 0.005383s (±0.001424s); valid time: 0.008468s; loss: 91.1228 (±57.3844); valid loss: 93.8711\n",
      "[Epoch 129/350, Step 5900, ETA 1m 4.324s] step time: 0.005389s (±0.001197s); valid time: 0.007964s; loss: 85.5888 (±5.09896); valid loss: 93.9599\n",
      "[Epoch 130/350, Step 5980, ETA 1m 3.738s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 3.617s] step time: 0.005403s (±0.001988s); valid time: 0.008026s; loss: 86.1702 (±5.83139); valid loss: 94.1115\n",
      "[Epoch 133/350, Step 6100, ETA 1m 2.929s] step time: 0.00549s (±0.0012s); valid time: 0.008228s; loss: 85.2093 (±3.89969); valid loss: 93.8192\n",
      "[Epoch 135/350, Step 6200, ETA 1m 2.254s] step time: 0.005553s (±0.001366s); valid time: 0.008352s; loss: 86.4688 (±10.6762); valid loss: 94.2014\n",
      "[Epoch 137/350, Step 6300, ETA 1m 1.596s] step time: 0.005645s (±0.001498s); valid time: 0.008355s; loss: 152.468 (±578.703); valid loss: 94.4782\n",
      "[Epoch 140/350, Step 6400, ETA 1m 0.9389s] step time: 0.005535s (±0.001503s); valid time: 0.008814s; loss: 85.3886 (±4.11703); valid loss: 93.9232\n",
      "[Epoch 140/350, Step 6440, ETA 1m 0.6574s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 0.2838s] step time: 0.005622s (±0.001762s); valid time: 0.008364s; loss: 85.6549 (±5.15034); valid loss: 93.97\n",
      "[Epoch 144/350, Step 6600, ETA 59.6s] step time: 0.005432s (±0.001615s); valid time: 0.008322s; loss: 85.0935 (±3.28309); valid loss: 93.9443\n",
      "[Epoch 146/350, Step 6700, ETA 58.94s] step time: 0.005554s (±0.001821s); valid time: 0.008366s; loss: 85.6323 (±3.43537); valid loss: 95.7802\n",
      "[Epoch 148/350, Step 6800, ETA 58.28s] step time: 0.005563s (±0.001456s); valid time: 0.009874s; loss: 149.939 (±644.613); valid loss: 94.1652\n",
      "[Epoch 150/350, Step 6900, ETA 57.61s] step time: 0.005471s (±0.001407s); valid time: 0.008304s; loss: 85.3237 (±3.71568); valid loss: 94.57\n",
      "[Epoch 150/350, Step 6900, ETA 57.61s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 56.96s] step time: 0.005517s (±0.001482s); valid time: 0.008488s; loss: 98.4242 (±122.467); valid loss: 94.4633\n",
      "[Epoch 155/350, Step 7100, ETA 56.32s] step time: 0.005647s (±0.001183s); valid time: 0.008271s; loss: 85.3621 (±3.57834); valid loss: 94.3125\n",
      "[Epoch 157/350, Step 7200, ETA 55.64s] step time: 0.005424s (±0.001296s); valid time: 0.008001s; loss: 85.682 (±5.02601); valid loss: 94.2259\n",
      "[Epoch 159/350, Step 7300, ETA 54.97s] step time: 0.005394s (±0.001426s); valid time: 0.008132s; loss: 85.4608 (±3.95233); valid loss: 94.1764\n",
      "[Epoch 160/350, Step 7360, ETA 54.56s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 54.31s] step time: 0.005502s (±0.001561s); valid time: 0.00799s; loss: 85.2366 (±3.40611); valid loss: 94.2727\n",
      "[Epoch 164/350, Step 7500, ETA 53.67s] step time: 0.005511s (±0.001527s); valid time: 0.008591s; loss: 87.0369 (±16.6375); valid loss: 94.0859\n",
      "[Epoch 166/350, Step 7600, ETA 53s] step time: 0.005377s (±0.001199s); valid time: 0.008405s; loss: 85.1521 (±3.00898); valid loss: 93.9922\n",
      "[Epoch 168/350, Step 7700, ETA 52.34s] step time: 0.005301s (±0.001024s); valid time: 0.008447s; loss: 85.3907 (±3.08088); valid loss: 94.1463\n",
      "[Epoch 170/350, Step 7800, ETA 51.69s] step time: 0.005487s (±0.001535s); valid time: 0.008307s; loss: 89.0961 (±40.1718); valid loss: 94.0812\n",
      "[Epoch 170/350, Step 7820, ETA 51.56s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 51.04s] step time: 0.005454s (±0.001688s); valid time: 0.008381s; loss: 85.4862 (±4.26524); valid loss: 94.2406\n",
      "[Epoch 174/350, Step 8000, ETA 50.39s] step time: 0.005501s (±0.001692s); valid time: 0.00837s; loss: 85.4829 (±3.54646); valid loss: 94.8131\n",
      "[Epoch 177/350, Step 8100, ETA 49.75s] step time: 0.005498s (±0.001303s); valid time: 0.008284s; loss: 87.6921 (±17.8747); valid loss: 94.2415\n",
      "[Epoch 179/350, Step 8200, ETA 49.1s] step time: 0.005497s (±0.001378s); valid time: 0.008483s; loss: 86.3546 (±10.3858); valid loss: 94.098\n",
      "[Epoch 180/350, Step 8280, ETA 48.56s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 48.45s] step time: 0.005365s (±0.00106s); valid time: 0.007788s; loss: 85.4246 (±4.41699); valid loss: 93.8768\n",
      "[Epoch 183/350, Step 8400, ETA 47.82s] step time: 0.005698s (±0.001611s); valid time: 0.008706s; loss: 85.1607 (±3.91217); valid loss: 94.2787\n",
      "[Epoch 185/350, Step 8500, ETA 47.16s] step time: 0.005381s (±0.00134s); valid time: 0.008312s; loss: 86.6414 (±14.5788); valid loss: 94.3894\n",
      "[Epoch 187/350, Step 8600, ETA 46.51s] step time: 0.005363s (±0.001343s); valid time: 0.008349s; loss: 85.4078 (±3.48373); valid loss: 94.2272\n",
      "[Epoch 190/350, Step 8700, ETA 45.88s] step time: 0.005531s (±0.001624s); valid time: 0.008279s; loss: 89.8869 (±47.4941); valid loss: 93.9727\n",
      "[Epoch 190/350, Step 8740, ETA 45.61s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 45.24s] step time: 0.005521s (±0.001643s); valid time: 0.007893s; loss: 85.199 (±3.53954); valid loss: 94.0584\n",
      "[Epoch 194/350, Step 8900, ETA 44.61s] step time: 0.005526s (±0.001603s); valid time: 0.008264s; loss: 85.4819 (±3.38518); valid loss: 93.9292\n",
      "[Epoch 196/350, Step 9000, ETA 43.96s] step time: 0.005366s (±0.001071s); valid time: 0.007923s; loss: 85.3273 (±3.39307); valid loss: 94.0761\n",
      "[Epoch 198/350, Step 9100, ETA 43.32s] step time: 0.005494s (±0.001618s); valid time: 0.008268s; loss: 85.4477 (±3.55586); valid loss: 93.9472\n",
      "[Epoch 200/350, Step 9200, ETA 42.68s] step time: 0.005445s (±0.00154s); valid time: 0.008571s; loss: 85.2843 (±3.79748); valid loss: 94.696\n",
      "[Epoch 200/350, Step 9200, ETA 42.68s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 42.04s] step time: 0.005326s (±0.001111s); valid time: 0.008277s; loss: 85.9865 (±7.23759); valid loss: 95.023\n",
      "[Epoch 205/350, Step 9400, ETA 41.41s] step time: 0.005423s (±0.001271s); valid time: 0.008314s; loss: 95.0663 (±98.182); valid loss: 93.7852\n",
      "[Epoch 207/350, Step 9500, ETA 40.76s] step time: 0.005335s (±0.001134s); valid time: 0.008155s; loss: 85.1937 (±3.46605); valid loss: 94.0032\n",
      "[Epoch 209/350, Step 9600, ETA 40.12s] step time: 0.005438s (±0.001543s); valid time: 0.008301s; loss: 85.5915 (±4.96071); valid loss: 93.9259\n",
      "[Epoch 210/350, Step 9660, ETA 39.73s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 39.49s] step time: 0.00545s (±0.002012s); valid time: 0.01592s; loss: 85.1709 (±3.47169); valid loss: 94.3243\n",
      "[Epoch 214/350, Step 9800, ETA 38.86s] step time: 0.00532s (±0.001397s); valid time: 0.008236s; loss: 85.2674 (±3.65005); valid loss: 94.1976\n",
      "[Epoch 216/350, Step 9900, ETA 38.23s] step time: 0.005524s (±0.001411s); valid time: 0.008119s; loss: 85.2732 (±3.74596); valid loss: 93.9319\n",
      "[Epoch 218/350, Step 10000, ETA 37.59s] step time: 0.005248s (±0.001054s); valid time: 0.008344s; loss: 85.2779 (±3.59001); valid loss: 94.4266\n",
      "[Epoch 220/350, Step 10100, ETA 36.96s] step time: 0.005575s (±0.001608s); valid time: 0.007885s; loss: 85.3442 (±3.65489); valid loss: 94.0655\n",
      "[Epoch 220/350, Step 10120, ETA 36.83s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 36.33s] step time: 0.00542s (±0.001368s); valid time: 0.007865s; loss: 85.9412 (±4.54136); valid loss: 94.3352\n",
      "[Epoch 224/350, Step 10300, ETA 35.71s] step time: 0.005628s (±0.001447s); valid time: 0.008294s; loss: 85.9353 (±5.10001); valid loss: 94.3114\n",
      "[Epoch 227/350, Step 10400, ETA 35.09s] step time: 0.005478s (±0.001622s); valid time: 0.008153s; loss: 86.9319 (±11.6237); valid loss: 94.6671\n",
      "[Epoch 229/350, Step 10500, ETA 34.46s] step time: 0.005409s (±0.001485s); valid time: 0.008475s; loss: 103.025 (±161.518); valid loss: 93.8434\n",
      "[Epoch 230/350, Step 10580, ETA 33.97s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 33.85s] step time: 0.005603s (±0.001251s); valid time: 0.008234s; loss: 90.3513 (±45.1874); valid loss: 94.203\n",
      "[Epoch 233/350, Step 10700, ETA 33.22s] step time: 0.005386s (±0.001355s); valid time: 0.008207s; loss: 85.2573 (±3.39847); valid loss: 94.1338\n",
      "[Epoch 235/350, Step 10800, ETA 32.59s] step time: 0.005531s (±0.001288s); valid time: 0.008537s; loss: 85.2752 (±3.64536); valid loss: 94.8187\n",
      "[Epoch 237/350, Step 10900, ETA 31.97s] step time: 0.005547s (±0.001578s); valid time: 0.008274s; loss: 88.5667 (±22.4172); valid loss: 94.0856\n",
      "[Epoch 240/350, Step 11000, ETA 31.35s] step time: 0.005333s (±0.0015s); valid time: 0.008359s; loss: 87.3547 (±20.8489); valid loss: 95.9971\n",
      "[Epoch 240/350, Step 11040, ETA 31.09s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 30.73s] step time: 0.005673s (±0.001916s); valid time: 0.008253s; loss: 85.3455 (±3.85818); valid loss: 94.4233\n",
      "[Epoch 244/350, Step 11200, ETA 30.1s] step time: 0.005383s (±0.001344s); valid time: 0.008198s; loss: 85.2145 (±3.49414); valid loss: 94.0731\n",
      "[Epoch 246/350, Step 11300, ETA 29.48s] step time: 0.005542s (±0.001778s); valid time: 0.008337s; loss: 107.026 (±213.426); valid loss: 94.0545\n",
      "[Epoch 248/350, Step 11400, ETA 28.85s] step time: 0.005317s (±0.001195s); valid time: 0.008068s; loss: 85.223 (±3.37531); valid loss: 94.0859\n",
      "[Epoch 250/350, Step 11500, ETA 28.24s] step time: 0.005646s (±0.001508s); valid time: 0.008822s; loss: 85.2527 (±3.56733); valid loss: 94.1484\n",
      "[Epoch 250/350, Step 11500, ETA 28.24s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 27.62s] step time: 0.005497s (±0.001508s); valid time: 0.008233s; loss: 85.2937 (±3.45718); valid loss: 94.6558\n",
      "[Epoch 255/350, Step 11700, ETA 27s] step time: 0.005373s (±0.001436s); valid time: 0.008534s; loss: 147.786 (±551.935); valid loss: 94.1834\n",
      "[Epoch 257/350, Step 11800, ETA 26.38s] step time: 0.005623s (±0.001604s); valid time: 0.008661s; loss: 85.1707 (±3.58369); valid loss: 94.1218\n",
      "[Epoch 259/350, Step 11900, ETA 25.76s] step time: 0.005548s (±0.00136s); valid time: 0.008069s; loss: 85.396 (±3.88865); valid loss: 94.704\n",
      "[Epoch 260/350, Step 11960, ETA 25.39s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.14s] step time: 0.005449s (±0.001627s); valid time: 0.008159s; loss: 85.633 (±5.01483); valid loss: 93.9358\n",
      "[Epoch 264/350, Step 12100, ETA 24.53s] step time: 0.005594s (±0.001468s); valid time: 0.008405s; loss: 85.181 (±3.63579); valid loss: 94.5602\n",
      "[Epoch 266/350, Step 12200, ETA 23.91s] step time: 0.00531s (±0.001043s); valid time: 0.008415s; loss: 85.0479 (±3.38296); valid loss: 94.1424\n",
      "[Epoch 268/350, Step 12300, ETA 23.29s] step time: 0.005529s (±0.001302s); valid time: 0.008741s; loss: 98.9431 (±135.022); valid loss: 94.0191\n",
      "[Epoch 270/350, Step 12400, ETA 22.68s] step time: 0.005573s (±0.001425s); valid time: 0.008368s; loss: 89.5098 (±43.6094); valid loss: 94.1735\n",
      "[Epoch 270/350, Step 12420, ETA 22.55s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.06s] step time: 0.005528s (±0.001487s); valid time: 0.008575s; loss: 94.742 (±59.5955); valid loss: 94.1274\n",
      "[Epoch 274/350, Step 12600, ETA 21.44s] step time: 0.005524s (±0.001603s); valid time: 0.01032s; loss: 89.4735 (±41.5329); valid loss: 94.4484\n",
      "[Epoch 277/350, Step 12700, ETA 20.83s] step time: 0.00535s (±0.001422s); valid time: 0.008134s; loss: 85.17 (±3.61376); valid loss: 94.0836\n",
      "[Epoch 279/350, Step 12800, ETA 20.21s] step time: 0.005648s (±0.001727s); valid time: 0.007979s; loss: 85.2587 (±3.56784); valid loss: 94.9729\n",
      "[Epoch 280/350, Step 12880, ETA 19.71s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.6s] step time: 0.005527s (±0.001234s); valid time: 0.007891s; loss: 85.1455 (±3.67812); valid loss: 94.4275\n",
      "[Epoch 283/350, Step 13000, ETA 18.98s] step time: 0.005593s (±0.00129s); valid time: 0.008006s; loss: 95.6904 (±96.7086); valid loss: 95.4497\n",
      "[Epoch 285/350, Step 13100, ETA 18.37s] step time: 0.005521s (±0.00176s); valid time: 0.01384s; loss: 85.2554 (±4.6716); valid loss: 94.0504\n",
      "[Epoch 287/350, Step 13200, ETA 17.75s] step time: 0.005447s (±0.001457s); valid time: 0.00858s; loss: 85.3523 (±4.13465); valid loss: 94.1864\n",
      "[Epoch 290/350, Step 13300, ETA 17.14s] step time: 0.005555s (±0.001496s); valid time: 0.007941s; loss: 85.3868 (±4.31163); valid loss: 94.0271\n",
      "[Epoch 290/350, Step 13340, ETA 16.89s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.53s] step time: 0.005632s (±0.001631s); valid time: 0.008177s; loss: 89.0238 (±34.6953); valid loss: 94.1757\n",
      "[Epoch 294/350, Step 13500, ETA 15.91s] step time: 0.005501s (±0.001704s); valid time: 0.008857s; loss: 85.3644 (±3.69828); valid loss: 94.3817\n",
      "[Epoch 296/350, Step 13600, ETA 15.3s] step time: 0.005472s (±0.001407s); valid time: 0.008347s; loss: 88.231 (±28.8133); valid loss: 95.6165\n",
      "[Epoch 298/350, Step 13700, ETA 14.68s] step time: 0.005579s (±0.001573s); valid time: 0.008408s; loss: 85.2835 (±3.8038); valid loss: 94.1449\n",
      "[Epoch 300/350, Step 13800, ETA 14.07s] step time: 0.005475s (±0.001288s); valid time: 0.008339s; loss: 85.2268 (±3.45513); valid loss: 95.0813\n",
      "[Epoch 300/350, Step 13800, ETA 14.07s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.45s] step time: 0.005413s (±0.001298s); valid time: 0.007939s; loss: 85.3078 (±3.69992); valid loss: 94.3536\n",
      "[Epoch 305/350, Step 14000, ETA 12.84s] step time: 0.005518s (±0.001469s); valid time: 0.008561s; loss: 86.1498 (±9.17237); valid loss: 94.1278\n",
      "[Epoch 307/350, Step 14100, ETA 12.22s] step time: 0.005313s (±0.001132s); valid time: 0.007946s; loss: 85.1865 (±3.83678); valid loss: 93.8451\n",
      "[Epoch 309/350, Step 14200, ETA 11.61s] step time: 0.005452s (±0.001267s); valid time: 0.008225s; loss: 91.2302 (±42.0314); valid loss: 94.0902\n",
      "[Epoch 310/350, Step 14260, ETA 11.24s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11s] step time: 0.005937s (±0.002357s); valid time: 0.008795s; loss: 85.551 (±3.92817); valid loss: 95.0828\n",
      "[Epoch 314/350, Step 14400, ETA 10.39s] step time: 0.00541s (±0.001385s); valid time: 0.008192s; loss: 86.3099 (±10.1648); valid loss: 94.1005\n",
      "[Epoch 316/350, Step 14500, ETA 9.778s] step time: 0.005603s (±0.001358s); valid time: 0.008268s; loss: 85.4209 (±4.58992); valid loss: 94.3167\n",
      "[Epoch 318/350, Step 14600, ETA 9.164s] step time: 0.005294s (±0.0009695s); valid time: 0.008459s; loss: 86.0485 (±5.69918); valid loss: 93.9098\n",
      "[Epoch 320/350, Step 14700, ETA 8.553s] step time: 0.005597s (±0.001638s); valid time: 0.008232s; loss: 109.804 (±222.588); valid loss: 94.0316\n",
      "[Epoch 320/350, Step 14720, ETA 8.429s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 7.939s] step time: 0.005312s (±0.001166s); valid time: 0.008497s; loss: 85.4916 (±4.5218); valid loss: 94.0975\n",
      "[Epoch 324/350, Step 14900, ETA 7.327s] step time: 0.005549s (±0.001639s); valid time: 0.008474s; loss: 85.4364 (±3.58048); valid loss: 94.2155\n",
      "[Epoch 327/350, Step 15000, ETA 6.716s] step time: 0.005422s (±0.00117s); valid time: 0.008004s; loss: 85.3018 (±3.56256); valid loss: 94.2335\n",
      "[Epoch 329/350, Step 15100, ETA 6.104s] step time: 0.005467s (±0.001343s); valid time: 0.008008s; loss: 88.0814 (±25.4399); valid loss: 94.2227\n",
      "[Epoch 330/350, Step 15180, ETA 5.614s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.492s] step time: 0.00538s (±0.001278s); valid time: 0.008585s; loss: 87.286 (±17.0443); valid loss: 95.1144\n",
      "[Epoch 333/350, Step 15300, ETA 4.881s] step time: 0.005501s (±0.001399s); valid time: 0.009159s; loss: 85.2773 (±3.65171); valid loss: 94.4001\n",
      "[Epoch 335/350, Step 15400, ETA 4.271s] step time: 0.005514s (±0.001353s); valid time: 0.008352s; loss: 85.295 (±4.09733); valid loss: 94.0731\n",
      "[Epoch 337/350, Step 15500, ETA 3.66s] step time: 0.005454s (±0.001566s); valid time: 0.008667s; loss: 85.3438 (±3.87966); valid loss: 94.009\n",
      "[Epoch 340/350, Step 15600, ETA 3.05s] step time: 0.005538s (±0.001387s); valid time: 0.008039s; loss: 111.158 (±258.993); valid loss: 94.116\n",
      "[Epoch 340/350, Step 15640, ETA 2.805s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.44s] step time: 0.005692s (±0.001605s); valid time: 0.00834s; loss: 85.3855 (±3.50088); valid loss: 94.4003\n",
      "[Epoch 344/350, Step 15800, ETA 1.83s] step time: 0.005456s (±0.001447s); valid time: 0.008152s; loss: 85.1594 (±3.06899); valid loss: 94.1217\n",
      "[Epoch 346/350, Step 15900, ETA 1.22s] step time: 0.005571s (±0.001232s); valid time: 0.008265s; loss: 84.9303 (±3.33955); valid loss: 97.2991\n",
      "[Epoch 348/350, Step 16000, ETA 0.61s] step time: 0.0057s (±0.001877s); valid time: 0.008569s; loss: 85.4461 (±3.86776); valid loss: 94.6914\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005335s (±0.001409s); valid time: 0.008394s; loss: 87.3211 (±20.4759); valid loss: 93.8813\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpuxlheanh/variables.dat-2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpuxlheanh/variables.dat-2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.77,\n",
      "\t(tp, fp, tn, fn)=(708, 2839, 11470, 885),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.44,\n",
      "\tf1=0.28,\n",
      "\troc_auc=0.62,\n",
      "\ty_pred%=0.22305370393661175,\n",
      "\ty_label%=0.10017607848069425,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_CVS.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 49.54s] step time: 0.009995s (±0.03282s); valid time: 0.1682s; loss: 138.73 (±10.7373); valid loss: 119.196 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 16.3s] step time: 0.006036s (±0.008124s); valid time: 0.08133s; loss: 116.815 (±4.00804); valid loss: 113.569 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 5.63s] step time: 0.006245s (±0.00826s); valid time: 0.08208s; loss: 111.956 (±3.60529); valid loss: 110.034 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 0.4914s] step time: 0.006389s (±0.0104s); valid time: 0.1033s; loss: 108.171 (±3.92735); valid loss: 109.342 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.09s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 56.86s] step time: 0.006295s (±0.008462s); valid time: 0.08468s; loss: 107.199 (±4.04103); valid loss: 109.057 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 54.48s] step time: 0.006294s (±0.008666s); valid time: 0.08583s; loss: 107.361 (±3.23564); valid loss: 108.938 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 52.65s] step time: 0.006432s (±0.009831s); valid time: 0.09855s; loss: 106.937 (±3.53141); valid loss: 108.82 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 50.37s] step time: 0.006037s (±0.008363s); valid time: 0.0831s; loss: 106.589 (±3.701); valid loss: 108.577 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 48.8s] step time: 0.00626s (±0.008801s); valid time: 0.08772s; loss: 106.014 (±3.34654); valid loss: 107.958 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 48.13s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 47.47s] step time: 0.006293s (±0.008675s); valid time: 0.08659s; loss: 105.18 (±4.10534); valid loss: 107.18 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 46.21s] step time: 0.006246s (±0.008675s); valid time: 0.08605s; loss: 104.637 (±3.53484); valid loss: 106.337 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.29s] step time: 0.006279s (±0.008548s); valid time: 0.08495s; loss: 103.47 (±3.56261); valid loss: 105.501 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.27s] step time: 0.006309s (±0.008899s); valid time: 0.08874s; loss: 102.701 (±3.95901); valid loss: 104.618 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 42.46s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 42.93s] step time: 0.005974s (±0.008186s); valid time: 0.08149s; loss: 102.106 (±3.28911); valid loss: 103.897 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 41.97s] step time: 0.00627s (±0.008612s); valid time: 0.08543s; loss: 101.495 (±3.50298); valid loss: 103.514 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 41.11s] step time: 0.006355s (±0.008661s); valid time: 0.0864s; loss: 101.297 (±3.4684); valid loss: 103.147 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 40.09s] step time: 0.006146s (±0.008269s); valid time: 0.0824s; loss: 100.936 (±3.74509); valid loss: 103.077 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 39.36s] step time: 0.006339s (±0.00871s); valid time: 0.08677s; loss: 100.552 (±3.71945); valid loss: 102.897 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 38.68s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 38.58s] step time: 0.006368s (±0.008667s); valid time: 0.08538s; loss: 100.518 (±3.31805); valid loss: 102.625 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 37.68s] step time: 0.006199s (±0.008524s); valid time: 0.085s; loss: 100.112 (±3.65605); valid loss: 102.597 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 36.85s] step time: 0.006267s (±0.009042s); valid time: 0.08989s; loss: 100.282 (±3.7223); valid loss: 102.358 (*)\n",
      "[Epoch 48/350, Step 2200, ETA 1m 35.51s] step time: 0.005424s (±0.0014s); valid time: 0.008576s; loss: 100.117 (±3.67153); valid loss: 102.399\n",
      "[Epoch 50/350, Step 2300, ETA 1m 34.63s] step time: 0.006077s (±0.008434s); valid time: 0.08409s; loss: 99.8995 (±4.14565); valid loss: 102.16 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 34.63s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 33.54s] step time: 0.005561s (±0.001536s); valid time: 0.008992s; loss: 99.7308 (±3.18448); valid loss: 102.163\n",
      "[Epoch 55/350, Step 2500, ETA 1m 32.23s] step time: 0.005209s (±0.001314s); valid time: 0.008837s; loss: 99.7219 (±3.29433); valid loss: 102.235\n",
      "[Epoch 57/350, Step 2600, ETA 1m 31.48s] step time: 0.006197s (±0.008446s); valid time: 0.08411s; loss: 99.8886 (±3.17721); valid loss: 102.099 (*)\n",
      "[Epoch 59/350, Step 2700, ETA 1m 30.79s] step time: 0.006249s (±0.008452s); valid time: 0.08363s; loss: 99.5206 (±3.96929); valid loss: 102 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 30.05s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 29.65s] step time: 0.005318s (±0.001486s); valid time: 0.008478s; loss: 99.6282 (±3.47772); valid loss: 102.075\n",
      "[Epoch 64/350, Step 2900, ETA 1m 28.71s] step time: 0.005583s (±0.001668s); valid time: 0.009449s; loss: 99.8633 (±4.82948); valid loss: 102.002\n",
      "[Epoch 66/350, Step 3000, ETA 1m 27.74s] step time: 0.005581s (±0.001719s); valid time: 0.008471s; loss: 99.469 (±3.77741); valid loss: 102.067\n",
      "[Epoch 68/350, Step 3100, ETA 1m 26.69s] step time: 0.005262s (±0.001135s); valid time: 0.00827s; loss: 99.3623 (±3.39371); valid loss: 102.012\n",
      "[Epoch 70/350, Step 3200, ETA 1m 25.66s] step time: 0.005322s (±0.001445s); valid time: 0.008213s; loss: 99.244 (±3.85215); valid loss: 102.018\n",
      "[Epoch 70/350, Step 3220, ETA 1m 25.49s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 24.74s] step time: 0.005469s (±0.001795s); valid time: 0.008424s; loss: 99.4331 (±3.41513); valid loss: 102.119\n",
      "[Epoch 74/350, Step 3400, ETA 1m 24.09s] step time: 0.006224s (±0.008347s); valid time: 0.08321s; loss: 99.4807 (±3.58232); valid loss: 101.838 (*)\n",
      "[Epoch 77/350, Step 3500, ETA 1m 23.19s] step time: 0.005375s (±0.001441s); valid time: 0.008556s; loss: 100.051 (±6.64075); valid loss: 101.918\n",
      "[Epoch 79/350, Step 3600, ETA 1m 22.35s] step time: 0.005606s (±0.001543s); valid time: 0.009204s; loss: 99.2973 (±4.03755); valid loss: 105.921\n",
      "[Epoch 80/350, Step 3680, ETA 1m 21.57s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 21.75s] step time: 0.006272s (±0.009352s); valid time: 0.09372s; loss: 99.2461 (±3.58108); valid loss: 101.782 (*)\n",
      "[Epoch 83/350, Step 3800, ETA 1m 20.87s] step time: 0.005456s (±0.001333s); valid time: 0.008385s; loss: 99.172 (±3.49714); valid loss: 101.79\n",
      "[Epoch 85/350, Step 3900, ETA 1m 20.23s] step time: 0.006163s (±0.00851s); valid time: 0.08306s; loss: 99.1104 (±3.01783); valid loss: 101.754 (*)\n",
      "[Epoch 87/350, Step 4000, ETA 1m 19.62s] step time: 0.006292s (±0.01017s); valid time: 0.0968s; loss: 99.0239 (±3.41135); valid loss: 101.664 (*)\n",
      "[Epoch 90/350, Step 4100, ETA 1m 18.78s] step time: 0.005399s (±0.001409s); valid time: 0.009073s; loss: 99.1405 (±3.41935); valid loss: 101.725\n",
      "[Epoch 90/350, Step 4140, ETA 1m 18.45s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 18.01s] step time: 0.005654s (±0.001718s); valid time: 0.008494s; loss: 99.1479 (±3.61775); valid loss: 101.742\n",
      "[Epoch 94/350, Step 4300, ETA 1m 17.39s] step time: 0.006246s (±0.008443s); valid time: 0.08431s; loss: 98.906 (±3.66406); valid loss: 101.608 (*)\n",
      "[Epoch 96/350, Step 4400, ETA 1m 16.58s] step time: 0.005509s (±0.001519s); valid time: 0.008593s; loss: 99.7565 (±3.99427); valid loss: 101.778\n",
      "[Epoch 98/350, Step 4500, ETA 1m 15.77s] step time: 0.005472s (±0.001458s); valid time: 0.00847s; loss: 98.8163 (±3.71233); valid loss: 101.771\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.12s] step time: 0.006106s (±0.008896s); valid time: 0.08884s; loss: 99.07 (±3.78147); valid loss: 101.593 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.12s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 14.35s] step time: 0.005471s (±0.001366s); valid time: 0.008233s; loss: 99.1162 (±4.30405); valid loss: 101.61\n",
      "[Epoch 105/350, Step 4800, ETA 1m 13.75s] step time: 0.006251s (±0.009689s); valid time: 0.09676s; loss: 99.3792 (±4.95667); valid loss: 101.533 (*)\n",
      "[Epoch 107/350, Step 4900, ETA 1m 13s] step time: 0.00565s (±0.001758s); valid time: 0.008742s; loss: 99.0323 (±3.26247); valid loss: 102.279\n",
      "[Epoch 109/350, Step 5000, ETA 1m 12.21s] step time: 0.005406s (±0.001235s); valid time: 0.008313s; loss: 98.8897 (±3.29576); valid loss: 101.592\n",
      "[Epoch 110/350, Step 5060, ETA 1m 11.71s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 11.43s] step time: 0.00545s (±0.001581s); valid time: 0.008437s; loss: 102.817 (±37.9207); valid loss: 101.592\n",
      "[Epoch 114/350, Step 5200, ETA 1m 10.71s] step time: 0.005581s (±0.001579s); valid time: 0.009129s; loss: 99.0307 (±3.30141); valid loss: 101.687\n",
      "[Epoch 116/350, Step 5300, ETA 1m 9.945s] step time: 0.005457s (±0.00163s); valid time: 0.008709s; loss: 98.9517 (±3.98972); valid loss: 104.406\n",
      "[Epoch 118/350, Step 5400, ETA 1m 9.172s] step time: 0.00538s (±0.001435s); valid time: 0.008558s; loss: 99.1512 (±3.51552); valid loss: 102.824\n",
      "[Epoch 120/350, Step 5500, ETA 1m 8.417s] step time: 0.005439s (±0.001463s); valid time: 0.008454s; loss: 98.6019 (±3.3685); valid loss: 101.692\n",
      "[Epoch 120/350, Step 5520, ETA 1m 8.242s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 7.624s] step time: 0.005202s (±0.001325s); valid time: 0.008403s; loss: 99.1541 (±3.43998); valid loss: 139.562\n",
      "[Epoch 124/350, Step 5700, ETA 1m 6.865s] step time: 0.005351s (±0.001384s); valid time: 0.008844s; loss: 98.8026 (±3.73627); valid loss: 101.54\n",
      "[Epoch 127/350, Step 5800, ETA 1m 6.142s] step time: 0.005419s (±0.001437s); valid time: 0.008829s; loss: 98.6449 (±3.8829); valid loss: 101.57\n",
      "[Epoch 129/350, Step 5900, ETA 1m 5.406s] step time: 0.005422s (±0.001417s); valid time: 0.008052s; loss: 99.2408 (±3.44345); valid loss: 105.414\n",
      "[Epoch 130/350, Step 5980, ETA 1m 4.784s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 4.681s] step time: 0.005428s (±0.001479s); valid time: 0.008769s; loss: 98.9657 (±5.08709); valid loss: 102.253\n",
      "[Epoch 133/350, Step 6100, ETA 1m 4.128s] step time: 0.006462s (±0.008456s); valid time: 0.08417s; loss: 98.9038 (±3.86351); valid loss: 101.516 (*)\n",
      "[Epoch 135/350, Step 6200, ETA 1m 3.404s] step time: 0.00544s (±0.001471s); valid time: 0.008716s; loss: 99.0282 (±4.23484); valid loss: 101.86\n",
      "[Epoch 137/350, Step 6300, ETA 1m 2.81s] step time: 0.006244s (±0.009143s); valid time: 0.09146s; loss: 99.1424 (±3.57319); valid loss: 101.492 (*)\n",
      "[Epoch 140/350, Step 6400, ETA 1m 2.126s] step time: 0.005556s (±0.001545s); valid time: 0.008528s; loss: 98.832 (±3.27739); valid loss: 101.507\n",
      "[Epoch 140/350, Step 6440, ETA 1m 1.823s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 1.423s] step time: 0.005413s (±0.001699s); valid time: 0.008117s; loss: 98.961 (±4.68547); valid loss: 101.516\n",
      "[Epoch 144/350, Step 6600, ETA 1m 0.7138s] step time: 0.005454s (±0.001726s); valid time: 0.00844s; loss: 98.9814 (±3.49374); valid loss: 102.073\n",
      "[Epoch 146/350, Step 6700, ETA 1m 0.006773s] step time: 0.005455s (±0.001579s); valid time: 0.009711s; loss: 99.0981 (±3.40159); valid loss: 101.986\n",
      "[Epoch 148/350, Step 6800, ETA 59.29s] step time: 0.005343s (±0.001321s); valid time: 0.008212s; loss: 99.1243 (±4.57238); valid loss: 101.544\n",
      "[Epoch 150/350, Step 6900, ETA 58.59s] step time: 0.005452s (±0.001527s); valid time: 0.00875s; loss: 99.8478 (±11.2691); valid loss: 105.638\n",
      "[Epoch 150/350, Step 6900, ETA 58.59s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 58.01s] step time: 0.006283s (±0.01008s); valid time: 0.1005s; loss: 125.282 (±261.41); valid loss: 101.469 (*)\n",
      "[Epoch 155/350, Step 7100, ETA 57.31s] step time: 0.005359s (±0.001167s); valid time: 0.008588s; loss: 99.2931 (±4.34352); valid loss: 101.826\n",
      "[Epoch 157/350, Step 7200, ETA 56.62s] step time: 0.0055s (±0.001418s); valid time: 0.008416s; loss: 98.9625 (±3.47229); valid loss: 101.515\n",
      "[Epoch 159/350, Step 7300, ETA 55.94s] step time: 0.005464s (±0.001608s); valid time: 0.009083s; loss: 98.7055 (±3.55096); valid loss: 107.198\n",
      "[Epoch 160/350, Step 7360, ETA 55.52s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 55.26s] step time: 0.00556s (±0.00172s); valid time: 0.008509s; loss: 98.947 (±3.52868); valid loss: 101.472\n",
      "[Epoch 164/350, Step 7500, ETA 54.59s] step time: 0.005446s (±0.001828s); valid time: 0.01253s; loss: 99.2166 (±5.26271); valid loss: 101.732\n",
      "[Epoch 166/350, Step 7600, ETA 53.91s] step time: 0.00554s (±0.001357s); valid time: 0.008644s; loss: 98.8928 (±3.51643); valid loss: 101.506\n",
      "[Epoch 168/350, Step 7700, ETA 53.25s] step time: 0.005554s (±0.001429s); valid time: 0.008451s; loss: 98.7753 (±3.83126); valid loss: 101.837\n",
      "[Epoch 170/350, Step 7800, ETA 52.57s] step time: 0.005508s (±0.001664s); valid time: 0.00869s; loss: 99.2383 (±4.12685); valid loss: 101.594\n",
      "[Epoch 170/350, Step 7820, ETA 52.43s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 51.9s] step time: 0.005472s (±0.001636s); valid time: 0.008919s; loss: 98.7366 (±3.93039); valid loss: 109.692\n",
      "[Epoch 174/350, Step 8000, ETA 51.23s] step time: 0.005458s (±0.001265s); valid time: 0.008803s; loss: 98.7816 (±3.89333); valid loss: 101.486\n",
      "[Epoch 177/350, Step 8100, ETA 50.57s] step time: 0.005544s (±0.001501s); valid time: 0.008608s; loss: 99.0413 (±3.70777); valid loss: 101.495\n",
      "[Epoch 179/350, Step 8200, ETA 49.91s] step time: 0.005494s (±0.001548s); valid time: 0.008253s; loss: 98.4921 (±3.66923); valid loss: 101.506\n",
      "[Epoch 180/350, Step 8280, ETA 49.38s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.26s] step time: 0.005641s (±0.001959s); valid time: 0.008972s; loss: 99.0378 (±3.51531); valid loss: 101.499\n",
      "[Epoch 183/350, Step 8400, ETA 48.59s] step time: 0.005502s (±0.001655s); valid time: 0.009823s; loss: 98.6088 (±3.43849); valid loss: 101.483\n",
      "[Epoch 185/350, Step 8500, ETA 47.93s] step time: 0.005524s (±0.001935s); valid time: 0.008716s; loss: 99.218 (±3.94597); valid loss: 103.233\n",
      "[Epoch 187/350, Step 8600, ETA 47.35s] step time: 0.006438s (±0.008518s); valid time: 0.08438s; loss: 98.7442 (±3.86124); valid loss: 101.45 (*)\n",
      "[Epoch 190/350, Step 8700, ETA 46.71s] step time: 0.005584s (±0.001699s); valid time: 0.008868s; loss: 167.377 (±682.952); valid loss: 101.699\n",
      "[Epoch 190/350, Step 8740, ETA 46.44s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 46.06s] step time: 0.005583s (±0.001842s); valid time: 0.008548s; loss: 99.4116 (±7.25892); valid loss: 104.347\n",
      "[Epoch 194/350, Step 8900, ETA 45.41s] step time: 0.005616s (±0.001742s); valid time: 0.008851s; loss: 98.9589 (±4.12085); valid loss: 101.809\n",
      "[Epoch 196/350, Step 9000, ETA 44.76s] step time: 0.005594s (±0.00162s); valid time: 0.009353s; loss: 99.16 (±3.62427); valid loss: 102.873\n",
      "[Epoch 198/350, Step 9100, ETA 44.12s] step time: 0.005721s (±0.001622s); valid time: 0.008703s; loss: 98.7765 (±3.57247); valid loss: 101.687\n",
      "[Epoch 200/350, Step 9200, ETA 43.52s] step time: 0.006239s (±0.008953s); valid time: 0.08927s; loss: 98.8224 (±3.42596); valid loss: 101.404 (*)\n",
      "[Epoch 200/350, Step 9200, ETA 43.52s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 42.87s] step time: 0.005439s (±0.001447s); valid time: 0.009107s; loss: 98.9534 (±3.52766); valid loss: 102.023\n",
      "[Epoch 205/350, Step 9400, ETA 42.22s] step time: 0.005593s (±0.001574s); valid time: 0.0089s; loss: 98.9888 (±3.67046); valid loss: 101.86\n",
      "[Epoch 207/350, Step 9500, ETA 41.57s] step time: 0.005588s (±0.001681s); valid time: 0.008401s; loss: 98.3973 (±3.8883); valid loss: 115.313\n",
      "[Epoch 209/350, Step 9600, ETA 40.92s] step time: 0.00549s (±0.001395s); valid time: 0.008798s; loss: 99.0871 (±3.8328); valid loss: 101.502\n",
      "[Epoch 210/350, Step 9660, ETA 40.52s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.28s] step time: 0.005649s (±0.001771s); valid time: 0.008369s; loss: 98.9247 (±3.80172); valid loss: 101.551\n",
      "[Epoch 214/350, Step 9800, ETA 39.63s] step time: 0.005387s (±0.001482s); valid time: 0.008898s; loss: 98.7747 (±4.04256); valid loss: 120.074\n",
      "[Epoch 216/350, Step 9900, ETA 38.98s] step time: 0.005461s (±0.001729s); valid time: 0.008838s; loss: 98.7592 (±3.45878); valid loss: 101.502\n",
      "[Epoch 218/350, Step 10000, ETA 38.33s] step time: 0.005543s (±0.00142s); valid time: 0.008697s; loss: 98.7119 (±4.10925); valid loss: 101.509\n",
      "[Epoch 220/350, Step 10100, ETA 37.69s] step time: 0.005543s (±0.001747s); valid time: 0.008918s; loss: 99.0707 (±3.23445); valid loss: 101.752\n",
      "[Epoch 220/350, Step 10120, ETA 37.55s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 37.04s] step time: 0.005436s (±0.001462s); valid time: 0.008495s; loss: 98.7245 (±3.52705); valid loss: 101.648\n",
      "[Epoch 224/350, Step 10300, ETA 36.38s] step time: 0.005388s (±0.001371s); valid time: 0.008569s; loss: 98.8957 (±3.77372); valid loss: 101.483\n",
      "[Epoch 227/350, Step 10400, ETA 35.75s] step time: 0.005525s (±0.001863s); valid time: 0.009227s; loss: 98.644 (±3.41993); valid loss: 101.688\n",
      "[Epoch 229/350, Step 10500, ETA 35.1s] step time: 0.005362s (±0.001369s); valid time: 0.008289s; loss: 98.8364 (±3.91638); valid loss: 101.585\n",
      "[Epoch 230/350, Step 10580, ETA 34.57s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.45s] step time: 0.005261s (±0.001306s); valid time: 0.008213s; loss: 99.5726 (±8.09648); valid loss: 101.483\n",
      "[Epoch 233/350, Step 10700, ETA 33.8s] step time: 0.005383s (±0.001623s); valid time: 0.008434s; loss: 98.7022 (±3.47134); valid loss: 101.444\n",
      "[Epoch 235/350, Step 10800, ETA 33.15s] step time: 0.005328s (±0.001482s); valid time: 0.00821s; loss: 98.9587 (±3.51618); valid loss: 101.536\n",
      "[Epoch 237/350, Step 10900, ETA 32.51s] step time: 0.005476s (±0.00169s); valid time: 0.008295s; loss: 98.8578 (±3.80797); valid loss: 104.37\n",
      "[Epoch 240/350, Step 11000, ETA 31.88s] step time: 0.005553s (±0.001561s); valid time: 0.008724s; loss: 98.8035 (±3.43998); valid loss: 101.51\n",
      "[Epoch 240/350, Step 11040, ETA 31.62s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.24s] step time: 0.005525s (±0.001741s); valid time: 0.008636s; loss: 98.9069 (±3.14243); valid loss: 101.516\n",
      "[Epoch 244/350, Step 11200, ETA 30.6s] step time: 0.005396s (±0.001733s); valid time: 0.008878s; loss: 98.6141 (±3.21173); valid loss: 101.68\n",
      "[Epoch 246/350, Step 11300, ETA 29.97s] step time: 0.005673s (±0.001803s); valid time: 0.01163s; loss: 99.0732 (±4.86912); valid loss: 101.563\n",
      "[Epoch 248/350, Step 11400, ETA 29.33s] step time: 0.005518s (±0.001626s); valid time: 0.009307s; loss: 99.3273 (±4.64129); valid loss: 101.571\n",
      "[Epoch 250/350, Step 11500, ETA 28.69s] step time: 0.005387s (±0.001318s); valid time: 0.008478s; loss: 98.7617 (±3.82949); valid loss: 101.883\n",
      "[Epoch 250/350, Step 11500, ETA 28.69s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.06s] step time: 0.005446s (±0.001432s); valid time: 0.008066s; loss: 98.683 (±3.81756); valid loss: 101.596\n",
      "[Epoch 255/350, Step 11700, ETA 27.43s] step time: 0.0056s (±0.001499s); valid time: 0.008437s; loss: 98.8907 (±3.65973); valid loss: 108.996\n",
      "[Epoch 257/350, Step 11800, ETA 26.8s] step time: 0.005582s (±0.001638s); valid time: 0.008396s; loss: 99.5378 (±8.19776); valid loss: 102.824\n",
      "[Epoch 259/350, Step 11900, ETA 26.17s] step time: 0.005658s (±0.001715s); valid time: 0.008297s; loss: 98.6095 (±3.66389); valid loss: 101.523\n",
      "[Epoch 260/350, Step 11960, ETA 25.79s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.54s] step time: 0.005429s (±0.001524s); valid time: 0.008499s; loss: 99.1949 (±3.85798); valid loss: 101.55\n",
      "[Epoch 264/350, Step 12100, ETA 24.91s] step time: 0.005439s (±0.001477s); valid time: 0.00905s; loss: 99.1489 (±4.72673); valid loss: 101.578\n",
      "[Epoch 266/350, Step 12200, ETA 24.27s] step time: 0.005328s (±0.001303s); valid time: 0.008332s; loss: 99.0878 (±3.94723); valid loss: 101.426\n",
      "[Epoch 268/350, Step 12300, ETA 23.65s] step time: 0.005635s (±0.001524s); valid time: 0.008613s; loss: 98.86 (±3.23339); valid loss: 101.478\n",
      "[Epoch 270/350, Step 12400, ETA 23.02s] step time: 0.005618s (±0.001758s); valid time: 0.008568s; loss: 98.6958 (±3.66168); valid loss: 101.614\n",
      "[Epoch 270/350, Step 12420, ETA 22.89s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.39s] step time: 0.005382s (±0.001523s); valid time: 0.008202s; loss: 98.7515 (±3.3885); valid loss: 101.485\n",
      "[Epoch 274/350, Step 12600, ETA 21.76s] step time: 0.005465s (±0.001637s); valid time: 0.008489s; loss: 98.9115 (±3.56201); valid loss: 108.98\n",
      "[Epoch 277/350, Step 12700, ETA 21.13s] step time: 0.005265s (±0.001339s); valid time: 0.009151s; loss: 99.0966 (±5.50572); valid loss: 101.798\n",
      "[Epoch 279/350, Step 12800, ETA 20.5s] step time: 0.00544s (±0.001171s); valid time: 0.008161s; loss: 99.0864 (±3.88845); valid loss: 101.817\n",
      "[Epoch 280/350, Step 12880, ETA 19.99s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.87s] step time: 0.005284s (±0.001213s); valid time: 0.008259s; loss: 98.8118 (±3.70325); valid loss: 153.424\n",
      "[Epoch 283/350, Step 13000, ETA 19.24s] step time: 0.005362s (±0.001488s); valid time: 0.008499s; loss: 99.014 (±4.69827); valid loss: 101.475\n",
      "[Epoch 285/350, Step 13100, ETA 18.61s] step time: 0.005437s (±0.001454s); valid time: 0.008638s; loss: 98.8043 (±3.23418); valid loss: 103.655\n",
      "[Epoch 287/350, Step 13200, ETA 17.98s] step time: 0.005507s (±0.001493s); valid time: 0.008427s; loss: 99.0658 (±3.7623); valid loss: 101.736\n",
      "[Epoch 290/350, Step 13300, ETA 17.36s] step time: 0.005518s (±0.001329s); valid time: 0.008375s; loss: 104.303 (±55.6248); valid loss: 101.537\n",
      "[Epoch 290/350, Step 13340, ETA 17.11s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.74s] step time: 0.005682s (±0.001964s); valid time: 0.009089s; loss: 99.1278 (±3.62166); valid loss: 102.746\n",
      "[Epoch 294/350, Step 13500, ETA 16.12s] step time: 0.00577s (±0.002045s); valid time: 0.008379s; loss: 98.8252 (±3.29857); valid loss: 101.523\n",
      "[Epoch 296/350, Step 13600, ETA 15.5s] step time: 0.005483s (±0.001203s); valid time: 0.008395s; loss: 98.3383 (±3.61005); valid loss: 101.507\n",
      "[Epoch 298/350, Step 13700, ETA 14.87s] step time: 0.005326s (±0.001394s); valid time: 0.008682s; loss: 99.6583 (±6.51639); valid loss: 102.488\n",
      "[Epoch 300/350, Step 13800, ETA 14.25s] step time: 0.005534s (±0.001702s); valid time: 0.008959s; loss: 98.6547 (±3.27436); valid loss: 129.428\n",
      "[Epoch 300/350, Step 13800, ETA 14.25s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.62s] step time: 0.005339s (±0.001337s); valid time: 0.009231s; loss: 98.9116 (±3.84931); valid loss: 101.547\n",
      "[Epoch 305/350, Step 14000, ETA 13s] step time: 0.005332s (±0.001433s); valid time: 0.008495s; loss: 99.335 (±5.96249); valid loss: 101.489\n",
      "[Epoch 307/350, Step 14100, ETA 12.37s] step time: 0.005383s (±0.001368s); valid time: 0.008492s; loss: 98.7281 (±4.06054); valid loss: 101.49\n",
      "[Epoch 309/350, Step 14200, ETA 11.75s] step time: 0.005456s (±0.001291s); valid time: 0.008717s; loss: 99.094 (±3.37081); valid loss: 101.648\n",
      "[Epoch 310/350, Step 14260, ETA 11.38s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.13s] step time: 0.005556s (±0.001806s); valid time: 0.008609s; loss: 99.0538 (±4.32341); valid loss: 101.565\n",
      "[Epoch 314/350, Step 14400, ETA 10.51s] step time: 0.005404s (±0.001385s); valid time: 0.008802s; loss: 98.771 (±3.59663); valid loss: 101.484\n",
      "[Epoch 316/350, Step 14500, ETA 9.891s] step time: 0.005594s (±0.001574s); valid time: 0.009831s; loss: 98.8972 (±3.34231); valid loss: 101.477\n",
      "[Epoch 318/350, Step 14600, ETA 9.271s] step time: 0.005488s (±0.001102s); valid time: 0.008665s; loss: 98.5998 (±4.18753); valid loss: 101.662\n",
      "[Epoch 320/350, Step 14700, ETA 8.65s] step time: 0.005399s (±0.001686s); valid time: 0.008369s; loss: 98.8112 (±3.58349); valid loss: 103.362\n",
      "[Epoch 320/350, Step 14720, ETA 8.525s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.029s] step time: 0.005419s (±0.001331s); valid time: 0.00845s; loss: 98.9631 (±3.7577); valid loss: 101.684\n",
      "[Epoch 324/350, Step 14900, ETA 7.41s] step time: 0.005506s (±0.00157s); valid time: 0.01101s; loss: 98.6722 (±3.40519); valid loss: 118.53\n",
      "[Epoch 327/350, Step 15000, ETA 6.792s] step time: 0.005562s (±0.001857s); valid time: 0.008443s; loss: 98.8288 (±3.7132); valid loss: 101.593\n",
      "[Epoch 329/350, Step 15100, ETA 6.173s] step time: 0.005387s (±0.001402s); valid time: 0.008805s; loss: 98.8671 (±3.94155); valid loss: 101.592\n",
      "[Epoch 330/350, Step 15180, ETA 5.678s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.555s] step time: 0.005544s (±0.001877s); valid time: 0.008766s; loss: 98.9209 (±3.77118); valid loss: 101.547\n",
      "[Epoch 333/350, Step 15300, ETA 4.936s] step time: 0.005403s (±0.001436s); valid time: 0.009013s; loss: 98.8919 (±3.85204); valid loss: 104.852\n",
      "[Epoch 335/350, Step 15400, ETA 4.318s] step time: 0.00542s (±0.001398s); valid time: 0.00819s; loss: 98.8397 (±4.09347); valid loss: 182.678\n",
      "[Epoch 337/350, Step 15500, ETA 3.701s] step time: 0.005656s (±0.001807s); valid time: 0.01028s; loss: 98.9766 (±3.24953); valid loss: 101.995\n",
      "[Epoch 340/350, Step 15600, ETA 3.084s] step time: 0.00557s (±0.001594s); valid time: 0.008468s; loss: 98.9454 (±4.03116); valid loss: 101.653\n",
      "[Epoch 340/350, Step 15640, ETA 2.837s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.467s] step time: 0.005656s (±0.001544s); valid time: 0.008361s; loss: 98.705 (±3.46242); valid loss: 102.381\n",
      "[Epoch 344/350, Step 15800, ETA 1.85s] step time: 0.005362s (±0.001278s); valid time: 0.008389s; loss: 98.889 (±3.96919); valid loss: 101.552\n",
      "[Epoch 346/350, Step 15900, ETA 1.233s] step time: 0.005415s (±0.001394s); valid time: 0.008893s; loss: 99.0508 (±3.44768); valid loss: 101.694\n",
      "[Epoch 348/350, Step 16000, ETA 0.6162s] step time: 0.00553s (±0.00173s); valid time: 0.00843s; loss: 98.6706 (±3.48776); valid loss: 101.457\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005289s (±0.001245s); valid time: 0.008571s; loss: 98.8974 (±3.63919); valid loss: 101.445\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5e03o42r/variables.dat-9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5e03o42r/variables.dat-9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.82,\n",
      "\t(tp, fp, tn, fn)=(432, 1697, 12630, 1094),\n",
      "\tprecision=0.2,\n",
      "\trecall=0.28,\n",
      "\tf1=0.24,\n",
      "\troc_auc=0.58,\n",
      "\ty_pred%=0.13429634769444268,\n",
      "\ty_label%=0.09625938308206648,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_FB.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 45.37s] step time: 0.009762s (±0.03308s); valid time: 0.1618s; loss: 119.569 (±17.6283); valid loss: 101.503 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 16.35s] step time: 0.006342s (±0.008188s); valid time: 0.08083s; loss: 84.9026 (±5.01004); valid loss: 87.9845 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 6.12s] step time: 0.006301s (±0.008739s); valid time: 0.08668s; loss: 78.4783 (±4.00844); valid loss: 83.1481 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 0.2289s] step time: 0.006231s (±0.008661s); valid time: 0.08625s; loss: 72.941 (±3.13677); valid loss: 80.0643 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.4s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 56.94s] step time: 0.006375s (±0.008332s); valid time: 0.08252s; loss: 69.8912 (±3.38472); valid loss: 78.3653 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 54.73s] step time: 0.006368s (±0.008911s); valid time: 0.08889s; loss: 68.5336 (±3.72211); valid loss: 77.4723 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 52.14s] step time: 0.006087s (±0.008331s); valid time: 0.08345s; loss: 67.813 (±3.932); valid loss: 76.9657 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 50.42s] step time: 0.006305s (±0.008451s); valid time: 0.08432s; loss: 66.8773 (±3.23359); valid loss: 76.439 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 48.77s] step time: 0.006197s (±0.008527s); valid time: 0.08404s; loss: 66.2912 (±3.34283); valid loss: 76.0643 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 48.1s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 47.49s] step time: 0.006219s (±0.008567s); valid time: 0.08571s; loss: 65.5391 (±3.62062); valid loss: 75.8265 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 46.1s] step time: 0.006081s (±0.008598s); valid time: 0.08566s; loss: 65.4714 (±3.33417); valid loss: 75.7741 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.24s] step time: 0.006363s (±0.00864s); valid time: 0.08569s; loss: 64.9627 (±3.45702); valid loss: 75.2698 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 43.16s] step time: 0.005399s (±0.001402s); valid time: 0.008556s; loss: 64.9062 (±3.3302); valid loss: 75.282\n",
      "[Epoch 30/350, Step 1380, ETA 1m 41.81s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 42.38s] step time: 0.006372s (±0.008453s); valid time: 0.08367s; loss: 64.5596 (±2.98716); valid loss: 74.9357 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 40.85s] step time: 0.005652s (±0.002319s); valid time: 0.00922s; loss: 64.3646 (±3.54272); valid loss: 74.9553\n",
      "[Epoch 35/350, Step 1600, ETA 1m 40.08s] step time: 0.006356s (±0.008568s); valid time: 0.08494s; loss: 64.0351 (±3.878); valid loss: 74.5869 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 39.47s] step time: 0.006533s (±0.009334s); valid time: 0.09259s; loss: 63.9103 (±3.51942); valid loss: 74.2943 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 38.18s] step time: 0.005568s (±0.001403s); valid time: 0.008704s; loss: 63.7168 (±3.46512); valid loss: 74.5435\n",
      "[Epoch 40/350, Step 1840, ETA 1m 37.52s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 37.41s] step time: 0.006262s (±0.008728s); valid time: 0.08686s; loss: 63.4918 (±3.4832); valid loss: 74.1093 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 36.19s] step time: 0.005634s (±0.001708s); valid time: 0.008729s; loss: 64.8731 (±10.3659); valid loss: 74.2844\n",
      "[Epoch 46/350, Step 2100, ETA 1m 34.88s] step time: 0.005412s (±0.001443s); valid time: 0.008684s; loss: 63.3292 (±3.78971); valid loss: 74.1906\n",
      "[Epoch 48/350, Step 2200, ETA 1m 34.19s] step time: 0.0063s (±0.008498s); valid time: 0.08464s; loss: 63.8436 (±6.40772); valid loss: 73.9741 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.13s] step time: 0.005678s (±0.001588s); valid time: 0.008475s; loss: 63.5258 (±4.44139); valid loss: 74.1\n",
      "[Epoch 50/350, Step 2300, ETA 1m 33.13s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 32.42s] step time: 0.006091s (±0.008306s); valid time: 0.0828s; loss: 62.9761 (±3.68047); valid loss: 73.8588 (*)\n",
      "[Epoch 55/350, Step 2500, ETA 1m 31.74s] step time: 0.006281s (±0.008351s); valid time: 0.08255s; loss: 63.1458 (±3.19691); valid loss: 73.7943 (*)\n",
      "[Epoch 57/350, Step 2600, ETA 1m 31.18s] step time: 0.006473s (±0.008608s); valid time: 0.0857s; loss: 63.2299 (±4.44022); valid loss: 73.6175 (*)\n",
      "[Epoch 59/350, Step 2700, ETA 1m 30.41s] step time: 0.006089s (±0.00856s); valid time: 0.08545s; loss: 62.8527 (±4.03883); valid loss: 73.5264 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 29.83s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 29.79s] step time: 0.006339s (±0.008345s); valid time: 0.08202s; loss: 62.8771 (±3.77054); valid loss: 73.4252 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 28.82s] step time: 0.005536s (±0.00168s); valid time: 0.009149s; loss: 62.7829 (±3.71315); valid loss: 73.4752\n",
      "[Epoch 66/350, Step 3000, ETA 1m 27.84s] step time: 0.005527s (±0.001481s); valid time: 0.008648s; loss: 62.5246 (±3.12903); valid loss: 73.5337\n",
      "[Epoch 68/350, Step 3100, ETA 1m 27.09s] step time: 0.006057s (±0.008378s); valid time: 0.0837s; loss: 62.447 (±3.36045); valid loss: 73.2769 (*)\n",
      "[Epoch 70/350, Step 3200, ETA 1m 26.06s] step time: 0.005325s (±0.00143s); valid time: 0.00861s; loss: 62.5306 (±3.77589); valid loss: 73.3958\n",
      "[Epoch 70/350, Step 3220, ETA 1m 25.9s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 25.45s] step time: 0.006338s (±0.008965s); valid time: 0.0889s; loss: 99.3943 (±362.588); valid loss: 73.1534 (*)\n",
      "[Epoch 74/350, Step 3400, ETA 1m 24.59s] step time: 0.005699s (±0.00161s); valid time: 0.008708s; loss: 62.2044 (±3.51053); valid loss: 73.19\n",
      "[Epoch 77/350, Step 3500, ETA 1m 23.71s] step time: 0.005501s (±0.001396s); valid time: 0.008809s; loss: 65.2776 (±29.6932); valid loss: 73.2769\n",
      "[Epoch 79/350, Step 3600, ETA 1m 22.83s] step time: 0.005549s (±0.001414s); valid time: 0.009205s; loss: 62.4233 (±3.84801); valid loss: 73.2765\n",
      "[Epoch 80/350, Step 3680, ETA 1m 22.07s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 21.93s] step time: 0.005434s (±0.001299s); valid time: 0.008735s; loss: 62.3732 (±3.71357); valid loss: 73.1877\n",
      "[Epoch 83/350, Step 3800, ETA 1m 21.37s] step time: 0.006488s (±0.008727s); valid time: 0.08674s; loss: 67.9987 (±36.4583); valid loss: 72.9648 (*)\n",
      "[Epoch 85/350, Step 3900, ETA 1m 20.53s] step time: 0.005525s (±0.001583s); valid time: 0.008941s; loss: 76.7045 (±142.126); valid loss: 73.0906\n",
      "[Epoch 87/350, Step 4000, ETA 1m 19.64s] step time: 0.005394s (±0.001441s); valid time: 0.008266s; loss: 63.166 (±12.7853); valid loss: 72.9716\n",
      "[Epoch 90/350, Step 4100, ETA 1m 18.84s] step time: 0.005524s (±0.001613s); valid time: 0.008373s; loss: 62.2085 (±4.11102); valid loss: 73.0151\n",
      "[Epoch 90/350, Step 4140, ETA 1m 18.51s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 18.37s] step time: 0.006737s (±0.009755s); valid time: 0.09624s; loss: 64.416 (±22.437); valid loss: 72.9491 (*)\n",
      "[Epoch 94/350, Step 4300, ETA 1m 17.74s] step time: 0.006214s (±0.008578s); valid time: 0.08543s; loss: 62.0746 (±4.21971); valid loss: 72.9075 (*)\n",
      "[Epoch 96/350, Step 4400, ETA 1m 16.9s] step time: 0.005452s (±0.001211s); valid time: 0.00874s; loss: 62.2699 (±3.46703); valid loss: 73.003\n",
      "[Epoch 98/350, Step 4500, ETA 1m 16.32s] step time: 0.006394s (±0.008535s); valid time: 0.08519s; loss: 66.9572 (±48.9077); valid loss: 72.7651 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.51s] step time: 0.005517s (±0.001495s); valid time: 0.009126s; loss: 62.0948 (±4.51694); valid loss: 73.0226\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.51s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 14.73s] step time: 0.005477s (±0.001653s); valid time: 0.008597s; loss: 66.2448 (±39.6596); valid loss: 72.9493\n",
      "[Epoch 105/350, Step 4800, ETA 1m 13.92s] step time: 0.005433s (±0.001469s); valid time: 0.008814s; loss: 61.9383 (±4.28977); valid loss: 72.869\n",
      "[Epoch 107/350, Step 4900, ETA 1m 13.33s] step time: 0.006343s (±0.008419s); valid time: 0.08337s; loss: 65.6428 (±36.0847); valid loss: 72.7273 (*)\n",
      "[Epoch 109/350, Step 5000, ETA 1m 12.58s] step time: 0.005592s (±0.001934s); valid time: 0.008619s; loss: 62.3589 (±7.80424); valid loss: 72.8888\n",
      "[Epoch 110/350, Step 5060, ETA 1m 12.09s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 11.79s] step time: 0.005461s (±0.001788s); valid time: 0.008589s; loss: 103.054 (±408.189); valid loss: 72.7714\n",
      "[Epoch 114/350, Step 5200, ETA 1m 11.21s] step time: 0.006268s (±0.008809s); valid time: 0.08777s; loss: 61.8192 (±4.05763); valid loss: 72.6319 (*)\n",
      "[Epoch 116/350, Step 5300, ETA 1m 10.43s] step time: 0.005474s (±0.001462s); valid time: 0.008117s; loss: 61.7032 (±3.20451); valid loss: 72.7389\n",
      "[Epoch 118/350, Step 5400, ETA 1m 9.641s] step time: 0.00525s (±0.00123s); valid time: 0.009007s; loss: 64.0948 (±22.7795); valid loss: 72.691\n",
      "[Epoch 120/350, Step 5500, ETA 1m 8.895s] step time: 0.005555s (±0.001414s); valid time: 0.008912s; loss: 62.4062 (±7.27137); valid loss: 72.797\n",
      "[Epoch 120/350, Step 5520, ETA 1m 8.721s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 8.16s] step time: 0.00558s (±0.001221s); valid time: 0.009275s; loss: 594.709 (±5300.99); valid loss: 72.6893\n",
      "[Epoch 124/350, Step 5700, ETA 1m 7.424s] step time: 0.00556s (±0.001732s); valid time: 0.008898s; loss: 61.9577 (±3.27031); valid loss: 72.8044\n",
      "[Epoch 127/350, Step 5800, ETA 1m 6.71s] step time: 0.005535s (±0.001757s); valid time: 0.008699s; loss: 61.9684 (±4.12727); valid loss: 72.8331\n",
      "[Epoch 129/350, Step 5900, ETA 1m 5.965s] step time: 0.005459s (±0.001386s); valid time: 0.008375s; loss: 61.6406 (±3.0609); valid loss: 72.714\n",
      "[Epoch 130/350, Step 5980, ETA 1m 5.354s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 5.382s] step time: 0.006378s (±0.01028s); valid time: 0.1021s; loss: 61.8199 (±4.13463); valid loss: 72.62 (*)\n",
      "[Epoch 133/350, Step 6100, ETA 1m 4.653s] step time: 0.005511s (±0.001267s); valid time: 0.009465s; loss: 62.0353 (±4.44736); valid loss: 72.6795\n",
      "[Epoch 135/350, Step 6200, ETA 1m 4.07s] step time: 0.006386s (±0.009005s); valid time: 0.08971s; loss: 71.2459 (±72.1632); valid loss: 72.5993 (*)\n",
      "[Epoch 137/350, Step 6300, ETA 1m 3.34s] step time: 0.005478s (±0.001164s); valid time: 0.008666s; loss: 64.9804 (±29.8008); valid loss: 72.6611\n",
      "[Epoch 140/350, Step 6400, ETA 1m 2.627s] step time: 0.005454s (±0.001713s); valid time: 0.008864s; loss: 61.5682 (±3.3422); valid loss: 72.7208\n",
      "[Epoch 140/350, Step 6440, ETA 1m 2.323s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 1.929s] step time: 0.005623s (±0.00155s); valid time: 0.008299s; loss: 61.965 (±3.68306); valid loss: 72.677\n",
      "[Epoch 144/350, Step 6600, ETA 1m 1.298s] step time: 0.006091s (±0.008578s); valid time: 0.08559s; loss: 61.573 (±3.50557); valid loss: 72.4741 (*)\n",
      "[Epoch 146/350, Step 6700, ETA 1m 0.6266s] step time: 0.005791s (±0.001834s); valid time: 0.008984s; loss: 62.6164 (±6.17829); valid loss: 72.5109\n",
      "[Epoch 148/350, Step 6800, ETA 59.91s] step time: 0.005444s (±0.001387s); valid time: 0.008415s; loss: 61.3199 (±3.57979); valid loss: 72.6267\n",
      "[Epoch 150/350, Step 6900, ETA 59.19s] step time: 0.005355s (±0.001106s); valid time: 0.008287s; loss: 61.5794 (±3.57558); valid loss: 72.7527\n",
      "[Epoch 150/350, Step 6900, ETA 59.19s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 58.5s] step time: 0.005519s (±0.001611s); valid time: 0.008425s; loss: 63.9989 (±21.8077); valid loss: 72.7601\n",
      "[Epoch 155/350, Step 7100, ETA 57.79s] step time: 0.005382s (±0.001293s); valid time: 0.008498s; loss: 61.7227 (±3.83002); valid loss: 72.6187\n",
      "[Epoch 157/350, Step 7200, ETA 57.08s] step time: 0.005444s (±0.001547s); valid time: 0.01275s; loss: 64.3298 (±28.2973); valid loss: 72.7285\n",
      "[Epoch 159/350, Step 7300, ETA 56.38s] step time: 0.005416s (±0.001325s); valid time: 0.008584s; loss: 62.8976 (±13.2762); valid loss: 72.6417\n",
      "[Epoch 160/350, Step 7360, ETA 55.95s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 55.69s] step time: 0.005516s (±0.001563s); valid time: 0.008517s; loss: 61.7041 (±3.64782); valid loss: 72.7053\n",
      "[Epoch 164/350, Step 7500, ETA 55.04s] step time: 0.005784s (±0.002213s); valid time: 0.008861s; loss: 64.0012 (±18.6749); valid loss: 72.6618\n",
      "[Epoch 166/350, Step 7600, ETA 54.43s] step time: 0.006168s (±0.009106s); valid time: 0.09076s; loss: 61.6934 (±3.76942); valid loss: 72.4651 (*)\n",
      "[Epoch 168/350, Step 7700, ETA 53.75s] step time: 0.005501s (±0.001329s); valid time: 0.008766s; loss: 1210.53 (±11432.2); valid loss: 72.6852\n",
      "[Epoch 170/350, Step 7800, ETA 53.04s] step time: 0.005373s (±0.00109s); valid time: 0.009171s; loss: 1573.05 (±15032.8); valid loss: 72.5282\n",
      "[Epoch 170/350, Step 7820, ETA 52.9s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 52.38s] step time: 0.005555s (±0.001438s); valid time: 0.008699s; loss: 61.2434 (±3.43684); valid loss: 72.5768\n",
      "[Epoch 174/350, Step 8000, ETA 51.69s] step time: 0.005498s (±0.001781s); valid time: 0.008762s; loss: 62.2166 (±5.66164); valid loss: 72.6351\n",
      "[Epoch 177/350, Step 8100, ETA 51.01s] step time: 0.005392s (±0.00137s); valid time: 0.008729s; loss: 114.783 (±528.303); valid loss: 72.5582\n",
      "[Epoch 179/350, Step 8200, ETA 50.33s] step time: 0.005432s (±0.00158s); valid time: 0.008926s; loss: 63.4853 (±17.477); valid loss: 72.5806\n",
      "[Epoch 180/350, Step 8280, ETA 49.77s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.64s] step time: 0.005376s (±0.001232s); valid time: 0.008616s; loss: 61.8285 (±3.93724); valid loss: 72.5763\n",
      "[Epoch 183/350, Step 8400, ETA 48.97s] step time: 0.005472s (±0.001459s); valid time: 0.00909s; loss: 61.5712 (±3.86953); valid loss: 72.6181\n",
      "[Epoch 185/350, Step 8500, ETA 48.29s] step time: 0.005483s (±0.001512s); valid time: 0.008655s; loss: 62.5786 (±11.5928); valid loss: 72.6241\n",
      "[Epoch 187/350, Step 8600, ETA 47.62s] step time: 0.005461s (±0.001325s); valid time: 0.009383s; loss: 74.626 (±121.88); valid loss: 72.6701\n",
      "[Epoch 190/350, Step 8700, ETA 46.96s] step time: 0.005466s (±0.001531s); valid time: 0.008798s; loss: 61.6617 (±3.49075); valid loss: 72.6314\n",
      "[Epoch 190/350, Step 8740, ETA 46.68s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 46.29s] step time: 0.005448s (±0.001552s); valid time: 0.008837s; loss: 62.3343 (±7.17784); valid loss: 72.4927\n",
      "[Epoch 194/350, Step 8900, ETA 45.62s] step time: 0.005466s (±0.001589s); valid time: 0.008852s; loss: 61.838 (±4.1115); valid loss: 72.626\n",
      "[Epoch 196/350, Step 9000, ETA 44.95s] step time: 0.005375s (±0.001239s); valid time: 0.008435s; loss: 61.5188 (±3.67333); valid loss: 72.5817\n",
      "[Epoch 198/350, Step 9100, ETA 44.29s] step time: 0.005552s (±0.001652s); valid time: 0.008386s; loss: 61.6428 (±3.69535); valid loss: 72.5832\n",
      "[Epoch 200/350, Step 9200, ETA 43.62s] step time: 0.005399s (±0.001523s); valid time: 0.008272s; loss: 991.799 (±9251.24); valid loss: 72.4948\n",
      "[Epoch 200/350, Step 9200, ETA 43.62s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 42.98s] step time: 0.005556s (±0.00151s); valid time: 0.008702s; loss: 186.098 (±1219.02); valid loss: 72.6851\n",
      "[Epoch 205/350, Step 9400, ETA 42.32s] step time: 0.005425s (±0.001583s); valid time: 0.008627s; loss: 1664.38 (±15821.6); valid loss: 72.6721\n",
      "[Epoch 207/350, Step 9500, ETA 41.64s] step time: 0.00528s (±0.001244s); valid time: 0.008692s; loss: 62.2429 (±9.30855); valid loss: 72.6192\n",
      "[Epoch 209/350, Step 9600, ETA 40.99s] step time: 0.005515s (±0.001608s); valid time: 0.008372s; loss: 61.7028 (±3.38003); valid loss: 72.6059\n",
      "[Epoch 210/350, Step 9660, ETA 40.59s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.34s] step time: 0.005514s (±0.001645s); valid time: 0.009424s; loss: 77.6675 (±162.58); valid loss: 72.5705\n",
      "[Epoch 214/350, Step 9800, ETA 39.7s] step time: 0.005517s (±0.001537s); valid time: 0.009297s; loss: 61.4981 (±3.38722); valid loss: 72.565\n",
      "[Epoch 216/350, Step 9900, ETA 39.05s] step time: 0.005497s (±0.001695s); valid time: 0.008437s; loss: 110.98 (±490.505); valid loss: 72.5552\n",
      "[Epoch 218/350, Step 10000, ETA 38.38s] step time: 0.005198s (±0.001042s); valid time: 0.008051s; loss: 70.115 (±86.9261); valid loss: 72.6065\n",
      "[Epoch 220/350, Step 10100, ETA 37.73s] step time: 0.005416s (±0.00155s); valid time: 0.008833s; loss: 61.7459 (±3.61041); valid loss: 72.6866\n",
      "[Epoch 220/350, Step 10120, ETA 37.59s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 37.08s] step time: 0.005504s (±0.001297s); valid time: 0.008487s; loss: 69.4459 (±69.841); valid loss: 72.5578\n",
      "[Epoch 224/350, Step 10300, ETA 36.43s] step time: 0.005477s (±0.001544s); valid time: 0.008301s; loss: 63.4797 (±18.4794); valid loss: 72.6138\n",
      "[Epoch 227/350, Step 10400, ETA 35.79s] step time: 0.005448s (±0.001636s); valid time: 0.009031s; loss: 61.6075 (±3.29572); valid loss: 72.611\n",
      "[Epoch 229/350, Step 10500, ETA 35.15s] step time: 0.005457s (±0.001454s); valid time: 0.008556s; loss: 61.676 (±3.88568); valid loss: 72.529\n",
      "[Epoch 230/350, Step 10580, ETA 34.63s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.56s] step time: 0.006485s (±0.009172s); valid time: 0.09194s; loss: 61.2967 (±3.82455); valid loss: 72.4602 (*)\n",
      "[Epoch 233/350, Step 10700, ETA 33.91s] step time: 0.005463s (±0.001205s); valid time: 0.008812s; loss: 61.6944 (±4.05117); valid loss: 72.6542\n",
      "[Epoch 235/350, Step 10800, ETA 33.27s] step time: 0.005619s (±0.001815s); valid time: 0.009155s; loss: 61.4928 (±3.26323); valid loss: 72.5717\n",
      "[Epoch 237/350, Step 10900, ETA 32.62s] step time: 0.005389s (±0.001491s); valid time: 0.008472s; loss: 62.5035 (±11.105); valid loss: 72.619\n",
      "[Epoch 240/350, Step 11000, ETA 31.99s] step time: 0.00556s (±0.001735s); valid time: 0.008839s; loss: 63.1702 (±15.8559); valid loss: 72.6263\n",
      "[Epoch 240/350, Step 11040, ETA 31.73s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.36s] step time: 0.00577s (±0.002023s); valid time: 0.009545s; loss: 61.5478 (±3.38316); valid loss: 72.6223\n",
      "[Epoch 244/350, Step 11200, ETA 30.72s] step time: 0.005427s (±0.001061s); valid time: 0.008806s; loss: 62.1854 (±5.32289); valid loss: 72.5737\n",
      "[Epoch 246/350, Step 11300, ETA 30.08s] step time: 0.005577s (±0.001558s); valid time: 0.008335s; loss: 62.2259 (±8.10941); valid loss: 72.6196\n",
      "[Epoch 248/350, Step 11400, ETA 29.44s] step time: 0.005493s (±0.001558s); valid time: 0.008499s; loss: 61.8099 (±4.97561); valid loss: 72.5065\n",
      "[Epoch 250/350, Step 11500, ETA 28.8s] step time: 0.005515s (±0.001265s); valid time: 0.008811s; loss: 61.5393 (±3.73885); valid loss: 72.5507\n",
      "[Epoch 250/350, Step 11500, ETA 28.8s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.17s] step time: 0.005511s (±0.001425s); valid time: 0.008765s; loss: 77.4876 (±139.395); valid loss: 72.6453\n",
      "[Epoch 255/350, Step 11700, ETA 27.54s] step time: 0.005613s (±0.001468s); valid time: 0.008503s; loss: 62.7504 (±15.3767); valid loss: 72.5394\n",
      "[Epoch 257/350, Step 11800, ETA 26.9s] step time: 0.00549s (±0.001443s); valid time: 0.00859s; loss: 61.999 (±4.29589); valid loss: 72.6686\n",
      "[Epoch 259/350, Step 11900, ETA 26.26s] step time: 0.005415s (±0.00172s); valid time: 0.01228s; loss: 61.683 (±3.68691); valid loss: 72.5408\n",
      "[Epoch 260/350, Step 11960, ETA 25.88s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.63s] step time: 0.00563s (±0.001596s); valid time: 0.008576s; loss: 61.6719 (±3.76607); valid loss: 72.639\n",
      "[Epoch 264/350, Step 12100, ETA 25.01s] step time: 0.005581s (±0.001659s); valid time: 0.008734s; loss: 62.0945 (±6.91258); valid loss: 72.6598\n",
      "[Epoch 266/350, Step 12200, ETA 24.4s] step time: 0.00636s (±0.00917s); valid time: 0.09123s; loss: 62.6569 (±6.42794); valid loss: 72.3837 (*)\n",
      "[Epoch 268/350, Step 12300, ETA 23.77s] step time: 0.005648s (±0.001598s); valid time: 0.008667s; loss: 61.9791 (±4.37294); valid loss: 72.5117\n",
      "[Epoch 270/350, Step 12400, ETA 23.13s] step time: 0.005498s (±0.00145s); valid time: 0.008529s; loss: 61.2327 (±3.83456); valid loss: 72.5532\n",
      "[Epoch 270/350, Step 12420, ETA 23s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.5s] step time: 0.005434s (±0.00126s); valid time: 0.008801s; loss: 61.6495 (±3.60281); valid loss: 72.55\n",
      "[Epoch 274/350, Step 12600, ETA 21.87s] step time: 0.00554s (±0.001709s); valid time: 0.008675s; loss: 61.602 (±3.28278); valid loss: 72.7151\n",
      "[Epoch 277/350, Step 12700, ETA 21.24s] step time: 0.00554s (±0.002036s); valid time: 0.009057s; loss: 61.5212 (±3.12271); valid loss: 72.4806\n",
      "[Epoch 279/350, Step 12800, ETA 20.61s] step time: 0.005569s (±0.001425s); valid time: 0.008517s; loss: 100.617 (±382.908); valid loss: 72.6026\n",
      "[Epoch 280/350, Step 12880, ETA 20.1s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.98s] step time: 0.0056s (±0.001899s); valid time: 0.008597s; loss: 64.4098 (±18.8337); valid loss: 72.6545\n",
      "[Epoch 283/350, Step 13000, ETA 19.35s] step time: 0.005291s (±0.001243s); valid time: 0.008419s; loss: 61.7846 (±4.48796); valid loss: 72.4794\n",
      "[Epoch 285/350, Step 13100, ETA 18.72s] step time: 0.005544s (±0.001598s); valid time: 0.008654s; loss: 62.1739 (±8.43637); valid loss: 72.6798\n",
      "[Epoch 287/350, Step 13200, ETA 18.08s] step time: 0.005379s (±0.001242s); valid time: 0.009011s; loss: 73.7065 (±89.4704); valid loss: 72.6494\n",
      "[Epoch 290/350, Step 13300, ETA 17.46s] step time: 0.005623s (±0.001545s); valid time: 0.008496s; loss: 61.4387 (±3.27921); valid loss: 72.5872\n",
      "[Epoch 290/350, Step 13340, ETA 17.2s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.83s] step time: 0.005528s (±0.001825s); valid time: 0.008647s; loss: 61.623 (±3.63805); valid loss: 72.5874\n",
      "[Epoch 294/350, Step 13500, ETA 16.2s] step time: 0.005574s (±0.001461s); valid time: 0.008359s; loss: 61.8969 (±3.68628); valid loss: 72.4516\n",
      "[Epoch 296/350, Step 13600, ETA 15.57s] step time: 0.005458s (±0.001544s); valid time: 0.008704s; loss: 79.8189 (±183.713); valid loss: 72.5506\n",
      "[Epoch 298/350, Step 13700, ETA 14.95s] step time: 0.005404s (±0.001257s); valid time: 0.008599s; loss: 61.7908 (±3.75593); valid loss: 72.5556\n",
      "[Epoch 300/350, Step 13800, ETA 14.32s] step time: 0.005524s (±0.001427s); valid time: 0.008685s; loss: 61.442 (±3.67199); valid loss: 72.6971\n",
      "[Epoch 300/350, Step 13800, ETA 14.32s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.69s] step time: 0.005441s (±0.001368s); valid time: 0.009224s; loss: 61.5098 (±3.86293); valid loss: 72.5032\n",
      "[Epoch 305/350, Step 14000, ETA 13.07s] step time: 0.005498s (±0.001618s); valid time: 0.008692s; loss: 61.791 (±3.71819); valid loss: 72.532\n",
      "[Epoch 307/350, Step 14100, ETA 12.44s] step time: 0.005661s (±0.00174s); valid time: 0.008529s; loss: 61.512 (±3.96519); valid loss: 72.5272\n",
      "[Epoch 309/350, Step 14200, ETA 11.82s] step time: 0.005449s (±0.001196s); valid time: 0.008647s; loss: 61.5268 (±3.60593); valid loss: 72.7196\n",
      "[Epoch 310/350, Step 14260, ETA 11.44s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.19s] step time: 0.005714s (±0.001968s); valid time: 0.008617s; loss: 62.3245 (±7.28559); valid loss: 72.6185\n",
      "[Epoch 314/350, Step 14400, ETA 10.57s] step time: 0.005657s (±0.001723s); valid time: 0.008088s; loss: 61.4227 (±3.39983); valid loss: 72.5709\n",
      "[Epoch 316/350, Step 14500, ETA 9.951s] step time: 0.005805s (±0.001715s); valid time: 0.009104s; loss: 61.5365 (±3.15107); valid loss: 72.6254\n",
      "[Epoch 318/350, Step 14600, ETA 9.327s] step time: 0.005397s (±0.001217s); valid time: 0.008478s; loss: 62.344 (±10.8194); valid loss: 72.5643\n",
      "[Epoch 320/350, Step 14700, ETA 8.702s] step time: 0.005465s (±0.001463s); valid time: 0.009453s; loss: 61.5975 (±3.70255); valid loss: 72.6352\n",
      "[Epoch 320/350, Step 14720, ETA 8.576s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.078s] step time: 0.00548s (±0.00136s); valid time: 0.008298s; loss: 66.0668 (±43.5904); valid loss: 72.5577\n",
      "[Epoch 324/350, Step 14900, ETA 7.456s] step time: 0.005592s (±0.001371s); valid time: 0.008578s; loss: 62.1184 (±7.66336); valid loss: 72.5085\n",
      "[Epoch 327/350, Step 15000, ETA 6.834s] step time: 0.005527s (±0.001726s); valid time: 0.008458s; loss: 62.2595 (±8.65044); valid loss: 72.6374\n",
      "[Epoch 329/350, Step 15100, ETA 6.21s] step time: 0.005423s (±0.001516s); valid time: 0.008765s; loss: 61.7137 (±3.7905); valid loss: 72.5051\n",
      "[Epoch 330/350, Step 15180, ETA 5.711s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.587s] step time: 0.005448s (±0.001382s); valid time: 0.008992s; loss: 61.4628 (±4.03912); valid loss: 72.535\n",
      "[Epoch 333/350, Step 15300, ETA 4.965s] step time: 0.005494s (±0.001683s); valid time: 0.009121s; loss: 61.7742 (±3.57018); valid loss: 72.5121\n",
      "[Epoch 335/350, Step 15400, ETA 4.343s] step time: 0.0055s (±0.001295s); valid time: 0.008461s; loss: 69.4847 (±78.5618); valid loss: 72.5301\n",
      "[Epoch 337/350, Step 15500, ETA 3.723s] step time: 0.00567s (±0.001635s); valid time: 0.01007s; loss: 61.546 (±3.14407); valid loss: 72.6801\n",
      "[Epoch 340/350, Step 15600, ETA 3.102s] step time: 0.005665s (±0.001653s); valid time: 0.009212s; loss: 61.6669 (±4.03498); valid loss: 72.5787\n",
      "[Epoch 340/350, Step 15640, ETA 2.854s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.482s] step time: 0.005574s (±0.001822s); valid time: 0.008366s; loss: 62.7012 (±12.477); valid loss: 72.5051\n",
      "[Epoch 344/350, Step 15800, ETA 1.861s] step time: 0.005637s (±0.001614s); valid time: 0.008608s; loss: 63.0396 (±13.4487); valid loss: 72.5243\n",
      "[Epoch 346/350, Step 15900, ETA 1.24s] step time: 0.005579s (±0.001529s); valid time: 0.008576s; loss: 61.9282 (±4.73882); valid loss: 72.694\n",
      "[Epoch 348/350, Step 16000, ETA 0.6202s] step time: 0.005642s (±0.001605s); valid time: 0.008234s; loss: 62.0187 (±4.9552); valid loss: 72.6219\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005774s (±0.001963s); valid time: 0.01241s; loss: 62.5522 (±10.6877); valid loss: 72.6599\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzbr0e_fp/variables.dat-12200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzbr0e_fp/variables.dat-12200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.1,\n",
      "\t(tp, fp, tn, fn)=(1582, 14250, 1, 0),\n",
      "\tprecision=0.1,\n",
      "\trecall=1.0,\n",
      "\tf1=0.18,\n",
      "\troc_auc=0.5,\n",
      "\ty_pred%=0.9999368407755953,\n",
      "\ty_label%=0.09991789300827386,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_GOOG.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 49.99s] step time: 0.01003s (±0.03343s); valid time: 0.1699s; loss: 126.934 (±10.5444); valid loss: 120.442 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 21.2s] step time: 0.006669s (±0.008946s); valid time: 0.08801s; loss: 101.067 (±5.61547); valid loss: 91.4859 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 8.844s] step time: 0.006238s (±0.008726s); valid time: 0.08681s; loss: 95.5151 (±3.83999); valid loss: 88.2349 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 3.135s] step time: 0.006447s (±0.009617s); valid time: 0.09611s; loss: 93.9099 (±3.81807); valid loss: 86.659 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 58.5s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 2m 0.1446s] step time: 0.006655s (±0.009805s); valid time: 0.09716s; loss: 93.2303 (±3.86139); valid loss: 85.9563 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 55.8s] step time: 0.005716s (±0.00191s); valid time: 0.009105s; loss: 93.1662 (±3.66592); valid loss: 85.9642\n",
      "[Epoch 16/350, Step 700, ETA 1m 53.58s] step time: 0.006322s (±0.008626s); valid time: 0.08624s; loss: 92.4533 (±3.99559); valid loss: 85.2282 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 51.91s] step time: 0.006434s (±0.008787s); valid time: 0.08759s; loss: 91.9625 (±3.74273); valid loss: 84.3221 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 49.99s] step time: 0.006158s (±0.00852s); valid time: 0.08524s; loss: 90.3718 (±3.80402); valid loss: 81.8093 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 49.69s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 48.87s] step time: 0.006515s (±0.009099s); valid time: 0.0901s; loss: 88.0492 (±3.32687); valid loss: 80.1969 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 47.48s] step time: 0.006261s (±0.008841s); valid time: 0.08783s; loss: 86.6363 (±4.04751); valid loss: 78.848 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 46.26s] step time: 0.006194s (±0.008563s); valid time: 0.0854s; loss: 85.8342 (±3.70104); valid loss: 78.0953 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.29s] step time: 0.005579s (±0.001549s); valid time: 0.009209s; loss: 84.9939 (±3.42133); valid loss: 78.3461\n",
      "[Epoch 30/350, Step 1380, ETA 1m 42.63s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 42.5s] step time: 0.005541s (±0.001558s); valid time: 0.009119s; loss: 84.5466 (±3.30959); valid loss: 78.2229\n",
      "[Epoch 33/350, Step 1500, ETA 1m 40.61s] step time: 0.005298s (±0.001355s); valid time: 0.008255s; loss: 84.3886 (±3.66066); valid loss: 78.3187\n",
      "[Epoch 35/350, Step 1600, ETA 1m 39.86s] step time: 0.006365s (±0.008403s); valid time: 0.08344s; loss: 84.1137 (±2.80246); valid loss: 77.9373 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 38.97s] step time: 0.006205s (±0.009047s); valid time: 0.08996s; loss: 83.8814 (±3.14524); valid loss: 77.824 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 38.28s] step time: 0.006305s (±0.008603s); valid time: 0.08524s; loss: 83.7212 (±3.7704); valid loss: 76.9867 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 37.64s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 37.11s] step time: 0.005765s (±0.001957s); valid time: 0.009121s; loss: 83.5198 (±3.75116); valid loss: 77.5274\n",
      "[Epoch 44/350, Step 2000, ETA 1m 35.68s] step time: 0.005319s (±0.001007s); valid time: 0.008655s; loss: 83.0915 (±3.24771); valid loss: 77.4833\n",
      "[Epoch 46/350, Step 2100, ETA 1m 34.45s] step time: 0.00546s (±0.001603s); valid time: 0.01204s; loss: 83.2998 (±4.22593); valid loss: 77.101\n",
      "[Epoch 48/350, Step 2200, ETA 1m 33.28s] step time: 0.005499s (±0.001606s); valid time: 0.01174s; loss: 83.9053 (±8.66566); valid loss: 77.4663\n",
      "[Epoch 50/350, Step 2300, ETA 1m 32.08s] step time: 0.005389s (±0.001518s); valid time: 0.008477s; loss: 82.876 (±3.45146); valid loss: 77.4764\n",
      "[Epoch 50/350, Step 2300, ETA 1m 32.08s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 31.51s] step time: 0.006231s (±0.008444s); valid time: 0.08452s; loss: 86.0061 (±28.7822); valid loss: 76.799 (*)\n",
      "[Epoch 55/350, Step 2500, ETA 1m 30.33s] step time: 0.005171s (±0.0009489s); valid time: 0.00823s; loss: 82.6609 (±3.52741); valid loss: 77.1966\n",
      "[Epoch 57/350, Step 2600, ETA 1m 29.24s] step time: 0.005358s (±0.001498s); valid time: 0.008296s; loss: 83.7042 (±7.76179); valid loss: 77.3487\n",
      "[Epoch 59/350, Step 2700, ETA 1m 28.35s] step time: 0.005697s (±0.002206s); valid time: 0.009383s; loss: 82.376 (±3.38938); valid loss: 77.2963\n",
      "[Epoch 60/350, Step 2760, ETA 1m 27.74s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 27.41s] step time: 0.005521s (±0.00185s); valid time: 0.008446s; loss: 82.4127 (±3.66066); valid loss: 77.2082\n",
      "[Epoch 64/350, Step 2900, ETA 1m 26.51s] step time: 0.005482s (±0.001786s); valid time: 0.009139s; loss: 82.3948 (±3.34828); valid loss: 77.1918\n",
      "[Epoch 66/350, Step 3000, ETA 1m 25.55s] step time: 0.005417s (±0.00154s); valid time: 0.008846s; loss: 395.157 (±3088.91); valid loss: 77.6378\n",
      "[Epoch 68/350, Step 3100, ETA 1m 24.62s] step time: 0.005283s (±0.001161s); valid time: 0.008546s; loss: 82.3664 (±4.07292); valid loss: 76.9724\n",
      "[Epoch 70/350, Step 3200, ETA 1m 23.68s] step time: 0.005333s (±0.001401s); valid time: 0.009041s; loss: 82.2903 (±3.76244); valid loss: 77.3401\n",
      "[Epoch 70/350, Step 3220, ETA 1m 23.51s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 22.82s] step time: 0.005469s (±0.001512s); valid time: 0.008561s; loss: 82.1863 (±3.57099); valid loss: 76.9559\n",
      "[Epoch 74/350, Step 3400, ETA 1m 21.97s] step time: 0.005486s (±0.001791s); valid time: 0.00902s; loss: 81.8644 (±3.94075); valid loss: 77.0781\n",
      "[Epoch 77/350, Step 3500, ETA 1m 21.26s] step time: 0.005704s (±0.001773s); valid time: 0.009031s; loss: 84.3573 (±17.9132); valid loss: 77.0442\n",
      "[Epoch 79/350, Step 3600, ETA 1m 20.44s] step time: 0.005483s (±0.001526s); valid time: 0.008977s; loss: 88.7753 (±69.7526); valid loss: 77.1156\n",
      "[Epoch 80/350, Step 3680, ETA 1m 19.72s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 19.6s] step time: 0.005357s (±0.001314s); valid time: 0.008745s; loss: 81.7665 (±3.45799); valid loss: 77.3229\n",
      "[Epoch 83/350, Step 3800, ETA 1m 18.81s] step time: 0.00548s (±0.001396s); valid time: 0.008592s; loss: 81.8387 (±3.75173); valid loss: 77.1625\n",
      "[Epoch 85/350, Step 3900, ETA 1m 18.05s] step time: 0.005568s (±0.00197s); valid time: 0.008764s; loss: 82.2586 (±5.37048); valid loss: 77.1844\n",
      "[Epoch 87/350, Step 4000, ETA 1m 17.47s] step time: 0.006157s (±0.008252s); valid time: 0.08226s; loss: 81.9698 (±4.00643); valid loss: 76.6594 (*)\n",
      "[Epoch 90/350, Step 4100, ETA 1m 16.78s] step time: 0.005652s (±0.001486s); valid time: 0.009153s; loss: 82.0853 (±4.21294); valid loss: 76.7146\n",
      "[Epoch 90/350, Step 4140, ETA 1m 16.43s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 16s] step time: 0.005437s (±0.001902s); valid time: 0.01008s; loss: 82.6182 (±9.54755); valid loss: 77.0885\n",
      "[Epoch 94/350, Step 4300, ETA 1m 15.3s] step time: 0.005722s (±0.001703s); valid time: 0.008948s; loss: 81.6959 (±3.60432); valid loss: 76.9047\n",
      "[Epoch 96/350, Step 4400, ETA 1m 14.57s] step time: 0.005561s (±0.001751s); valid time: 0.008891s; loss: 81.707 (±3.62134); valid loss: 77.1003\n",
      "[Epoch 98/350, Step 4500, ETA 1m 13.98s] step time: 0.006111s (±0.008995s); valid time: 0.08755s; loss: 81.8057 (±4.02261); valid loss: 76.5902 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 13.27s] step time: 0.005626s (±0.002255s); valid time: 0.01775s; loss: 81.604 (±3.56879); valid loss: 76.78\n",
      "[Epoch 100/350, Step 4600, ETA 1m 13.27s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 12.56s] step time: 0.005478s (±0.001776s); valid time: 0.008858s; loss: 84.4675 (±22.9151); valid loss: 77.638\n",
      "[Epoch 105/350, Step 4800, ETA 1m 11.78s] step time: 0.005308s (±0.001588s); valid time: 0.008325s; loss: 81.9952 (±5.577); valid loss: 77.0609\n",
      "[Epoch 107/350, Step 4900, ETA 1m 11.07s] step time: 0.005573s (±0.001453s); valid time: 0.008209s; loss: 81.2918 (±3.60086); valid loss: 77.2227\n",
      "[Epoch 109/350, Step 5000, ETA 1m 10.35s] step time: 0.00554s (±0.001552s); valid time: 0.008826s; loss: 82.5762 (±7.97291); valid loss: 76.8685\n",
      "[Epoch 110/350, Step 5060, ETA 1m 9.941s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 9.675s] step time: 0.00565s (±0.001891s); valid time: 0.008269s; loss: 81.2221 (±3.52072); valid loss: 76.9378\n",
      "[Epoch 114/350, Step 5200, ETA 1m 8.992s] step time: 0.005502s (±0.001639s); valid time: 0.008614s; loss: 81.2484 (±3.90145); valid loss: 77.1111\n",
      "[Epoch 116/350, Step 5300, ETA 1m 8.263s] step time: 0.005408s (±0.001655s); valid time: 0.008884s; loss: 81.5749 (±3.25993); valid loss: 77.0136\n",
      "[Epoch 118/350, Step 5400, ETA 1m 7.566s] step time: 0.005422s (±0.001258s); valid time: 0.008549s; loss: 81.6222 (±3.60398); valid loss: 77.0017\n",
      "[Epoch 120/350, Step 5500, ETA 1m 6.848s] step time: 0.005362s (±0.001354s); valid time: 0.009075s; loss: 81.527 (±4.63018); valid loss: 77.1521\n",
      "[Epoch 120/350, Step 5520, ETA 1m 6.689s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 6.134s] step time: 0.005316s (±0.001288s); valid time: 0.008616s; loss: 84.4764 (±30.2608); valid loss: 76.8443\n",
      "[Epoch 124/350, Step 5700, ETA 1m 5.42s] step time: 0.005386s (±0.001236s); valid time: 0.008645s; loss: 81.7211 (±4.52783); valid loss: 77.038\n",
      "[Epoch 127/350, Step 5800, ETA 1m 4.786s] step time: 0.005692s (±0.001394s); valid time: 0.008679s; loss: 89.8338 (±73.0592); valid loss: 77.381\n",
      "[Epoch 129/350, Step 5900, ETA 1m 4.077s] step time: 0.005381s (±0.001413s); valid time: 0.008055s; loss: 83.9026 (±25.0955); valid loss: 77.4793\n",
      "[Epoch 130/350, Step 5980, ETA 1m 3.502s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 3.383s] step time: 0.005395s (±0.001511s); valid time: 0.0083s; loss: 81.1004 (±3.60515); valid loss: 76.9966\n",
      "[Epoch 133/350, Step 6100, ETA 1m 2.713s] step time: 0.00558s (±0.001645s); valid time: 0.008418s; loss: 81.695 (±4.96007); valid loss: 77.386\n",
      "[Epoch 135/350, Step 6200, ETA 1m 2.002s] step time: 0.005293s (±0.001607s); valid time: 0.009538s; loss: 81.5562 (±3.61871); valid loss: 77.2336\n",
      "[Epoch 137/350, Step 6300, ETA 1m 1.332s] step time: 0.005535s (±0.001673s); valid time: 0.009177s; loss: 83.626 (±26.0257); valid loss: 77.4153\n",
      "[Epoch 140/350, Step 6400, ETA 1m 0.6699s] step time: 0.005453s (±0.001241s); valid time: 0.008502s; loss: 82.2925 (±9.72067); valid loss: 76.9871\n",
      "[Epoch 140/350, Step 6440, ETA 1m 0.4168s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 0.02555s] step time: 0.005629s (±0.001815s); valid time: 0.008624s; loss: 82.1231 (±9.26329); valid loss: 77.1137\n",
      "[Epoch 144/350, Step 6600, ETA 59.35s] step time: 0.005437s (±0.001496s); valid time: 0.008405s; loss: 81.1558 (±3.55919); valid loss: 77.1822\n",
      "[Epoch 146/350, Step 6700, ETA 58.65s] step time: 0.005281s (±0.001311s); valid time: 0.008371s; loss: 82.8214 (±16.6727); valid loss: 77.3406\n",
      "[Epoch 148/350, Step 6800, ETA 58.01s] step time: 0.005642s (±0.001514s); valid time: 0.008634s; loss: 81.2845 (±3.57149); valid loss: 77.5651\n",
      "[Epoch 150/350, Step 6900, ETA 57.34s] step time: 0.00541s (±0.001542s); valid time: 0.008555s; loss: 83.5309 (±23.0965); valid loss: 77.3025\n",
      "[Epoch 150/350, Step 6900, ETA 57.34s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 56.68s] step time: 0.005416s (±0.001338s); valid time: 0.008459s; loss: 84.3533 (±28.8144); valid loss: 77.3498\n",
      "[Epoch 155/350, Step 7100, ETA 56s] step time: 0.005186s (±0.001573s); valid time: 0.01467s; loss: 80.849 (±3.32473); valid loss: 77.1152\n",
      "[Epoch 157/350, Step 7200, ETA 55.33s] step time: 0.005388s (±0.001364s); valid time: 0.008771s; loss: 81.15 (±3.41601); valid loss: 77.0002\n",
      "[Epoch 159/350, Step 7300, ETA 54.66s] step time: 0.005371s (±0.001406s); valid time: 0.008165s; loss: 81.2932 (±3.29856); valid loss: 77.1704\n",
      "[Epoch 160/350, Step 7360, ETA 54.25s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 54.02s] step time: 0.005625s (±0.002027s); valid time: 0.008705s; loss: 86.7878 (±57.4); valid loss: 77.3323\n",
      "[Epoch 164/350, Step 7500, ETA 53.39s] step time: 0.005567s (±0.001672s); valid time: 0.01229s; loss: 83.5369 (±22.8973); valid loss: 77.2049\n",
      "[Epoch 166/350, Step 7600, ETA 52.73s] step time: 0.005393s (±0.001579s); valid time: 0.008286s; loss: 82.6076 (±13.1262); valid loss: 77.0569\n",
      "[Epoch 168/350, Step 7700, ETA 52.1s] step time: 0.005594s (±0.001708s); valid time: 0.008508s; loss: 81.1613 (±3.1423); valid loss: 77.1817\n",
      "[Epoch 170/350, Step 7800, ETA 51.44s] step time: 0.005394s (±0.001388s); valid time: 0.008459s; loss: 81.1856 (±3.42977); valid loss: 76.8361\n",
      "[Epoch 170/350, Step 7820, ETA 51.31s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 50.8s] step time: 0.005569s (±0.001483s); valid time: 0.008677s; loss: 81.061 (±3.50959); valid loss: 77.0841\n",
      "[Epoch 174/350, Step 8000, ETA 50.15s] step time: 0.005415s (±0.001468s); valid time: 0.008699s; loss: 82.1415 (±6.97395); valid loss: 77.0916\n",
      "[Epoch 177/350, Step 8100, ETA 49.51s] step time: 0.0054s (±0.001325s); valid time: 0.009145s; loss: 82.3317 (±14.5309); valid loss: 77.0282\n",
      "[Epoch 179/350, Step 8200, ETA 48.87s] step time: 0.005525s (±0.00151s); valid time: 0.008413s; loss: 85.1307 (±39.0646); valid loss: 77.1639\n",
      "[Epoch 180/350, Step 8280, ETA 48.36s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 48.24s] step time: 0.005491s (±0.001546s); valid time: 0.01006s; loss: 81.1725 (±3.31493); valid loss: 77.0554\n",
      "[Epoch 183/350, Step 8400, ETA 47.6s] step time: 0.005462s (±0.001524s); valid time: 0.008686s; loss: 81.3315 (±5.12756); valid loss: 77.418\n",
      "[Epoch 185/350, Step 8500, ETA 46.94s] step time: 0.005325s (±0.001415s); valid time: 0.009052s; loss: 81.0659 (±3.37617); valid loss: 77.0876\n",
      "[Epoch 187/350, Step 8600, ETA 46.31s] step time: 0.005484s (±0.001652s); valid time: 0.008583s; loss: 83.404 (±23.1801); valid loss: 77.6571\n",
      "[Epoch 190/350, Step 8700, ETA 45.69s] step time: 0.005595s (±0.001596s); valid time: 0.008772s; loss: 90.9405 (±61.9811); valid loss: 77.019\n",
      "[Epoch 190/350, Step 8740, ETA 45.43s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 45.05s] step time: 0.005408s (±0.001532s); valid time: 0.008327s; loss: 85.3544 (±41.5804); valid loss: 77.1432\n",
      "[Epoch 194/350, Step 8900, ETA 44.42s] step time: 0.005457s (±0.001244s); valid time: 0.008664s; loss: 81.1435 (±4.08768); valid loss: 77.4471\n",
      "[Epoch 196/350, Step 9000, ETA 43.78s] step time: 0.005397s (±0.001442s); valid time: 0.008563s; loss: 81.3317 (±4.13859); valid loss: 77.3707\n",
      "[Epoch 198/350, Step 9100, ETA 43.14s] step time: 0.005389s (±0.001196s); valid time: 0.008312s; loss: 81.2112 (±3.79885); valid loss: 76.9061\n",
      "[Epoch 200/350, Step 9200, ETA 42.52s] step time: 0.005628s (±0.001562s); valid time: 0.008417s; loss: 83.1019 (±12.9915); valid loss: 77.1956\n",
      "[Epoch 200/350, Step 9200, ETA 42.52s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 41.9s] step time: 0.00559s (±0.001859s); valid time: 0.01425s; loss: 894.03 (±8089.27); valid loss: 77.1933\n",
      "[Epoch 205/350, Step 9400, ETA 41.27s] step time: 0.005574s (±0.001646s); valid time: 0.009017s; loss: 81.3288 (±3.38536); valid loss: 77.2589\n",
      "[Epoch 207/350, Step 9500, ETA 40.63s] step time: 0.005249s (±0.00119s); valid time: 0.008369s; loss: 81.1883 (±3.5614); valid loss: 76.9078\n",
      "[Epoch 209/350, Step 9600, ETA 39.99s] step time: 0.005419s (±0.001624s); valid time: 0.00877s; loss: 80.989 (±4.00283); valid loss: 77.1991\n",
      "[Epoch 210/350, Step 9660, ETA 39.61s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 39.38s] step time: 0.005775s (±0.002062s); valid time: 0.008436s; loss: 81.2076 (±3.58615); valid loss: 77.0704\n",
      "[Epoch 214/350, Step 9800, ETA 38.75s] step time: 0.005364s (±0.001617s); valid time: 0.008748s; loss: 80.9337 (±3.73492); valid loss: 77.0971\n",
      "[Epoch 216/350, Step 9900, ETA 38.13s] step time: 0.005597s (±0.001571s); valid time: 0.00847s; loss: 83.515 (±16.3338); valid loss: 77.2273\n",
      "[Epoch 218/350, Step 10000, ETA 37.49s] step time: 0.005202s (±0.001194s); valid time: 0.008288s; loss: 81.9479 (±7.43458); valid loss: 77.1659\n",
      "[Epoch 220/350, Step 10100, ETA 36.86s] step time: 0.005434s (±0.001612s); valid time: 0.008697s; loss: 99.198 (±168.216); valid loss: 77.3159\n",
      "[Epoch 220/350, Step 10120, ETA 36.73s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 36.23s] step time: 0.005487s (±0.001746s); valid time: 0.008606s; loss: 80.9662 (±3.77529); valid loss: 77.198\n",
      "[Epoch 224/350, Step 10300, ETA 35.6s] step time: 0.005417s (±0.001387s); valid time: 0.009509s; loss: 3258.46 (±31387.4); valid loss: 77.111\n",
      "[Epoch 227/350, Step 10400, ETA 34.99s] step time: 0.005561s (±0.001669s); valid time: 0.008401s; loss: 198.533 (±1169.78); valid loss: 77.353\n",
      "[Epoch 229/350, Step 10500, ETA 34.37s] step time: 0.005505s (±0.00129s); valid time: 0.008284s; loss: 1686.23 (±15865.7); valid loss: 77.2834\n",
      "[Epoch 230/350, Step 10580, ETA 33.87s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 33.75s] step time: 0.005541s (±0.001478s); valid time: 0.008571s; loss: 81.7728 (±5.61307); valid loss: 77.3921\n",
      "[Epoch 233/350, Step 10700, ETA 33.12s] step time: 0.005429s (±0.002158s); valid time: 0.01187s; loss: 116.843 (±315.093); valid loss: 77.0973\n",
      "[Epoch 235/350, Step 10800, ETA 32.5s] step time: 0.005446s (±0.001483s); valid time: 0.008524s; loss: 81.4696 (±3.55964); valid loss: 77.1217\n",
      "[Epoch 237/350, Step 10900, ETA 31.87s] step time: 0.005408s (±0.001628s); valid time: 0.00822s; loss: 81.0962 (±3.78649); valid loss: 77.1618\n",
      "[Epoch 240/350, Step 11000, ETA 31.25s] step time: 0.005359s (±0.00144s); valid time: 0.009045s; loss: 81.3765 (±4.8739); valid loss: 77.2885\n",
      "[Epoch 240/350, Step 11040, ETA 30.99s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 30.63s] step time: 0.005562s (±0.00198s); valid time: 0.008451s; loss: 93.2253 (±95.5339); valid loss: 77.0285\n",
      "[Epoch 244/350, Step 11200, ETA 30.02s] step time: 0.005584s (±0.001392s); valid time: 0.008781s; loss: 81.119 (±3.60634); valid loss: 76.969\n",
      "[Epoch 246/350, Step 11300, ETA 29.4s] step time: 0.005514s (±0.001557s); valid time: 0.008576s; loss: 81.4276 (±5.61475); valid loss: 77.2436\n",
      "[Epoch 248/350, Step 11400, ETA 28.78s] step time: 0.005456s (±0.0016s); valid time: 0.008714s; loss: 81.0435 (±3.4415); valid loss: 76.9132\n",
      "[Epoch 250/350, Step 11500, ETA 28.16s] step time: 0.005573s (±0.001438s); valid time: 0.008518s; loss: 81.0844 (±3.55009); valid loss: 76.9922\n",
      "[Epoch 250/350, Step 11500, ETA 28.16s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 27.54s] step time: 0.005388s (±0.001389s); valid time: 0.008324s; loss: 81.0219 (±3.61438); valid loss: 77.3959\n",
      "[Epoch 255/350, Step 11700, ETA 26.93s] step time: 0.005537s (±0.001339s); valid time: 0.008844s; loss: 81.1878 (±3.90111); valid loss: 77.0518\n",
      "[Epoch 257/350, Step 11800, ETA 26.31s] step time: 0.005512s (±0.001501s); valid time: 0.00878s; loss: 81.2487 (±3.31623); valid loss: 77.3832\n",
      "[Epoch 259/350, Step 11900, ETA 25.69s] step time: 0.005478s (±0.001553s); valid time: 0.008291s; loss: 82.027 (±11.0287); valid loss: 77.0402\n",
      "[Epoch 260/350, Step 11960, ETA 25.32s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.08s] step time: 0.005502s (±0.001587s); valid time: 0.008582s; loss: 81.4858 (±5.2752); valid loss: 77.1883\n",
      "[Epoch 264/350, Step 12100, ETA 24.47s] step time: 0.005654s (±0.001862s); valid time: 0.008969s; loss: 83.8118 (±24.1538); valid loss: 77.1266\n",
      "[Epoch 266/350, Step 12200, ETA 23.85s] step time: 0.005349s (±0.001516s); valid time: 0.00853s; loss: 80.9877 (±3.56417); valid loss: 77.1804\n",
      "[Epoch 268/350, Step 12300, ETA 23.23s] step time: 0.005412s (±0.001199s); valid time: 0.008635s; loss: 220.531 (±1369.1); valid loss: 77.57\n",
      "[Epoch 270/350, Step 12400, ETA 22.62s] step time: 0.005517s (±0.001832s); valid time: 0.01149s; loss: 81.0949 (±4.10316); valid loss: 77.3143\n",
      "[Epoch 270/350, Step 12420, ETA 22.49s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22s] step time: 0.005579s (±0.001384s); valid time: 0.008682s; loss: 81.9883 (±7.74424); valid loss: 77.1897\n",
      "[Epoch 274/350, Step 12600, ETA 21.39s] step time: 0.005597s (±0.001398s); valid time: 0.008363s; loss: 83.5443 (±26.6958); valid loss: 77.004\n",
      "[Epoch 277/350, Step 12700, ETA 20.77s] step time: 0.005353s (±0.001312s); valid time: 0.008615s; loss: 81.1888 (±3.64898); valid loss: 77.2144\n",
      "[Epoch 279/350, Step 12800, ETA 20.16s] step time: 0.005518s (±0.001648s); valid time: 0.008761s; loss: 204.05 (±1225.53); valid loss: 76.9628\n",
      "[Epoch 280/350, Step 12880, ETA 19.66s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.54s] step time: 0.005426s (±0.001201s); valid time: 0.008561s; loss: 82.2259 (±9.88127); valid loss: 77.1354\n",
      "[Epoch 283/350, Step 13000, ETA 18.93s] step time: 0.005479s (±0.001582s); valid time: 0.008289s; loss: 81.1323 (±3.57055); valid loss: 76.8044\n",
      "[Epoch 285/350, Step 13100, ETA 18.32s] step time: 0.005595s (±0.001504s); valid time: 0.008807s; loss: 82.5377 (±11.7242); valid loss: 77.2814\n",
      "[Epoch 287/350, Step 13200, ETA 17.7s] step time: 0.005503s (±0.001363s); valid time: 0.008389s; loss: 80.9341 (±3.36777); valid loss: 77.5842\n",
      "[Epoch 290/350, Step 13300, ETA 17.09s] step time: 0.005622s (±0.001624s); valid time: 0.008546s; loss: 89.8058 (±78.5213); valid loss: 76.8988\n",
      "[Epoch 290/350, Step 13340, ETA 16.85s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.48s] step time: 0.005601s (±0.0019s); valid time: 0.008444s; loss: 89.5031 (±84.3627); valid loss: 77.3068\n",
      "[Epoch 294/350, Step 13500, ETA 15.88s] step time: 0.005674s (±0.001429s); valid time: 0.008546s; loss: 81.2739 (±3.35645); valid loss: 77.3123\n",
      "[Epoch 296/350, Step 13600, ETA 15.26s] step time: 0.005644s (±0.001494s); valid time: 0.00885s; loss: 103.682 (±186.799); valid loss: 77.1262\n",
      "[Epoch 298/350, Step 13700, ETA 14.65s] step time: 0.005403s (±0.001247s); valid time: 0.00829s; loss: 81.1721 (±4.56699); valid loss: 77.0467\n",
      "[Epoch 300/350, Step 13800, ETA 14.04s] step time: 0.005604s (±0.00185s); valid time: 0.008557s; loss: 83.3579 (±21.0815); valid loss: 77.348\n",
      "[Epoch 300/350, Step 13800, ETA 14.04s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.43s] step time: 0.005553s (±0.001257s); valid time: 0.009017s; loss: 81.75 (±6.46688); valid loss: 77.0445\n",
      "[Epoch 305/350, Step 14000, ETA 12.82s] step time: 0.00553s (±0.001739s); valid time: 0.008697s; loss: 81.0845 (±3.43838); valid loss: 77.2913\n",
      "[Epoch 307/350, Step 14100, ETA 12.21s] step time: 0.005553s (±0.001538s); valid time: 0.00809s; loss: 81.2874 (±3.7695); valid loss: 77.1292\n",
      "[Epoch 309/350, Step 14200, ETA 11.59s] step time: 0.00544s (±0.001352s); valid time: 0.008696s; loss: 84.975 (±35.7565); valid loss: 77.2632\n",
      "[Epoch 310/350, Step 14260, ETA 11.23s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 10.99s] step time: 0.005803s (±0.002015s); valid time: 0.008438s; loss: 97.208 (±139.281); valid loss: 77.2092\n",
      "[Epoch 314/350, Step 14400, ETA 10.37s] step time: 0.005404s (±0.001572s); valid time: 0.008827s; loss: 83.1085 (±20.6445); valid loss: 77.3946\n",
      "[Epoch 316/350, Step 14500, ETA 9.762s] step time: 0.005433s (±0.001436s); valid time: 0.008573s; loss: 81.1134 (±3.97309); valid loss: 77.1114\n",
      "[Epoch 318/350, Step 14600, ETA 9.151s] step time: 0.00543s (±0.001384s); valid time: 0.008393s; loss: 81.1838 (±3.72919); valid loss: 77.3248\n",
      "[Epoch 320/350, Step 14700, ETA 8.537s] step time: 0.005305s (±0.001473s); valid time: 0.008785s; loss: 85.1609 (±36.36); valid loss: 77.1516\n",
      "[Epoch 320/350, Step 14720, ETA 8.414s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 7.926s] step time: 0.005496s (±0.001544s); valid time: 0.008838s; loss: 182.586 (±1010.17); valid loss: 77.3453\n",
      "[Epoch 324/350, Step 14900, ETA 7.314s] step time: 0.005313s (±0.001257s); valid time: 0.008548s; loss: 85.5759 (±44.4235); valid loss: 77.1327\n",
      "[Epoch 327/350, Step 15000, ETA 6.704s] step time: 0.005477s (±0.001495s); valid time: 0.008975s; loss: 111.32 (±277.252); valid loss: 77.1709\n",
      "[Epoch 329/350, Step 15100, ETA 6.092s] step time: 0.005211s (±0.001424s); valid time: 0.008382s; loss: 81.2779 (±3.62259); valid loss: 77.4295\n",
      "[Epoch 330/350, Step 15180, ETA 5.603s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.482s] step time: 0.005467s (±0.001824s); valid time: 0.01129s; loss: 80.851 (±3.83299); valid loss: 77.0714\n",
      "[Epoch 333/350, Step 15300, ETA 4.872s] step time: 0.005572s (±0.001737s); valid time: 0.008931s; loss: 81.0963 (±3.64544); valid loss: 77.0531\n",
      "[Epoch 335/350, Step 15400, ETA 4.262s] step time: 0.005359s (±0.001562s); valid time: 0.009061s; loss: 81.0181 (±3.79625); valid loss: 77.1851\n",
      "[Epoch 337/350, Step 15500, ETA 3.653s] step time: 0.005546s (±0.001697s); valid time: 0.008787s; loss: 94.5291 (±110.762); valid loss: 77.2359\n",
      "[Epoch 340/350, Step 15600, ETA 3.044s] step time: 0.005595s (±0.002045s); valid time: 0.008998s; loss: 88.471 (±61.7179); valid loss: 77.3157\n",
      "[Epoch 340/350, Step 15640, ETA 2.801s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.435s] step time: 0.005531s (±0.001942s); valid time: 0.013s; loss: 80.844 (±3.53015); valid loss: 77.3856\n",
      "[Epoch 344/350, Step 15800, ETA 1.826s] step time: 0.005565s (±0.001482s); valid time: 0.009044s; loss: 138.383 (±571.174); valid loss: 77.0259\n",
      "[Epoch 346/350, Step 15900, ETA 1.217s] step time: 0.005321s (±0.00149s); valid time: 0.008921s; loss: 81.8611 (±6.30445); valid loss: 76.8996\n",
      "[Epoch 348/350, Step 16000, ETA 0.6085s] step time: 0.00542s (±0.001556s); valid time: 0.008673s; loss: 81.0296 (±3.70187); valid loss: 77.3134\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005302s (±0.001182s); valid time: 0.008439s; loss: 84.5998 (±34.8798); valid loss: 77.0511\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_g6q5onl/variables.dat-4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_g6q5onl/variables.dat-4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.79,\n",
      "\t(tp, fp, tn, fn)=(557, 2375, 12035, 875),\n",
      "\tprecision=0.19,\n",
      "\trecall=0.39,\n",
      "\tf1=0.26,\n",
      "\troc_auc=0.61,\n",
      "\ty_pred%=0.18507764171190505,\n",
      "\ty_label%=0.09039262719353618,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_IBM.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 50.16s] step time: 0.01006s (±0.03392s); valid time: 0.1661s; loss: 122.302 (±8.81306); valid loss: 121.787 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 19.5s] step time: 0.006407s (±0.008047s); valid time: 0.08008s; loss: 101.783 (±4.48672); valid loss: 107.813 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 7.719s] step time: 0.006131s (±0.008541s); valid time: 0.08505s; loss: 96.3092 (±3.43859); valid loss: 104.718 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 1.281s] step time: 0.006188s (±0.008229s); valid time: 0.08205s; loss: 94.6467 (±3.48211); valid loss: 102.847 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.96s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 57.6s] step time: 0.006322s (±0.008845s); valid time: 0.08773s; loss: 94.1812 (±3.44534); valid loss: 101.806 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 55.27s] step time: 0.006355s (±0.008856s); valid time: 0.08715s; loss: 93.4679 (±3.50996); valid loss: 100.429 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 52.98s] step time: 0.006274s (±0.00863s); valid time: 0.08561s; loss: 91.9303 (±3.24443); valid loss: 98.9461 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 50.92s] step time: 0.00607s (±0.008638s); valid time: 0.08688s; loss: 90.0493 (±3.37672); valid loss: 98.2259 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 49.3s] step time: 0.006255s (±0.008416s); valid time: 0.08297s; loss: 88.6718 (±3.4315); valid loss: 96.9014 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 48.52s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 47.77s] step time: 0.006195s (±0.00854s); valid time: 0.08525s; loss: 87.4351 (±3.10617); valid loss: 95.1947 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 46.29s] step time: 0.00611s (±0.007693s); valid time: 0.07677s; loss: 86.8201 (±2.72696); valid loss: 94.5549 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.56s] step time: 0.00648s (±0.01004s); valid time: 0.09987s; loss: 85.9939 (±2.83481); valid loss: 93.9004 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.31s] step time: 0.006144s (±0.008271s); valid time: 0.08205s; loss: 85.8311 (±2.69967); valid loss: 93.8115 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 42.57s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 43.15s] step time: 0.006137s (±0.008549s); valid time: 0.08538s; loss: 85.3781 (±3.25701); valid loss: 93.2289 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 41.49s] step time: 0.00556s (±0.001605s); valid time: 0.007887s; loss: 85.0721 (±3.06532); valid loss: 93.3332\n",
      "[Epoch 35/350, Step 1600, ETA 1m 40.74s] step time: 0.006371s (±0.008711s); valid time: 0.08708s; loss: 85.1038 (±2.69373); valid loss: 93.0053 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 39.9s] step time: 0.006314s (±0.008778s); valid time: 0.08789s; loss: 84.9877 (±2.79526); valid loss: 92.7832 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 38.49s] step time: 0.005463s (±0.001438s); valid time: 0.008893s; loss: 84.8641 (±3.2081); valid loss: 93.1088\n",
      "[Epoch 40/350, Step 1840, ETA 1m 37.88s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 37.88s] step time: 0.006524s (±0.008989s); valid time: 0.08839s; loss: 84.722 (±3.33242); valid loss: 92.7337 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 37.05s] step time: 0.006238s (±0.00824s); valid time: 0.08245s; loss: 84.3188 (±3.36287); valid loss: 92.5561 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 36.25s] step time: 0.006241s (±0.008809s); valid time: 0.08822s; loss: 84.7634 (±3.29333); valid loss: 92.3548 (*)\n",
      "[Epoch 48/350, Step 2200, ETA 1m 35.36s] step time: 0.006102s (±0.008227s); valid time: 0.08153s; loss: 83.9369 (±2.84964); valid loss: 92.3332 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 34.56s] step time: 0.006195s (±0.008323s); valid time: 0.08251s; loss: 84.2282 (±3.42488); valid loss: 92.2754 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 34.56s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 33.83s] step time: 0.006207s (±0.008605s); valid time: 0.08494s; loss: 83.9995 (±3.01196); valid loss: 92.1766 (*)\n",
      "[Epoch 55/350, Step 2500, ETA 1m 33.13s] step time: 0.006358s (±0.008625s); valid time: 0.08281s; loss: 84.1067 (±3.30847); valid loss: 92.0383 (*)\n",
      "[Epoch 57/350, Step 2600, ETA 1m 31.86s] step time: 0.005257s (±0.001061s); valid time: 0.00822s; loss: 83.9877 (±3.50447); valid loss: 92.1557\n",
      "[Epoch 59/350, Step 2700, ETA 1m 31.1s] step time: 0.006192s (±0.008458s); valid time: 0.08437s; loss: 83.9769 (±2.85864); valid loss: 91.9414 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 30.38s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 30.42s] step time: 0.006317s (±0.008646s); valid time: 0.0836s; loss: 83.6981 (±3.18478); valid loss: 91.6536 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 29.45s] step time: 0.005581s (±0.001688s); valid time: 0.008513s; loss: 83.8072 (±3.36117); valid loss: 91.87\n",
      "[Epoch 66/350, Step 3000, ETA 1m 28.4s] step time: 0.005465s (±0.001488s); valid time: 0.00916s; loss: 83.5339 (±2.88343); valid loss: 91.7563\n",
      "[Epoch 68/350, Step 3100, ETA 1m 27.29s] step time: 0.005167s (±0.0009596s); valid time: 0.008265s; loss: 83.7929 (±3.24491); valid loss: 91.8543\n",
      "[Epoch 70/350, Step 3200, ETA 1m 26.58s] step time: 0.006172s (±0.008489s); valid time: 0.08446s; loss: 83.4255 (±3.20518); valid loss: 91.5106 (*)\n",
      "[Epoch 70/350, Step 3220, ETA 1m 26.34s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 25.56s] step time: 0.005333s (±0.001238s); valid time: 0.008983s; loss: 83.1941 (±3.26828); valid loss: 91.5376\n",
      "[Epoch 74/350, Step 3400, ETA 1m 24.88s] step time: 0.006194s (±0.008575s); valid time: 0.08498s; loss: 83.5391 (±3.14363); valid loss: 91.4762 (*)\n",
      "[Epoch 77/350, Step 3500, ETA 1m 24.05s] step time: 0.005664s (±0.001784s); valid time: 0.0086s; loss: 83.4022 (±3.25834); valid loss: 91.5645\n",
      "[Epoch 79/350, Step 3600, ETA 1m 23.43s] step time: 0.006332s (±0.008581s); valid time: 0.08597s; loss: 83.3697 (±2.84661); valid loss: 91.2498 (*)\n",
      "[Epoch 80/350, Step 3680, ETA 1m 22.64s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 22.5s] step time: 0.005331s (±0.001246s); valid time: 0.008341s; loss: 83.0194 (±3.39185); valid loss: 91.3919\n",
      "[Epoch 83/350, Step 3800, ETA 1m 21.89s] step time: 0.00637s (±0.009803s); valid time: 0.09736s; loss: 83.2475 (±2.96619); valid loss: 91.1309 (*)\n",
      "[Epoch 85/350, Step 3900, ETA 1m 21.25s] step time: 0.006283s (±0.009597s); valid time: 0.09598s; loss: 83.2438 (±3.17184); valid loss: 90.7832 (*)\n",
      "[Epoch 87/350, Step 4000, ETA 1m 20.43s] step time: 0.005688s (±0.001432s); valid time: 0.008323s; loss: 83.1301 (±3.39592); valid loss: 91.3071\n",
      "[Epoch 90/350, Step 4100, ETA 1m 19.56s] step time: 0.005365s (±0.001349s); valid time: 0.008449s; loss: 83.0272 (±3.12553); valid loss: 90.9255\n",
      "[Epoch 90/350, Step 4140, ETA 1m 19.23s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 18.75s] step time: 0.005641s (±0.001866s); valid time: 0.008189s; loss: 83.1507 (±3.27902); valid loss: 90.9224\n",
      "[Epoch 94/350, Step 4300, ETA 1m 17.89s] step time: 0.005434s (±0.001938s); valid time: 0.008276s; loss: 83.0959 (±3.00066); valid loss: 90.9602\n",
      "[Epoch 96/350, Step 4400, ETA 1m 17.05s] step time: 0.005476s (±0.001333s); valid time: 0.008207s; loss: 82.9466 (±2.99849); valid loss: 90.867\n",
      "[Epoch 98/350, Step 4500, ETA 1m 16.45s] step time: 0.006364s (±0.008398s); valid time: 0.08298s; loss: 82.8201 (±3.28762); valid loss: 90.6718 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.6s] step time: 0.005329s (±0.001305s); valid time: 0.008376s; loss: 83.0687 (±2.90494); valid loss: 90.9014\n",
      "[Epoch 100/350, Step 4600, ETA 1m 15.6s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 14.8s] step time: 0.005415s (±0.001773s); valid time: 0.008785s; loss: 82.9026 (±3.05144); valid loss: 90.7482\n",
      "[Epoch 105/350, Step 4800, ETA 1m 14.01s] step time: 0.005508s (±0.001569s); valid time: 0.008847s; loss: 82.8096 (±3.2483); valid loss: 90.8356\n",
      "[Epoch 107/350, Step 4900, ETA 1m 13.16s] step time: 0.005246s (±0.001329s); valid time: 0.008469s; loss: 83.5134 (±3.81086); valid loss: 90.7023\n",
      "[Epoch 109/350, Step 5000, ETA 1m 12.53s] step time: 0.006151s (±0.008334s); valid time: 0.08277s; loss: 82.5724 (±3.35415); valid loss: 90.5535 (*)\n",
      "[Epoch 110/350, Step 5060, ETA 1m 12.03s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 11.91s] step time: 0.006252s (±0.009196s); valid time: 0.09156s; loss: 83.0685 (±2.73141); valid loss: 90.4246 (*)\n",
      "[Epoch 114/350, Step 5200, ETA 1m 11.17s] step time: 0.005536s (±0.001662s); valid time: 0.008774s; loss: 82.5981 (±3.33704); valid loss: 90.8042\n",
      "[Epoch 116/350, Step 5300, ETA 1m 10.43s] step time: 0.00563s (±0.001576s); valid time: 0.008564s; loss: 82.9909 (±2.91016); valid loss: 90.5453\n",
      "[Epoch 118/350, Step 5400, ETA 1m 9.639s] step time: 0.005304s (±0.001151s); valid time: 0.008751s; loss: 82.7341 (±3.22144); valid loss: 90.5086\n",
      "[Epoch 120/350, Step 5500, ETA 1m 8.886s] step time: 0.005525s (±0.001642s); valid time: 0.008549s; loss: 82.7454 (±3.09619); valid loss: 90.5307\n",
      "[Epoch 120/350, Step 5520, ETA 1m 8.711s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 8.122s] step time: 0.005417s (±0.00174s); valid time: 0.008519s; loss: 82.6804 (±2.98645); valid loss: 90.5481\n",
      "[Epoch 124/350, Step 5700, ETA 1m 7.347s] step time: 0.005336s (±0.001285s); valid time: 0.008141s; loss: 82.8756 (±3.30014); valid loss: 90.727\n",
      "[Epoch 127/350, Step 5800, ETA 1m 6.678s] step time: 0.005757s (±0.001724s); valid time: 0.008809s; loss: 82.6912 (±3.37394); valid loss: 90.5169\n",
      "[Epoch 129/350, Step 5900, ETA 1m 5.914s] step time: 0.005344s (±0.001352s); valid time: 0.008106s; loss: 82.8399 (±2.84475); valid loss: 90.5277\n",
      "[Epoch 130/350, Step 5980, ETA 1m 5.315s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 5.187s] step time: 0.005498s (±0.00162s); valid time: 0.00841s; loss: 82.6511 (±2.84132); valid loss: 90.5168\n",
      "[Epoch 133/350, Step 6100, ETA 1m 4.514s] step time: 0.005781s (±0.001936s); valid time: 0.01093s; loss: 82.5866 (±3.01593); valid loss: 90.58\n",
      "[Epoch 135/350, Step 6200, ETA 1m 3.943s] step time: 0.006449s (±0.008941s); valid time: 0.08785s; loss: 82.7218 (±3.04088); valid loss: 90.3446 (*)\n",
      "[Epoch 137/350, Step 6300, ETA 1m 3.228s] step time: 0.005527s (±0.001331s); valid time: 0.008163s; loss: 82.7289 (±3.03402); valid loss: 90.5063\n",
      "[Epoch 140/350, Step 6400, ETA 1m 2.529s] step time: 0.005526s (±0.00166s); valid time: 0.008137s; loss: 82.7962 (±3.17553); valid loss: 90.4064\n",
      "[Epoch 140/350, Step 6440, ETA 1m 2.221s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 1.809s] step time: 0.005411s (±0.001461s); valid time: 0.00803s; loss: 82.5909 (±3.40941); valid loss: 90.4267\n",
      "[Epoch 144/350, Step 6600, ETA 1m 1.071s] step time: 0.005334s (±0.001313s); valid time: 0.008116s; loss: 82.468 (±3.15318); valid loss: 90.4431\n",
      "[Epoch 146/350, Step 6700, ETA 1m 0.3607s] step time: 0.005484s (±0.001345s); valid time: 0.008138s; loss: 83.0119 (±2.95581); valid loss: 90.4674\n",
      "[Epoch 148/350, Step 6800, ETA 59.66s] step time: 0.005519s (±0.001489s); valid time: 0.008323s; loss: 82.6109 (±2.90394); valid loss: 90.4812\n",
      "[Epoch 150/350, Step 6900, ETA 58.94s] step time: 0.005328s (±0.001251s); valid time: 0.008225s; loss: 82.6063 (±3.26507); valid loss: 90.6976\n",
      "[Epoch 150/350, Step 6900, ETA 58.94s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 58.34s] step time: 0.006191s (±0.00794s); valid time: 0.07896s; loss: 82.698 (±3.63619); valid loss: 90.3429 (*)\n",
      "[Epoch 155/350, Step 7100, ETA 57.65s] step time: 0.005519s (±0.001762s); valid time: 0.008208s; loss: 82.7519 (±3.14671); valid loss: 90.6146\n",
      "[Epoch 157/350, Step 7200, ETA 57.04s] step time: 0.006184s (±0.008563s); valid time: 0.08415s; loss: 82.5706 (±3.0288); valid loss: 90.2411 (*)\n",
      "[Epoch 159/350, Step 7300, ETA 56.35s] step time: 0.005579s (±0.001492s); valid time: 0.008597s; loss: 82.5678 (±3.52508); valid loss: 90.5233\n",
      "[Epoch 160/350, Step 7360, ETA 55.93s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 55.66s] step time: 0.005468s (±0.001654s); valid time: 0.008435s; loss: 82.6706 (±2.96587); valid loss: 90.6174\n",
      "[Epoch 164/350, Step 7500, ETA 54.97s] step time: 0.005429s (±0.001416s); valid time: 0.008404s; loss: 82.5918 (±3.23435); valid loss: 90.4701\n",
      "[Epoch 166/350, Step 7600, ETA 54.26s] step time: 0.00528s (±0.001322s); valid time: 0.009026s; loss: 82.6077 (±3.29343); valid loss: 90.6674\n",
      "[Epoch 168/350, Step 7700, ETA 53.56s] step time: 0.005355s (±0.001666s); valid time: 0.01199s; loss: 82.4551 (±3.02725); valid loss: 90.3893\n",
      "[Epoch 170/350, Step 7800, ETA 52.88s] step time: 0.005481s (±0.001754s); valid time: 0.008376s; loss: 82.8326 (±2.91321); valid loss: 90.4363\n",
      "[Epoch 170/350, Step 7820, ETA 52.74s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 52.21s] step time: 0.005598s (±0.001575s); valid time: 0.009053s; loss: 82.6218 (±3.64976); valid loss: 90.4661\n",
      "[Epoch 174/350, Step 8000, ETA 51.54s] step time: 0.005515s (±0.001337s); valid time: 0.008344s; loss: 82.59 (±3.01117); valid loss: 90.4129\n",
      "[Epoch 177/350, Step 8100, ETA 50.87s] step time: 0.00548s (±0.001328s); valid time: 0.008212s; loss: 82.5585 (±3.41389); valid loss: 90.4532\n",
      "[Epoch 179/350, Step 8200, ETA 50.2s] step time: 0.005593s (±0.001386s); valid time: 0.008299s; loss: 82.5697 (±2.77811); valid loss: 90.3973\n",
      "[Epoch 180/350, Step 8280, ETA 49.65s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.55s] step time: 0.005549s (±0.001767s); valid time: 0.01257s; loss: 82.5633 (±3.0911); valid loss: 90.3951\n",
      "[Epoch 183/350, Step 8400, ETA 48.88s] step time: 0.005517s (±0.001514s); valid time: 0.008689s; loss: 82.3826 (±3.50753); valid loss: 90.4602\n",
      "[Epoch 185/350, Step 8500, ETA 48.23s] step time: 0.005726s (±0.001755s); valid time: 0.008571s; loss: 82.7731 (±3.1454); valid loss: 90.3547\n",
      "[Epoch 187/350, Step 8600, ETA 47.55s] step time: 0.005386s (±0.00141s); valid time: 0.00882s; loss: 82.5665 (±3.11531); valid loss: 90.5453\n",
      "[Epoch 190/350, Step 8700, ETA 46.91s] step time: 0.005637s (±0.001734s); valid time: 0.01301s; loss: 82.4034 (±2.9717); valid loss: 90.2635\n",
      "[Epoch 190/350, Step 8740, ETA 46.64s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 46.26s] step time: 0.005662s (±0.00184s); valid time: 0.009585s; loss: 82.5961 (±2.96842); valid loss: 90.3225\n",
      "[Epoch 194/350, Step 8900, ETA 45.6s] step time: 0.005501s (±0.001483s); valid time: 0.00845s; loss: 82.7087 (±3.02049); valid loss: 90.3201\n",
      "[Epoch 196/350, Step 9000, ETA 44.93s] step time: 0.005463s (±0.001456s); valid time: 0.008108s; loss: 82.5825 (±3.12354); valid loss: 90.5669\n",
      "[Epoch 198/350, Step 9100, ETA 44.27s] step time: 0.005465s (±0.001786s); valid time: 0.008454s; loss: 82.5834 (±2.77613); valid loss: 90.3699\n",
      "[Epoch 200/350, Step 9200, ETA 43.6s] step time: 0.005363s (±0.001504s); valid time: 0.008671s; loss: 82.6477 (±3.08303); valid loss: 90.5374\n",
      "[Epoch 200/350, Step 9200, ETA 43.6s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 42.94s] step time: 0.005464s (±0.001512s); valid time: 0.009128s; loss: 82.6426 (±3.37286); valid loss: 90.4511\n",
      "[Epoch 205/350, Step 9400, ETA 42.29s] step time: 0.005504s (±0.001565s); valid time: 0.008899s; loss: 82.4707 (±2.76754); valid loss: 90.3818\n",
      "[Epoch 207/350, Step 9500, ETA 41.62s] step time: 0.005378s (±0.00163s); valid time: 0.008307s; loss: 82.6711 (±3.11452); valid loss: 90.4231\n",
      "[Epoch 209/350, Step 9600, ETA 40.97s] step time: 0.005464s (±0.001337s); valid time: 0.007922s; loss: 82.4338 (±3.12428); valid loss: 90.4421\n",
      "[Epoch 210/350, Step 9660, ETA 40.59s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.33s] step time: 0.00576s (±0.002234s); valid time: 0.008888s; loss: 82.7027 (±3.06255); valid loss: 90.3896\n",
      "[Epoch 214/350, Step 9800, ETA 39.69s] step time: 0.005617s (±0.001426s); valid time: 0.008547s; loss: 82.4982 (±2.86486); valid loss: 90.3588\n",
      "[Epoch 216/350, Step 9900, ETA 39.04s] step time: 0.005455s (±0.001338s); valid time: 0.00856s; loss: 82.5596 (±3.07701); valid loss: 90.4078\n",
      "[Epoch 218/350, Step 10000, ETA 38.38s] step time: 0.00536s (±0.001348s); valid time: 0.008246s; loss: 82.4709 (±3.08167); valid loss: 90.4457\n",
      "[Epoch 220/350, Step 10100, ETA 37.72s] step time: 0.005348s (±0.001307s); valid time: 0.008533s; loss: 82.6829 (±3.15333); valid loss: 90.301\n",
      "[Epoch 220/350, Step 10120, ETA 37.59s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 37.08s] step time: 0.005566s (±0.001436s); valid time: 0.008414s; loss: 82.5084 (±3.4634); valid loss: 90.4056\n",
      "[Epoch 224/350, Step 10300, ETA 36.44s] step time: 0.005615s (±0.001466s); valid time: 0.009599s; loss: 82.6962 (±2.98358); valid loss: 90.4501\n",
      "[Epoch 227/350, Step 10400, ETA 35.81s] step time: 0.005613s (±0.001672s); valid time: 0.009173s; loss: 82.5897 (±2.8007); valid loss: 90.59\n",
      "[Epoch 229/350, Step 10500, ETA 35.15s] step time: 0.005344s (±0.001735s); valid time: 0.01181s; loss: 82.5373 (±3.16547); valid loss: 90.532\n",
      "[Epoch 230/350, Step 10580, ETA 34.63s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.51s] step time: 0.005453s (±0.001177s); valid time: 0.008421s; loss: 82.6198 (±2.8589); valid loss: 90.2491\n",
      "[Epoch 233/350, Step 10700, ETA 33.87s] step time: 0.005453s (±0.001488s); valid time: 0.008403s; loss: 82.52 (±2.89017); valid loss: 90.4627\n",
      "[Epoch 235/350, Step 10800, ETA 33.22s] step time: 0.005331s (±0.00108s); valid time: 0.008752s; loss: 82.5361 (±3.41202); valid loss: 90.3815\n",
      "[Epoch 237/350, Step 10900, ETA 32.57s] step time: 0.005377s (±0.001438s); valid time: 0.008121s; loss: 82.5552 (±3.14335); valid loss: 90.2455\n",
      "[Epoch 240/350, Step 11000, ETA 31.92s] step time: 0.005236s (±0.00126s); valid time: 0.008583s; loss: 82.4128 (±3.43917); valid loss: 90.4261\n",
      "[Epoch 240/350, Step 11040, ETA 31.67s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.29s] step time: 0.005615s (±0.001647s); valid time: 0.008873s; loss: 82.4694 (±2.82123); valid loss: 90.4291\n",
      "[Epoch 244/350, Step 11200, ETA 30.65s] step time: 0.00554s (±0.001397s); valid time: 0.008693s; loss: 82.8714 (±2.91892); valid loss: 90.4366\n",
      "[Epoch 246/350, Step 11300, ETA 30.01s] step time: 0.005427s (±0.001354s); valid time: 0.008543s; loss: 82.5388 (±2.8909); valid loss: 90.4481\n",
      "[Epoch 248/350, Step 11400, ETA 29.38s] step time: 0.005664s (±0.001975s); valid time: 0.01038s; loss: 82.4929 (±2.88479); valid loss: 90.6123\n",
      "[Epoch 250/350, Step 11500, ETA 28.75s] step time: 0.005514s (±0.001376s); valid time: 0.008626s; loss: 82.5725 (±2.81458); valid loss: 90.4136\n",
      "[Epoch 250/350, Step 11500, ETA 28.75s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.11s] step time: 0.005432s (±0.00136s); valid time: 0.008406s; loss: 82.589 (±3.47228); valid loss: 90.3498\n",
      "[Epoch 255/350, Step 11700, ETA 27.47s] step time: 0.005181s (±0.001754s); valid time: 0.01731s; loss: 82.5815 (±2.83883); valid loss: 90.631\n",
      "[Epoch 257/350, Step 11800, ETA 26.83s] step time: 0.005274s (±0.001126s); valid time: 0.00867s; loss: 82.4291 (±3.18974); valid loss: 90.3585\n",
      "[Epoch 259/350, Step 11900, ETA 26.19s] step time: 0.005523s (±0.001807s); valid time: 0.009991s; loss: 82.7309 (±2.90174); valid loss: 90.3204\n",
      "[Epoch 260/350, Step 11960, ETA 25.81s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.56s] step time: 0.005533s (±0.001334s); valid time: 0.009015s; loss: 82.3229 (±3.18369); valid loss: 90.335\n",
      "[Epoch 264/350, Step 12100, ETA 24.94s] step time: 0.005641s (±0.001884s); valid time: 0.008922s; loss: 82.7779 (±3.1193); valid loss: 90.4152\n",
      "[Epoch 266/350, Step 12200, ETA 24.3s] step time: 0.005304s (±0.001309s); valid time: 0.008303s; loss: 82.4408 (±2.99357); valid loss: 90.4986\n",
      "[Epoch 268/350, Step 12300, ETA 23.67s] step time: 0.005488s (±0.001884s); valid time: 0.00894s; loss: 82.6925 (±3.0215); valid loss: 90.5207\n",
      "[Epoch 270/350, Step 12400, ETA 23.04s] step time: 0.005636s (±0.001831s); valid time: 0.008942s; loss: 82.6009 (±3.06589); valid loss: 90.2651\n",
      "[Epoch 270/350, Step 12420, ETA 22.91s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.41s] step time: 0.005384s (±0.001302s); valid time: 0.008154s; loss: 82.6069 (±2.76789); valid loss: 90.47\n",
      "[Epoch 274/350, Step 12600, ETA 21.78s] step time: 0.005533s (±0.001573s); valid time: 0.008511s; loss: 82.4076 (±3.07478); valid loss: 90.5205\n",
      "[Epoch 277/350, Step 12700, ETA 21.15s] step time: 0.00543s (±0.001227s); valid time: 0.008146s; loss: 82.7139 (±2.64726); valid loss: 90.3589\n",
      "[Epoch 279/350, Step 12800, ETA 20.52s] step time: 0.005456s (±0.001387s); valid time: 0.008404s; loss: 82.4607 (±3.41252); valid loss: 90.4922\n",
      "[Epoch 280/350, Step 12880, ETA 20.02s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.89s] step time: 0.005432s (±0.001462s); valid time: 0.008394s; loss: 82.526 (±3.48557); valid loss: 90.2944\n",
      "[Epoch 283/350, Step 13000, ETA 19.26s] step time: 0.00536s (±0.001379s); valid time: 0.009207s; loss: 82.5372 (±2.85388); valid loss: 90.3272\n",
      "[Epoch 285/350, Step 13100, ETA 18.64s] step time: 0.005586s (±0.001818s); valid time: 0.008182s; loss: 82.7665 (±3.12911); valid loss: 90.5991\n",
      "[Epoch 287/350, Step 13200, ETA 18.01s] step time: 0.005309s (±0.001379s); valid time: 0.00896s; loss: 82.5947 (±3.02624); valid loss: 90.5587\n",
      "[Epoch 290/350, Step 13300, ETA 17.38s] step time: 0.005515s (±0.001653s); valid time: 0.008274s; loss: 82.4924 (±3.42612); valid loss: 90.5518\n",
      "[Epoch 290/350, Step 13340, ETA 17.13s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.76s] step time: 0.00547s (±0.001756s); valid time: 0.008565s; loss: 82.679 (±3.20272); valid loss: 90.2851\n",
      "[Epoch 294/350, Step 13500, ETA 16.14s] step time: 0.005733s (±0.001432s); valid time: 0.008002s; loss: 82.4544 (±2.98558); valid loss: 90.5273\n",
      "[Epoch 296/350, Step 13600, ETA 15.51s] step time: 0.005394s (±0.001598s); valid time: 0.008502s; loss: 82.7024 (±3.46102); valid loss: 90.6186\n",
      "[Epoch 298/350, Step 13700, ETA 14.88s] step time: 0.005376s (±0.001519s); valid time: 0.009395s; loss: 82.5745 (±2.8946); valid loss: 90.2731\n",
      "[Epoch 300/350, Step 13800, ETA 14.26s] step time: 0.005436s (±0.001588s); valid time: 0.008323s; loss: 82.5767 (±3.37838); valid loss: 90.3948\n",
      "[Epoch 300/350, Step 13800, ETA 14.26s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.63s] step time: 0.005362s (±0.00135s); valid time: 0.008686s; loss: 82.5397 (±3.25017); valid loss: 90.3266\n",
      "[Epoch 305/350, Step 14000, ETA 13.01s] step time: 0.005441s (±0.001465s); valid time: 0.008649s; loss: 82.6949 (±3.28045); valid loss: 90.4112\n",
      "[Epoch 307/350, Step 14100, ETA 12.39s] step time: 0.005544s (±0.001659s); valid time: 0.008168s; loss: 82.7305 (±3.13836); valid loss: 90.41\n",
      "[Epoch 309/350, Step 14200, ETA 11.76s] step time: 0.005352s (±0.001574s); valid time: 0.008883s; loss: 82.4289 (±2.95961); valid loss: 90.3292\n",
      "[Epoch 310/350, Step 14260, ETA 11.39s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.14s] step time: 0.005561s (±0.001915s); valid time: 0.008397s; loss: 82.5792 (±3.42245); valid loss: 90.3176\n",
      "[Epoch 314/350, Step 14400, ETA 10.52s] step time: 0.005416s (±0.001477s); valid time: 0.008166s; loss: 82.5631 (±3.10275); valid loss: 90.3478\n",
      "[Epoch 316/350, Step 14500, ETA 9.898s] step time: 0.005362s (±0.001304s); valid time: 0.008333s; loss: 82.6881 (±4.1276); valid loss: 90.3842\n",
      "[Epoch 318/350, Step 14600, ETA 9.275s] step time: 0.005354s (±0.001381s); valid time: 0.00894s; loss: 82.7605 (±3.22268); valid loss: 90.3291\n",
      "[Epoch 320/350, Step 14700, ETA 8.66s] step time: 0.006078s (±0.008248s); valid time: 0.08234s; loss: 82.4721 (±3.23496); valid loss: 90.2383 (*)\n",
      "[Epoch 320/350, Step 14720, ETA 8.536s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.043s] step time: 0.005839s (±0.00223s); valid time: 0.008715s; loss: 82.6332 (±3.21899); valid loss: 90.3133\n",
      "[Epoch 324/350, Step 14900, ETA 7.421s] step time: 0.005341s (±0.001367s); valid time: 0.008053s; loss: 82.6401 (±2.87965); valid loss: 90.2634\n",
      "[Epoch 327/350, Step 15000, ETA 6.802s] step time: 0.005538s (±0.001542s); valid time: 0.01002s; loss: 82.5353 (±3.07219); valid loss: 90.5205\n",
      "[Epoch 329/350, Step 15100, ETA 6.182s] step time: 0.005442s (±0.001289s); valid time: 0.00854s; loss: 82.7456 (±3.00643); valid loss: 90.3523\n",
      "[Epoch 330/350, Step 15180, ETA 5.685s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.562s] step time: 0.005378s (±0.001252s); valid time: 0.008387s; loss: 82.3997 (±3.04328); valid loss: 90.2506\n",
      "[Epoch 333/350, Step 15300, ETA 4.943s] step time: 0.005456s (±0.001259s); valid time: 0.008375s; loss: 82.6629 (±2.92916); valid loss: 90.4869\n",
      "[Epoch 335/350, Step 15400, ETA 4.323s] step time: 0.005263s (±0.001264s); valid time: 0.008425s; loss: 82.4321 (±3.17469); valid loss: 90.2647\n",
      "[Epoch 337/350, Step 15500, ETA 3.704s] step time: 0.005405s (±0.001369s); valid time: 0.008578s; loss: 82.6205 (±2.94099); valid loss: 90.409\n",
      "[Epoch 340/350, Step 15600, ETA 3.086s] step time: 0.005359s (±0.001786s); valid time: 0.008525s; loss: 82.5696 (±3.41502); valid loss: 91.0566\n",
      "[Epoch 340/350, Step 15640, ETA 2.839s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.469s] step time: 0.005646s (±0.001653s); valid time: 0.008395s; loss: 82.4926 (±2.81895); valid loss: 90.2874\n",
      "[Epoch 344/350, Step 15800, ETA 1.851s] step time: 0.005538s (±0.002369s); valid time: 0.01176s; loss: 82.7241 (±2.93167); valid loss: 90.3928\n",
      "[Epoch 346/350, Step 15900, ETA 1.234s] step time: 0.005384s (±0.001324s); valid time: 0.008466s; loss: 82.5314 (±2.88977); valid loss: 90.5057\n",
      "[Epoch 348/350, Step 16000, ETA 0.6167s] step time: 0.005553s (±0.001483s); valid time: 0.008994s; loss: 82.2879 (±3.29703); valid loss: 90.3446\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.00534s (±0.00131s); valid time: 0.008454s; loss: 82.8168 (±3.33898); valid loss: 90.4516\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpr349y2d5/variables.dat-14700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpr349y2d5/variables.dat-14700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.75,\n",
      "\t(tp, fp, tn, fn)=(481, 2829, 11474, 1109),\n",
      "\tprecision=0.15,\n",
      "\trecall=0.3,\n",
      "\tf1=0.2,\n",
      "\troc_auc=0.55,\n",
      "\ty_pred%=0.2082677908513182,\n",
      "\ty_label%=0.10004404454791417,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_KO.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 47.26s] step time: 0.009876s (±0.03331s); valid time: 0.172s; loss: 147.308 (±13.669); valid loss: 105.108 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 16.02s] step time: 0.006203s (±0.008427s); valid time: 0.08374s; loss: 114.385 (±11.4196); valid loss: 85.5606 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 6.15s] step time: 0.006393s (±0.008404s); valid time: 0.08341s; loss: 92.0141 (±5.70013); valid loss: 77.5818 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 0.2919s] step time: 0.006242s (±0.008845s); valid time: 0.08341s; loss: 86.9552 (±5.37637); valid loss: 75.1592 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.41s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 56.93s] step time: 0.006337s (±0.008605s); valid time: 0.08494s; loss: 84.185 (±4.37366); valid loss: 71.8639 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 54.88s] step time: 0.006343s (±0.008286s); valid time: 0.08197s; loss: 80.9737 (±5.18583); valid loss: 68.6232 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 53.22s] step time: 0.006527s (±0.008389s); valid time: 0.08357s; loss: 77.6405 (±3.85074); valid loss: 65.7968 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 51.11s] step time: 0.006117s (±0.008962s); valid time: 0.08985s; loss: 74.7373 (±4.82865); valid loss: 63.4264 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 49.68s] step time: 0.006375s (±0.008662s); valid time: 0.08542s; loss: 72.643 (±3.8233); valid loss: 61.5402 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 49s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 48.2s] step time: 0.006245s (±0.008474s); valid time: 0.08403s; loss: 70.8928 (±4.06125); valid loss: 60.6766 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 47.01s] step time: 0.006357s (±0.009538s); valid time: 0.09519s; loss: 69.8078 (±3.43295); valid loss: 59.8882 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.94s] step time: 0.006258s (±0.008792s); valid time: 0.08801s; loss: 68.6757 (±3.87043); valid loss: 59.2537 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.8s] step time: 0.006267s (±0.008982s); valid time: 0.08899s; loss: 67.8074 (±3.924); valid loss: 58.3651 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 43.08s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 43.59s] step time: 0.006142s (±0.008166s); valid time: 0.08106s; loss: 67.5288 (±3.78419); valid loss: 57.8474 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 42.54s] step time: 0.006231s (±0.008533s); valid time: 0.08493s; loss: 66.9254 (±4.28395); valid loss: 57.5178 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 41.47s] step time: 0.006155s (±0.008534s); valid time: 0.08481s; loss: 65.9582 (±3.49171); valid loss: 57.3814 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 40.94s] step time: 0.006721s (±0.009245s); valid time: 0.09163s; loss: 66.1247 (±4.23534); valid loss: 56.9397 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 40.36s] step time: 0.00655s (±0.008903s); valid time: 0.08779s; loss: 65.4078 (±4.03969); valid loss: 56.6678 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 39.81s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 39.68s] step time: 0.006535s (±0.009034s); valid time: 0.0892s; loss: 64.8086 (±4.02386); valid loss: 56.355 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 38.78s] step time: 0.006265s (±0.008662s); valid time: 0.08642s; loss: 65.2896 (±3.93666); valid loss: 56.1977 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 37.97s] step time: 0.00638s (±0.008922s); valid time: 0.0885s; loss: 64.2901 (±3.17841); valid loss: 55.8582 (*)\n",
      "[Epoch 48/350, Step 2200, ETA 1m 37.23s] step time: 0.00644s (±0.008606s); valid time: 0.08578s; loss: 64.365 (±3.37171); valid loss: 55.8241 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 36.31s] step time: 0.00616s (±0.008329s); valid time: 0.08287s; loss: 63.984 (±3.62978); valid loss: 55.3944 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 36.31s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 35.68s] step time: 0.006423s (±0.008596s); valid time: 0.08504s; loss: 63.7087 (±4.10462); valid loss: 55.3079 (*)\n",
      "[Epoch 55/350, Step 2500, ETA 1m 34.81s] step time: 0.006201s (±0.008559s); valid time: 0.08501s; loss: 63.5757 (±3.39015); valid loss: 55.1016 (*)\n",
      "[Epoch 57/350, Step 2600, ETA 1m 33.93s] step time: 0.006163s (±0.008406s); valid time: 0.084s; loss: 63.396 (±4.12598); valid loss: 54.8882 (*)\n",
      "[Epoch 59/350, Step 2700, ETA 1m 33.16s] step time: 0.006352s (±0.008496s); valid time: 0.08452s; loss: 62.9967 (±3.22719); valid loss: 54.7229 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 32.4s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 32.39s] step time: 0.006331s (±0.008775s); valid time: 0.08646s; loss: 63.1061 (±3.84388); valid loss: 54.6015 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 31.31s] step time: 0.005522s (±0.001516s); valid time: 0.008146s; loss: 62.9818 (±3.9422); valid loss: 54.7022\n",
      "[Epoch 66/350, Step 3000, ETA 1m 30.56s] step time: 0.006315s (±0.008675s); valid time: 0.08594s; loss: 62.7096 (±3.35644); valid loss: 54.5581 (*)\n",
      "[Epoch 68/350, Step 3100, ETA 1m 29.79s] step time: 0.006155s (±0.008962s); valid time: 0.0854s; loss: 62.7652 (±3.68554); valid loss: 54.4864 (*)\n",
      "[Epoch 70/350, Step 3200, ETA 1m 28.97s] step time: 0.006136s (±0.008523s); valid time: 0.08549s; loss: 62.6645 (±3.74163); valid loss: 54.4232 (*)\n",
      "[Epoch 70/350, Step 3220, ETA 1m 28.72s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 28.26s] step time: 0.006357s (±0.00862s); valid time: 0.08541s; loss: 62.3787 (±3.97691); valid loss: 54.2074 (*)\n",
      "[Epoch 74/350, Step 3400, ETA 1m 27.51s] step time: 0.00626s (±0.009384s); valid time: 0.09375s; loss: 62.1111 (±3.55367); valid loss: 54.0488 (*)\n",
      "[Epoch 77/350, Step 3500, ETA 1m 26.53s] step time: 0.005511s (±0.001212s); valid time: 0.008126s; loss: 62.2274 (±3.28709); valid loss: 54.0496\n",
      "[Epoch 79/350, Step 3600, ETA 1m 25.85s] step time: 0.00644s (±0.008617s); valid time: 0.08587s; loss: 62.1164 (±3.399); valid loss: 53.9707 (*)\n",
      "[Epoch 80/350, Step 3680, ETA 1m 25.07s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 25.21s] step time: 0.006509s (±0.009728s); valid time: 0.09679s; loss: 62.2745 (±3.78182); valid loss: 53.9025 (*)\n",
      "[Epoch 83/350, Step 3800, ETA 1m 24.27s] step time: 0.005628s (±0.001733s); valid time: 0.008705s; loss: 62.0464 (±3.7637); valid loss: 53.9064\n",
      "[Epoch 85/350, Step 3900, ETA 1m 23.35s] step time: 0.005633s (±0.001759s); valid time: 0.00763s; loss: 61.9642 (±3.41438); valid loss: 53.9053\n",
      "[Epoch 87/350, Step 4000, ETA 1m 22.62s] step time: 0.006226s (±0.008699s); valid time: 0.08598s; loss: 61.7899 (±4.27308); valid loss: 53.6994 (*)\n",
      "[Epoch 90/350, Step 4100, ETA 1m 22.03s] step time: 0.006585s (±0.00858s); valid time: 0.08566s; loss: 61.9304 (±3.2582); valid loss: 53.6036 (*)\n",
      "[Epoch 90/350, Step 4140, ETA 1m 21.63s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 21.15s] step time: 0.005668s (±0.001901s); valid time: 0.008872s; loss: 61.6592 (±3.7984); valid loss: 53.7736\n",
      "[Epoch 94/350, Step 4300, ETA 1m 20.24s] step time: 0.005489s (±0.001375s); valid time: 0.008141s; loss: 61.7015 (±3.47532); valid loss: 53.6092\n",
      "[Epoch 96/350, Step 4400, ETA 1m 19.58s] step time: 0.006414s (±0.008564s); valid time: 0.08552s; loss: 61.689 (±3.6938); valid loss: 53.4651 (*)\n",
      "[Epoch 98/350, Step 4500, ETA 1m 18.9s] step time: 0.006378s (±0.008634s); valid time: 0.08619s; loss: 61.6482 (±3.65231); valid loss: 53.4422 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 18.01s] step time: 0.005497s (±0.001303s); valid time: 0.008409s; loss: 61.6646 (±4.11501); valid loss: 53.4698\n",
      "[Epoch 100/350, Step 4600, ETA 1m 18.02s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 17.39s] step time: 0.00634s (±0.00854s); valid time: 0.08518s; loss: 61.6117 (±3.2484); valid loss: 53.3444 (*)\n",
      "[Epoch 105/350, Step 4800, ETA 1m 16.76s] step time: 0.006542s (±0.009263s); valid time: 0.08636s; loss: 61.5815 (±3.89714); valid loss: 53.3295 (*)\n",
      "[Epoch 107/350, Step 4900, ETA 1m 16.05s] step time: 0.00619s (±0.008456s); valid time: 0.08479s; loss: 61.3104 (±3.8722); valid loss: 53.2781 (*)\n",
      "[Epoch 109/350, Step 5000, ETA 1m 15.2s] step time: 0.00554s (±0.001401s); valid time: 0.008394s; loss: 61.8226 (±3.61553); valid loss: 53.2915\n",
      "[Epoch 110/350, Step 5060, ETA 1m 14.66s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 14.33s] step time: 0.005447s (±0.001599s); valid time: 0.008281s; loss: 61.2761 (±3.69005); valid loss: 53.3365\n",
      "[Epoch 114/350, Step 5200, ETA 1m 13.7s] step time: 0.006413s (±0.008936s); valid time: 0.0873s; loss: 61.4405 (±3.6337); valid loss: 53.1705 (*)\n",
      "[Epoch 116/350, Step 5300, ETA 1m 12.9s] step time: 0.00567s (±0.00199s); valid time: 0.009612s; loss: 61.3904 (±3.63056); valid loss: 53.3662\n",
      "[Epoch 118/350, Step 5400, ETA 1m 12.05s] step time: 0.005316s (±0.0009427s); valid time: 0.008497s; loss: 61.2147 (±3.55294); valid loss: 53.3095\n",
      "[Epoch 120/350, Step 5500, ETA 1m 11.23s] step time: 0.005529s (±0.001276s); valid time: 0.008259s; loss: 61.4595 (±3.87457); valid loss: 53.2738\n",
      "[Epoch 120/350, Step 5520, ETA 1m 11.05s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 10.4s] step time: 0.0054s (±0.001527s); valid time: 0.008125s; loss: 61.4497 (±4.01234); valid loss: 53.2789\n",
      "[Epoch 124/350, Step 5700, ETA 1m 9.6s] step time: 0.00555s (±0.001697s); valid time: 0.007946s; loss: 61.3177 (±3.42218); valid loss: 53.3242\n",
      "[Epoch 127/350, Step 5800, ETA 1m 8.829s] step time: 0.005552s (±0.001476s); valid time: 0.007922s; loss: 61.1552 (±3.88467); valid loss: 53.2467\n",
      "[Epoch 129/350, Step 5900, ETA 1m 8.141s] step time: 0.006122s (±0.008424s); valid time: 0.08405s; loss: 61.5287 (±3.81444); valid loss: 53.1018 (*)\n",
      "[Epoch 130/350, Step 5980, ETA 1m 7.492s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 7.35s] step time: 0.005467s (±0.001367s); valid time: 0.00803s; loss: 60.966 (±3.63783); valid loss: 53.1978\n",
      "[Epoch 133/350, Step 6100, ETA 1m 6.693s] step time: 0.006282s (±0.009405s); valid time: 0.09346s; loss: 61.6583 (±3.77653); valid loss: 53.0855 (*)\n",
      "[Epoch 135/350, Step 6200, ETA 1m 5.921s] step time: 0.005547s (±0.001535s); valid time: 0.008161s; loss: 60.9012 (±3.91693); valid loss: 53.1603\n",
      "[Epoch 137/350, Step 6300, ETA 1m 5.137s] step time: 0.005449s (±0.001574s); valid time: 0.008454s; loss: 61.2305 (±3.67477); valid loss: 53.1173\n",
      "[Epoch 140/350, Step 6400, ETA 1m 4.501s] step time: 0.006263s (±0.008767s); valid time: 0.08725s; loss: 61.3336 (±4.04913); valid loss: 53.0302 (*)\n",
      "[Epoch 140/350, Step 6440, ETA 1m 4.215s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 3.88s] step time: 0.006475s (±0.008598s); valid time: 0.08521s; loss: 61.0769 (±3.3406); valid loss: 53.0058 (*)\n",
      "[Epoch 144/350, Step 6600, ETA 1m 3.141s] step time: 0.005667s (±0.001653s); valid time: 0.008195s; loss: 61.1072 (±3.6245); valid loss: 53.1957\n",
      "[Epoch 146/350, Step 6700, ETA 1m 2.379s] step time: 0.005487s (±0.001477s); valid time: 0.008663s; loss: 61.3997 (±3.74088); valid loss: 53.0943\n",
      "[Epoch 148/350, Step 6800, ETA 1m 1.746s] step time: 0.006403s (±0.008757s); valid time: 0.08742s; loss: 61.0921 (±3.30497); valid loss: 52.9975 (*)\n",
      "[Epoch 150/350, Step 6900, ETA 1m 1.03s] step time: 0.005755s (±0.002133s); valid time: 0.009183s; loss: 61.1919 (±3.76093); valid loss: 53.0682\n",
      "[Epoch 150/350, Step 6900, ETA 1m 1.031s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 1m 0.2827s] step time: 0.005394s (±0.001368s); valid time: 0.008029s; loss: 61.255 (±3.72508); valid loss: 53.0193\n",
      "[Epoch 155/350, Step 7100, ETA 59.57s] step time: 0.005748s (±0.001632s); valid time: 0.008172s; loss: 60.9605 (±3.51937); valid loss: 53.1325\n",
      "[Epoch 157/350, Step 7200, ETA 58.92s] step time: 0.00625s (±0.008724s); valid time: 0.08739s; loss: 61.2046 (±3.6662); valid loss: 52.9056 (*)\n",
      "[Epoch 159/350, Step 7300, ETA 58.18s] step time: 0.005516s (±0.001807s); valid time: 0.011s; loss: 61.319 (±3.78128); valid loss: 53.0899\n",
      "[Epoch 160/350, Step 7360, ETA 57.74s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 57.48s] step time: 0.005777s (±0.002223s); valid time: 0.008521s; loss: 61.0998 (±3.3458); valid loss: 52.9762\n",
      "[Epoch 164/350, Step 7500, ETA 56.84s] step time: 0.006255s (±0.008993s); valid time: 0.08851s; loss: 60.9803 (±3.67669); valid loss: 52.8175 (*)\n",
      "[Epoch 166/350, Step 7600, ETA 56.12s] step time: 0.00558s (±0.001349s); valid time: 0.008113s; loss: 61.1278 (±4.14226); valid loss: 52.9415\n",
      "[Epoch 168/350, Step 7700, ETA 55.37s] step time: 0.005318s (±0.001222s); valid time: 0.008113s; loss: 60.9111 (±4.00266); valid loss: 52.8507\n",
      "[Epoch 170/350, Step 7800, ETA 54.65s] step time: 0.005512s (±0.001385s); valid time: 0.008152s; loss: 61.3492 (±3.57744); valid loss: 52.822\n",
      "[Epoch 170/350, Step 7820, ETA 54.49s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 53.93s] step time: 0.005494s (±0.001639s); valid time: 0.007792s; loss: 61.1609 (±3.53387); valid loss: 53.0225\n",
      "[Epoch 174/350, Step 8000, ETA 53.19s] step time: 0.00532s (±0.001343s); valid time: 0.009427s; loss: 60.9164 (±3.96783); valid loss: 52.9973\n",
      "[Epoch 177/350, Step 8100, ETA 52.49s] step time: 0.005547s (±0.00157s); valid time: 0.008719s; loss: 61.1045 (±3.81675); valid loss: 53.094\n",
      "[Epoch 179/350, Step 8200, ETA 51.77s] step time: 0.005518s (±0.001753s); valid time: 0.008458s; loss: 61.173 (±3.56046); valid loss: 52.9333\n",
      "[Epoch 180/350, Step 8280, ETA 51.19s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 51.06s] step time: 0.005423s (±0.001181s); valid time: 0.00828s; loss: 60.8376 (±3.59887); valid loss: 52.9905\n",
      "[Epoch 183/350, Step 8400, ETA 50.36s] step time: 0.005613s (±0.00172s); valid time: 0.00893s; loss: 61.3141 (±3.66294); valid loss: 52.8758\n",
      "[Epoch 185/350, Step 8500, ETA 49.66s] step time: 0.005527s (±0.001273s); valid time: 0.008337s; loss: 61.2138 (±3.97224); valid loss: 52.9077\n",
      "[Epoch 187/350, Step 8600, ETA 49.04s] step time: 0.006455s (±0.008858s); valid time: 0.08742s; loss: 60.9697 (±3.28047); valid loss: 52.8167 (*)\n",
      "[Epoch 190/350, Step 8700, ETA 48.34s] step time: 0.005406s (±0.00117s); valid time: 0.008598s; loss: 61.0883 (±3.8187); valid loss: 52.8687\n",
      "[Epoch 190/350, Step 8740, ETA 48.06s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 47.64s] step time: 0.005576s (±0.001762s); valid time: 0.008058s; loss: 61.2043 (±3.92788); valid loss: 52.9217\n",
      "[Epoch 194/350, Step 8900, ETA 46.94s] step time: 0.005462s (±0.001488s); valid time: 0.008179s; loss: 60.7987 (±3.49964); valid loss: 52.82\n",
      "[Epoch 196/350, Step 9000, ETA 46.25s] step time: 0.005517s (±0.001781s); valid time: 0.008634s; loss: 61.1744 (±3.87408); valid loss: 52.9068\n",
      "[Epoch 198/350, Step 9100, ETA 45.56s] step time: 0.005577s (±0.001546s); valid time: 0.008468s; loss: 60.9802 (±3.49965); valid loss: 52.911\n",
      "[Epoch 200/350, Step 9200, ETA 44.86s] step time: 0.005423s (±0.001192s); valid time: 0.008088s; loss: 61.1136 (±3.48895); valid loss: 52.8707\n",
      "[Epoch 200/350, Step 9200, ETA 44.86s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 44.19s] step time: 0.005648s (±0.001554s); valid time: 0.008326s; loss: 61.0615 (±3.89431); valid loss: 52.8778\n",
      "[Epoch 205/350, Step 9400, ETA 43.5s] step time: 0.005492s (±0.001525s); valid time: 0.008271s; loss: 61.1397 (±4.0662); valid loss: 52.8431\n",
      "[Epoch 207/350, Step 9500, ETA 42.81s] step time: 0.005407s (±0.001306s); valid time: 0.008426s; loss: 60.9124 (±3.4849); valid loss: 52.9322\n",
      "[Epoch 209/350, Step 9600, ETA 42.13s] step time: 0.005526s (±0.001339s); valid time: 0.008019s; loss: 61.1959 (±3.87086); valid loss: 52.8692\n",
      "[Epoch 210/350, Step 9660, ETA 41.71s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 41.45s] step time: 0.005602s (±0.001563s); valid time: 0.00864s; loss: 61.0952 (±3.63854); valid loss: 52.889\n",
      "[Epoch 214/350, Step 9800, ETA 40.78s] step time: 0.005489s (±0.00158s); valid time: 0.00817s; loss: 60.8434 (±3.86522); valid loss: 52.8308\n",
      "[Epoch 216/350, Step 9900, ETA 40.1s] step time: 0.005455s (±0.001392s); valid time: 0.008178s; loss: 61.062 (±3.1911); valid loss: 52.9288\n",
      "[Epoch 218/350, Step 10000, ETA 39.43s] step time: 0.005589s (±0.001419s); valid time: 0.008253s; loss: 61.1797 (±3.86252); valid loss: 52.9445\n",
      "[Epoch 220/350, Step 10100, ETA 38.75s] step time: 0.005459s (±0.001318s); valid time: 0.007958s; loss: 61.1258 (±3.50065); valid loss: 53.0519\n",
      "[Epoch 220/350, Step 10120, ETA 38.61s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 38.07s] step time: 0.005453s (±0.001438s); valid time: 0.008932s; loss: 61.0338 (±3.81429); valid loss: 52.889\n",
      "[Epoch 224/350, Step 10300, ETA 37.45s] step time: 0.006472s (±0.008952s); valid time: 0.08897s; loss: 60.8768 (±3.6218); valid loss: 52.8108 (*)\n",
      "[Epoch 227/350, Step 10400, ETA 36.78s] step time: 0.005376s (±0.001545s); valid time: 0.00821s; loss: 61.1731 (±4.07987); valid loss: 52.9335\n",
      "[Epoch 229/350, Step 10500, ETA 36.15s] step time: 0.00634s (±0.008194s); valid time: 0.08231s; loss: 60.7779 (±3.53902); valid loss: 52.728 (*)\n",
      "[Epoch 230/350, Step 10580, ETA 35.6s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 35.47s] step time: 0.005351s (±0.001528s); valid time: 0.007977s; loss: 61.5228 (±3.90027); valid loss: 53.0275\n",
      "[Epoch 233/350, Step 10700, ETA 34.8s] step time: 0.005464s (±0.001348s); valid time: 0.008467s; loss: 60.6628 (±3.4695); valid loss: 52.8855\n",
      "[Epoch 235/350, Step 10800, ETA 34.13s] step time: 0.005502s (±0.001849s); valid time: 0.008214s; loss: 61.2444 (±3.47793); valid loss: 53.0223\n",
      "[Epoch 237/350, Step 10900, ETA 33.47s] step time: 0.005565s (±0.001627s); valid time: 0.008125s; loss: 61.0512 (±3.68544); valid loss: 52.8222\n",
      "[Epoch 240/350, Step 11000, ETA 32.81s] step time: 0.00554s (±0.001571s); valid time: 0.008112s; loss: 61.091 (±3.51514); valid loss: 52.8629\n",
      "[Epoch 240/350, Step 11040, ETA 32.54s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 32.15s] step time: 0.005607s (±0.001807s); valid time: 0.008115s; loss: 61.0789 (±3.86211); valid loss: 52.9265\n",
      "[Epoch 244/350, Step 11200, ETA 31.49s] step time: 0.005474s (±0.001436s); valid time: 0.008194s; loss: 60.9019 (±3.79837); valid loss: 52.9285\n",
      "[Epoch 246/350, Step 11300, ETA 30.83s] step time: 0.005515s (±0.001523s); valid time: 0.008399s; loss: 61.0695 (±4.1029); valid loss: 52.8334\n",
      "[Epoch 248/350, Step 11400, ETA 30.16s] step time: 0.005429s (±0.00128s); valid time: 0.00818s; loss: 61.1256 (±3.60125); valid loss: 52.8896\n",
      "[Epoch 250/350, Step 11500, ETA 29.51s] step time: 0.005609s (±0.001768s); valid time: 0.008567s; loss: 60.9305 (±3.70711); valid loss: 52.8096\n",
      "[Epoch 250/350, Step 11500, ETA 29.51s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.85s] step time: 0.005502s (±0.001648s); valid time: 0.008297s; loss: 61.2498 (±3.46839); valid loss: 52.806\n",
      "[Epoch 255/350, Step 11700, ETA 28.2s] step time: 0.005599s (±0.001853s); valid time: 0.008165s; loss: 60.8085 (±3.833); valid loss: 52.8321\n",
      "[Epoch 257/350, Step 11800, ETA 27.54s] step time: 0.005433s (±0.00148s); valid time: 0.007842s; loss: 61.2859 (±3.77867); valid loss: 52.8954\n",
      "[Epoch 259/350, Step 11900, ETA 26.88s] step time: 0.005314s (±0.001327s); valid time: 0.008083s; loss: 60.8548 (±3.70289); valid loss: 52.9849\n",
      "[Epoch 260/350, Step 11960, ETA 26.49s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 26.24s] step time: 0.006005s (±0.001932s); valid time: 0.007771s; loss: 61.0063 (±3.99299); valid loss: 52.8723\n",
      "[Epoch 264/350, Step 12100, ETA 25.59s] step time: 0.005465s (±0.001381s); valid time: 0.008943s; loss: 61.1298 (±3.77407); valid loss: 52.9413\n",
      "[Epoch 266/350, Step 12200, ETA 24.93s] step time: 0.005408s (±0.001253s); valid time: 0.008339s; loss: 61.0379 (±3.85881); valid loss: 52.9152\n",
      "[Epoch 268/350, Step 12300, ETA 24.28s] step time: 0.005536s (±0.00121s); valid time: 0.007947s; loss: 60.9011 (±3.94446); valid loss: 52.9719\n",
      "[Epoch 270/350, Step 12400, ETA 23.65s] step time: 0.006262s (±0.008154s); valid time: 0.08058s; loss: 61.1713 (±4.1302); valid loss: 52.7153 (*)\n",
      "[Epoch 270/350, Step 12420, ETA 23.52s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 23s] step time: 0.00544s (±0.001543s); valid time: 0.008416s; loss: 60.7855 (±3.75752); valid loss: 52.8922\n",
      "[Epoch 274/350, Step 12600, ETA 22.35s] step time: 0.005459s (±0.001542s); valid time: 0.008362s; loss: 60.9984 (±3.71952); valid loss: 52.9266\n",
      "[Epoch 277/350, Step 12700, ETA 21.7s] step time: 0.005429s (±0.001386s); valid time: 0.008652s; loss: 61.1832 (±3.52202); valid loss: 52.9217\n",
      "[Epoch 279/350, Step 12800, ETA 21.05s] step time: 0.005569s (±0.001616s); valid time: 0.008585s; loss: 61.0404 (±3.59051); valid loss: 52.9096\n",
      "[Epoch 280/350, Step 12880, ETA 20.53s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 20.4s] step time: 0.005382s (±0.001328s); valid time: 0.00807s; loss: 61.1652 (±3.45674); valid loss: 52.8421\n",
      "[Epoch 283/350, Step 13000, ETA 19.75s] step time: 0.00552s (±0.001594s); valid time: 0.008388s; loss: 60.78 (±4.11392); valid loss: 52.948\n",
      "[Epoch 285/350, Step 13100, ETA 19.11s] step time: 0.005449s (±0.001715s); valid time: 0.008111s; loss: 60.9099 (±3.81911); valid loss: 52.9433\n",
      "[Epoch 287/350, Step 13200, ETA 18.46s] step time: 0.005335s (±0.001269s); valid time: 0.008454s; loss: 61.1308 (±3.74873); valid loss: 53.0124\n",
      "[Epoch 290/350, Step 13300, ETA 17.81s] step time: 0.005569s (±0.001876s); valid time: 0.008774s; loss: 60.7237 (±3.52843); valid loss: 52.9496\n",
      "[Epoch 290/350, Step 13340, ETA 17.55s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 17.17s] step time: 0.005513s (±0.00178s); valid time: 0.009512s; loss: 61.2297 (±3.5787); valid loss: 52.8499\n",
      "[Epoch 294/350, Step 13500, ETA 16.53s] step time: 0.00572s (±0.001934s); valid time: 0.008562s; loss: 61.0294 (±3.50344); valid loss: 52.8377\n",
      "[Epoch 296/350, Step 13600, ETA 15.89s] step time: 0.00561s (±0.001959s); valid time: 0.01171s; loss: 61.1281 (±3.84098); valid loss: 52.9943\n",
      "[Epoch 298/350, Step 13700, ETA 15.25s] step time: 0.005581s (±0.001731s); valid time: 0.008646s; loss: 60.8386 (±3.2823); valid loss: 52.9502\n",
      "[Epoch 300/350, Step 13800, ETA 14.61s] step time: 0.005728s (±0.001962s); valid time: 0.008386s; loss: 61.1465 (±3.8224); valid loss: 53.0784\n",
      "[Epoch 300/350, Step 13800, ETA 14.61s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.97s] step time: 0.005396s (±0.001485s); valid time: 0.008187s; loss: 60.8824 (±3.74708); valid loss: 52.8603\n",
      "[Epoch 305/350, Step 14000, ETA 13.33s] step time: 0.005437s (±0.00113s); valid time: 0.008017s; loss: 61.2122 (±3.81564); valid loss: 52.8785\n",
      "[Epoch 307/350, Step 14100, ETA 12.69s] step time: 0.005412s (±0.001274s); valid time: 0.008531s; loss: 61.241 (±3.29632); valid loss: 52.8764\n",
      "[Epoch 309/350, Step 14200, ETA 12.05s] step time: 0.00562s (±0.001496s); valid time: 0.008325s; loss: 60.8102 (±3.95365); valid loss: 52.9382\n",
      "[Epoch 310/350, Step 14260, ETA 11.67s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.42s] step time: 0.005803s (±0.002001s); valid time: 0.008615s; loss: 60.8293 (±3.41357); valid loss: 52.8263\n",
      "[Epoch 314/350, Step 14400, ETA 10.78s] step time: 0.005434s (±0.001349s); valid time: 0.008552s; loss: 61.1721 (±3.29263); valid loss: 52.8546\n",
      "[Epoch 316/350, Step 14500, ETA 10.14s] step time: 0.005648s (±0.001317s); valid time: 0.008431s; loss: 61.1715 (±4.08863); valid loss: 53.0172\n",
      "[Epoch 318/350, Step 14600, ETA 9.505s] step time: 0.005553s (±0.001193s); valid time: 0.009198s; loss: 60.9046 (±3.68404); valid loss: 53.0747\n",
      "[Epoch 320/350, Step 14700, ETA 8.872s] step time: 0.005912s (±0.002014s); valid time: 0.008274s; loss: 61.0258 (±3.33509); valid loss: 52.8705\n",
      "[Epoch 320/350, Step 14720, ETA 8.743s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.236s] step time: 0.005628s (±0.001617s); valid time: 0.008617s; loss: 61.2214 (±3.65376); valid loss: 52.8522\n",
      "[Epoch 324/350, Step 14900, ETA 7.599s] step time: 0.00547s (±0.001418s); valid time: 0.008537s; loss: 60.9399 (±3.459); valid loss: 52.9675\n",
      "[Epoch 327/350, Step 15000, ETA 6.965s] step time: 0.005636s (±0.001542s); valid time: 0.008818s; loss: 60.9244 (±3.49908); valid loss: 52.9553\n",
      "[Epoch 329/350, Step 15100, ETA 6.33s] step time: 0.00564s (±0.001545s); valid time: 0.008549s; loss: 61.0336 (±3.70069); valid loss: 53.0141\n",
      "[Epoch 330/350, Step 15180, ETA 5.822s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.696s] step time: 0.005505s (±0.001166s); valid time: 0.008127s; loss: 61.3658 (±3.87462); valid loss: 52.8308\n",
      "[Epoch 333/350, Step 15300, ETA 5.061s] step time: 0.005547s (±0.00166s); valid time: 0.008067s; loss: 60.6017 (±3.63309); valid loss: 52.9143\n",
      "[Epoch 335/350, Step 15400, ETA 4.426s] step time: 0.005419s (±0.001808s); valid time: 0.01367s; loss: 61.1953 (±3.9248); valid loss: 53.0051\n",
      "[Epoch 337/350, Step 15500, ETA 3.794s] step time: 0.005749s (±0.001775s); valid time: 0.008579s; loss: 60.9203 (±3.65034); valid loss: 52.7867\n",
      "[Epoch 340/350, Step 15600, ETA 3.16s] step time: 0.00537s (±0.001269s); valid time: 0.008624s; loss: 61.1547 (±3.67214); valid loss: 52.8295\n",
      "[Epoch 340/350, Step 15640, ETA 2.907s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.528s] step time: 0.005746s (±0.001763s); valid time: 0.008085s; loss: 61.0679 (±3.38238); valid loss: 52.8956\n",
      "[Epoch 344/350, Step 15800, ETA 1.895s] step time: 0.005578s (±0.001439s); valid time: 0.009034s; loss: 60.852 (±4.11932); valid loss: 52.7538\n",
      "[Epoch 346/350, Step 15900, ETA 1.263s] step time: 0.00557s (±0.001484s); valid time: 0.008462s; loss: 61.057 (±3.91433); valid loss: 52.9276\n",
      "[Epoch 348/350, Step 16000, ETA 0.6315s] step time: 0.005658s (±0.001537s); valid time: 0.008102s; loss: 61.0463 (±3.67446); valid loss: 52.8537\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005455s (±0.00137s); valid time: 0.008783s; loss: 61.0085 (±3.6177); valid loss: 52.9762\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmperg9fg2x/variables.dat-12400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmperg9fg2x/variables.dat-12400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.13,\n",
      "\t(tp, fp, tn, fn)=(1571, 13825, 439, 16),\n",
      "\tprecision=0.1,\n",
      "\trecall=0.99,\n",
      "\tf1=0.19,\n",
      "\troc_auc=0.51,\n",
      "\ty_pred%=0.9712951864235695,\n",
      "\ty_label%=0.10011986625449498,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_PFE.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 47.02s] step time: 0.009864s (±0.03389s); valid time: 0.1648s; loss: 123.302 (±6.67921); valid loss: 141.191 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 16.58s] step time: 0.006257s (±0.008039s); valid time: 0.0802s; loss: 106.751 (±4.19564); valid loss: 128.202 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 5.577s] step time: 0.006169s (±0.008729s); valid time: 0.08713s; loss: 100.191 (±3.99025); valid loss: 125.182 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 0.2506s] step time: 0.006326s (±0.008331s); valid time: 0.08313s; loss: 97.8016 (±4.02232); valid loss: 124.749 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.02s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 57.02s] step time: 0.0064s (±0.009426s); valid time: 0.08844s; loss: 97.5816 (±3.11877); valid loss: 124.463 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 54.16s] step time: 0.006111s (±0.008354s); valid time: 0.08345s; loss: 96.9367 (±3.38084); valid loss: 124.4 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 52.44s] step time: 0.006468s (±0.008791s); valid time: 0.08729s; loss: 96.6097 (±4.01298); valid loss: 124.221 (*)\n",
      "[Epoch 18/350, Step 800, ETA 1m 51s] step time: 0.006374s (±0.008527s); valid time: 0.08485s; loss: 96.747 (±3.74054); valid loss: 124.176 (*)\n",
      "[Epoch 20/350, Step 900, ETA 1m 49.37s] step time: 0.006217s (±0.008724s); valid time: 0.08695s; loss: 96.0378 (±4.01898); valid loss: 124.163 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 48.65s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 47.96s] step time: 0.006258s (±0.008567s); valid time: 0.08534s; loss: 96.0905 (±3.43703); valid loss: 123.985 (*)\n",
      "[Epoch 24/350, Step 1100, ETA 1m 46.77s] step time: 0.006339s (±0.01117s); valid time: 0.1018s; loss: 95.9053 (±3.50004); valid loss: 123.923 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 45.79s] step time: 0.006259s (±0.008395s); valid time: 0.08383s; loss: 95.7664 (±3.80741); valid loss: 123.782 (*)\n",
      "[Epoch 29/350, Step 1300, ETA 1m 44.66s] step time: 0.006265s (±0.008552s); valid time: 0.08461s; loss: 95.3083 (±4.20859); valid loss: 123.398 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 42.89s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 43.59s] step time: 0.006253s (±0.008808s); valid time: 0.08661s; loss: 95.1253 (±3.77893); valid loss: 122.756 (*)\n",
      "[Epoch 33/350, Step 1500, ETA 1m 42.61s] step time: 0.006304s (±0.008452s); valid time: 0.08389s; loss: 94.6639 (±3.72363); valid loss: 121.904 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 41.54s] step time: 0.006154s (±0.008961s); valid time: 0.08886s; loss: 93.9627 (±4.28057); valid loss: 120.834 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 40.55s] step time: 0.006198s (±0.009028s); valid time: 0.0903s; loss: 93.2346 (±3.71239); valid loss: 119.89 (*)\n",
      "[Epoch 40/350, Step 1800, ETA 1m 39.69s] step time: 0.006213s (±0.008848s); valid time: 0.08828s; loss: 92.8206 (±5.06169); valid loss: 119.226 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 39s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 38.96s] step time: 0.006456s (±0.008593s); valid time: 0.08461s; loss: 92.4068 (±4.12491); valid loss: 118.685 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 38.14s] step time: 0.006353s (±0.01006s); valid time: 0.09995s; loss: 91.9264 (±3.92242); valid loss: 118.333 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 37.23s] step time: 0.006176s (±0.008442s); valid time: 0.08384s; loss: 91.0618 (±3.53845); valid loss: 118.106 (*)\n",
      "[Epoch 48/350, Step 2200, ETA 1m 36.36s] step time: 0.006192s (±0.008708s); valid time: 0.08679s; loss: 91.7586 (±5.5384); valid loss: 117.878 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 35.58s] step time: 0.006328s (±0.009518s); valid time: 0.09426s; loss: 3190.71 (±30841); valid loss: 117.55 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 35.58s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 34.3s] step time: 0.005338s (±0.001288s); valid time: 0.008397s; loss: 126.301 (±327.79); valid loss: 117.552\n",
      "[Epoch 55/350, Step 2500, ETA 1m 33.44s] step time: 0.006006s (±0.008588s); valid time: 0.0861s; loss: 101.132 (±102.293); valid loss: 117.489 (*)\n",
      "[Epoch 57/350, Step 2600, ETA 1m 32.56s] step time: 0.006012s (±0.007803s); valid time: 0.07807s; loss: 91.3513 (±5.57994); valid loss: 117.426 (*)\n",
      "[Epoch 59/350, Step 2700, ETA 1m 31.92s] step time: 0.006471s (±0.008489s); valid time: 0.08417s; loss: 90.6279 (±4.69395); valid loss: 117.276 (*)\n",
      "[Epoch 60/350, Step 2760, ETA 1m 31.2s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 31.28s] step time: 0.00649s (±0.009172s); valid time: 0.09139s; loss: 90.6929 (±5.39066); valid loss: 117.266 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 30.55s] step time: 0.006196s (±0.008629s); valid time: 0.08637s; loss: 91.0315 (±7.2768); valid loss: 117.158 (*)\n",
      "[Epoch 66/350, Step 3000, ETA 1m 29.47s] step time: 0.005501s (±0.001365s); valid time: 0.008126s; loss: 257.297 (±1661.39); valid loss: 117.23\n",
      "[Epoch 68/350, Step 3100, ETA 1m 28.7s] step time: 0.006047s (±0.008374s); valid time: 0.08385s; loss: 91.4731 (±11.3638); valid loss: 117.107 (*)\n",
      "[Epoch 70/350, Step 3200, ETA 1m 27.95s] step time: 0.00618s (±0.00883s); valid time: 0.08805s; loss: 91.9972 (±15.0508); valid loss: 117.097 (*)\n",
      "[Epoch 70/350, Step 3220, ETA 1m 27.69s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 27.17s] step time: 0.006084s (±0.008715s); valid time: 0.08706s; loss: 101.427 (±112.433); valid loss: 117.064 (*)\n",
      "[Epoch 74/350, Step 3400, ETA 1m 26.16s] step time: 0.00547s (±0.001684s); valid time: 0.008323s; loss: 90.8819 (±7.89043); valid loss: 117.127\n",
      "[Epoch 77/350, Step 3500, ETA 1m 25.53s] step time: 0.006342s (±0.008327s); valid time: 0.08301s; loss: 90.6678 (±9.61773); valid loss: 117.052 (*)\n",
      "[Epoch 79/350, Step 3600, ETA 1m 24.61s] step time: 0.005527s (±0.001544s); valid time: 0.008246s; loss: 111.815 (±209.021); valid loss: 117.083\n",
      "[Epoch 80/350, Step 3680, ETA 1m 23.77s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 23.62s] step time: 0.005365s (±0.001164s); valid time: 0.008237s; loss: 90.1511 (±4.0806); valid loss: 117.067\n",
      "[Epoch 83/350, Step 3800, ETA 1m 22.89s] step time: 0.006119s (±0.008524s); valid time: 0.08491s; loss: 90.9115 (±11.6056); valid loss: 116.905 (*)\n",
      "[Epoch 85/350, Step 3900, ETA 1m 21.96s] step time: 0.005456s (±0.001517s); valid time: 0.008078s; loss: 89.8706 (±3.8122); valid loss: 117.001\n",
      "[Epoch 87/350, Step 4000, ETA 1m 21.06s] step time: 0.005496s (±0.001417s); valid time: 0.008352s; loss: 92.6568 (±22.3319); valid loss: 117.033\n",
      "[Epoch 90/350, Step 4100, ETA 1m 20.27s] step time: 0.005708s (±0.001501s); valid time: 0.007978s; loss: 104.127 (±140.75); valid loss: 116.93\n",
      "[Epoch 90/350, Step 4140, ETA 1m 19.88s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 19.35s] step time: 0.005357s (±0.001496s); valid time: 0.007935s; loss: 681.412 (±5749.88); valid loss: 116.969\n",
      "[Epoch 94/350, Step 4300, ETA 1m 18.55s] step time: 0.00572s (±0.002763s); valid time: 0.02206s; loss: 90.0601 (±4.39596); valid loss: 116.98\n",
      "[Epoch 96/350, Step 4400, ETA 1m 17.71s] step time: 0.005505s (±0.001535s); valid time: 0.008169s; loss: 5806.77 (±56874); valid loss: 116.988\n",
      "[Epoch 98/350, Step 4500, ETA 1m 16.79s] step time: 0.005211s (±0.001287s); valid time: 0.008353s; loss: 90.7554 (±9.51701); valid loss: 116.918\n",
      "[Epoch 100/350, Step 4600, ETA 1m 16.14s] step time: 0.006215s (±0.008505s); valid time: 0.08386s; loss: 291.299 (±2005.14); valid loss: 116.89 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 16.14s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 15.31s] step time: 0.005372s (±0.001232s); valid time: 0.008795s; loss: 90.7203 (±8.20826); valid loss: 116.946\n",
      "[Epoch 105/350, Step 4800, ETA 1m 14.66s] step time: 0.006167s (±0.008797s); valid time: 0.08737s; loss: 218.865 (±917.003); valid loss: 116.872 (*)\n",
      "[Epoch 107/350, Step 4900, ETA 1m 13.99s] step time: 0.006061s (±0.008246s); valid time: 0.08188s; loss: 100.116 (±102.138); valid loss: 116.864 (*)\n",
      "[Epoch 109/350, Step 5000, ETA 1m 13.31s] step time: 0.006073s (±0.008796s); valid time: 0.08738s; loss: 92.2743 (±24.1918); valid loss: 116.816 (*)\n",
      "[Epoch 110/350, Step 5060, ETA 1m 12.86s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 12.56s] step time: 0.00569s (±0.001713s); valid time: 0.008542s; loss: 97.8988 (±80.9297); valid loss: 116.915\n",
      "[Epoch 114/350, Step 5200, ETA 1m 11.76s] step time: 0.0053s (±0.001608s); valid time: 0.008343s; loss: 96.5002 (±65.1101); valid loss: 116.854\n",
      "[Epoch 116/350, Step 5300, ETA 1m 10.96s] step time: 0.005444s (±0.00124s); valid time: 0.008296s; loss: 90.9121 (±10.5302); valid loss: 116.929\n",
      "[Epoch 118/350, Step 5400, ETA 1m 10.17s] step time: 0.005382s (±0.00121s); valid time: 0.008423s; loss: 107.85 (±164.792); valid loss: 116.841\n",
      "[Epoch 120/350, Step 5500, ETA 1m 9.397s] step time: 0.005488s (±0.001295s); valid time: 0.008408s; loss: 89.8612 (±4.94473); valid loss: 116.857\n",
      "[Epoch 120/350, Step 5520, ETA 1m 9.222s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 8.643s] step time: 0.005553s (±0.001658s); valid time: 0.009437s; loss: 90.0252 (±4.22153); valid loss: 116.882\n",
      "[Epoch 124/350, Step 5700, ETA 1m 8.012s] step time: 0.006209s (±0.009338s); valid time: 0.09321s; loss: 1724.48 (±15180.7); valid loss: 116.807 (*)\n",
      "[Epoch 127/350, Step 5800, ETA 1m 7.276s] step time: 0.005512s (±0.001524s); valid time: 0.008358s; loss: 91.1418 (±12.0791); valid loss: 116.884\n",
      "[Epoch 129/350, Step 5900, ETA 1m 6.499s] step time: 0.005362s (±0.001564s); valid time: 0.008328s; loss: 90.6833 (±11.3398); valid loss: 116.886\n",
      "[Epoch 130/350, Step 5980, ETA 1m 5.881s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 5.896s] step time: 0.006296s (±0.009487s); valid time: 0.09393s; loss: 91.2544 (±14.5216); valid loss: 116.787 (*)\n",
      "[Epoch 133/350, Step 6100, ETA 1m 5.149s] step time: 0.00543s (±0.001535s); valid time: 0.00807s; loss: 120.827 (±311.671); valid loss: 116.851\n",
      "[Epoch 135/350, Step 6200, ETA 1m 4.443s] step time: 0.00571s (±0.001615s); valid time: 0.008375s; loss: 90.6668 (±5.86734); valid loss: 116.792\n",
      "[Epoch 137/350, Step 6300, ETA 1m 3.694s] step time: 0.005435s (±0.001547s); valid time: 0.008187s; loss: 132.3 (±426.055); valid loss: 116.891\n",
      "[Epoch 140/350, Step 6400, ETA 1m 2.981s] step time: 0.005525s (±0.001505s); valid time: 0.008466s; loss: 91.016 (±13.8189); valid loss: 116.87\n",
      "[Epoch 140/350, Step 6440, ETA 1m 2.667s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 2.253s] step time: 0.005487s (±0.001701s); valid time: 0.008793s; loss: 90.1468 (±7.87807); valid loss: 116.832\n",
      "[Epoch 144/350, Step 6600, ETA 1m 1.538s] step time: 0.005545s (±0.00173s); valid time: 0.008102s; loss: 89.6739 (±3.61148); valid loss: 116.804\n",
      "[Epoch 146/350, Step 6700, ETA 1m 0.9464s] step time: 0.006419s (±0.009862s); valid time: 0.09851s; loss: 918.377 (±8246.72); valid loss: 116.785 (*)\n",
      "[Epoch 148/350, Step 6800, ETA 1m 0.3004s] step time: 0.006043s (±0.008238s); valid time: 0.08201s; loss: 89.6997 (±3.39244); valid loss: 116.777 (*)\n",
      "[Epoch 150/350, Step 6900, ETA 59.58s] step time: 0.0055s (±0.001525s); valid time: 0.008367s; loss: 238.921 (±1472.37); valid loss: 116.878\n",
      "[Epoch 150/350, Step 6900, ETA 59.58s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 58.88s] step time: 0.005463s (±0.00131s); valid time: 0.008457s; loss: 89.5728 (±3.8873); valid loss: 116.779\n",
      "[Epoch 155/350, Step 7100, ETA 58.16s] step time: 0.005452s (±0.001735s); valid time: 0.01035s; loss: 428.588 (±3371.12); valid loss: 116.843\n",
      "[Epoch 157/350, Step 7200, ETA 57.43s] step time: 0.005305s (±0.001483s); valid time: 0.008252s; loss: 89.5454 (±4.44662); valid loss: 116.848\n",
      "[Epoch 159/350, Step 7300, ETA 56.7s] step time: 0.005352s (±0.001629s); valid time: 0.008656s; loss: 92.1596 (±20.3122); valid loss: 116.797\n",
      "[Epoch 160/350, Step 7360, ETA 56.28s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 56.02s] step time: 0.00565s (±0.001926s); valid time: 0.008194s; loss: 499.165 (±4073.68); valid loss: 116.812\n",
      "[Epoch 164/350, Step 7500, ETA 55.32s] step time: 0.005335s (±0.001806s); valid time: 0.01196s; loss: 128.063 (±356.563); valid loss: 116.904\n",
      "[Epoch 166/350, Step 7600, ETA 54.61s] step time: 0.005447s (±0.00147s); valid time: 0.00832s; loss: 89.881 (±4.05051); valid loss: 116.812\n",
      "[Epoch 168/350, Step 7700, ETA 54.01s] step time: 0.006306s (±0.009283s); valid time: 0.0922s; loss: 90.4861 (±9.16079); valid loss: 116.746 (*)\n",
      "[Epoch 170/350, Step 7800, ETA 53.33s] step time: 0.005562s (±0.001403s); valid time: 0.008367s; loss: 10207.6 (±100671); valid loss: 116.822\n",
      "[Epoch 170/350, Step 7820, ETA 53.18s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 52.65s] step time: 0.00563s (±0.001738s); valid time: 0.008596s; loss: 92.114 (±24.1112); valid loss: 116.821\n",
      "[Epoch 174/350, Step 8000, ETA 52.03s] step time: 0.0062s (±0.008756s); valid time: 0.08761s; loss: 1278.43 (±11826.2); valid loss: 116.741 (*)\n",
      "[Epoch 177/350, Step 8100, ETA 51.35s] step time: 0.005435s (±0.001551s); valid time: 0.008266s; loss: 874.805 (±7812.51); valid loss: 116.774\n",
      "[Epoch 179/350, Step 8200, ETA 50.65s] step time: 0.005387s (±0.001276s); valid time: 0.008658s; loss: 89.7665 (±4.06896); valid loss: 116.793\n",
      "[Epoch 180/350, Step 8280, ETA 50.07s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.95s] step time: 0.005339s (±0.001278s); valid time: 0.008252s; loss: 90.0102 (±7.01518); valid loss: 116.764\n",
      "[Epoch 183/350, Step 8400, ETA 49.27s] step time: 0.005526s (±0.001591s); valid time: 0.008872s; loss: 171.029 (±733.003); valid loss: 116.787\n",
      "[Epoch 185/350, Step 8500, ETA 48.58s] step time: 0.005391s (±0.00121s); valid time: 0.009131s; loss: 744.69 (±6193.63); valid loss: 116.835\n",
      "[Epoch 187/350, Step 8600, ETA 47.91s] step time: 0.005534s (±0.001501s); valid time: 0.008084s; loss: 90.5075 (±7.71329); valid loss: 116.782\n",
      "[Epoch 190/350, Step 8700, ETA 47.31s] step time: 0.006214s (±0.009565s); valid time: 0.0957s; loss: 91.3245 (±12.6102); valid loss: 116.732 (*)\n",
      "[Epoch 190/350, Step 8740, ETA 47.03s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 46.63s] step time: 0.005438s (±0.001585s); valid time: 0.008498s; loss: 1871.36 (±17636.9); valid loss: 116.801\n",
      "[Epoch 194/350, Step 8900, ETA 45.96s] step time: 0.005573s (±0.001547s); valid time: 0.008088s; loss: 105.024 (±119.254); valid loss: 116.736\n",
      "[Epoch 196/350, Step 9000, ETA 45.28s] step time: 0.005392s (±0.001365s); valid time: 0.008874s; loss: 104.253 (±103.032); valid loss: 116.738\n",
      "[Epoch 198/350, Step 9100, ETA 44.61s] step time: 0.005432s (±0.001521s); valid time: 0.008151s; loss: 147.731 (±567.68); valid loss: 116.76\n",
      "[Epoch 200/350, Step 9200, ETA 43.93s] step time: 0.005435s (±0.001838s); valid time: 0.008447s; loss: 89.6241 (±4.15287); valid loss: 116.804\n",
      "[Epoch 200/350, Step 9200, ETA 43.93s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 43.28s] step time: 0.005625s (±0.001719s); valid time: 0.00917s; loss: 127.431 (±372.301); valid loss: 116.841\n",
      "[Epoch 205/350, Step 9400, ETA 42.62s] step time: 0.00557s (±0.001854s); valid time: 0.00813s; loss: 91.5673 (±13.8787); valid loss: 116.756\n",
      "[Epoch 207/350, Step 9500, ETA 41.94s] step time: 0.005233s (±0.00102s); valid time: 0.008396s; loss: 89.6633 (±3.25439); valid loss: 116.74\n",
      "[Epoch 209/350, Step 9600, ETA 41.29s] step time: 0.005638s (±0.001516s); valid time: 0.008352s; loss: 89.5899 (±3.96958); valid loss: 116.874\n",
      "[Epoch 210/350, Step 9660, ETA 40.88s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.62s] step time: 0.005394s (±0.001872s); valid time: 0.008115s; loss: 95.2159 (±54.4944); valid loss: 116.785\n",
      "[Epoch 214/350, Step 9800, ETA 39.97s] step time: 0.0055s (±0.001434s); valid time: 0.008473s; loss: 89.5483 (±3.48175); valid loss: 116.753\n",
      "[Epoch 216/350, Step 9900, ETA 39.31s] step time: 0.005466s (±0.001667s); valid time: 0.008343s; loss: 148.915 (±447.13); valid loss: 116.781\n",
      "[Epoch 218/350, Step 10000, ETA 38.66s] step time: 0.005472s (±0.001024s); valid time: 0.008959s; loss: 154.589 (±630.725); valid loss: 116.735\n",
      "[Epoch 220/350, Step 10100, ETA 38.01s] step time: 0.005635s (±0.00168s); valid time: 0.008539s; loss: 143.045 (±512.434); valid loss: 116.833\n",
      "[Epoch 220/350, Step 10120, ETA 37.87s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 37.35s] step time: 0.005412s (±0.001628s); valid time: 0.008053s; loss: 92.1851 (±24.6955); valid loss: 116.851\n",
      "[Epoch 224/350, Step 10300, ETA 36.69s] step time: 0.005329s (±0.001482s); valid time: 0.008356s; loss: 90.323 (±6.95852); valid loss: 116.773\n",
      "[Epoch 227/350, Step 10400, ETA 36.03s] step time: 0.00534s (±0.001642s); valid time: 0.0109s; loss: 89.9937 (±4.59262); valid loss: 116.818\n",
      "[Epoch 229/350, Step 10500, ETA 35.38s] step time: 0.005575s (±0.001678s); valid time: 0.00889s; loss: 91.099 (±16.12); valid loss: 116.847\n",
      "[Epoch 230/350, Step 10580, ETA 34.86s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.73s] step time: 0.005473s (±0.001542s); valid time: 0.008023s; loss: 94.4799 (±47.0268); valid loss: 116.854\n",
      "[Epoch 233/350, Step 10700, ETA 34.08s] step time: 0.005403s (±0.001312s); valid time: 0.00805s; loss: 89.4284 (±3.90298); valid loss: 116.819\n",
      "[Epoch 235/350, Step 10800, ETA 33.43s] step time: 0.005478s (±0.00145s); valid time: 0.008373s; loss: 89.8939 (±3.60953); valid loss: 116.787\n",
      "[Epoch 237/350, Step 10900, ETA 32.78s] step time: 0.005427s (±0.001286s); valid time: 0.008239s; loss: 557.502 (±4658.23); valid loss: 116.852\n",
      "[Epoch 240/350, Step 11000, ETA 32.14s] step time: 0.005572s (±0.00171s); valid time: 0.008306s; loss: 207.467 (±1137.41); valid loss: 116.845\n",
      "[Epoch 240/350, Step 11040, ETA 31.87s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.5s] step time: 0.005646s (±0.002121s); valid time: 0.008613s; loss: 100.251 (±92.3051); valid loss: 116.828\n",
      "[Epoch 244/350, Step 11200, ETA 30.85s] step time: 0.00539s (±0.001275s); valid time: 0.008087s; loss: 89.537 (±3.39187); valid loss: 116.827\n",
      "[Epoch 246/350, Step 11300, ETA 30.24s] step time: 0.006311s (±0.008681s); valid time: 0.08686s; loss: 89.6662 (±3.64912); valid loss: 116.728 (*)\n",
      "[Epoch 248/350, Step 11400, ETA 29.59s] step time: 0.005468s (±0.001273s); valid time: 0.00917s; loss: 89.8677 (±3.88508); valid loss: 116.836\n",
      "[Epoch 250/350, Step 11500, ETA 28.96s] step time: 0.005594s (±0.001364s); valid time: 0.008271s; loss: 93.2612 (±38.1494); valid loss: 116.849\n",
      "[Epoch 250/350, Step 11500, ETA 28.96s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 28.32s] step time: 0.00551s (±0.001536s); valid time: 0.008595s; loss: 436.686 (±3349.68); valid loss: 116.784\n",
      "[Epoch 255/350, Step 11700, ETA 27.67s] step time: 0.005253s (±0.0009715s); valid time: 0.009144s; loss: 97.2033 (±78.5258); valid loss: 116.834\n",
      "[Epoch 257/350, Step 11800, ETA 27.03s] step time: 0.005511s (±0.00163s); valid time: 0.008562s; loss: 171.008 (±804.04); valid loss: 116.817\n",
      "[Epoch 259/350, Step 11900, ETA 26.39s] step time: 0.00538s (±0.001931s); valid time: 0.01599s; loss: 43511.5 (±430550); valid loss: 116.831\n",
      "[Epoch 260/350, Step 11960, ETA 26.01s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.76s] step time: 0.00591s (±0.002191s); valid time: 0.008498s; loss: 89.6572 (±3.39433); valid loss: 116.826\n",
      "[Epoch 264/350, Step 12100, ETA 25.13s] step time: 0.005674s (±0.001644s); valid time: 0.00866s; loss: 90.0852 (±5.51167); valid loss: 116.813\n",
      "[Epoch 266/350, Step 12200, ETA 24.5s] step time: 0.005578s (±0.00133s); valid time: 0.008541s; loss: 90.1006 (±5.44573); valid loss: 116.914\n",
      "[Epoch 268/350, Step 12300, ETA 23.86s] step time: 0.005431s (±0.001319s); valid time: 0.008254s; loss: 109.725 (±200.562); valid loss: 116.797\n",
      "[Epoch 270/350, Step 12400, ETA 23.22s] step time: 0.005403s (±0.001278s); valid time: 0.008271s; loss: 89.4675 (±3.88331); valid loss: 116.816\n",
      "[Epoch 270/350, Step 12420, ETA 23.09s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.58s] step time: 0.005412s (±0.001654s); valid time: 0.009109s; loss: 90.6939 (±8.3289); valid loss: 116.752\n",
      "[Epoch 274/350, Step 12600, ETA 21.94s] step time: 0.005397s (±0.00138s); valid time: 0.01129s; loss: 93.2978 (±39.7889); valid loss: 116.879\n",
      "[Epoch 277/350, Step 12700, ETA 21.31s] step time: 0.00548s (±0.001464s); valid time: 0.009297s; loss: 115.531 (±257.388); valid loss: 116.751\n",
      "[Epoch 279/350, Step 12800, ETA 20.68s] step time: 0.005797s (±0.00202s); valid time: 0.01238s; loss: 53268.6 (±528922); valid loss: 116.812\n",
      "[Epoch 280/350, Step 12880, ETA 20.17s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 20.05s] step time: 0.0054s (±0.00127s); valid time: 0.008297s; loss: 226.514 (±1362.61); valid loss: 116.739\n",
      "[Epoch 283/350, Step 13000, ETA 19.41s] step time: 0.005522s (±0.001448s); valid time: 0.008677s; loss: 407.295 (±3163.18); valid loss: 116.832\n",
      "[Epoch 285/350, Step 13100, ETA 18.78s] step time: 0.005395s (±0.001233s); valid time: 0.008609s; loss: 547.867 (±4557.74); valid loss: 116.84\n",
      "[Epoch 287/350, Step 13200, ETA 18.14s] step time: 0.00546s (±0.001481s); valid time: 0.008574s; loss: 91.4411 (±15.2347); valid loss: 116.767\n",
      "[Epoch 290/350, Step 13300, ETA 17.52s] step time: 0.005556s (±0.001558s); valid time: 0.008752s; loss: 89.7202 (±3.7973); valid loss: 116.774\n",
      "[Epoch 290/350, Step 13340, ETA 17.26s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.89s] step time: 0.005657s (±0.001886s); valid time: 0.008497s; loss: 93.3752 (±27.587); valid loss: 116.769\n",
      "[Epoch 294/350, Step 13500, ETA 16.25s] step time: 0.005369s (±0.001272s); valid time: 0.007952s; loss: 89.7291 (±4.93242); valid loss: 116.77\n",
      "[Epoch 296/350, Step 13600, ETA 15.62s] step time: 0.005289s (±0.001486s); valid time: 0.008224s; loss: 89.3814 (±3.60793); valid loss: 116.74\n",
      "[Epoch 298/350, Step 13700, ETA 14.99s] step time: 0.005461s (±0.001511s); valid time: 0.008403s; loss: 91.4712 (±15.0124); valid loss: 116.768\n",
      "[Epoch 300/350, Step 13800, ETA 14.36s] step time: 0.005589s (±0.001654s); valid time: 0.00852s; loss: 93.3943 (±33.6661); valid loss: 116.758\n",
      "[Epoch 300/350, Step 13800, ETA 14.36s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.73s] step time: 0.005426s (±0.001404s); valid time: 0.008597s; loss: 199.546 (±1085.6); valid loss: 116.817\n",
      "[Epoch 305/350, Step 14000, ETA 13.11s] step time: 0.005508s (±0.001744s); valid time: 0.008796s; loss: 102.664 (±132.452); valid loss: 116.803\n",
      "[Epoch 307/350, Step 14100, ETA 12.48s] step time: 0.005345s (±0.001276s); valid time: 0.008214s; loss: 17253.4 (±170737); valid loss: 116.817\n",
      "[Epoch 309/350, Step 14200, ETA 11.85s] step time: 0.005547s (±0.001909s); valid time: 0.008044s; loss: 3824.06 (±37147.7); valid loss: 116.785\n",
      "[Epoch 310/350, Step 14260, ETA 11.47s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.22s] step time: 0.005593s (±0.001708s); valid time: 0.008267s; loss: 92.8144 (±23.1691); valid loss: 116.813\n",
      "[Epoch 314/350, Step 14400, ETA 10.6s] step time: 0.005533s (±0.001862s); valid time: 0.008316s; loss: 89.454 (±4.135); valid loss: 116.811\n",
      "[Epoch 316/350, Step 14500, ETA 9.971s] step time: 0.00543s (±0.001691s); valid time: 0.008055s; loss: 165.489 (±753.79); valid loss: 116.79\n",
      "[Epoch 318/350, Step 14600, ETA 9.346s] step time: 0.005588s (±0.001874s); valid time: 0.008698s; loss: 157.107 (±654.873); valid loss: 116.848\n",
      "[Epoch 320/350, Step 14700, ETA 8.721s] step time: 0.00556s (±0.001647s); valid time: 0.008839s; loss: 96.4295 (±62.9783); valid loss: 116.841\n",
      "[Epoch 320/350, Step 14720, ETA 8.594s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.094s] step time: 0.005382s (±0.001446s); valid time: 0.008837s; loss: 90.2008 (±7.58888); valid loss: 116.789\n",
      "[Epoch 324/350, Step 14900, ETA 7.47s] step time: 0.005582s (±0.001538s); valid time: 0.008116s; loss: 90.7476 (±11.2426); valid loss: 116.825\n",
      "[Epoch 327/350, Step 15000, ETA 6.847s] step time: 0.00554s (±0.001612s); valid time: 0.008037s; loss: 91.9119 (±20.741); valid loss: 116.885\n",
      "[Epoch 329/350, Step 15100, ETA 6.223s] step time: 0.005505s (±0.001366s); valid time: 0.008511s; loss: 281.202 (±1906.09); valid loss: 116.81\n",
      "[Epoch 330/350, Step 15180, ETA 5.722s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.598s] step time: 0.005396s (±0.001727s); valid time: 0.007946s; loss: 90.1493 (±4.88174); valid loss: 116.796\n",
      "[Epoch 333/350, Step 15300, ETA 4.974s] step time: 0.0053s (±0.0012s); valid time: 0.008486s; loss: 89.3964 (±3.95018); valid loss: 116.821\n",
      "[Epoch 335/350, Step 15400, ETA 4.351s] step time: 0.005582s (±0.001692s); valid time: 0.00837s; loss: 93.1982 (±27.6094); valid loss: 116.767\n",
      "[Epoch 337/350, Step 15500, ETA 3.728s] step time: 0.005329s (±0.00151s); valid time: 0.00791s; loss: 89.7221 (±3.91339); valid loss: 116.836\n",
      "[Epoch 340/350, Step 15600, ETA 3.109s] step time: 0.006339s (±0.008591s); valid time: 0.08548s; loss: 691.805 (±5622.2); valid loss: 116.724 (*)\n",
      "[Epoch 340/350, Step 15640, ETA 2.859s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.487s] step time: 0.005449s (±0.001798s); valid time: 0.008344s; loss: 89.4975 (±3.45806); valid loss: 116.818\n",
      "[Epoch 344/350, Step 15800, ETA 1.864s] step time: 0.00538s (±0.001976s); valid time: 0.00841s; loss: 144406 (±1.43585e+06); valid loss: 116.838\n",
      "[Epoch 346/350, Step 15900, ETA 1.243s] step time: 0.005589s (±0.001624s); valid time: 0.008411s; loss: 108.914 (±192.426); valid loss: 116.805\n",
      "[Epoch 348/350, Step 16000, ETA 0.6212s] step time: 0.00556s (±0.002074s); valid time: 0.01133s; loss: 132.521 (±418.958); valid loss: 116.752\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005347s (±0.001316s); valid time: 0.008474s; loss: 11836.4 (±116810); valid loss: 116.789\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmploykse05/variables.dat-15600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmploykse05/variables.dat-15600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.65,\n",
      "\t(tp, fp, tn, fn)=(819, 4847, 9423, 769),\n",
      "\tprecision=0.14,\n",
      "\trecall=0.52,\n",
      "\tf1=0.23,\n",
      "\troc_auc=0.59,\n",
      "\ty_pred%=0.35729600201790895,\n",
      "\ty_label%=0.10013873123975281,\n",
      ")\n",
      "Testing on realTweets/Twitter_volume_UPS.csv ...\n",
      "Trainable Parameters                   (11,896 in total)\n",
      "--------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (100,)       100\n",
      "donut/p_x_given_z/x_mean/kernel         (16, 100)  1,600\n",
      "donut/p_x_given_z/x_std/bias            (100,)       100\n",
      "donut/p_x_given_z/x_std/kernel          (16, 100)  1,600\n",
      "donut/q_z_given_x/z_mean/bias           (8,)           8\n",
      "donut/q_z_given_x/z_mean/kernel         (64, 8)      512\n",
      "donut/q_z_given_x/z_std/bias            (8,)           8\n",
      "donut/q_z_given_x/z_std/kernel          (64, 8)      512\n",
      "sequential/forward/_0/dense/bias        (64,)         64\n",
      "sequential/forward/_0/dense/kernel      (8, 64)      512\n",
      "sequential/forward/_1/dense_1/bias      (32,)         32\n",
      "sequential/forward/_1/dense_1/kernel    (64, 32)   2,048\n",
      "sequential/forward/_2/dense_2/bias      (16,)         16\n",
      "sequential/forward/_2/dense_2/kernel    (32, 16)     512\n",
      "sequential_1/forward/_0/dense_3/bias    (16,)         16\n",
      "sequential_1/forward/_0/dense_3/kernel  (100, 16)  1,600\n",
      "sequential_1/forward/_1/dense_4/bias    (32,)         32\n",
      "sequential_1/forward/_1/dense_4/kernel  (16, 32)     512\n",
      "sequential_1/forward/_2/dense_5/bias    (64,)         64\n",
      "sequential_1/forward/_2/dense_5/kernel  (32, 64)   2,048\n",
      "\n",
      "[Epoch 3/350, Step 100, ETA 2m 46.48s] step time: 0.009834s (±0.03401s); valid time: 0.168s; loss: 118.884 (±23.654); valid loss: 80.0055 (*)\n",
      "[Epoch 5/350, Step 200, ETA 2m 15.42s] step time: 0.006173s (±0.008305s); valid time: 0.08174s; loss: 26.6983 (±24.3036); valid loss: -16.4192 (*)\n",
      "[Epoch 7/350, Step 300, ETA 2m 4.613s] step time: 0.006174s (±0.008425s); valid time: 0.08355s; loss: -1.47143 (±6.42933); valid loss: -24.0866 (*)\n",
      "[Epoch 9/350, Step 400, ETA 2m 0.07442s] step time: 0.006463s (±0.00903s); valid time: 0.08979s; loss: -8.45344 (±6.98797); valid loss: -29.5323 (*)\n",
      "[Epoch 10/350, Step 460, ETA 1m 56.18s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/350, Step 500, ETA 1m 56.84s] step time: 0.006378s (±0.009174s); valid time: 0.09087s; loss: -13.8414 (±6.6187); valid loss: -33.2974 (*)\n",
      "[Epoch 14/350, Step 600, ETA 1m 54.78s] step time: 0.006395s (±0.008559s); valid time: 0.0848s; loss: -17.92 (±5.73422); valid loss: -36.0494 (*)\n",
      "[Epoch 16/350, Step 700, ETA 1m 50.78s] step time: 0.005473s (±0.001568s); valid time: 0.008224s; loss: -20.7282 (±6.39239); valid loss: -35.5747\n",
      "[Epoch 18/350, Step 800, ETA 1m 47.7s] step time: 0.005407s (±0.001019s); valid time: 0.008427s; loss: -22.9992 (±6.93113); valid loss: -35.9916\n",
      "[Epoch 20/350, Step 900, ETA 1m 46.49s] step time: 0.006276s (±0.00793s); valid time: 0.07867s; loss: -24.5072 (±5.82141); valid loss: -37.1045 (*)\n",
      "[Epoch 20/350, Step 920, ETA 1m 45.96s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/350, Step 1000, ETA 1m 44.18s] step time: 0.005468s (±0.001585s); valid time: 0.009984s; loss: -25.6568 (±5.8285); valid loss: -36.9341\n",
      "[Epoch 24/350, Step 1100, ETA 1m 43.29s] step time: 0.006296s (±0.00867s); valid time: 0.08678s; loss: -26.5429 (±5.61741); valid loss: -38.6106 (*)\n",
      "[Epoch 27/350, Step 1200, ETA 1m 41.58s] step time: 0.005498s (±0.00142s); valid time: 0.008009s; loss: -27.3599 (±6.60735); valid loss: -37.7008\n",
      "[Epoch 29/350, Step 1300, ETA 1m 40.96s] step time: 0.006414s (±0.009042s); valid time: 0.09s; loss: -28.0983 (±6.32531); valid loss: -38.8765 (*)\n",
      "[Epoch 30/350, Step 1380, ETA 1m 39.72s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 31/350, Step 1400, ETA 1m 39.53s] step time: 0.005605s (±0.001489s); valid time: 0.008693s; loss: -28.5842 (±5.48619); valid loss: -38.3043\n",
      "[Epoch 33/350, Step 1500, ETA 1m 38.74s] step time: 0.006186s (±0.008558s); valid time: 0.08512s; loss: -29.8149 (±5.67393); valid loss: -40.1516 (*)\n",
      "[Epoch 35/350, Step 1600, ETA 1m 38.05s] step time: 0.006234s (±0.008478s); valid time: 0.0842s; loss: -29.4818 (±5.47593); valid loss: -41.056 (*)\n",
      "[Epoch 37/350, Step 1700, ETA 1m 36.65s] step time: 0.005461s (±0.001469s); valid time: 0.008087s; loss: -30.1206 (±5.81528); valid loss: -40.8713\n",
      "[Epoch 40/350, Step 1800, ETA 1m 35.98s] step time: 0.006145s (±0.008528s); valid time: 0.08475s; loss: -30.3882 (±5.78614); valid loss: -41.2064 (*)\n",
      "[Epoch 40/350, Step 1840, ETA 1m 35.44s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 42/350, Step 1900, ETA 1m 35.38s] step time: 0.00634s (±0.00902s); valid time: 0.08916s; loss: -31.1544 (±5.09106); valid loss: -41.5369 (*)\n",
      "[Epoch 44/350, Step 2000, ETA 1m 34.97s] step time: 0.006484s (±0.009018s); valid time: 0.08835s; loss: -31.34 (±5.6169); valid loss: -42.6131 (*)\n",
      "[Epoch 46/350, Step 2100, ETA 1m 33.7s] step time: 0.005393s (±0.001576s); valid time: 0.008412s; loss: -31.2462 (±6.1316); valid loss: -41.7981\n",
      "[Epoch 48/350, Step 2200, ETA 1m 32.56s] step time: 0.005492s (±0.001735s); valid time: 0.007978s; loss: -31.6404 (±5.01786); valid loss: -42.4747\n",
      "[Epoch 50/350, Step 2300, ETA 1m 31.89s] step time: 0.006202s (±0.008535s); valid time: 0.08464s; loss: -32.0358 (±5.35462); valid loss: -43.5809 (*)\n",
      "[Epoch 50/350, Step 2300, ETA 1m 31.9s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 53/350, Step 2400, ETA 1m 30.86s] step time: 0.005446s (±0.001421s); valid time: 0.008392s; loss: -32.2866 (±5.48206); valid loss: -42.4651\n",
      "[Epoch 55/350, Step 2500, ETA 1m 29.89s] step time: 0.005612s (±0.001608s); valid time: 0.008368s; loss: -32.2878 (±5.66723); valid loss: -43.3377\n",
      "[Epoch 57/350, Step 2600, ETA 1m 28.86s] step time: 0.005422s (±0.001844s); valid time: 0.008519s; loss: -32.432 (±4.96685); valid loss: -42.7589\n",
      "[Epoch 59/350, Step 2700, ETA 1m 27.89s] step time: 0.005485s (±0.001681s); valid time: 0.008334s; loss: -32.5247 (±5.68316); valid loss: -43.388\n",
      "[Epoch 60/350, Step 2760, ETA 1m 27.32s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 61/350, Step 2800, ETA 1m 27.46s] step time: 0.00658s (±0.01027s); valid time: 0.1022s; loss: -32.462 (±5.27879); valid loss: -43.7466 (*)\n",
      "[Epoch 64/350, Step 2900, ETA 1m 26.88s] step time: 0.006166s (±0.008541s); valid time: 0.08415s; loss: -33.1332 (±5.77181); valid loss: -44.3079 (*)\n",
      "[Epoch 66/350, Step 3000, ETA 1m 26.01s] step time: 0.005632s (±0.001708s); valid time: 0.008886s; loss: -33.0188 (±5.49976); valid loss: -44.3063\n",
      "[Epoch 68/350, Step 3100, ETA 1m 25.08s] step time: 0.005342s (±0.001255s); valid time: 0.008648s; loss: -32.965 (±4.97466); valid loss: -44.0638\n",
      "[Epoch 70/350, Step 3200, ETA 1m 24.14s] step time: 0.005373s (±0.001439s); valid time: 0.007923s; loss: -32.8323 (±5.70851); valid loss: -44.0827\n",
      "[Epoch 70/350, Step 3220, ETA 1m 23.92s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/350, Step 3300, ETA 1m 23.34s] step time: 0.005663s (±0.00169s); valid time: 0.008203s; loss: -33.4163 (±4.9485); valid loss: -44.0501\n",
      "[Epoch 74/350, Step 3400, ETA 1m 22.72s] step time: 0.006139s (±0.008615s); valid time: 0.08546s; loss: -33.2063 (±5.49952); valid loss: -44.3274 (*)\n",
      "[Epoch 77/350, Step 3500, ETA 1m 22.18s] step time: 0.006225s (±0.008334s); valid time: 0.08312s; loss: -33.2713 (±6.1952); valid loss: -44.5351 (*)\n",
      "[Epoch 79/350, Step 3600, ETA 1m 21.35s] step time: 0.005565s (±0.0019s); valid time: 0.01095s; loss: -33.4786 (±5.5114); valid loss: -44.4673\n",
      "[Epoch 80/350, Step 3680, ETA 1m 20.64s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 81/350, Step 3700, ETA 1m 20.51s] step time: 0.005435s (±0.001569s); valid time: 0.008687s; loss: -33.5019 (±5.81661); valid loss: -44.5351\n",
      "[Epoch 83/350, Step 3800, ETA 1m 19.95s] step time: 0.006311s (±0.00867s); valid time: 0.0862s; loss: -33.6532 (±4.98996); valid loss: -44.7839 (*)\n",
      "[Epoch 85/350, Step 3900, ETA 1m 19.1s] step time: 0.005415s (±0.001471s); valid time: 0.008358s; loss: -33.5313 (±5.31474); valid loss: -44.3572\n",
      "[Epoch 87/350, Step 4000, ETA 1m 18.48s] step time: 0.006143s (±0.008829s); valid time: 0.08735s; loss: -33.2847 (±5.59406); valid loss: -45.0154 (*)\n",
      "[Epoch 90/350, Step 4100, ETA 1m 17.68s] step time: 0.005402s (±0.002138s); valid time: 0.01743s; loss: -33.6164 (±5.61918); valid loss: -44.8249\n",
      "[Epoch 90/350, Step 4140, ETA 1m 17.33s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 92/350, Step 4200, ETA 1m 16.91s] step time: 0.00556s (±0.001801s); valid time: 0.008874s; loss: -33.4337 (±4.93461); valid loss: -44.4868\n",
      "[Epoch 94/350, Step 4300, ETA 1m 16.16s] step time: 0.005591s (±0.001758s); valid time: 0.01491s; loss: -34.0674 (±5.23637); valid loss: -44.5295\n",
      "[Epoch 96/350, Step 4400, ETA 1m 15.36s] step time: 0.005413s (±0.001234s); valid time: 0.008284s; loss: -34.0588 (±4.74256); valid loss: -44.9293\n",
      "[Epoch 98/350, Step 4500, ETA 1m 14.6s] step time: 0.005529s (±0.00179s); valid time: 0.008209s; loss: -33.2866 (±5.48141); valid loss: -44.8379\n",
      "[Epoch 100/350, Step 4600, ETA 1m 14.05s] step time: 0.006328s (±0.009003s); valid time: 0.0901s; loss: -33.8351 (±5.36313); valid loss: -45.1436 (*)\n",
      "[Epoch 100/350, Step 4600, ETA 1m 14.05s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 103/350, Step 4700, ETA 1m 13.31s] step time: 0.005482s (±0.001452s); valid time: 0.008391s; loss: -33.658 (±5.25121); valid loss: -44.981\n",
      "[Epoch 105/350, Step 4800, ETA 1m 12.53s] step time: 0.005281s (±0.001174s); valid time: 0.008094s; loss: -34.2939 (±5.64152); valid loss: -44.8333\n",
      "[Epoch 107/350, Step 4900, ETA 1m 11.77s] step time: 0.005449s (±0.001534s); valid time: 0.009212s; loss: -33.3625 (±5.26334); valid loss: -45.1199\n",
      "[Epoch 109/350, Step 5000, ETA 1m 11.02s] step time: 0.005443s (±0.001528s); valid time: 0.008402s; loss: -33.8085 (±5.72956); valid loss: -44.43\n",
      "[Epoch 110/350, Step 5060, ETA 1m 10.57s] Learning rate decreased to 4.22351360321045e-05\n",
      "[Epoch 111/350, Step 5100, ETA 1m 10.3s] step time: 0.005588s (±0.001685s); valid time: 0.008839s; loss: -34.2674 (±5.68416); valid loss: -44.8387\n",
      "[Epoch 114/350, Step 5200, ETA 1m 9.769s] step time: 0.006294s (±0.008791s); valid time: 0.08707s; loss: -33.8633 (±5.90994); valid loss: -45.1657 (*)\n",
      "[Epoch 116/350, Step 5300, ETA 1m 9.061s] step time: 0.005608s (±0.001993s); valid time: 0.00835s; loss: -34.0316 (±5.09029); valid loss: -44.9736\n",
      "[Epoch 118/350, Step 5400, ETA 1m 8.306s] step time: 0.005323s (±0.001233s); valid time: 0.008693s; loss: -34.0833 (±5.88755); valid loss: -44.9318\n",
      "[Epoch 120/350, Step 5500, ETA 1m 7.728s] step time: 0.006244s (±0.00888s); valid time: 0.08862s; loss: -34.0869 (±5.0786); valid loss: -45.4361 (*)\n",
      "[Epoch 120/350, Step 5520, ETA 1m 7.602s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/350, Step 5600, ETA 1m 7.107s] step time: 0.005969s (±0.001869s); valid time: 0.008709s; loss: -33.7584 (±5.30233); valid loss: -45.1016\n",
      "[Epoch 124/350, Step 5700, ETA 1m 6.393s] step time: 0.005527s (±0.001307s); valid time: 0.008373s; loss: -34.1152 (±5.7224); valid loss: -45.1342\n",
      "[Epoch 127/350, Step 5800, ETA 1m 5.704s] step time: 0.005536s (±0.001541s); valid time: 0.008239s; loss: -34.063 (±5.3187); valid loss: -45.1183\n",
      "[Epoch 129/350, Step 5900, ETA 1m 4.973s] step time: 0.005386s (±0.001413s); valid time: 0.008271s; loss: -33.9249 (±5.10542); valid loss: -45.1619\n",
      "[Epoch 130/350, Step 5980, ETA 1m 4.395s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 131/350, Step 6000, ETA 1m 4.271s] step time: 0.005387s (±0.001069s); valid time: 0.00809s; loss: -34.0193 (±5.7605); valid loss: -44.9616\n",
      "[Epoch 133/350, Step 6100, ETA 1m 3.534s] step time: 0.005299s (±0.001305s); valid time: 0.007983s; loss: -34.3954 (±5.09964); valid loss: -45.2546\n",
      "[Epoch 135/350, Step 6200, ETA 1m 2.839s] step time: 0.005522s (±0.001214s); valid time: 0.007996s; loss: -33.9154 (±5.80056); valid loss: -45.0555\n",
      "[Epoch 137/350, Step 6300, ETA 1m 2.159s] step time: 0.005597s (±0.001357s); valid time: 0.008663s; loss: -34.0657 (±5.58308); valid loss: -45.3634\n",
      "[Epoch 140/350, Step 6400, ETA 1m 1.481s] step time: 0.005492s (±0.00146s); valid time: 0.008708s; loss: -34.1143 (±5.52411); valid loss: -45.4079\n",
      "[Epoch 140/350, Step 6440, ETA 1m 1.207s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 142/350, Step 6500, ETA 1m 0.8209s] step time: 0.005649s (±0.001669s); valid time: 0.008032s; loss: -34.0949 (±5.64835); valid loss: -45.0961\n",
      "[Epoch 144/350, Step 6600, ETA 1m 0.1295s] step time: 0.00546s (±0.00133s); valid time: 0.008314s; loss: -34.1634 (±5.37573); valid loss: -45.2213\n",
      "[Epoch 146/350, Step 6700, ETA 59.43s] step time: 0.005411s (±0.001693s); valid time: 0.008008s; loss: -34.3207 (±6.08359); valid loss: -45.0854\n",
      "[Epoch 148/350, Step 6800, ETA 58.86s] step time: 0.006282s (±0.008929s); valid time: 0.08915s; loss: -34.0306 (±5.55696); valid loss: -45.4702 (*)\n",
      "[Epoch 150/350, Step 6900, ETA 58.17s] step time: 0.005511s (±0.001242s); valid time: 0.007957s; loss: -34.116 (±5.09115); valid loss: -44.7971\n",
      "[Epoch 150/350, Step 6900, ETA 58.17s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 153/350, Step 7000, ETA 57.54s] step time: 0.00564s (±0.001803s); valid time: 0.008595s; loss: -34.1661 (±5.39628); valid loss: -45.0864\n",
      "[Epoch 155/350, Step 7100, ETA 56.86s] step time: 0.005496s (±0.001319s); valid time: 0.008707s; loss: -34.259 (±5.49142); valid loss: -44.9871\n",
      "[Epoch 157/350, Step 7200, ETA 56.17s] step time: 0.00547s (±0.001435s); valid time: 0.007947s; loss: -34.0839 (±5.83657); valid loss: -45.3369\n",
      "[Epoch 159/350, Step 7300, ETA 55.5s] step time: 0.005527s (±0.001486s); valid time: 0.008417s; loss: -34.1849 (±5.37441); valid loss: -45.1199\n",
      "[Epoch 160/350, Step 7360, ETA 55.09s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 161/350, Step 7400, ETA 54.84s] step time: 0.005583s (±0.00135s); valid time: 0.008474s; loss: -33.9478 (±5.1967); valid loss: -44.8745\n",
      "[Epoch 164/350, Step 7500, ETA 54.2s] step time: 0.005641s (±0.001576s); valid time: 0.008732s; loss: -34.5037 (±5.3497); valid loss: -45.0569\n",
      "[Epoch 166/350, Step 7600, ETA 53.53s] step time: 0.005484s (±0.001429s); valid time: 0.009101s; loss: -34.1835 (±4.91406); valid loss: -45.1831\n",
      "[Epoch 168/350, Step 7700, ETA 52.87s] step time: 0.005553s (±0.001439s); valid time: 0.0085s; loss: -34.1483 (±5.32622); valid loss: -45.3647\n",
      "[Epoch 170/350, Step 7800, ETA 52.21s] step time: 0.005562s (±0.001455s); valid time: 0.008612s; loss: -33.9417 (±5.96305); valid loss: -45.1245\n",
      "[Epoch 170/350, Step 7820, ETA 52.07s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/350, Step 7900, ETA 51.64s] step time: 0.006383s (±0.00891s); valid time: 0.0893s; loss: -34.3452 (±5.97834); valid loss: -45.4979 (*)\n",
      "[Epoch 174/350, Step 8000, ETA 50.98s] step time: 0.005465s (±0.001736s); valid time: 0.009348s; loss: -34.3133 (±5.64068); valid loss: -45.194\n",
      "[Epoch 177/350, Step 8100, ETA 50.32s] step time: 0.005441s (±0.001239s); valid time: 0.008054s; loss: -34.2968 (±4.9193); valid loss: -45.2377\n",
      "[Epoch 179/350, Step 8200, ETA 49.68s] step time: 0.005749s (±0.001816s); valid time: 0.00828s; loss: -34.312 (±5.7225); valid loss: -45.4025\n",
      "[Epoch 180/350, Step 8280, ETA 49.15s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 181/350, Step 8300, ETA 49.02s] step time: 0.005519s (±0.001442s); valid time: 0.008609s; loss: -34.4339 (±5.00576); valid loss: -45.2736\n",
      "[Epoch 183/350, Step 8400, ETA 48.37s] step time: 0.005538s (±0.001551s); valid time: 0.008429s; loss: -34.0746 (±5.26987); valid loss: -45.4147\n",
      "[Epoch 185/350, Step 8500, ETA 47.78s] step time: 0.006318s (±0.009179s); valid time: 0.08685s; loss: -34.0606 (±5.11661); valid loss: -45.5854 (*)\n",
      "[Epoch 187/350, Step 8600, ETA 47.1s] step time: 0.005254s (±0.001618s); valid time: 0.008798s; loss: -34.2353 (±6.05066); valid loss: -45.1576\n",
      "[Epoch 190/350, Step 8700, ETA 46.49s] step time: 0.005801s (±0.002152s); valid time: 0.008965s; loss: -34.3502 (±5.19042); valid loss: -45.3033\n",
      "[Epoch 190/350, Step 8740, ETA 46.23s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 192/350, Step 8800, ETA 45.86s] step time: 0.005752s (±0.001891s); valid time: 0.008273s; loss: -34.5234 (±5.97946); valid loss: -45.3709\n",
      "[Epoch 194/350, Step 8900, ETA 45.21s] step time: 0.00559s (±0.001735s); valid time: 0.009512s; loss: -33.6781 (±5.5633); valid loss: -45.3142\n",
      "[Epoch 196/350, Step 9000, ETA 44.56s] step time: 0.005559s (±0.001666s); valid time: 0.008166s; loss: -34.4052 (±4.84235); valid loss: -45.3478\n",
      "[Epoch 198/350, Step 9100, ETA 43.9s] step time: 0.005381s (±0.001412s); valid time: 0.008019s; loss: -34.3148 (±5.37577); valid loss: -45.2704\n",
      "[Epoch 200/350, Step 9200, ETA 43.26s] step time: 0.005569s (±0.001512s); valid time: 0.008182s; loss: -34.2449 (±5.42393); valid loss: -44.7891\n",
      "[Epoch 200/350, Step 9200, ETA 43.26s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 203/350, Step 9300, ETA 42.61s] step time: 0.005468s (±0.001773s); valid time: 0.008565s; loss: -34.2424 (±5.47413); valid loss: -45.1067\n",
      "[Epoch 205/350, Step 9400, ETA 41.96s] step time: 0.005311s (±0.001047s); valid time: 0.008497s; loss: -34.395 (±5.27035); valid loss: -45.3357\n",
      "[Epoch 207/350, Step 9500, ETA 41.31s] step time: 0.005444s (±0.001769s); valid time: 0.008629s; loss: -34.0855 (±5.36135); valid loss: -45.177\n",
      "[Epoch 209/350, Step 9600, ETA 40.65s] step time: 0.005291s (±0.001293s); valid time: 0.008289s; loss: -34.3136 (±5.36712); valid loss: -45.2037\n",
      "[Epoch 210/350, Step 9660, ETA 40.26s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 211/350, Step 9700, ETA 40.07s] step time: 0.00648s (±0.009842s); valid time: 0.08867s; loss: -34.1236 (±4.94111); valid loss: -45.7313 (*)\n",
      "[Epoch 214/350, Step 9800, ETA 39.42s] step time: 0.005452s (±0.00161s); valid time: 0.008462s; loss: -34.1068 (±6.02472); valid loss: -45.0642\n",
      "[Epoch 216/350, Step 9900, ETA 38.77s] step time: 0.005376s (±0.001305s); valid time: 0.008469s; loss: -34.4881 (±6.03227); valid loss: -45.2543\n",
      "[Epoch 218/350, Step 10000, ETA 38.13s] step time: 0.005442s (±0.00201s); valid time: 0.008366s; loss: -34.375 (±5.96998); valid loss: -45.0659\n",
      "[Epoch 220/350, Step 10100, ETA 37.48s] step time: 0.005442s (±0.001562s); valid time: 0.008155s; loss: -33.9375 (±5.44294); valid loss: -45.3917\n",
      "[Epoch 220/350, Step 10120, ETA 37.35s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/350, Step 10200, ETA 36.84s] step time: 0.005434s (±0.001371s); valid time: 0.008447s; loss: -34.4924 (±5.21642); valid loss: -45.1135\n",
      "[Epoch 224/350, Step 10300, ETA 36.19s] step time: 0.005416s (±0.001499s); valid time: 0.00811s; loss: -34.0097 (±5.10998); valid loss: -44.8521\n",
      "[Epoch 227/350, Step 10400, ETA 35.57s] step time: 0.00564s (±0.001546s); valid time: 0.008662s; loss: -34.4017 (±5.53447); valid loss: -44.8676\n",
      "[Epoch 229/350, Step 10500, ETA 34.93s] step time: 0.005569s (±0.001397s); valid time: 0.008403s; loss: -34.0481 (±5.93307); valid loss: -45.0414\n",
      "[Epoch 230/350, Step 10580, ETA 34.41s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/350, Step 10600, ETA 34.29s] step time: 0.005405s (±0.001389s); valid time: 0.008049s; loss: -34.3562 (±5.43548); valid loss: -45.4239\n",
      "[Epoch 233/350, Step 10700, ETA 33.65s] step time: 0.005443s (±0.001484s); valid time: 0.008046s; loss: -34.5075 (±4.72563); valid loss: -45.107\n",
      "[Epoch 235/350, Step 10800, ETA 33.01s] step time: 0.00538s (±0.001704s); valid time: 0.01356s; loss: -33.7279 (±6.22741); valid loss: -45.381\n",
      "[Epoch 237/350, Step 10900, ETA 32.38s] step time: 0.005561s (±0.0015s); valid time: 0.007859s; loss: -34.5068 (±5.00509); valid loss: -45.3967\n",
      "[Epoch 240/350, Step 11000, ETA 31.75s] step time: 0.005555s (±0.001638s); valid time: 0.008041s; loss: -34.3373 (±5.37896); valid loss: -45.6352\n",
      "[Epoch 240/350, Step 11040, ETA 31.49s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 242/350, Step 11100, ETA 31.12s] step time: 0.005467s (±0.001881s); valid time: 0.008667s; loss: -34.3673 (±5.28828); valid loss: -45.6417\n",
      "[Epoch 244/350, Step 11200, ETA 30.47s] step time: 0.005328s (±0.001551s); valid time: 0.008465s; loss: -33.996 (±5.48501); valid loss: -45.5233\n",
      "[Epoch 246/350, Step 11300, ETA 29.84s] step time: 0.005556s (±0.001681s); valid time: 0.008258s; loss: -34.6949 (±5.77397); valid loss: -44.652\n",
      "[Epoch 248/350, Step 11400, ETA 29.21s] step time: 0.005588s (±0.001764s); valid time: 0.008777s; loss: -34.138 (±5.09531); valid loss: -45.3757\n",
      "[Epoch 250/350, Step 11500, ETA 28.58s] step time: 0.005546s (±0.001585s); valid time: 0.008252s; loss: -34.2108 (±5.15149); valid loss: -45.325\n",
      "[Epoch 250/350, Step 11500, ETA 28.58s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 253/350, Step 11600, ETA 27.96s] step time: 0.005511s (±0.001365s); valid time: 0.008117s; loss: -34.2918 (±6.01672); valid loss: -45.0779\n",
      "[Epoch 255/350, Step 11700, ETA 27.32s] step time: 0.005436s (±0.001469s); valid time: 0.008324s; loss: -34.2972 (±5.70373); valid loss: -45.4651\n",
      "[Epoch 257/350, Step 11800, ETA 26.69s] step time: 0.005346s (±0.001532s); valid time: 0.008041s; loss: -34.3157 (±4.87629); valid loss: -45.5324\n",
      "[Epoch 259/350, Step 11900, ETA 26.06s] step time: 0.005575s (±0.00167s); valid time: 0.01055s; loss: -34.2841 (±5.41846); valid loss: -45.3489\n",
      "[Epoch 260/350, Step 11960, ETA 25.69s] Learning rate decreased to 5.644075936237502e-07\n",
      "[Epoch 261/350, Step 12000, ETA 25.45s] step time: 0.005971s (±0.002198s); valid time: 0.008354s; loss: -34.4979 (±5.15985); valid loss: -45.3494\n",
      "[Epoch 264/350, Step 12100, ETA 24.83s] step time: 0.005545s (±0.00177s); valid time: 0.007869s; loss: -33.9343 (±5.54966); valid loss: -45.254\n",
      "[Epoch 266/350, Step 12200, ETA 24.2s] step time: 0.005477s (±0.001453s); valid time: 0.008351s; loss: -33.9345 (±5.16294); valid loss: -45.3256\n",
      "[Epoch 268/350, Step 12300, ETA 23.56s] step time: 0.00525s (±0.001058s); valid time: 0.008033s; loss: -34.5055 (±5.52568); valid loss: -45.4973\n",
      "[Epoch 270/350, Step 12400, ETA 22.93s] step time: 0.005499s (±0.001629s); valid time: 0.008916s; loss: -34.0873 (±5.60637); valid loss: -45.0818\n",
      "[Epoch 270/350, Step 12420, ETA 22.81s] Learning rate decreased to 4.233056952178126e-07\n",
      "[Epoch 272/350, Step 12500, ETA 22.31s] step time: 0.005484s (±0.001858s); valid time: 0.01251s; loss: -34.6619 (±5.11936); valid loss: -45.3797\n",
      "[Epoch 274/350, Step 12600, ETA 21.68s] step time: 0.005565s (±0.001616s); valid time: 0.008576s; loss: -34.3792 (±6.17029); valid loss: -45.6142\n",
      "[Epoch 277/350, Step 12700, ETA 21.06s] step time: 0.005696s (±0.001683s); valid time: 0.008115s; loss: -34.0164 (±4.7594); valid loss: -45.1396\n",
      "[Epoch 279/350, Step 12800, ETA 20.44s] step time: 0.005395s (±0.001332s); valid time: 0.00808s; loss: -34.6177 (±5.88027); valid loss: -45.5676\n",
      "[Epoch 280/350, Step 12880, ETA 19.93s] Learning rate decreased to 3.1747927141335945e-07\n",
      "[Epoch 281/350, Step 12900, ETA 19.81s] step time: 0.005444s (±0.001924s); valid time: 0.008589s; loss: -34.1448 (±5.87972); valid loss: -45.0685\n",
      "[Epoch 283/350, Step 13000, ETA 19.18s] step time: 0.005446s (±0.00146s); valid time: 0.007972s; loss: -34.2789 (±5.86257); valid loss: -45.2375\n",
      "[Epoch 285/350, Step 13100, ETA 18.56s] step time: 0.005459s (±0.001629s); valid time: 0.008447s; loss: -34.1578 (±4.94134); valid loss: -45.6348\n",
      "[Epoch 287/350, Step 13200, ETA 17.94s] step time: 0.005546s (±0.001679s); valid time: 0.008457s; loss: -34.2355 (±5.76356); valid loss: -44.9806\n",
      "[Epoch 290/350, Step 13300, ETA 17.32s] step time: 0.005472s (±0.00124s); valid time: 0.008256s; loss: -34.2962 (±5.6143); valid loss: -45.1916\n",
      "[Epoch 290/350, Step 13340, ETA 17.06s] Learning rate decreased to 2.3810945356001957e-07\n",
      "[Epoch 292/350, Step 13400, ETA 16.7s] step time: 0.005674s (±0.00208s); valid time: 0.00854s; loss: -34.0054 (±5.71637); valid loss: -45.1166\n",
      "[Epoch 294/350, Step 13500, ETA 16.07s] step time: 0.005395s (±0.001298s); valid time: 0.008486s; loss: -34.8721 (±5.33088); valid loss: -45.3297\n",
      "[Epoch 296/350, Step 13600, ETA 15.45s] step time: 0.005571s (±0.001637s); valid time: 0.008434s; loss: -33.7599 (±5.86138); valid loss: -45.3149\n",
      "[Epoch 298/350, Step 13700, ETA 14.83s] step time: 0.005442s (±0.001636s); valid time: 0.008353s; loss: -34.6364 (±5.71486); valid loss: -45.5953\n",
      "[Epoch 300/350, Step 13800, ETA 14.2s] step time: 0.005326s (±0.001474s); valid time: 0.008068s; loss: -34.0955 (±6.48601); valid loss: -45.0533\n",
      "[Epoch 300/350, Step 13800, ETA 14.2s] Learning rate decreased to 1.7858209017001467e-07\n",
      "[Epoch 303/350, Step 13900, ETA 13.59s] step time: 0.00573s (±0.001766s); valid time: 0.008812s; loss: -34.2613 (±5.42454); valid loss: -45.3525\n",
      "[Epoch 305/350, Step 14000, ETA 12.97s] step time: 0.005296s (±0.001085s); valid time: 0.00844s; loss: -34.2166 (±5.7826); valid loss: -45.5293\n",
      "[Epoch 307/350, Step 14100, ETA 12.35s] step time: 0.005629s (±0.001708s); valid time: 0.009411s; loss: -34.3521 (±5.66641); valid loss: -44.9368\n",
      "[Epoch 309/350, Step 14200, ETA 11.73s] step time: 0.00566s (±0.002083s); valid time: 0.009062s; loss: -33.9518 (±6.08667); valid loss: -45.3134\n",
      "[Epoch 310/350, Step 14260, ETA 11.36s] Learning rate decreased to 1.33936567627511e-07\n",
      "[Epoch 311/350, Step 14300, ETA 11.11s] step time: 0.005527s (±0.001813s); valid time: 0.008381s; loss: -34.9112 (±6.1727); valid loss: -44.8318\n",
      "[Epoch 314/350, Step 14400, ETA 10.49s] step time: 0.005543s (±0.001736s); valid time: 0.009445s; loss: -34.0927 (±6.11076); valid loss: -45.2118\n",
      "[Epoch 316/350, Step 14500, ETA 9.871s] step time: 0.005462s (±0.001475s); valid time: 0.008101s; loss: -34.1135 (±6.061); valid loss: -45.2653\n",
      "[Epoch 318/350, Step 14600, ETA 9.252s] step time: 0.005435s (±0.001309s); valid time: 0.008197s; loss: -34.6755 (±5.08965); valid loss: -45.1118\n",
      "[Epoch 320/350, Step 14700, ETA 8.631s] step time: 0.005271s (±0.001118s); valid time: 0.008097s; loss: -34.1377 (±5.17889); valid loss: -45.1427\n",
      "[Epoch 320/350, Step 14720, ETA 8.508s] Learning rate decreased to 1.0045242572063325e-07\n",
      "[Epoch 322/350, Step 14800, ETA 8.012s] step time: 0.005444s (±0.001377s); valid time: 0.008556s; loss: -34.3686 (±4.41489); valid loss: -45.4924\n",
      "[Epoch 324/350, Step 14900, ETA 7.394s] step time: 0.005473s (±0.001646s); valid time: 0.008248s; loss: -34.1983 (±5.5478); valid loss: -45.3269\n",
      "[Epoch 327/350, Step 15000, ETA 6.775s] step time: 0.005273s (±0.001168s); valid time: 0.008648s; loss: -34.2649 (±5.56955); valid loss: -45.2303\n",
      "[Epoch 329/350, Step 15100, ETA 6.158s] step time: 0.005461s (±0.00125s); valid time: 0.00827s; loss: -34.2098 (±5.07396); valid loss: -44.9715\n",
      "[Epoch 330/350, Step 15180, ETA 5.663s] Learning rate decreased to 7.533931929047494e-08\n",
      "[Epoch 331/350, Step 15200, ETA 5.54s] step time: 0.005344s (±0.001296s); valid time: 0.008102s; loss: -34.1942 (±5.32684); valid loss: -45.1238\n",
      "[Epoch 333/350, Step 15300, ETA 4.923s] step time: 0.005371s (±0.001567s); valid time: 0.008464s; loss: -34.3195 (±5.88897); valid loss: -45.3289\n",
      "[Epoch 335/350, Step 15400, ETA 4.306s] step time: 0.005316s (±0.001505s); valid time: 0.008263s; loss: -34.3995 (±5.16868); valid loss: -44.8996\n",
      "[Epoch 337/350, Step 15500, ETA 3.69s] step time: 0.00544s (±0.001693s); valid time: 0.009558s; loss: -34.3546 (±5.78612); valid loss: -45.2419\n",
      "[Epoch 340/350, Step 15600, ETA 3.076s] step time: 0.005789s (±0.001961s); valid time: 0.008377s; loss: -33.9934 (±5.40789); valid loss: -45.6833\n",
      "[Epoch 340/350, Step 15640, ETA 2.829s] Learning rate decreased to 5.650448946785621e-08\n",
      "[Epoch 342/350, Step 15700, ETA 2.46s] step time: 0.005549s (±0.001512s); valid time: 0.008438s; loss: -34.5322 (±5.37029); valid loss: -45.0162\n",
      "[Epoch 344/350, Step 15800, ETA 1.845s] step time: 0.00562s (±0.001657s); valid time: 0.008239s; loss: -34.577 (±5.48485); valid loss: -45.1166\n",
      "[Epoch 346/350, Step 15900, ETA 1.23s] step time: 0.005294s (±0.001502s); valid time: 0.008419s; loss: -33.981 (±5.08686); valid loss: -45.0013\n",
      "[Epoch 348/350, Step 16000, ETA 0.6147s] step time: 0.005494s (±0.001417s); valid time: 0.007882s; loss: -34.3967 (±5.1633); valid loss: -45.234\n",
      "[Epoch 350/350, Step 16100, ETA 0s] step time: 0.005455s (±0.001464s); valid time: 0.008126s; loss: -34.0484 (±4.96543); valid loss: -44.9394\n",
      "[Epoch 350/350, Step 16100, ETA 0s] Learning rate decreased to 4.237836710089216e-08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp78u9qaff/variables.dat-9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmp78u9qaff/variables.dat-9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(accuracy=0.71,\n",
      "\t(tp, fp, tn, fn)=(873, 3882, 10399, 712),\n",
      "\tprecision=0.18,\n",
      "\trecall=0.55,\n",
      "\tf1=0.28,\n",
      "\troc_auc=0.64,\n",
      "\ty_pred%=0.2996974662800958,\n",
      "\ty_label%=0.0998991554266986,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "datasets = NabDataset.datasets()\n",
    "index = []\n",
    "all_results = []\n",
    "all_anomalies = []\n",
    "for dataset_name, files in datasets.items():\n",
    "    if dataset_name == \"artificialNoAnomaly\":\n",
    "        continue\n",
    "    for filename in files:\n",
    "        print(f\"Testing on {dataset_name}/{filename} ...\")\n",
    "        dataset = NabDataset(dataset=dataset_name, file=filename)\n",
    "        x, y = dataset.data\n",
    "\n",
    "        if np.any(x.index.duplicated()):\n",
    "            print(\"reindexing\")\n",
    "            new_idx = fake_index(x)\n",
    "            x.index = new_idx\n",
    "            y.index = new_idx\n",
    "\n",
    "        with tf.Graph().as_default(), tf.device('/gpu:0'):\n",
    "            model = Donut(window_size=100, latent_size=8, layers=(64,32,16))\n",
    "            model.train(x, labels=y, epochs=350)\n",
    "            \n",
    "            scores = model.predict(x)\n",
    "        \n",
    "        result, threshold = best_result(scores, y, upper_range=np.max(scores)+0.1, steps=400)\n",
    "        print(result)\n",
    "        \n",
    "        anomalies = (scores > threshold).astype(np.float32)\n",
    "    \n",
    "        index.append(f\"{dataset_name}/{filename}\")\n",
    "        all_results.append(result)\n",
    "        all_anomalies.append(anomalies)\n",
    "        \n",
    "save_results(index, all_results, all_anomalies, \"donut\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nab_exp.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
